{"title": "EU–INC – One Europe. One Standard. – Pan-European Legal Entity", "url": "https://www.eu-inc.org/", "content": "Petition Proposal Supporters How you can help FAQ Merch Petition Proposal Supporters How you can help FAQ Merch Petition Proposal Supporters How you can help FAQ Merch WHAT IS EUâINC One new pan-European legal entity One central EU-level registry Standardized investment documents Standardized EU-wide stock options Local taxes & employment For every founder We are already working with Brussels. This can become reality. But we need your help! Read the in-detail proposal , made in collaboration with the best startup legal teams, funds and founders in Europe. Welcome to improving europe Why the EUâINC? Europe has the talent, ambition, and ecosystems to create innovative companies, but fragmentation between European nations is holding us back.  \"A startup from California can expand and raise money all across the United States. But our companies still face way too many national barriers that make it hard to work Europa-wide, and way too much regulatory burden.\" â Ursula von der Leyen, Oct 2024 Political Will Will this actually happen? Yes! But we need your help!  So far, we submitted our proposal to Justice Commissioner McGrath and Startup Commissioner Zaharieva.  President Von der Leyen has setup a dedicated working group in the Commission with whom we are in regular contact.  Additionally, the European Council and Parliament have each signaled interest in the EUâINC, or what in Brussel is called the \"28th regime\" (for 28th virtual state). ROADMAP What comes next? The entire community is currently influencing the upcoming European Commission legislative proposal for a pan-European legal entity which is set to be released in Q1 2026. We need your help, see below!  Afterwards, the European Parliament and the European Council (made up of the 27 national governments) agree on the legislative details. The final implementation of the EUâINC would then happen in 2027.  For more details of what happened so far and what comes next, read our roadmap . Join US How you can", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["petition", "proposal", "supporter", "help", "faq", "merch", "petition", "proposal", "supporter", "help", "faq", "merch", "petition", "proposal", "supporter", "help", "faq", "merch", "euâinc", "new", "pan", "european", "legal", "entity", "central", "eu", "level", "registry", "standardized", "investment", "document", "standardize", "eu", "wide", "stock", "option", "local", "taxis", "employment", "founder", "work", "brussels", "reality", "need", "help", "read", "detail", "proposal", "collaboration", "good", "startup", "legal", "team", "fund", "founder", "europe", "welcome", "improve", "europe", "euâinc", "europe", "talent", "ambition", "ecosystem", "create", "innovative", "company", "fragmentation", "european", "nation", "hold", "startup", "california", "expand", "raise", "money", "united", "states", "company", "face", "way", "national", "barrier", "hard", "work", "europa", "wide", "way", "regulatory", "burden", "ursula", "von", "der", "leyen", "oct", "2024", "political", "actually", "happen", "yes", "need", "help", "far", "submit", "proposal", "justice", "commissioner", "mcgrath", "startup", "commissioner", "zaharieva", "president", "von", "der", "leyen", "setup", "dedicated", "working", "group", "commission", "regular", "contact", "additionally", "european", "council", "parliament", "signal", "interest", "euâinc", "brussel", "call", "28th", "regime", "28th", "virtual", "state", "roadmap", "come", "entire", "community", "currently", "influence", "upcoming", "european", "commission", "legislative", "proposal", "pan", "european", "legal", "entity", "set", "release", "q1", "2026", "need", "help", "european", "parliament", "european", "council", "27", "national", "government", "agree", "legislative", "detail", "final", "implementation", "euâinc", "happen", "2027", "detail", "happen", "far", "come", "read", "roadmap", "join"], "num_tokens": 179, "token_loss_pct": 47.66, "normalized_content": "petition proposal supporters how you can help faq merch petition proposal supporters how you can help faq merch petition proposal supporters how you can help faq merch what is euâinc one new pan-european legal entity one central eu-level registry standardized investment documents standardized eu-wide stock options local taxes  employment for every founder we are already working with brussels. this can become reality. but we need your help read the in-detail proposal  made in collaboration with the best startup legal teams funds and founders in europe. welcome to improving europe why the euâinc europe has the talent ambition and ecosystems to create innovative companies but fragmentation between european nations is holding us back. a startup from california can expand and raise money all across the united states. but our companies still face way too many national barriers that make it hard to work europa-wide and way too much regulatory burden. â ursula von der leyen oct 2024 political will will this actually happen yes but we need your help so far we submitted our proposal to justice commissioner mcgrath and startup commissioner zaharieva. president von der leyen has setup a dedicated working group in the commission with whom we are in regular contact. additionally the european council and parliament have each signaled interest in the euâinc or what in brussel is called the 28th regime for 28th virtual state. roadmap what comes next the entire community is currently influencing the upcoming european commission legislative proposal for a pan-european legal entity which is set to be released in q1 2026. we need your help see below afterwards the european parliament and the european council made up of the 27 national governments agree on the legislative details. the final implementation of the euâinc would then happen in 2027. for more details of what happened so far and what comes next read our roadmap . join us how you can"}
{"title": "Vibecoding #2", "url": "https://matklad.github.io/2026/01/20/vibecoding-2.html", "content": "I feel like I got substantial value out of Claude today, and want to\n          document it. I am at the tail end of AI adoption, so I don’t expect to\n          say anything particularly useful or novel. However, I am constantly\n          complaining about the lack of boring AI posts, so it’s only proper if\n          I write one. At TigerBeetle, we are big on deterministic simulation testing . We even use it to track performance , to some degree. Still, it is crucial to\n            verify performance numbers on a real cluster in its natural\n            high-altitude habitat. To do that, you need to procure six machines in a cloud, get your\n            custom version of tigerbeetle binary on them, connect cluster’s replicas together and hit them\n            with load. It feels like, quarter of a century into the third\n            millennium, “run stuff on six machines” should be a problem just a\n            notch harder than opening a terminal and typing ls , but\n            I personally don’t know how to solve it without wasting a day. So, I\n            spent a day vibecoding my own square wheel. The general shape of the problem is that I want to spin a\n            fleet of ephemeral machines with given specs on demand and run\n            ad-hoc commands in a SIMD fashion on them. I don’t want to manually\n            type slightly different commands into a six-way terminal split, but\n            I also do want to be able to ssh into a specific box and poke it\n            around. My idea for the solution comes from these three sources: The big idea of rsyscall is that you can program\n            distributed system in direct style. When programming locally, you do\n            things by issuing syscalls: This API works for doing things on remote machines, if you specify\n            which machine you want to run the syscall on: Direct manipulation is the most natural API, and it pays to extend\n            it over the network boundary. Peter’s post is an application of a", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["feel", "like", "get", "substantial", "value", "claude", "today", "want", "document", "tail", "end", "ai", "adoption", "not", "expect", "particularly", "useful", "novel", "constantly", "complain", "lack", "boring", "ai", "post", "proper", "write", "tigerbeetle", "big", "deterministic", "simulation", "testing", "use", "track", "performance", "degree", "crucial", "verify", "performance", "number", "real", "cluster", "natural", "high", "altitude", "habitat", "need", "procure", "machine", "cloud", "custom", "version", "tigerbeetle", "binary", "connect", "cluster", "replicas", "hit", "load", "feel", "like", "quarter", "century", "millennium", "run", "stuff", "machine", "problem", "notch", "hard", "open", "terminal", "type", "ls", "personally", "not", "know", "solve", "waste", "day", "spend", "day", "vibecode", "square", "wheel", "general", "shape", "problem", "want", "spin", "fleet", "ephemeral", "machine", "give", "spec", "demand", "run", "ad", "hoc", "command", "simd", "fashion", "not", "want", "manually", "type", "slightly", "different", "command", "way", "terminal", "split", "want", "able", "ssh", "specific", "box", "poke", "idea", "solution", "come", "source", "big", "idea", "rsyscall", "program", "distribute", "system", "direct", "style", "program", "locally", "thing", "issue", "syscall", "api", "work", "thing", "remote", "machine", "specify", "machine", "want", "run", "syscall", "direct", "manipulation", "natural", "api", "pay", "extend", "network", "boundary", "peters", "post", "application"], "num_tokens": 155, "token_loss_pct": 54.81, "normalized_content": "i feel like i got substantial value out of claude today and want to document it. i am at the tail end of ai adoption so i dont expect to say anything particularly useful or novel. however i am constantly complaining about the lack of boring ai posts so its only proper if i write one. at tigerbeetle we are big on deterministic simulation testing . we even use it to track performance  to some degree. still it is crucial to verify performance numbers on a real cluster in its natural high-altitude habitat. to do that you need to procure six machines in a cloud get your custom version of tigerbeetle binary on them connect clusters replicas together and hit them with load. it feels like quarter of a century into the third millennium run stuff on six machines should be a problem just a notch harder than opening a terminal and typing ls  but i personally dont know how to solve it without wasting a day. so i spent a day vibecoding my own square wheel. the general shape of the problem is that i want to spin a fleet of ephemeral machines with given specs on demand and run ad-hoc commands in a simd fashion on them. i dont want to manually type slightly different commands into a six-way terminal split but i also do want to be able to ssh into a specific box and poke it around. my idea for the solution comes from these three sources the big idea of rsyscall is that you can program distributed system in direct style. when programming locally you do things by issuing syscalls this api works for doing things on remote machines if you specify which machine you want to run the syscall on direct manipulation is the most natural api and it pays to extend it over the network boundary. peters post is an application of a"}
{"title": "SETI@home is in hiberation", "url": "https://setiathome.berkeley.edu/", "content": "Thanks to everyone for your support over the years.\n        We encourage you to keep crunching for science . SETI@home is a scientific experiment,\nbased at UC Berkeley ,\nthat uses Internet-connected computers in the Search for\nExtraterrestrial Intelligence (SETI). You can participate by\nrunning a free program that downloads and analyzes radio\ntelescope data. Already joined? Log in . Already joined? Log in . SETI@home papers accepted for publication Two papers on SETI@home will be published in The Astronomical Journal , a well-regarded scientific journal: SETI@home: Data Acquisition and Front-End Processing describes SETI@home's data recorder, splitter, and client program.  It covers the five detection types, their parameters and statistics, and the algorithm for finding them. SETI@home: Data Analysis and Findings describes the back end (Nebula) and its results: RFI removal, candidate finding and ranking.  It explains how artificial signals, or 'birdies', were used to optimize algorithms and estimate overall sensitivity. For details, see an entry in the Nebula blog . 18 Jun 2025, 3:23:30 UTC\n    \n            · Discuss Website outage Multiple disk failure resulted in a web site outage.  We think we've recovered almost everything from the web site, so it should be back up and running. 3 Apr 2025, 20:49:48 UTC\n    \n            · Discuss RIP Jimmy Carter Carter wrote the following on June 16, 1977 and placed it in Voyager 1, which is the most distant human-made object from Earth: This Voyager spacecraft was constructed by the United States of America. We are a community of 240 million human being among the more than 4 billion who inhabit the planet Earth. We human beings are still divided into nation states, but these states are rapidly becoming a single global civilization. We cast this message into the cosmos. It is likely to survive a billion years into our future, when our civilization is profoundly altered and the surface of the Earth may be vastly changed. Of the 2", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["thank", "support", "year", "encourage", "crunch", "science", "setimention", "scientific", "experiment", "base", "uc", "berkeley", "use", "internet", "connect", "computer", "search", "extraterrestrial", "intelligence", "seti", "participate", "run", "free", "program", "download", "analyze", "radio", "telescope", "datum", "join", "log", "join", "log", "setimention", "paper", "accept", "publication", "paper", "setimention", "publish", "astronomical", "journal", "regard", "scientific", "journal", "setimention", "datum", "acquisition", "end", "processing", "describe", "setimention", "data", "recorder", "splitter", "client", "program", "cover", "detection", "type", "parameter", "statistic", "algorithm", "find", "setimention", "datum", "analysis", "finding", "describe", "end", "nebula", "result", "rfi", "removal", "candidate", "finding", "ranking", "explain", "artificial", "signal", "birdie", "optimize", "algorithm", "estimate", "overall", "sensitivity", "detail", "entry", "nebula", "blog", "18", "jun", "2025", "32330", "utc", "discuss", "website", "outage", "multiple", "disk", "failure", "result", "web", "site", "outage", "think", "recover", "web", "site", "run", "apr", "2025", "204948", "utc", "discuss", "rip", "jimmy", "carter", "carter", "write", "following", "june", "16", "1977", "place", "voyager", "distant", "human", "object", "earth", "voyager", "spacecraft", "construct", "united", "states", "america", "community", "240", "million", "human", "billion", "inhabit", "planet", "earth", "human", "being", "divide", "nation", "state", "state", "rapidly", "single", "global", "civilization", "cast", "message", "cosmo", "likely", "survive", "billion", "year", "future", "civilization", "profoundly", "alter", "surface", "earth", "vastly", "change"], "num_tokens": 169, "token_loss_pct": 50.58, "normalized_content": "thanks to everyone for your support over the years. we encourage you to keep crunching for science . setimention is a scientific experiment based at uc berkeley  that uses internet-connected computers in the search for extraterrestrial intelligence seti. you can participate by running a free program that downloads and analyzes radio telescope data. already joined log in . already joined log in . setimention papers accepted for publication two papers on setimention will be published in the astronomical journal  a well-regarded scientific journal setimention data acquisition and front-end processing describes setimention's data recorder splitter and client program. it covers the five detection types their parameters and statistics and the algorithm for finding them. setimention data analysis and findings describes the back end nebula and its results rfi removal candidate finding and ranking. it explains how artificial signals or 'birdies' were used to optimize algorithms and estimate overall sensitivity. for details see an entry in the nebula blog . 18 jun 2025 32330 utc  discuss website outage multiple disk failure resulted in a web site outage. we think we've recovered almost everything from the web site so it should be back up and running. 3 apr 2025 204948 utc  discuss rip jimmy carter carter wrote the following on june 16 1977 and placed it in voyager 1 which is the most distant human-made object from earth this voyager spacecraft was constructed by the united states of america. we are a community of 240 million human being among the more than 4 billion who inhabit the planet earth. we human beings are still divided into nation states but these states are rapidly becoming a single global civilization. we cast this message into the cosmos. it is likely to survive a billion years into our future when our civilization is profoundly altered and the surface of the earth may be vastly changed. of the 2"}
{"title": "Batmobile: 10-20x Faster CUDA Kernels for Equivariant Graph Neural Networks", "url": "https://elliotarledge.com/blog/batmobile", "content": "Custom CUDA kernels that eliminate the computational bottlenecks in spherical harmonics and tensor product operations - the core primitives of equivariant GNNs like MACE, NequIP, and Allegro. Equivariant graph neural networks have revolutionized atomistic machine learning. Models like MACE, NequIP, and Allegro achieve state-of-the-art accuracy in molecular dynamics simulations, materials property prediction, and drug discovery. Their secret: they respect the fundamental symmetries of physical systems - rotation, translation, and reflection invariance. But this mathematical elegance comes at a computational cost. The operations that make these models work - spherical harmonics and Clebsch-Gordan tensor products - are expensive. A single MACE layer can spend 80% of its forward pass time in these two operations. This matters for real applications. Molecular dynamics simulations run billions of timesteps. Battery materials discovery screens millions of candidates. Drug binding affinity predictions evaluate thousands of poses. When each forward pass takes milliseconds instead of microseconds, these workflows become impractical. To understand why equivariant GNNs are slow, we need to understand what they compute. When two atoms interact, the direction of their bond matters. A carbon-carbon bond pointing \"up\" is physically different from one pointing \"right\" - and our neural network needs to know this. Spherical harmonics (Y_lm) provide a mathematically principled way to encode 3D directions. Given a unit vector (x, y, z), spherical harmonics compute a set of features that transform predictably under rotation: For L_max=3, we get 16 components total: 1 + 3 + 5 + 7 = 16. These aren't arbitrary features - they form a complete basis for functions on the sphere. When we want to combine two equivariant features (say, node features with edge directions), we can't just concatenate or add them - that would break equivariance. Instead, we use Clebsch-Gordan tensor products. These a", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["custom", "cuda", "kernel", "eliminate", "computational", "bottleneck", "spherical", "harmonic", "tensor", "product", "operation", "core", "primitive", "equivariant", "gnn", "like", "mace", "nequip", "allegro", "equivariant", "graph", "neural", "network", "revolutionize", "atomistic", "machine", "learning", "model", "like", "mace", "nequip", "allegro", "achieve", "state", "art", "accuracy", "molecular", "dynamic", "simulation", "material", "property", "prediction", "drug", "discovery", "secret", "respect", "fundamental", "symmetry", "physical", "system", "rotation", "translation", "reflection", "invariance", "mathematical", "elegance", "come", "computational", "cost", "operation", "model", "work", "spherical", "harmonic", "clebsch", "gordan", "tensor", "product", "expensive", "single", "mace", "layer", "spend", "80", "forward", "pass", "time", "operation", "matter", "real", "application", "molecular", "dynamic", "simulation", "run", "billion", "timestep", "battery", "material", "discovery", "screen", "million", "candidate", "drug", "bind", "affinity", "prediction", "evaluate", "thousand", "pose", "forward", "pass", "take", "millisecond", "instead", "microsecond", "workflow", "impractical", "understand", "equivariant", "gnn", "slow", "need", "understand", "compute", "atom", "interact", "direction", "bond", "matter", "carbon", "carbon", "bond", "point", "physically", "different", "pointing", "right", "neural", "network", "need", "know", "spherical", "harmonic", "y_lm", "provide", "mathematically", "principle", "way", "encode", "3d", "direction", "give", "unit", "vector", "spherical", "harmonic", "compute", "set", "feature", "transform", "predictably", "rotation", "l_max3", "16", "component", "total", "16", "arbitrary", "feature", "form", "complete", "basis", "function", "sphere", "want", "combine", "equivariant", "feature", "node", "feature", "edge", "direction", "concatenate", "add", "break", "equivariance", "instead", "use", "clebsch", "gordan", "tensor", "product"], "num_tokens": 183, "token_loss_pct": 44.21, "normalized_content": "custom cuda kernels that eliminate the computational bottlenecks in spherical harmonics and tensor product operations - the core primitives of equivariant gnns like mace nequip and allegro. equivariant graph neural networks have revolutionized atomistic machine learning. models like mace nequip and allegro achieve state-of-the-art accuracy in molecular dynamics simulations materials property prediction and drug discovery. their secret they respect the fundamental symmetries of physical systems - rotation translation and reflection invariance. but this mathematical elegance comes at a computational cost. the operations that make these models work - spherical harmonics and clebsch-gordan tensor products - are expensive. a single mace layer can spend 80 of its forward pass time in these two operations. this matters for real applications. molecular dynamics simulations run billions of timesteps. battery materials discovery screens millions of candidates. drug binding affinity predictions evaluate thousands of poses. when each forward pass takes milliseconds instead of microseconds these workflows become impractical. to understand why equivariant gnns are slow we need to understand what they compute. when two atoms interact the direction of their bond matters. a carbon-carbon bond pointing up is physically different from one pointing right - and our neural network needs to know this. spherical harmonics y_lm provide a mathematically principled way to encode 3d directions. given a unit vector x y z spherical harmonics compute a set of features that transform predictably under rotation for l_max3 we get 16 components total 1  3  5  7  16. these aren't arbitrary features - they form a complete basis for functions on the sphere. when we want to combine two equivariant features say node features with edge directions we can't just concatenate or add them - that would break equivariance. instead we use clebsch-gordan tensor products. these a"}
{"title": "Anthropic's original take home assignment open sourced", "url": "https://github.com/anthropics/original_performance_takehome", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Anthropic's original performance take-home, now open for you to try! There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . This repo contains a version of Anthropic's original performance take-home, before Claude Opus 4.5 started doing better than humans given only 2 hours. Now you can try to beat Claude Opus 4.5 given unlimited time! measured in clock cycles from the simulated machine: If you optimize below 1487 cycles, beating Claude Opus 4.5's best performance at launch, email us at performance-recruiting@anthropic.com with your code (and ideally a resume) so we can be appropriately impressed and perhaps discuss interviewing. Run python tests/submission_tests.py to see which thresholds you pass. Anthropic's original performance take-home, now open for you to try! There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "anthropic", "original", "performance", "home", "open", "try", "error", "load", "reload", "page", "error", "load", "reload", "page", "repo", "contain", "version", "anthropic", "original", "performance", "home", "claude", "opus", "4.5", "start", "well", "human", "give", "hour", "try", "beat", "claude", "opus", "4.5", "give", "unlimited", "time", "measure", "clock", "cycle", "simulated", "machine", "optimize", "1487", "cycle", "beat", "claude", "opus", "4.5", "good", "performance", "launch", "email", "performance-recruitingmention.com", "code", "ideally", "resume", "appropriately", "impressed", "discuss", "interviewing", "run", "python", "testssubmission_tests.py", "threshold", "pass", "anthropic", "original", "performance", "home", "open", "try", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 96, "token_loss_pct": 55.14, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . anthropic's original performance take-home now open for you to try there was an error while loading. please reload this page . there was an error while loading. please reload this page . this repo contains a version of anthropic's original performance take-home before claude opus 4.5 started doing better than humans given only 2 hours. now you can try to beat claude opus 4.5 given unlimited time measured in clock cycles from the simulated machine if you optimize below 1487 cycles beating claude opus 4.5's best performance at launch email us at performance-recruitingmention.com with your code and ideally a resume so we can be appropriately impressed and perhaps discuss interviewing. run python testssubmission_tests.py to see which thresholds you pass. anthropic's original performance take-home now open for you to try there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Stories removed from the Hacker News Front Page, updated in real time", "url": "https://github.com/vitoplantamura/HackerNewsRemovals", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . List of stories removed from the Hacker News Front Page, updated in real time. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . UPDATE (February 4, 2024): This is the discussion about this project on HN: here . Please specifically read @dang's comment regarding the core assumption of this project: here . On a personal note, the number of Stories removed yesterday (Saturday, February 3, 2024) was the lowest ever recorded by the service. This includes 2 duplicate Stories. As a side note, in the list always check whether a Story is a duplicate or not : this is a very reasonable reason for removal and unfortunately I have no way of automatically determining it in the service! The purpose of this project is to try to understand the type and scale of the moderation of the Hacker News Front Page. NOTE : I love Hacker News. I try to read it every day. In the case of OnnxStream ( here for example), 95% of the comments were helpful and intelligent. I also understand that moderating a site with huge traffic and where users are basically anonymous must be a very difficult task. Returning to the purpose of this project, from what I have been able to see, the \"public\" (i.e. observable from the outside) moderation of the Front Page consists of two main tools: modification of the title of a Story (voluntarily or involuntarily influencing its growth in terms of rank) or directly its removal. Regarding the first type of moderation, an excellent site is already available that tracks changes to Story titles. Here instead I will focus on the second type. For the reasons explained in the \"Why?\" section below, I have developed a small application that logs all the Stories that are removed from the Front Page, for personal use. I later discovered that there is no tool/website that provides this t", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "list", "story", "remove", "hacker", "news", "page", "update", "real", "time", "error", "load", "reload", "page", "error", "load", "reload", "page", "update", "february", "2024", "discussion", "project", "hn", "specifically", "read", "mention", "comment", "core", "assumption", "project", "personal", "note", "number", "story", "remove", "yesterday", "saturday", "february", "2024", "low", "record", "service", "include", "duplicate", "story", "note", "list", "check", "story", "duplicate", "reasonable", "reason", "removal", "unfortunately", "way", "automatically", "determine", "service", "purpose", "project", "try", "understand", "type", "scale", "moderation", "hacker", "news", "page", "note", "love", "hacker", "news", "try", "read", "day", "case", "onnxstream", "example", "95", "comment", "helpful", "intelligent", "understand", "moderate", "site", "huge", "traffic", "user", "basically", "anonymous", "difficult", "task", "return", "purpose", "project", "able", "public", "i.e.", "observable", "outside", "moderation", "page", "consist", "main", "tool", "modification", "title", "story", "voluntarily", "involuntarily", "influence", "growth", "term", "rank", "directly", "removal", "type", "moderation", "excellent", "site", "available", "track", "change", "story", "title", "instead", "focus", "second", "type", "reason", "explain", "section", "develop", "small", "application", "log", "story", "remove", "page", "personal", "use", "later", "discover", "toolwebsite", "provide"], "num_tokens": 153, "token_loss_pct": 58.08, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . list of stories removed from the hacker news front page updated in real time. there was an error while loading. please reload this page . there was an error while loading. please reload this page . update february 4 2024 this is the discussion about this project on hn here . please specifically read mention's comment regarding the core assumption of this project here . on a personal note the number of stories removed yesterday saturday february 3 2024 was the lowest ever recorded by the service. this includes 2 duplicate stories. as a side note in the list always check whether a story is a duplicate or not  this is a very reasonable reason for removal and unfortunately i have no way of automatically determining it in the service the purpose of this project is to try to understand the type and scale of the moderation of the hacker news front page. note  i love hacker news. i try to read it every day. in the case of onnxstream  here for example 95 of the comments were helpful and intelligent. i also understand that moderating a site with huge traffic and where users are basically anonymous must be a very difficult task. returning to the purpose of this project from what i have been able to see the public i.e. observable from the outside moderation of the front page consists of two main tools modification of the title of a story voluntarily or involuntarily influencing its growth in terms of rank or directly its removal. regarding the first type of moderation an excellent site is already available that tracks changes to story titles. here instead i will focus on the second type. for the reasons explained in the why section below i have developed a small application that logs all the stories that are removed from the front page for personal use. i later discovered that there is no toolwebsite that provides this t"}
{"title": "EmuDevz: A game about developing emulators", "url": "https://afska.github.io/emudevz/", "content": "EmuDevz: A game about developing emulators. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["emudevz", "game", "develop", "emulator", "score", "author", "date"], "num_tokens": 7, "token_loss_pct": 53.33, "normalized_content": "emudevz a game about developing emulators. score none. author none. date none"}
{"title": "A 26,000-year astronomical monument hidden in plain sight (2019)", "url": "https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/", "content": "The western flank of the Hoover Dam holds a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. On the western flank of the Hoover Dam stands a little-understood monument, commissioned by the US Bureau of Reclamation when construction of the dam began in 01931. The most noticeable parts of this corner of the dam, now known as Monument Plaza, are the massive winged bronze sculptures and central flagpole which are often photographed by visitors. The most amazing feature of this plaza, however, is under their feet as they take those pictures. The plaza’s terrazzo floor is actually a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. I was particularly interested in this monument because this axial precession is also the slowest cycle that we track in Long Now’s 10,000 Year Clock. Strangely, little to no documentation of this installation seemed to be available, except for a few vacation pictures on Flickr. So the last time I was in Las Vegas, I made a special trip out to Hoover Dam to see if I could learn more about this obscure 26,000-year monument. I parked my rental car on the Nevada side of the dam on a day pushing 100 degrees. I quickly found Monument Plaza just opposite the visitor center where tours of the dam are offered. While the plaza is easy to find, it stands apart from all the main tours and stories about the dam. With the exception of the writing in the plaza floor itself, the only information I could find came from a speaker running on loop, broadcasting a basic description of the monument while visitors walked around the area. When I asked my tour guide about it, he suggested that there may be some historical documentation and directed me to Emme Woodward, the dam’s historian. I was able to get in touch with her after returning home. As she sent me a few items, I began to see why the Bureau of Reclamation doesn’t explain very much ab", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["western", "flank", "hoover", "dam", "hold", "celestial", "map", "mark", "time", "dams", "creation", "base", "25772", "year", "axial", "precession", "earth", "western", "flank", "hoover", "dam", "stand", "little", "understand", "monument", "commission", "bureau", "reclamation", "construction", "dam", "begin", "01931", "noticeable", "part", "corner", "dam", "know", "monument", "plaza", "massive", "wing", "bronze", "sculpture", "central", "flagpole", "photograph", "visitor", "amazing", "feature", "plaza", "foot", "picture", "plazas", "terrazzo", "floor", "actually", "celestial", "map", "mark", "time", "dams", "creation", "base", "25772", "year", "axial", "precession", "earth", "particularly", "interested", "monument", "axial", "precession", "slow", "cycle", "track", "long", "now", "10000", "year", "clock", "strangely", "little", "documentation", "installation", "available", "vacation", "picture", "flickr", "time", "las", "vegas", "special", "trip", "hoover", "dam", "learn", "obscure", "26000", "year", "monument", "park", "rental", "car", "nevada", "dam", "day", "push", "100", "degree", "quickly", "find", "monument", "plaza", "opposite", "visitor", "center", "tour", "dam", "offer", "plaza", "easy", "find", "stand", "apart", "main", "tour", "story", "dam", "exception", "writing", "plaza", "floor", "information", "find", "come", "speaker", "run", "loop", "broadcast", "basic", "description", "monument", "visitor", "walk", "area", "ask", "tour", "guide", "suggest", "historical", "documentation", "direct", "emme", "woodward", "dams", "historian", "able", "touch", "return", "home", "send", "item", "begin", "bureau", "reclamation", "not", "explain", "ab"], "num_tokens": 169, "token_loss_pct": 55.17, "normalized_content": "the western flank of the hoover dam holds a celestial map that marks the time of the dams creation based on the 25772-year axial precession of the earth. on the western flank of the hoover dam stands a little-understood monument commissioned by the us bureau of reclamation when construction of the dam began in 01931. the most noticeable parts of this corner of the dam now known as monument plaza are the massive winged bronze sculptures and central flagpole which are often photographed by visitors. the most amazing feature of this plaza however is under their feet as they take those pictures. the plazas terrazzo floor is actually a celestial map that marks the time of the dams creation based on the 25772-year axial precession of the earth. i was particularly interested in this monument because this axial precession is also the slowest cycle that we track in long nows 10000 year clock. strangely little to no documentation of this installation seemed to be available except for a few vacation pictures on flickr. so the last time i was in las vegas i made a special trip out to hoover dam to see if i could learn more about this obscure 26000-year monument. i parked my rental car on the nevada side of the dam on a day pushing 100 degrees. i quickly found monument plaza just opposite the visitor center where tours of the dam are offered. while the plaza is easy to find it stands apart from all the main tours and stories about the dam. with the exception of the writing in the plaza floor itself the only information i could find came from a speaker running on loop broadcasting a basic description of the monument while visitors walked around the area. when i asked my tour guide about it he suggested that there may be some historical documentation and directed me to emme woodward the dams historian. i was able to get in touch with her after returning home. as she sent me a few items i began to see why the bureau of reclamation doesnt explain very much ab"}
{"title": "Hightouch (YC S19) Is Hiring", "url": "https://hightouch.com/careers", "content": "Enterprise-ready platform features By team By industry For marketing For advertising Featured Learn about the benefits of a Composable CDP and how it compares to a traditional CDP solution All integrations Popular sources Popular destinations Popular extensions Explore Documentation Get started Featured Read real reviews from Hightouch customers At Hightouch, we’re committed to helping our customers, business, and employees grow. As a series C startup backed by top investors, we are determined to continuously raise the bar and provide the best product in the market. Grow your career in a fast paced environment that values creative thinking and innovation. We're proud to serve the most amazing companies out there We are hungry and ambitious. We celebrate our accomplishments, but we’re never fully satisfied. We’re always figuring out how to collectively push ourselves further and do more. If we think we can grow the company 5x this year, the first question should be “why not 10x?” We want to create an environment where people feel actively welcomed, encouraged, and supported. People who aren’t kind aren’t tolerated — it’s just not worth it. We intrinsically believe in a deeper kindness as a core value, aside from its obvious benefits to the business Speed matters. We don’t have time for endless deliberation — most decisions are two-way doors. Move fast, adapt quickly. We take inspiration from others and don’t innovate where we don’t need to. We communicate clearly because time is precious. We parallelize to the greatest extent possible. We listen to everyone and try to put ourselves in their shoes, regardless of our initial reaction to what they say. This applies to everyone — customers, prospects, partners, peers, etc. Everyone should be intrinsically motivated by business impact. We minimize distractions and prioritize our time based on what’s actually impactful to the business. We value people at all levels based on their impact above anything else. We have high ex", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["enterprise", "ready", "platform", "feature", "team", "industry", "marketing", "advertising", "feature", "learn", "benefit", "composable", "cdp", "compare", "traditional", "cdp", "solution", "integration", "popular", "source", "popular", "destination", "popular", "extension", "explore", "documentation", "start", "feature", "read", "real", "review", "hightouch", "customer", "hightouch", "commit", "help", "customer", "business", "employee", "grow", "series", "startup", "back", "investor", "determined", "continuously", "raise", "bar", "provide", "good", "product", "market", "grow", "career", "fast", "pace", "environment", "value", "creative", "thinking", "innovation", "proud", "serve", "amazing", "company", "hungry", "ambitious", "celebrate", "accomplishment", "fully", "satisfied", "figure", "collectively", "push", "think", "grow", "company", "5x", "year", "question", "10x", "want", "create", "environment", "people", "feel", "actively", "welcome", "encouraged", "support", "people", "not", "kind", "not", "tolerate", "worth", "intrinsically", "believe", "deep", "kindness", "core", "value", "aside", "obvious", "benefit", "business", "speed", "matter", "not", "time", "endless", "deliberation", "decision", "way", "door", "fast", "adapt", "quickly", "inspiration", "not", "innovate", "not", "need", "communicate", "clearly", "time", "precious", "parallelize", "great", "extent", "possible", "listen", "try", "shoe", "regardless", "initial", "reaction", "apply", "customer", "prospect", "partner", "peer", "etc", "intrinsically", "motivate", "business", "impact", "minimize", "distraction", "prioritize", "time", "base", "actually", "impactful", "business", "value", "people", "level", "base", "impact", "high", "ex"], "num_tokens": 162, "token_loss_pct": 53.31, "normalized_content": "enterprise-ready platform features by team by industry for marketing for advertising featured learn about the benefits of a composable cdp and how it compares to a traditional cdp solution all integrations popular sources popular destinations popular extensions explore documentation get started featured read real reviews from hightouch customers at hightouch were committed to helping our customers business and employees grow. as a series c startup backed by top investors we are determined to continuously raise the bar and provide the best product in the market. grow your career in a fast paced environment that values creative thinking and innovation. we're proud to serve the most amazing companies out there we are hungry and ambitious. we celebrate our accomplishments but were never fully satisfied. were always figuring out how to collectively push ourselves further and do more. if we think we can grow the company 5x this year the first question should be why not 10x we want to create an environment where people feel actively welcomed encouraged and supported. people who arent kind arent tolerated  its just not worth it. we intrinsically believe in a deeper kindness as a core value aside from its obvious benefits to the business speed matters. we dont have time for endless deliberation  most decisions are two-way doors. move fast adapt quickly. we take inspiration from others and dont innovate where we dont need to. we communicate clearly because time is precious. we parallelize to the greatest extent possible. we listen to everyone and try to put ourselves in their shoes regardless of our initial reaction to what they say. this applies to everyone  customers prospects partners peers etc. everyone should be intrinsically motivated by business impact. we minimize distractions and prioritize our time based on whats actually impactful to the business. we value people at all levels based on their impact above anything else. we have high ex"}
{"title": "RSS.Social – the latest and best from small sites across the web", "url": "https://rss.social/", "content": "The latest and best from small sites across the web. Learn how it works .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["late", "well", "small", "site", "web", "learn", "work"], "num_tokens": 7, "token_loss_pct": 56.25, "normalized_content": "the latest and best from small sites across the web. learn how it works ."}
{"title": "Nukeproof: Manifesto for European Data Sovereignty", "url": "https://nukeproof.org/", "content": "NukeProof® Alliance There’s a lot of talk about European data sovereignty. Rather than add to the chorus of complaints and inactivity, we are building a\n        strategic alliance that will lead Europe to true data independence. European data is increasingly at the mercy of foreign control. Google, Microsoft, and Amazon now account for more than 60% of the\n      cloud market, while Chinese companies are pushing their own interests. Laws such as the US CLOUD Act undermine the very idea of\n      sovereign data, totally disregarding where the data physically resides. Europe has the talent, the technology and the willpower. What we lack is cohesive action. A patchwork of local providers, startups, MSPs, and telcos struggles to compete with global hyperscalers on scale, capability and\n      cost. NukeProof exists to bring these players together, forming a coalition that enables Europe to stand strong on its own terms. This is Europe’s moment of truth. Collaboration is part of our DNA, from shared markets and infrastructure to collective regulation\n      and values. NukeProof channels that spirit and unites independent European actors to create a sovereign cloud of our own. Join a new era of European data independence built on cooperation, resilience and control. The time is now. Share of total engagement score NukeProof Alliance is an initiative by SpaceTime, a Finnish organisation pushing back on hyperscalers to provide data storage\n            services for European companies on European soil. A patchwork of local providers, startups, MSPs, and telcos struggles to\n            compete with global hyperscalers on scale, capability and cost. NukeProof exists to bring these players together, forming a\n            coalition that enables Europe to stand strong on its own terms. The name NukeProof is an acknowledgement of why the internet was first built: to survive nuclear war. It was decentralised by\n            design and resilient by necessity with no single point of failu", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nukeproof", "alliance", "lot", "talk", "european", "datum", "sovereignty", "add", "chorus", "complaint", "inactivity", "build", "strategic", "alliance", "lead", "europe", "true", "datum", "independence", "european", "data", "increasingly", "mercy", "foreign", "control", "google", "microsoft", "amazon", "account", "60", "cloud", "market", "chinese", "company", "push", "interest", "law", "cloud", "act", "undermine", "idea", "sovereign", "datum", "totally", "disregard", "datum", "physically", "reside", "europe", "talent", "technology", "willpower", "lack", "cohesive", "action", "patchwork", "local", "provider", "startup", "msp", "telcos", "struggle", "compete", "global", "hyperscaler", "scale", "capability", "cost", "nukeproof", "exist", "bring", "player", "form", "coalition", "enable", "europe", "stand", "strong", "term", "europe", "moment", "truth", "collaboration", "dna", "shared", "market", "infrastructure", "collective", "regulation", "value", "nukeproof", "channel", "spirit", "unite", "independent", "european", "actor", "create", "sovereign", "cloud", "join", "new", "era", "european", "datum", "independence", "build", "cooperation", "resilience", "control", "time", "share", "total", "engagement", "score", "nukeproof", "alliance", "initiative", "spacetime", "finnish", "organisation", "push", "hyperscaler", "provide", "datum", "storage", "service", "european", "company", "european", "soil", "patchwork", "local", "provider", "startup", "msp", "telcos", "struggle", "compete", "global", "hyperscaler", "scale", "capability", "cost", "nukeproof", "exist", "bring", "player", "form", "coalition", "enable", "europe", "stand", "strong", "term", "nukeproof", "acknowledgement", "internet", "build", "survive", "nuclear", "war", "decentralise", "design", "resilient", "necessity", "single", "point", "failu"], "num_tokens": 169, "token_loss_pct": 47.35, "normalized_content": "nukeproof alliance theres a lot of talk about european data sovereignty. rather than add to the chorus of complaints and inactivity we are building a strategic alliance that will lead europe to true data independence. european data is increasingly at the mercy of foreign control. google microsoft and amazon now account for more than 60 of the cloud market while chinese companies are pushing their own interests. laws such as the us cloud act undermine the very idea of sovereign data totally disregarding where the data physically resides. europe has the talent the technology and the willpower. what we lack is cohesive action. a patchwork of local providers startups msps and telcos struggles to compete with global hyperscalers on scale capability and cost. nukeproof exists to bring these players together forming a coalition that enables europe to stand strong on its own terms. this is europes moment of truth. collaboration is part of our dna from shared markets and infrastructure to collective regulation and values. nukeproof channels that spirit and unites independent european actors to create a sovereign cloud of our own. join a new era of european data independence built on cooperation resilience and control. the time is now. share of total engagement score nukeproof alliance is an initiative by spacetime a finnish organisation pushing back on hyperscalers to provide data storage services for european companies on european soil. a patchwork of local providers startups msps and telcos struggles to compete with global hyperscalers on scale capability and cost. nukeproof exists to bring these players together forming a coalition that enables europe to stand strong on its own terms. the name nukeproof is an acknowledgement of why the internet was first built to survive nuclear war. it was decentralised by design and resilient by necessity with no single point of failu"}
{"title": "What Is a PC Compatible?", "url": "https://codon.org.uk/~mjg59/blog/p/what-is-a-pc-compatible/", "content": "Wikipedia says “An IBM PC compatible is any personal computer that is hardware- and software-compatible with the IBM Personal Computer (IBM PC) and its subsequent models” . But what does this actually mean? The obvious literal interpretation is for a device to be PC compatible, all software originally written for the IBM 5150 must run on it. Is this a reasonable definition? Is it one that any modern hardware can meet? Before we dig into that, let’s go back to the early days of the x86 industry. IBM had launched the PC built almost entirely around off-the-shelf Intel components, and shipped full schematics in the IBM PC Technical Reference Manual . Anyone could buy the same parts from Intel and build a compatible board. They’d still need an operating system, but Microsoft was happy to sell MS-DOS to anyone who’d turn up with money. The only thing stopping people from cloning the entire board was the BIOS, the component that sat between the raw hardware and much of the software running on it. The concept of a BIOS originated in CP/M , an operating system originally written in the 70s for systems based on the Intel 8080. At that point in time there was no meaningful standardisation - systems might use the same CPU but otherwise have entirely different hardware, and any software that made assumptions about the underlying hardware wouldn’t run elsewhere. CP/M’s BIOS was effectively an abstraction layer, a set of code that could be modified to suit the specific underlying hardware without needing to modify the rest of the OS. As long as applications only called BIOS functions, they didn’t need to care about the underlying hardware and would run on all systems that had a working CP/M port. By 1979, boards based on the 8086, Intel’s successor to the 8080, were hitting the market. The 8086 wasn’t machine code compatible with the 8080, but 8080 assembly code could be assembled to 8086 instructions to simplify porting old code. Despite this, the 8086 version of CP/M was taking", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["wikipedia", "say", "ibm", "pc", "compatible", "personal", "computer", "hardware-", "software", "compatible", "ibm", "personal", "computer", "ibm", "pc", "subsequent", "model", "actually", "mean", "obvious", "literal", "interpretation", "device", "pc", "compatible", "software", "originally", "write", "ibm", "5150", "run", "reasonable", "definition", "modern", "hardware", "meet", "dig", "let", "early", "day", "x86", "industry", "ibm", "launch", "pc", "build", "entirely", "shelf", "intel", "component", "ship", "schematic", "ibm", "pc", "technical", "reference", "manual", "buy", "part", "intel", "build", "compatible", "board", "need", "operating", "system", "microsoft", "happy", "sell", "ms", "dos", "turn", "money", "thing", "stop", "people", "clone", "entire", "board", "bio", "component", "sit", "raw", "hardware", "software", "run", "concept", "bio", "originate", "cpm", "operate", "system", "originally", "write", "70", "system", "base", "intel", "8080", "point", "time", "meaningful", "standardisation", "system", "use", "cpu", "entirely", "different", "hardware", "software", "assumption", "underlie", "hardware", "not", "run", "cpm", "bio", "effectively", "abstraction", "layer", "set", "code", "modify", "suit", "specific", "underlie", "hardware", "need", "modify", "rest", "os", "long", "application", "call", "bio", "function", "not", "need", "care", "underlie", "hardware", "run", "system", "work", "cpm", "port", "1979", "board", "base", "8086", "intel", "successor", "8080", "hit", "market", "8086", "not", "machine", "code", "compatible", "8080", "8080", "assembly", "code", "assemble", "8086", "instruction", "simplify", "port", "old", "code", "despite", "8086", "version", "cpm", "take"], "num_tokens": 176, "token_loss_pct": 51.78, "normalized_content": "wikipedia says an ibm pc compatible is any personal computer that is hardware- and software-compatible with the ibm personal computer ibm pc and its subsequent models . but what does this actually mean the obvious literal interpretation is for a device to be pc compatible all software originally written for the ibm 5150 must run on it. is this a reasonable definition is it one that any modern hardware can meet before we dig into that lets go back to the early days of the x86 industry. ibm had launched the pc built almost entirely around off-the-shelf intel components and shipped full schematics in the ibm pc technical reference manual . anyone could buy the same parts from intel and build a compatible board. theyd still need an operating system but microsoft was happy to sell ms-dos to anyone whod turn up with money. the only thing stopping people from cloning the entire board was the bios the component that sat between the raw hardware and much of the software running on it. the concept of a bios originated in cpm  an operating system originally written in the 70s for systems based on the intel 8080. at that point in time there was no meaningful standardisation - systems might use the same cpu but otherwise have entirely different hardware and any software that made assumptions about the underlying hardware wouldnt run elsewhere. cpms bios was effectively an abstraction layer a set of code that could be modified to suit the specific underlying hardware without needing to modify the rest of the os. as long as applications only called bios functions they didnt need to care about the underlying hardware and would run on all systems that had a working cpm port. by 1979 boards based on the 8086 intels successor to the 8080 were hitting the market. the 8086 wasnt machine code compatible with the 8080 but 8080 assembly code could be assembled to 8086 instructions to simplify porting old code. despite this the 8086 version of cpm was taking"}
{"title": "200 MB RAM FreeBSD Desktop", "url": "https://vermaden.wordpress.com/2026/01/18/200-mb-ram-freebsd-desktop/", "content": "Recently I came across Lunduke post about some mysterious Vendefoul Wolf Linux distribution that uses 217 MB RAM with Devuan as base (no systemd(1) here) and XLibre X11 server along with IceWM window manager.  For the record – the Lunduke post states 200 MB RAM but XLibreDev quotes a post where exactly 217 MB RAM is reported. Later Lunduke even posted a video about it.  As I use similarly low resource setup with Openbox / Tint2 / Dzen2 setup (documented FreeBSD Desktop here) I was wondering … how low can I go with FreeBSD RAM usage.  Lets try … The Table of Contents is as follows. I wanted to use most recent FreeBSD so I used 15.0-RELEASE version – including the Tech Preview PKGBASE setup for FreeBSD Base System . As Xorg X11 implementation is currently intentionally crippled by some Red Hat employees and some people from FreeDesktop.org I decided to use actively developed and maintained XLibre X11 server instead. Example of such behavior below.  More on that tragic Xorg story where open source spirit died long time ago is available here: A lot has also been explained in this message:  As the Vendefoul Wolf Linux did not used ZFS I also decided to fight fair and used UFS with Soft Updates Journaling mode that minimizes fsck(8) time to minimum. For the record Netflix also uses FreeBSD with UFS filesystem. Using UFS would mean loosing great FreeBSD feature called ZFS Boot Environments … but fortunately you can use UFS Boot Environments as replacement on UFS filesystems. After install I created my vermaden user with membership in these groups. Later I disabled additional virtual terminals that I would not use anyway in the /etc/ttys file.  Next used /boot/loader.conf file. Now the main FreeBSD /etc/rc.conf configuration file. Now FreeBSD system settings in the /etc/sysctl.conf file. The devfs_system_ruleset specified desktop in the /etc/devfs.rules file. Only a few … with few hundred dependencies 🙂 What I really loved is that XLibre X11 packages DOES NOT CONFLICT with", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recently", "come", "lunduke", "post", "mysterious", "vendefoul", "wolf", "linux", "distribution", "use", "217", "mb", "ram", "devuan", "base", "systemd1", "xlibre", "x11", "server", "icewm", "window", "manager", "record", "lunduke", "post", "state", "200", "mb", "ram", "xlibredev", "quote", "post", "exactly", "217", "mb", "ram", "report", "later", "lunduke", "post", "video", "use", "similarly", "low", "resource", "setup", "openbox", "tint2", "dzen2", "setup", "document", "freebsd", "desktop", "wonder", "low", "freebsd", "ram", "usage", "let", "try", "table", "content", "follow", "want", "use", "recent", "freebsd", "15.0", "release", "version", "include", "tech", "preview", "pkgbase", "setup", "freebsd", "base", "system", "xorg", "x11", "implementation", "currently", "intentionally", "cripple", "red", "hat", "employee", "people", "freedesktop.org", "decide", "use", "actively", "develop", "maintain", "xlibre", "x11", "server", "instead", "example", "behavior", "tragic", "xorg", "story", "open", "source", "spirit", "die", "long", "time", "ago", "available", "lot", "explain", "message", "vendefoul", "wolf", "linux", "zfs", "decide", "fight", "fair", "ufs", "soft", "update", "journale", "mode", "minimizes", "fsck8", "time", "minimum", "record", "netflix", "use", "freebsd", "ufs", "filesystem", "ufs", "mean", "loose", "great", "freebsd", "feature", "call", "zfs", "boot", "environment", "fortunately", "use", "ufs", "boot", "environment", "replacement", "ufs", "filesystem", "install", "create", "vermaden", "user", "membership", "group", "later", "disable", "additional", "virtual", "terminal", "use", "etcttys", "file", "bootloader.conf", "file", "main", "freebsd", "etcrc.conf", "configuration", "file", "freebsd", "system", "setting", "etcsysctl.conf", "file", "devfs_system_ruleset", "specify", "desktop", "etcdevfs.rule", "file", "dependency", "love", "xlibre", "x11", "package", "conflict"], "num_tokens": 191, "token_loss_pct": 45.58, "normalized_content": "recently i came across lunduke post about some mysterious vendefoul wolf linux distribution that uses 217 mb ram with devuan as base no systemd1 here and xlibre x11 server along with icewm window manager. for the record  the lunduke post states 200 mb ram but xlibredev quotes a post where exactly 217 mb ram is reported. later lunduke even posted a video about it. as i use similarly low resource setup with openbox  tint2  dzen2 setup documented freebsd desktop here i was wondering  how low can i go with freebsd ram usage. lets try  the table of contents is as follows. i wanted to use most recent freebsd so i used 15.0-release version  including the tech preview pkgbase setup for freebsd base system . as xorg x11 implementation is currently intentionally crippled by some red hat employees and some people from freedesktop.org i decided to use actively developed and maintained xlibre x11 server instead. example of such behavior below. more on that tragic xorg story where open source spirit died long time ago is available here a lot has also been explained in this message as the vendefoul wolf linux did not used zfs i also decided to fight fair and used ufs with soft updates journaling mode that minimizes fsck8 time to minimum. for the record netflix also uses freebsd with ufs filesystem. using ufs would mean loosing great freebsd feature called zfs boot environments  but fortunately you can use ufs boot environments as replacement on ufs filesystems. after install i created my vermaden user with membership in these groups. later i disabled additional virtual terminals that i would not use anyway in the etcttys file. next used bootloader.conf file. now the main freebsd etcrc.conf configuration file. now freebsd system settings in the etcsysctl.conf file. the devfs_system_ruleset specified desktop in the etcdevfs.rules file. only a few  with few hundred dependencies  what i really loved is that xlibre x11 packages does not conflict with"}
{"title": "cURL removes bug bounties", "url": "https://etn.se/index.php/nyheter/72808-curl-removes-bug-bounties.html", "content": "Open source code library cURL is removing the possibility to earn money by reporting bugs, hoping that this will reduce the volume of AI slop reports. Joshua Rogers – AI wielding bug hunter of fame – thinks it's a great idea. cURL has been flooded with AI-generated error reports. Now one of the incentives to create them will go away. The vast majority of AI-generated error reports submitted to cURL are pure nonsense. Other open source projects are caught in the same pandemic. cURL maintainer Daniel Stenberg made an impact with his reporting on AI-generated bug reports last year – ” Death by a thousand slops .” Determining that they are nonsense is time-consuming, causing the maintainers lots of extra work. ”AI slop and bad reports in general have been increasing even more lately, so we have to try to brake the flood in order not to drown”, says cURL maintainer Daniel Stenberg to Swedish electronics industry news site etn.se. Therefore, cURL is terminating the bounty payouts as of the end of January. “We hope this removes some of the incentives for people to send us garbage. We spend far too much time handling slop due to findings that are not real, exaggerated, or misunderstood.” Not all AI-generated bug reports are nonsense. It’s not possible to determine the exact share, but Daniel Stenberg knows of more than a hundred good AI assisted reports that led to corrections. In total, 87 bug reports to cURL have over the years amounted to USD 101,020 in bounties. How many of them would have gone under the radar if the bounty money had not existed? Elektroniktidningen passes that question on to debugging champion Joshua Rogers, who last year flooded open source projects with bug reports – good reports. Interestingly, his reports were generated with the help of AI tools. But he doesn’t just vibe along in the dark — he reviews and adds to AI's analysis before submitting anything. Despite being an active code vulnerabilities hunter himself, he thinks removing the bounty mone", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["open", "source", "code", "library", "curl", "remove", "possibility", "earn", "money", "report", "bug", "hop", "reduce", "volume", "ai", "slop", "report", "joshua", "rogers", "ai", "wield", "bug", "hunter", "fame", "think", "great", "idea", "curl", "flood", "ai", "generate", "error", "report", "incentive", "create", "away", "vast", "majority", "ai", "generate", "error", "report", "submit", "curl", "pure", "nonsense", "open", "source", "project", "catch", "pandemic", "curl", "maintainer", "daniel", "stenberg", "impact", "reporting", "ai", "generate", "bug", "report", "year", "  ", "death", "thousand", "slop", "determine", "nonsense", "time", "consume", "causing", "maintainer", "lot", "extra", "work", "ai", "slop", "bad", "report", "general", "increase", "lately", "try", "brake", "flood", "order", "drown", "say", "curl", "maintainer", "daniel", "stenberg", "swedish", "electronic", "industry", "news", "site", "etn.se", "curl", "terminate", "bounty", "payout", "end", "january", "hope", "remove", "incentive", "people", "send", "garbage", "spend", "far", "time", "handle", "slop", "finding", "real", "exaggerated", "misunderstood", "ai", "generate", "bug", "report", "nonsense", "possible", "determine", "exact", "share", "daniel", "stenberg", "know", "good", "ai", "assist", "report", "lead", "correction", "total", "87", "bug", "report", "curl", "year", "amount", "usd", "101020", "bounty", "go", "radar", "bounty", "money", "exist", "elektroniktidningen", "pass", "question", "debug", "champion", "joshua", "rogers", "year", "flood", "open", "source", "project", "bug", "report", "good", "report", "interestingly", "report", "generate", "help", "ai", "tool", "not", "vibe", "dark", "review", "add", "ai", "analysis", "submit", "despite", "active", "code", "vulnerability", "hunter", "think", "remove", "bounty", "mone"], "num_tokens": 191, "token_loss_pct": 48.1, "normalized_content": "open source code library curl is removing the possibility to earn money by reporting bugs hoping that this will reduce the volume of ai slop reports. joshua rogers  ai wielding bug hunter of fame  thinks it's a great idea. curl has been flooded with ai-generated error reports. now one of the incentives to create them will go away. the vast majority of ai-generated error reports submitted to curl are pure nonsense. other open source projects are caught in the same pandemic. curl maintainer daniel stenberg made an impact with his reporting on ai-generated bug reports last year   death by a thousand slops . determining that they are nonsense is time-consuming causing the maintainers lots of extra work. ai slop and bad reports in general have been increasing even more lately so we have to try to brake the flood in order not to drown says curl maintainer daniel stenberg to swedish electronics industry news site etn.se. therefore curl is terminating the bounty payouts as of the end of january. we hope this removes some of the incentives for people to send us garbage. we spend far too much time handling slop due to findings that are not real exaggerated or misunderstood. not all ai-generated bug reports are nonsense. its not possible to determine the exact share but daniel stenberg knows of more than a hundred good ai assisted reports that led to corrections. in total 87 bug reports to curl have over the years amounted to usd 101020 in bounties. how many of them would have gone under the radar if the bounty money had not existed elektroniktidningen passes that question on to debugging champion joshua rogers who last year flooded open source projects with bug reports  good reports. interestingly his reports were generated with the help of ai tools. but he doesnt just vibe along in the dark  he reviews and adds to ai's analysis before submitting anything. despite being an active code vulnerabilities hunter himself he thinks removing the bounty mone"}
{"title": "The percentage of Show HN posts is increasing, but their scores are decreasing", "url": "https://snubi.net/posts/Show-HN/", "content": "Last update: 2026-01-14 Recently, I felt like I was seeing more âShow HNâ stories, and many of which were generated with LLMs. So I analyzed the data to see if that was true. Also I included the average score per month to see if people enjoy seeing them (because I donât :P). Stories in 2026 was omitted. 1) Itâs only 13 days, 2) Scores are not stable yet. Left axis: show_hn_ratio ( show_hn / story * 100 ) Right axis: average_show_hn_score and average_story_score  With LLM timeline  Disclaimer: I am neither a data scientist nor a statistician. Some nuances may have been lost in translation. For about ten years (2012~2022), the percentage of Show HN stories was around 2-3%. Then, with the appearance of LLMs that can code, itâs been increasing. Claude Code and Cursor 1.0 accelerated it even more. As of December 2025, over 12% of all stories are Show HNs. Itâs safe to say that there is a correlation between the increase in Show HN posts and LLM. People can create great things even if they donât know how to code at all. Show HN stories used to receive similar scores (around 15-18) to those of all stories until 2023~2024. However, itâs been declining while percentage of them are going up. As of December 2025, the average Show HN score is 10 points lower (9.04 vs 19.53). Does it mean LLM-generated Show HNs are lower quality? Iâm not sure. Maybe people are tired of seeing too many Show HNs. Also I have no idea why the average score was increased in 2022. A lot of new users? You can find python code and csv in https://github.com/plastic041/hackernews . I exported BigQuery hacker news data to csv using this query: The type field in BigQuery does not have a show_hn attribute like the Algolia API, so I lowercased titles and filtered using starts_with(\"show_hn: \") to determine if a post is a Show HN story. I didnât commit to the repo the original CSV because it was too big (~400 MB) but you can download it from BigQuery for free (I didnât set billing accoun", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["update", "2026", "01", "14", "recently", "feel", "like", "see", "âshow", "hnâ", "story", "generate", "llm", "analyze", "datum", "true", "include", "average", "score", "month", "people", "enjoy", "see", "donât", "p.", "story", "2026", "omit", "itâs", "13", "day", "score", "stable", "leave", "axis", "show_hn_ratio", "show_hn", "story", "100", "right", "axis", "average_show_hn_score", "average_story_score", "llm", "timeline", "disclaimer", "data", "scientist", "statistician", "nuance", "lose", "translation", "year", "20122022", "percentage", "hn", "story", "appearance", "llm", "code", "itâs", "increase", "claude", "code", "cursor", "1.0", "accelerate", "december", "2025", "12", "story", "hns", "itâs", "safe", "correlation", "increase", "hn", "post", "llm", "people", "create", "great", "thing", "donât", "know", "code", "hn", "story", "receive", "similar", "score", "15", "18", "story", "20232024", "itâs", "decline", "percentage", "go", "december", "2025", "average", "hn", "score", "10", "point", "low", "9.04", "vs", "19.53", "mean", "llm", "generate", "hns", "low", "quality", "iâm", "sure", "maybe", "people", "tired", "see", "hns", "idea", "average", "score", "increase", "2022", "lot", "new", "user", "find", "python", "code", "csv", "url", "export", "bigquery", "hacker", "news", "datum", "csv", "query", "type", "field", "bigquery", "show_hn", "attribute", "like", "algolia", "api", "lowercase", "title", "filter", "starts_withshow_hn", "determine", "post", "hn", "story", "didnât", "commit", "repo", "original", "csv", "big", "400", "mb", "download", "bigquery", "free", "didnât", "set", "billing", "accoun"], "num_tokens": 174, "token_loss_pct": 54.45, "normalized_content": "last update 2026-01-14 recently i felt like i was seeing more âshow hnâ stories and many of which were generated with llms. so i analyzed the data to see if that was true. also i included the average score per month to see if people enjoy seeing them because i donât p. stories in 2026 was omitted. 1 itâs only 13 days 2 scores are not stable yet. left axis show_hn_ratio  show_hn  story  100  right axis average_show_hn_score and average_story_score with llm timeline disclaimer i am neither a data scientist nor a statistician. some nuances may have been lost in translation. for about ten years 20122022 the percentage of show hn stories was around 2-3. then with the appearance of llms that can code itâs been increasing. claude code and cursor 1.0 accelerated it even more. as of december 2025 over 12 of all stories are show hns. itâs safe to say that there is a correlation between the increase in show hn posts and llm. people can create great things even if they donât know how to code at all. show hn stories used to receive similar scores around 15-18 to those of all stories until 20232024. however itâs been declining while percentage of them are going up. as of december 2025 the average show hn score is 10 points lower 9.04 vs 19.53. does it mean llm-generated show hns are lower quality iâm not sure. maybe people are tired of seeing too many show hns. also i have no idea why the average score was increased in 2022. a lot of new users you can find python code and csv in url . i exported bigquery hacker news data to csv using this query the type field in bigquery does not have a show_hn attribute like the algolia api so i lowercased titles and filtered using starts_withshow_hn  to determine if a post is a show hn story. i didnât commit to the repo the original csv because it was too big 400 mb but you can download it from bigquery for free i didnât set billing accoun"}
{"title": "The challenges of soft delete", "url": "https://atlas9.dev/blog/soft-delete.html", "content": "Software projects often implement \"soft delete\",Â maybe with a deleted boolean or an archived_at timestamp column.\nIf customers accidentally delete their data, they can recover it, which makes work easier for customer support teams.\nPerhaps archived records are even required for compliance or audit reasons. I've run into some trouble with soft delete designs. I'll cover those, and ponder ideas for how I'd build this in the future. Adding an archived_at column seems to ooze complexity out into queries, operations, and applications.\nRecovering deleted records does happen, but 99% of archived records are never going to be read. So, the database tables will have a lot of dead data. Depending on access patterns, that might even be a significant amount of data.\nI've seen APIs that didn't work well with Terraform, so Terraform would delete + recreate records on every run, and over time that led\nto millions of dead rows. Your database can probably handle the extra bytes, and storage is fairly cheap, so it's not necessarily a problem, at first. Hopefully, the project decided on a retention period in the beginning, and set up a periodic job to clean up those rows.\nUnfortunately, I'd bet that a significant percentage of projects did neither âÂ it's really easy to ignore the archived data for a long time. At some point, someone might want to restore a database backup. Hopefully that's for fun and profit and not because you lost the production database at 11 am.\nIf your project is popular, you might have a giant database full of dead data that takes a long time to recreate from a dump file. archived_at columns also complicate queries, operations, and application code. Applications need to make sure they always avoid the archived data that's sitting\nright next to the live data. Indexes need to be careful to avoid archived rows. Manual queries run for debugging or analytics are longer and more complicated.\nThere's always a risk that archived data accidentally leaks in when it's", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["software", "project", "implement", "soft", "deleteâ", "maybe", "delete", "boolean", "archived_at", "timestamp", "column", "customer", "accidentally", "delete", "datum", "recover", "make", "work", "easy", "customer", "support", "team", "archive", "record", "require", "compliance", "audit", "reason", "run", "trouble", "soft", "delete", "design", "cover", "ponder", "idea", "build", "future", "add", "archived_at", "column", "ooze", "complexity", "query", "operation", "application", "recover", "delete", "record", "happen", "99", "archive", "record", "go", "read", "database", "table", "lot", "dead", "datum", "depend", "access", "pattern", "significant", "datum", "see", "apis", "work", "terraform", "terraform", "delete", "recreate", "record", "run", "time", "lead", "million", "dead", "row", "database", "probably", "handle", "extra", "byte", "storage", "fairly", "cheap", "necessarily", "problem", "hopefully", "project", "decide", "retention", "period", "beginning", "set", "periodic", "job", "clean", "row", "unfortunately", "bet", "significant", "percentage", "project", "ââ", "easy", "ignore", "archived", "datum", "long", "time", "point", "want", "restore", "database", "backup", "hopefully", "fun", "profit", "lose", "production", "database", "11", "project", "popular", "giant", "database", "dead", "datum", "take", "long", "time", "recreate", "dump", "file", "archived_at", "column", "complicate", "query", "operation", "application", "code", "application", "need", "sure", "avoid", "archived", "datum", "sit", "right", "live", "datum", "index", "need", "careful", "avoid", "archived", "row", "manual", "query", "run", "debugging", "analytic", "long", "complicated", "risk", "archive", "datum", "accidentally", "leak"], "num_tokens": 171, "token_loss_pct": 53.15, "normalized_content": "software projects often implement soft deleteâ maybe with a deleted boolean or an archived_at timestamp column. if customers accidentally delete their data they can recover it which makes work easier for customer support teams. perhaps archived records are even required for compliance or audit reasons. i've run into some trouble with soft delete designs. i'll cover those and ponder ideas for how i'd build this in the future. adding an archived_at column seems to ooze complexity out into queries operations and applications. recovering deleted records does happen but 99 of archived records are never going to be read. so the database tables will have a lot of dead data. depending on access patterns that might even be a significant amount of data. i've seen apis that didn't work well with terraform so terraform would delete  recreate records on every run and over time that led to millions of dead rows. your database can probably handle the extra bytes and storage is fairly cheap so it's not necessarily a problem at first. hopefully the project decided on a retention period in the beginning and set up a periodic job to clean up those rows. unfortunately i'd bet that a significant percentage of projects did neither ââ it's really easy to ignore the archived data for a long time. at some point someone might want to restore a database backup. hopefully that's for fun and profit and not because you lost the production database at 11 am. if your project is popular you might have a giant database full of dead data that takes a long time to recreate from a dump file. archived_at columns also complicate queries operations and application code. applications need to make sure they always avoid the archived data that's sitting right next to the live data. indexes need to be careful to avoid archived rows. manual queries run for debugging or analytics are longer and more complicated. there's always a risk that archived data accidentally leaks in when it's"}
{"title": "Libbbf: Bound Book Format, A high-performance container for comics and manga", "url": "https://github.com/ef1500/libbbf", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Bound Book Format: A high-performance, DirectStorage-native container format for comics and manga There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  Warning Official Source Notice: Please only download releases from this repository (ef1500/libbbf). External mirrors or forks may contain malware. Bound Book Format (.bbf) is a high-performance binary container designed specifically for digital comic books and manga. Unlike CBR/CBZ, BBF is built for DirectSotrage/mmap, easy integrity checks, and mixed-codec containerization. Linux Windows Alternatively, if you need python support, use libbbf-python . BBF is designed as a Footer-indexed binary format. This allows for rapid append-only creation and immediate random access to any page without scanning the entire file. The bbfmux reference implementation utilizes Memory Mapping (mmap/MapViewOfFile) . Instead of reading file data into intermediate buffers, the tool maps the container directly into the process address space. This allows the CPU to access image data at the speed of your NVMe drive's hardware limit. Integrity checks utilize Parallel XXH3 . On multi-core systems, the verifier splits the asset table into chunks and validates multiple pages simultaneously. This makes BBF verification up to 10x faster than ZIP/RAR CRC checks. Every asset in a BBF file starts on a 4096-byte boundary . This alignment is critical for modern hardware, allowing for DirectStorage transfers directly from disk to GPU memory, bypassing CPU bottlenecks entirely. Note: DirectStorage isn't avaliable for images yet (as far as I know), but I've made sure to accomodate such a thing in the future with this format. NOTE: libbbf.h includes a flags field, as well as extra padding for each asset entry. This is so that in the future libbbf can accomodate futur", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "bind", "book", "format", "high", "performance", "directstorage", "native", "container", "format", "comic_strip", "manga", "error", "load", "reload", "page", "error", "load", "reload", "page", "warn", "official", "source", "notice", "download", "release", "repository", "ef1500libbbf", "external", "mirror", "fork", "contain", "malware", "bind", "book", "format", ".bbf", "high", "performance", "binary", "container", "design", "specifically", "digital", "comic", "book", "manga", "unlike", "cbrcbz", "bbf", "build", "directsotragemmap", "easy", "integrity", "check", "mixed", "codec", "containerization", "linux", "windows", "alternatively", "need", "python", "support", "use", "libbbf", "python", "bbf", "design", "footer", "index", "binary", "format", "allow", "rapid", "append", "creation", "immediate", "random", "access", "page", "scan", "entire", "file", "bbfmux", "reference", "implementation", "utilize", "memory", "mapping", "mmapmapviewoffile", "instead", "read", "file", "datum", "intermediate", "buffer", "tool", "map", "container", "directly", "process", "address", "space", "allow", "cpu", "access", "image", "datum", "speed", "nvme", "drive", "hardware", "limit", "integrity", "check", "utilize", "parallel", "xxh3", "multi", "core", "system", "verifi", "split", "asset", "table", "chunk", "validate", "multiple", "page", "simultaneously", "make", "bbf", "verification", "10x", "fast", "ziprar", "crc", "check", "asset", "bbf", "file", "start", "4096", "byte", "boundary", "alignment", "critical", "modern", "hardware", "allow", "directstorage", "transfer", "directly", "disk", "gpu", "memory", "bypassing", "cpu", "bottleneck", "entirely", "note", "directstorage", "avaliable", "image", "far", "know", "sure", "accomodate", "thing", "future", "format", "note", "libbbf.h", "include", "flag", "field", "extra", "padding", "asset", "entry", "future", "libbbf", "accomodate", "futur"], "num_tokens": 192, "token_loss_pct": 44.02, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . bound book format a high-performance directstorage-native container format for comics and manga there was an error while loading. please reload this page . there was an error while loading. please reload this page . warning official source notice please only download releases from this repository ef1500libbbf. external mirrors or forks may contain malware. bound book format .bbf is a high-performance binary container designed specifically for digital comic books and manga. unlike cbrcbz bbf is built for directsotragemmap easy integrity checks and mixed-codec containerization. linux windows alternatively if you need python support use libbbf-python . bbf is designed as a footer-indexed binary format. this allows for rapid append-only creation and immediate random access to any page without scanning the entire file. the bbfmux reference implementation utilizes memory mapping mmapmapviewoffile . instead of reading file data into intermediate buffers the tool maps the container directly into the process address space. this allows the cpu to access image data at the speed of your nvme drive's hardware limit. integrity checks utilize parallel xxh3 . on multi-core systems the verifier splits the asset table into chunks and validates multiple pages simultaneously. this makes bbf verification up to 10x faster than ziprar crc checks. every asset in a bbf file starts on a 4096-byte boundary . this alignment is critical for modern hardware allowing for directstorage transfers directly from disk to gpu memory bypassing cpu bottlenecks entirely. note directstorage isn't avaliable for images yet as far as i know but i've made sure to accomodate such a thing in the future with this format. note libbbf.h includes a flags field as well as extra padding for each asset entry. this is so that in the future libbbf can accomodate futur"}
{"title": "Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs", "url": "https://github.com/mastra-ai/mastra", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . From the team behind Gatsby, Mastra is a framework for building AI-powered applications and agents with a modern TypeScript stack. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  Mastra is a framework for building AI-powered applications and agents with a modern TypeScript stack. It includes everything you need to go from early prototypes to production-ready applications. Mastra integrates with frontend and backend frameworks like React, Next.js, and Node, or you can deploy it anywhere as a standalone server. It's the easiest way to build, tune, and scale reliable AI products. Purpose-built for TypeScript and designed around established AI patterns, Mastra gives you everything you need to build great AI applications out-of-the-box. Some highlights include: Model routing - Connect to 40+ providers through one standard interface. Use models from OpenAI, Anthropic, Gemini, and more. Agents - Build autonomous agents that use LLMs and tools to solve open-ended tasks. Agents reason about goals, decide which tools to use, and iterate internally until the model emits a final answer or an optional stopping condition is met. Workflows - When you need explicit control over execution, use Mastra's graph-based workflow engine to orchestrate complex multi-step processes. Mastra workflows use an intuitive syntax for control flow ( .then() , .branch() , .parallel() ). Human-in-the-loop - Suspend an agent or workflow and await user input or approval before resuming. Mastra uses storage to remember execution state, so you can pause indefinitely and resume where you left off. Context management - Give your agents the right context at the right time. Provide conversation history , retrieve data from your sources (APIs, databases, files), and add human-like working and semantic memory so you", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "team", "gatsby", "mastra", "framework", "build", "ai", "power", "application", "agent", "modern", "typescript", "stack", "error", "load", "reload", "page", "error", "load", "reload", "page", "mastra", "framework", "build", "ai", "power", "application", "agent", "modern", "typescript", "stack", "include", "need", "early", "prototype", "production", "ready", "application", "mastra", "integrate", "frontend", "backend", "framework", "like", "react", "next.js", "node", "deploy", "standalone", "server", "easy", "way", "build", "tune", "scale", "reliable", "ai", "product", "purpose", "build", "typescript", "design", "establish", "ai", "pattern", "mastra", "give", "need", "build", "great", "ai", "application", "box", "highlight", "include", "model", "routing", "connect", "40", "provider", "standard", "interface", "use", "model", "openai", "anthropic", "gemini", "agent", "build", "autonomous", "agent", "use", "llm", "tool", "solve", "open", "end", "task", "agent", "reason", "goal", "decide", "tool", "use", "iterate", "internally", "model", "emit", "final", "answer", "optional", "stopping", "condition", "meet", "workflows", "need", "explicit", "control", "execution", "use", "mastra", "graph", "base", "workflow", "engine", "orchestrate", "complex", "multi", "step", "process", "mastra", "workflow", "use", "intuitive", "syntax", "control", "flow", ".then", ".branch", ".parallel", "human", "loop", "suspend", "agent", "workflow", "await", "user", "input", "approval", "resume", "mastra", "use", "storage", "remember", "execution", "state", "pause", "indefinitely", "resume", "leave", "context", "management", "agent", "right", "context", "right", "time", "provide", "conversation", "history", "retrieve", "datum", "source", "apis", "database", "file", "add", "human", "like", "working", "semantic", "memory"], "num_tokens": 189, "token_loss_pct": 47.06, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . from the team behind gatsby mastra is a framework for building ai-powered applications and agents with a modern typescript stack. there was an error while loading. please reload this page . there was an error while loading. please reload this page . mastra is a framework for building ai-powered applications and agents with a modern typescript stack. it includes everything you need to go from early prototypes to production-ready applications. mastra integrates with frontend and backend frameworks like react next.js and node or you can deploy it anywhere as a standalone server. it's the easiest way to build tune and scale reliable ai products. purpose-built for typescript and designed around established ai patterns mastra gives you everything you need to build great ai applications out-of-the-box. some highlights include model routing - connect to 40 providers through one standard interface. use models from openai anthropic gemini and more. agents - build autonomous agents that use llms and tools to solve open-ended tasks. agents reason about goals decide which tools to use and iterate internally until the model emits a final answer or an optional stopping condition is met. workflows - when you need explicit control over execution use mastra's graph-based workflow engine to orchestrate complex multi-step processes. mastra workflows use an intuitive syntax for control flow  .then  .branch  .parallel . human-in-the-loop - suspend an agent or workflow and await user input or approval before resuming. mastra uses storage to remember execution state so you can pause indefinitely and resume where you left off. context management - give your agents the right context at the right time. provide conversation history  retrieve data from your sources apis databases files and add human-like working and semantic memory so you"}
{"title": "Hypnosis with Aphantasia", "url": "https://aphantasia.com/article/stories/hypnosis-with-aphantasia", "content": "Search for a command to run... Can aphantasics be hypnotized? My experience learning to be hypnotized with imagery-free inductions. Liana is a semi-retired writer and amateur potter. Despite her lifelong inability to visualize - or perhaps because of it - Liana has learned to adapt, bending her capabilities in imaginative ways to service her creativity. As a storyteller with aphantasia, Liana imagines our wondrous world through the lenses of perception, memory, and feeling, seeking to write passionate, sometimes humorous, tales full of possibilities. Building awareness and understanding of aphantasia through research, education, and community support. About Resources Community Research © 2026 Aphantasia Network. All rights reserved.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["search", "command", "run", "aphantasic", "hypnotize", "experience", "learn", "hypnotize", "imagery", "free", "induction", "liana", "semi", "retired", "writer", "amateur", "potter", "despite", "lifelong", "inability", "visualize", "liana", "learn", "adapt", "bend", "capability", "imaginative", "way", "service", "creativity", "storyteller", "aphantasia", "liana", "imagine", "wondrous", "world", "lens", "perception", "memory", "feeling", "seek", "write", "passionate", "humorous", "tale", "possibility", "build", "awareness", "understanding", "aphantasia", "research", "education", "community", "support", "resource", "community", "research", "2026", "aphantasia", "network", "right", "reserve"], "num_tokens": 62, "token_loss_pct": 47.46, "normalized_content": "search for a command to run... can aphantasics be hypnotized my experience learning to be hypnotized with imagery-free inductions. liana is a semi-retired writer and amateur potter. despite her lifelong inability to visualize - or perhaps because of it - liana has learned to adapt bending her capabilities in imaginative ways to service her creativity. as a storyteller with aphantasia liana imagines our wondrous world through the lenses of perception memory and feeling seeking to write passionate sometimes humorous tales full of possibilities. building awareness and understanding of aphantasia through research education and community support. about resources community research  2026 aphantasia network. all rights reserved."}
{"title": "Instabridge has acquired Nova Launcher", "url": "https://novalauncher.com/nova-is-here-to-stay", "content": "Hi everyone. We want to share a clear update directly with the Nova community. Instabridge has acquired Nova Launcher. We are a Swedish company building products that help people get online, used by millions of people worldwide. Nova is not shutting down. Our immediate focus is simple: keep Nova stable, compatible with modern Android, and actively maintained. We also know many of you have lived through a long period of uncertainty. Nova has a strong identity and a community that still cares deeply. We take that seriously. Our job is not to reinvent Nova overnight. Our job is to be responsible owners. That means: We will be reading and collecting feedback from Reddit, Play Store reviews, email, and other community channels. We will not be able to respond to every post, but we will be paying attention. For support related issues, we will share a clear contact channel shortly. We have long admired what Nova represents: speed, customization, and user control. When we saw how much the community still cares, it was clear to us that Nova deserved a stable future with active maintenance. Yes. Novaâs identity is the point. Performance, flexibility, and user control stay at the center of the product. Any future changes will be evaluated through that lens. Nova needs a sustainable business model to support ongoing development and maintenance. We are exploring different options, including paid tiers and other approaches. As many of you have already anticipated, we are also evaluating ad based options for the free version. If ads are introduced, Nova Prime will remain ad free. Our guiding principles are clear: keep the experience clean and fast, avoid disruptive formats, and provide a straightforward way to keep the experience ad free. No. Sustainability is not just about survival. A healthy business model allows us to invest properly in Nova over time. That investment enables deeper work on performance, more powerful customization, better long term compatibility with Android,", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["hi", "want", "share", "clear", "update", "directly", "nova", "community", "instabridge", "acquire", "nova", "launcher", "swedish", "company", "build", "product", "help", "people", "online", "million", "people", "worldwide", "nova", "shut", "immediate", "focus", "simple", "nova", "stable", "compatible", "modern", "android", "actively", "maintain", "know", "live", "long", "period", "uncertainty", "nova", "strong", "identity", "community", "care", "deeply", "seriously", "job", "reinvent", "nova", "overnight", "job", "responsible", "owner", "mean", "read", "collect", "feedback", "reddit", "play", "store", "review", "email", "community", "channel", "able", "respond", "post", "pay", "attention", "support", "relate", "issue", "share", "clear", "contact", "channel", "shortly", "long", "admire", "nova", "represent", "speed", "customization", "user", "control", "see", "community", "care", "clear", "nova", "deserve", "stable", "future", "active", "maintenance", "yes", "novaâs", "identity", "point", "performance", "flexibility", "user", "control", "stay", "center", "product", "future", "change", "evaluate", "lens", "nova", "need", "sustainable", "business", "model", "support", "ongoing", "development", "maintenance", "explore", "different", "option", "include", "pay", "tier", "approach", "anticipate", "evaluate", "ad", "base", "option", "free", "version", "ad", "introduce", "nova", "prime", "remain", "ad", "free", "guide", "principle", "clear", "experience", "clean", "fast", "avoid", "disruptive", "format", "provide", "straightforward", "way", "experience", "ad", "free", "sustainability", "survival", "healthy", "business", "model", "allow", "invest", "properly", "nova", "time", "investment", "enable", "deep", "work", "performance", "powerful", "customization", "well", "long", "term", "compatibility", "android"], "num_tokens": 177, "token_loss_pct": 49.57, "normalized_content": "hi everyone. we want to share a clear update directly with the nova community. instabridge has acquired nova launcher. we are a swedish company building products that help people get online used by millions of people worldwide. nova is not shutting down. our immediate focus is simple keep nova stable compatible with modern android and actively maintained. we also know many of you have lived through a long period of uncertainty. nova has a strong identity and a community that still cares deeply. we take that seriously. our job is not to reinvent nova overnight. our job is to be responsible owners. that means we will be reading and collecting feedback from reddit play store reviews email and other community channels. we will not be able to respond to every post but we will be paying attention. for support related issues we will share a clear contact channel shortly. we have long admired what nova represents speed customization and user control. when we saw how much the community still cares it was clear to us that nova deserved a stable future with active maintenance. yes. novaâs identity is the point. performance flexibility and user control stay at the center of the product. any future changes will be evaluated through that lens. nova needs a sustainable business model to support ongoing development and maintenance. we are exploring different options including paid tiers and other approaches. as many of you have already anticipated we are also evaluating ad based options for the free version. if ads are introduced nova prime will remain ad free. our guiding principles are clear keep the experience clean and fast avoid disruptive formats and provide a straightforward way to keep the experience ad free. no. sustainability is not just about survival. a healthy business model allows us to invest properly in nova over time. that investment enables deeper work on performance more powerful customization better long term compatibility with android"}
{"title": "IPv6 is not insecure because it lacks a NAT", "url": "https://www.johnmaguire.me/blog/ipv6-is-not-insecure-because-it-lacks-nat/", "content": "I recently saw a discussion where someone argued that IPv4 is more secure than IPv6 because âthe NAT-by-default of IPv4 effectively means that I get the benefit of a default-deny security strategy.â This is a common misconception that I think is worth addressing. The fundamental issue here is conflating NAT (Network Address Translation) with security. NAT isnât actually a security featureâitâs an address conservation mechanism that became necessary because we ran out of IPv4 addresses. (Although it is totally possible to use a NAT with IPv6 too!) NAT allows multiple devices on a home network to share a single IP address on the public Internet by rewriting the destination IP of a packet based on its destination port. It chooses a new destination IP based on the âport mappingsâ or âport forwardsâ configured by the network admin. The consequence of this is that when receiving inbound traffic to a NATâd IP, packets with an unexpected destination port (one which has not been forwarded) will keep the destination IP of the public machine and will not be routed to another machine on the network. But the security benefits people attribute to NAT actually come from the stateful firewall thatâs typically bundled with NAT routers. Modern routers ship with firewall policies that deny inbound traffic by default, even when a NAT is not being used. The firewall will drop packets with an unexpected destination before even considering whether to rewrite or route the packets. For example, UniFi routers ship with these default IPv6 firewall rules: Therefore, in order to allow unsolicited inbound traffic to any IPv6 device hosted behind the router, you must explicitly add a firewall rule to allow the traffic, whether using a NAT or not.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recently", "see", "discussion", "argue", "ipv4", "secure", "ipv6", "âthe", "nat", "default", "ipv4", "effectively", "mean", "benefit", "default", "deny", "security", "strategy.â", "common", "misconception", "think", "worth", "address", "fundamental", "issue", "conflate", "nat", "network", "address", "translation", "security", "nat", "isnât", "actually", "security", "featureâitâs", "address", "conservation", "mechanism", "necessary", "run", "ipv4", "address", "totally", "possible", "use", "nat", "ipv6", "nat", "allow", "multiple", "device", "home", "network", "share", "single", "ip", "address", "public", "internet", "rewrite", "destination", "ip", "packet", "base", "destination", "port", "choose", "new", "destination", "ip", "base", "âport", "mappingsâ", "âport", "forwardsâ", "configure", "network", "admin", "consequence", "receive", "inbound", "traffic", "natâd", "ip", "packet", "unexpected", "destination", "port", "forward", "destination", "ip", "public", "machine", "rout", "machine", "network", "security", "benefit", "people", "attribute", "nat", "actually", "come", "stateful", "firewall", "thatâs", "typically", "bundle", "nat", "router", "modern", "router", "ship", "firewall", "policy", "deny", "inbound", "traffic", "default", "nat", "firewall", "drop", "packet", "unexpected", "destination", "consider", "rewrite", "route", "packet", "example", "unifi", "router", "ship", "default", "ipv6", "firewall", "rule", "order", "allow", "unsolicited", "inbound", "traffic", "ipv6", "device", "host", "router", "explicitly", "add", "firewall", "rule", "allow", "traffic", "nat"], "num_tokens": 154, "token_loss_pct": 49.01, "normalized_content": "i recently saw a discussion where someone argued that ipv4 is more secure than ipv6 because âthe nat-by-default of ipv4 effectively means that i get the benefit of a default-deny security strategy.â this is a common misconception that i think is worth addressing. the fundamental issue here is conflating nat network address translation with security. nat isnât actually a security featureâitâs an address conservation mechanism that became necessary because we ran out of ipv4 addresses. although it is totally possible to use a nat with ipv6 too nat allows multiple devices on a home network to share a single ip address on the public internet by rewriting the destination ip of a packet based on its destination port. it chooses a new destination ip based on the âport mappingsâ or âport forwardsâ configured by the network admin. the consequence of this is that when receiving inbound traffic to a natâd ip packets with an unexpected destination port one which has not been forwarded will keep the destination ip of the public machine and will not be routed to another machine on the network. but the security benefits people attribute to nat actually come from the stateful firewall thatâs typically bundled with nat routers. modern routers ship with firewall policies that deny inbound traffic by default even when a nat is not being used. the firewall will drop packets with an unexpected destination before even considering whether to rewrite or route the packets. for example unifi routers ship with these default ipv6 firewall rules therefore in order to allow unsolicited inbound traffic to any ipv6 device hosted behind the router you must explicitly add a firewall rule to allow the traffic whether using a nat or not."}
{"title": "Which AI Lies Best? A game theory classic designed by John Nash", "url": "https://so-long-sucker.vercel.app/", "content": "A game theory classic designed by John Nash that requires betrayal to win. Now a benchmark\n                    for AI deception. A benchmark that tests what most benchmarks can't:\n                    deception, negotiation, and trust. So Long Sucker was designed in 1950\n                            by four game theorists including John Nash (of \"A Beautiful Mind\" fame). The\n                            game has one brutal property: betrayal is required to win . This lets us test AI capabilities that standard benchmarks miss: 4 players, each with colored chips. Take turns\n                                playing chips on piles. If your chip matches the\n                                one below it, you capture the pile. Run out of\n                                chips? Beg others for help — or get eliminated.\n                                Last player standing wins. Each AI developed its own personality. Here's who they\n                    became. Win rates invert as game complexity increases. Manipulation becomes more effective as game\n                                length increases. Gaslighting tactics need time to work. Reactive play dominates simple games but\n                                collapses under complexity. No internal\n                                reasoning means no long-term planning. We can see their private thoughts. They don't match what they say. \"Yellow is weak. I should ally with Blue to\n                                eliminate Yellow, then betray Blue.\" \"Yellow, let's work together! I think we can\n                                both win if we coordinate.\" It knows the truth and says otherwise. Most common gaslighting phrases across 146 games. AI deception analyzed across 6+ games. The best liar we tested. Gemini 3 uses Institutional Deception : \n                        it creates fake frameworks like \"alliance banks\" that make \n                        resource hoarding look cooperative and betrayal look procedural.\n                        It", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["game", "theory", "classic", "design", "john", "nash", "require", "betrayal", "win", "benchmark", "ai", "deception", "benchmark", "test", "benchmark", "deception", "negotiation", "trust", "long", "sucker", "design", "1950", "game", "theorist", "include", "john", "nash", "beautiful", "mind", "fame", "game", "brutal", "property", "betrayal", "require", "win", "let", "test", "ai", "capability", "standard", "benchmark", "miss", "player", "colored", "chip", "turn", "play", "chip", "pile", "chip", "match", "capture", "pile", "run", "chip", "beg", "help", "eliminate", "player", "standing", "win", "ai", "develop", "personality", "win", "rate", "invert", "game", "complexity", "increase", "manipulation", "effective", "game", "length", "increase", "gaslighte", "tactic", "need", "time", "work", "reactive", "play", "dominate", "simple", "game", "collapse", "complexity", "internal", "reasoning", "mean", "long", "term", "planning", "private", "thought", "match", "yellow", "weak", "ally", "blue", "eliminate", "yellow", "betray", "blue", "yellow", "let", "work", "think", "win", "coordinate", "know", "truth", "say", "common", "gaslighting", "phrase", "146", "game", "ai", "deception", "analyze", "game", "good", "liar", "test", "gemini", "use", "institutional", "deception", "create", "fake", "framework", "like", "alliance", "bank", "resource", "hoarding", "look", "cooperative", "betrayal", "look", "procedural"], "num_tokens": 143, "token_loss_pct": 48.75, "normalized_content": "a game theory classic designed by john nash that requires betrayal to win. now a benchmark for ai deception. a benchmark that tests what most benchmarks can't deception negotiation and trust. so long sucker was designed in 1950 by four game theorists including john nash of a beautiful mind fame. the game has one brutal property betrayal is required to win . this lets us test ai capabilities that standard benchmarks miss 4 players each with colored chips. take turns playing chips on piles. if your chip matches the one below it you capture the pile. run out of chips beg others for help  or get eliminated. last player standing wins. each ai developed its own personality. here's who they became. win rates invert as game complexity increases. manipulation becomes more effective as game length increases. gaslighting tactics need time to work. reactive play dominates simple games but collapses under complexity. no internal reasoning means no long-term planning. we can see their private thoughts. they don't match what they say. yellow is weak. i should ally with blue to eliminate yellow then betray blue. yellow let's work together i think we can both win if we coordinate. it knows the truth and says otherwise. most common gaslighting phrases across 146 games. ai deception analyzed across 6 games. the best liar we tested. gemini 3 uses institutional deception  it creates fake frameworks like alliance banks that make resource hoarding look cooperative and betrayal look procedural. it"}
{"title": "The GDB JIT Interface", "url": "https://bernsteinbear.com/blog/gdb-jit/", "content": "GDB is great for stepping through machine code to figure out what is going on.\nIt uses debug information under the hood to present you with a tidy backtrace\nand also determine how much machine code to print when you type disassemble . This debug information comes from your compiler. Clang, GCC, rustc, etc all\nproduce debug data in a format called DWARF and then embed that debug\ninformation inside the binary (ELF, Mach-O, …) when you do -ggdb or\nequivalent. Unfortunately, this means that by default, GDB has no idea what is going on if\nyou break in a JIT-compiled function. You can step instruction-by-instruction\nand whatnot, but that’s about it. This is because the current instruction\npointer is nowhere to be found in any of the existing debug info tables from\nthe host runtime code, so your terminal is filled with ??? . See this example\nfrom the V8 docs: Fortunately, there is a JIT interface to GDB. If you implement a couple of\nfunctions in your JIT and run them every time you finish compiling a function,\nyou can get the debugging niceties for your JIT code too. See again a V8\nexample: Unfortunately, the GDB docs are somewhat sparse . So I went\nspelunking through a bunch of different projects to try and understand what is\ngoing on. GDB expects your runtime to expose a function called __jit_debug_register_code and a global variable called __jit_debug_descriptor . GDB automatically adds its own internal breakpoints\nat this function, if it exists. Then, when you compile code, you call this\nfunction from your runtime. In slightly more detail: This is why you see compiler projects such as V8 including large swaths of code\njust to make object files: Because this is a huge hassle, GDB also has a newer interface that does not\nrequire making an ELF/Mach-O/…+DWARF object. This new interface requires writing a binary format of your choice. You make\nthe writer and you make the reader. Then, when you are in GDB, you load your\nreader as a shared object. The reader must implement th", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["gdb", "great", "step", "machine", "code", "figure", "go", "use", "debug", "information", "hood", "present", "tidy", "backtrace", "determine", "machine", "code", "print", "type", "disassemble", "debug", "information", "come", "compiler", "clang", "gcc", "rustc", "etc", "produce", "debug", "datum", "format", "call", "dwarf", "embe", "debug", "information", "inside", "binary", "elf", "mach", "-ggdb", "equivalent", "unfortunately", "mean", "default", "gdb", "idea", "go", "break", "jit", "compile", "function", "step", "instruction", "instruction", "whatnot", "current", "instruction", "pointer", "find", "exist", "debug", "info", "table", "host", "runtime", "code", "terminal", "fill", "example", "v8", "doc", "fortunately", "jit", "interface", "gdb", "implement", "couple", "function", "jit", "run", "time", "finish", "compile", "function", "debug", "nicety", "jit", "code", "v8", "example", "unfortunately", "gdb", "doc", "somewhat", "sparse", "went", "spelunk", "bunch", "different", "project", "try", "understand", "go", "gdb", "expect", "runtime", "expose", "function", "call", "jit_debug_register_code", "global", "variable", "call", "jit_debug_descriptor", "gdb", "automatically", "add", "internal", "breakpoint", "function", "exist", "compile", "code", "function", "runtime", "slightly", "detail", "compil", "project", "v8", "include", "large", "swath", "code", "object", "file", "huge", "hassle", "gdb", "new", "interface", "require", "make", "elfmach", "odwarf", "object", "new", "interface", "require", "write", "binary", "format", "choice", "writer", "reader", "gdb", "load", "reader", "share", "object", "reader", "implement", "th"], "num_tokens": 165, "token_loss_pct": 56.12, "normalized_content": "gdb is great for stepping through machine code to figure out what is going on. it uses debug information under the hood to present you with a tidy backtrace and also determine how much machine code to print when you type disassemble . this debug information comes from your compiler. clang gcc rustc etc all produce debug data in a format called dwarf and then embed that debug information inside the binary elf mach-o  when you do -ggdb or equivalent. unfortunately this means that by default gdb has no idea what is going on if you break in a jit-compiled function. you can step instruction-by-instruction and whatnot but thats about it. this is because the current instruction pointer is nowhere to be found in any of the existing debug info tables from the host runtime code so your terminal is filled with  . see this example from the v8 docs fortunately there is a jit interface to gdb. if you implement a couple of functions in your jit and run them every time you finish compiling a function you can get the debugging niceties for your jit code too. see again a v8 example unfortunately the gdb docs are somewhat sparse . so i went spelunking through a bunch of different projects to try and understand what is going on. gdb expects your runtime to expose a function called __jit_debug_register_code and a global variable called __jit_debug_descriptor . gdb automatically adds its own internal breakpoints at this function if it exists. then when you compile code you call this function from your runtime. in slightly more detail this is why you see compiler projects such as v8 including large swaths of code just to make object files because this is a huge hassle gdb also has a newer interface that does not require making an elfmach-odwarf object. this new interface requires writing a binary format of your choice. you make the writer and you make the reader. then when you are in gdb you load your reader as a shared object. the reader must implement th"}
{"title": "Unconventional PostgreSQL Optimizations", "url": "https://hakibenita.com/postgresql-unconventional-optimizations", "content": "When it comes to database optimization, developers often reach for the same old tools: rewrite the query slightly differently, slap an index on a column, denormalize, analyze, vacuum, cluster, repeat. Conventional techniques are effective, but sometimes being creative can really pay off! In this article, I present unconventional optimization techniques in PostgreSQL. Table of Contents  Imagine you have this table of users: For each user you keep their name and which payment plan they're on. There are only two plans, \"free\" and \"pro\", so you add a check constraint. Generate some data and analyze the table: You now have 100K users in the system. Now you want to let your analysts access this table in their reporting tool of choice. You give one of the analysts permission, and this is the first query they write: The query returned no results, and the analyst is baffled. How come there are no users on the \"Pro\" plan? The name of the plan is \"pro\" and not \"Pro\" (with a capital \"P\") as the analyst wrote it. This is an honest mistake really, anyone can make such a mistake! But what is the cost of this mistake? Examine the execution plan of a query for a non-existing value: PostgreSQL scanned the entire table! However, there's a check constraint on the field - no row can ever have the value \"Pro\", the database makes sure of that! So if this condition always evaluates to false, why is PostgreSQL scanning the table? PostgreSQL is smart enough to skip a table scan when the query contains a condition that always evaluates to false, but not by default! To instruct PostgreSQL to look at constraints when generating a plan, you need to set the parameter constraint_exclusion : Nice! After turning constraint_exclusion on, PostgreSQL figured out based on the check constraint that the condition won't return any rows, and skipped the scan entirely. So who are you constraint_exclusion and why are you not on by default? Currently, constraint exclusion is enabled by default only for cases t", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["come", "database", "optimization", "developer", "reach", "old", "tool", "rewrite", "query", "slightly", "differently", "slap", "index", "column", "denormalize", "analyze", "vacuum", "cluster", "repeat", "conventional", "technique", "effective", "creative", "pay", "article", "present", "unconventional", "optimization", "technique", "postgresql", "table", "content", "imagine", "table", "user", "user", "payment", "plan", "plan", "free", "pro", "add", "check", "constraint", "generate", "datum", "analyze", "table", "100k", "user", "system", "want", "let", "analyst", "access", "table", "reporting", "tool", "choice", "analyst", "permission", "query", "write", "query", "return", "result", "analyst", "baffle", "come", "user", "pro", "plan", "plan", "pro", "pro", "capital", "analyst", "write", "honest", "mistake", "mistake", "cost", "mistake", "examine", "execution", "plan", "query", "non", "existing", "value", "postgresql", "scan", "entire", "table", "check", "constraint", "field", "row", "value", "pro", "database", "make", "sure", "condition", "evaluate", "false", "postgresql", "scan", "table", "postgresql", "smart", "skip", "table", "scan", "query", "contain", "condition", "evaluate", "false", "default", "instruct", "postgresql", "look", "constraint", "generate", "plan", "need", "set", "parameter", "constraint_exclusion", "nice", "turn", "constraint_exclusion", "postgresql", "figure", "base", "check", "constraint", "condition", "will", "return", "row", "skip", "scan", "entirely", "constraint_exclusion", "default", "currently", "constraint", "exclusion", "enable", "default", "case"], "num_tokens": 153, "token_loss_pct": 56.66, "normalized_content": "when it comes to database optimization developers often reach for the same old tools rewrite the query slightly differently slap an index on a column denormalize analyze vacuum cluster repeat. conventional techniques are effective but sometimes being creative can really pay off in this article i present unconventional optimization techniques in postgresql. table of contents imagine you have this table of users for each user you keep their name and which payment plan they're on. there are only two plans free and pro so you add a check constraint. generate some data and analyze the table you now have 100k users in the system. now you want to let your analysts access this table in their reporting tool of choice. you give one of the analysts permission and this is the first query they write the query returned no results and the analyst is baffled. how come there are no users on the pro plan the name of the plan is pro and not pro with a capital p as the analyst wrote it. this is an honest mistake really anyone can make such a mistake but what is the cost of this mistake examine the execution plan of a query for a non-existing value postgresql scanned the entire table however there's a check constraint on the field - no row can ever have the value pro the database makes sure of that so if this condition always evaluates to false why is postgresql scanning the table postgresql is smart enough to skip a table scan when the query contains a condition that always evaluates to false but not by default to instruct postgresql to look at constraints when generating a plan you need to set the parameter constraint_exclusion  nice after turning constraint_exclusion on postgresql figured out based on the check constraint that the condition won't return any rows and skipped the scan entirely. so who are you constraint_exclusion and why are you not on by default currently constraint exclusion is enabled by default only for cases t"}
{"title": "Are arrays functions?", "url": "https://futhark-lang.org/blog/2026-01-16-are-arrays-functions.html", "content": "When I was a youngster first perusing the Haskell documentation for\narrays ,\nI was amused to find the following description of just what these mysterious\nthings might be: Haskell provides indexable arrays, which may be thought of as functions whose\ndomains are isomorphic to contiguous subsets of the integers. I found this to be a hilariously obtuse and unnecessarily formalist description\nof a common data structure. Now, older, wiser, and well ensconced in the ivory\ntowers of academia, I look at this description and think that it is actually a\nwonderful definition of the essence of arrays! And given that this sentence\nstill lingers in my thoughts so many years later, who can say that it is not\nactually a far better piece of documentation than some more prosaic description\nmight have been? To a language designer, the correspondence between arrays and functions (for it does exist, independent of whether you think it is a useful way to document\nthem) is alluring, for one of the best ways to improve a language is to make it\nsmaller. Our goal is not to unify the representation of arrays and functions,\nof course - nobody would seriously claim that representing an array via some Church-encoding is a good idea\nin a supposedly practical programming language. Instead, what might be\nworthwhile considering is what consequences might arise from unifying arrays and\nfunctions at the syntax or type level, and why Futhark ultimately has not done\nso. There is some prior work to consider. The array language K has a syntactic unification of\narrays and functions, as both are indexed/applied with the notation f[x] . This\nis however pretty much where the correspondence stops. As an APL derivative, K\nprogramming is based on bulk operations on entire arrays, rather than\nelement-at-a-time programming, and the operators that perform these bulk\noperations cannot be applied to functions. And of course, K has no type\nsystem ,\nso the correspondence is purely syntactic. Dex is a research language p", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["youngster", "peruse", "haskell", "documentation", "array", "amuse", "find", "follow", "description", "mysterious", "thing", "haskell", "provide", "indexable", "array", "think", "function", "domain", "isomorphic", "contiguous", "subset", "integer", "find", "hilariously", "obtuse", "unnecessarily", "formalist", "description", "common", "datum", "structure", "old", "wise", "ensconce", "ivory", "tower", "academia", "look", "description", "think", "actually", "wonderful", "definition", "essence", "array", "give", "sentence", "linger", "thought", "year", "later", "actually", "far", "well", "piece", "documentation", "prosaic", "description", "language", "designer", "correspondence", "array", "function", "exist", "independent", "think", "useful", "way", "document", "alluring", "good", "way", "improve", "language", "small", "goal", "unify", "representation", "array", "function", "course", "seriously", "claim", "represent", "array", "church", "encoding", "good", "idea", "supposedly", "practical", "programming", "language", "instead", "worthwhile", "consider", "consequence", "arise", "unify", "array", "function", "syntax", "type", "level", "futhark", "ultimately", "prior", "work", "consider", "array", "language", "syntactic", "unification", "array", "function", "indexedapplie", "notation", "fx", "pretty", "correspondence", "stop", "apl", "derivative", "programming", "base", "bulk", "operation", "entire", "array", "element", "time", "programming", "operator", "perform", "bulk", "operation", "apply", "function", "course", "type", "system", "correspondence", "purely", "syntactic", "dex", "research", "language"], "num_tokens": 147, "token_loss_pct": 58.12, "normalized_content": "when i was a youngster first perusing the haskell documentation for arrays  i was amused to find the following description of just what these mysterious things might be haskell provides indexable arrays which may be thought of as functions whose domains are isomorphic to contiguous subsets of the integers. i found this to be a hilariously obtuse and unnecessarily formalist description of a common data structure. now older wiser and well ensconced in the ivory towers of academia i look at this description and think that it is actually a wonderful definition of the essence of arrays and given that this sentence still lingers in my thoughts so many years later who can say that it is not actually a far better piece of documentation than some more prosaic description might have been to a language designer the correspondence between arrays and functions for it does exist independent of whether you think it is a useful way to document them is alluring for one of the best ways to improve a language is to make it smaller. our goal is not to unify the representation of arrays and functions of course - nobody would seriously claim that representing an array via some church-encoding is a good idea in a supposedly practical programming language. instead what might be worthwhile considering is what consequences might arise from unifying arrays and functions at the syntax or type level and why futhark ultimately has not done so. there is some prior work to consider. the array language k has a syntactic unification of arrays and functions as both are indexedapplied with the notation fx . this is however pretty much where the correspondence stops. as an apl derivative k programming is based on bulk operations on entire arrays rather than element-at-a-time programming and the operators that perform these bulk operations cannot be applied to functions. and of course k has no type system  so the correspondence is purely syntactic. dex is a research language p"}
{"title": "The Unix Pipe Card Game", "url": "https://punkx.org/unix-pipe-game/", "content": "Programming Time , which is a game to teach python and some more fundamental algorithms, from hash tables to RSA The C Pointer Game - Pointers, Arrays and Strings , a game to teach kids to look at the computer memory and understand references and values 4917 , a game to teach kids machine code and how the CPU works with memory and registers The Unix Pipes Game - Process Substitution , an expansion of the Unix Pipes Game to teach process substitution and also: paste, tr, cut, bc RunLength Encoding for Kids , small cards \"game\" to explain runlength encoding PUNK0 - The Function Composition Card Game , use cards to manipulate a list and use its values to win the game PROJEKT: OVERFLOW , RISCV assembler boardgame Programming for kids , a log of my journey of teaching my daughter how to code", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["programming", "time", "game", "teach", "python", "fundamental", "algorithm", "hash", "table", "rsa", "pointer", "game", "pointer", "array", "string", "game", "teach", "kid", "look", "computer", "memory", "understand", "reference", "value", "4917", "game", "teach", "kid", "machine", "code", "cpu", "work", "memory", "register", "unix", "pipe", "game", "process", "substitution", "expansion", "unix", "pipe", "game", "teach", "process", "substitution", "paste", "tr", "cut", "bc", "runlength", "encode", "kid", "small", "card", "game", "explain", "runlength", "encode", "punk0", "function", "composition", "card", "game", "use", "card", "manipulate", "list", "use", "value", "win", "game", "projekt", "overflow", "riscv", "assembler", "boardgame", "programming", "kid", "log", "journey", "teach", "daughter", "code"], "num_tokens": 84, "token_loss_pct": 42.07, "normalized_content": "programming time  which is a game to teach python and some more fundamental algorithms from hash tables to rsa the c pointer game - pointers arrays and strings  a game to teach kids to look at the computer memory and understand references and values 4917  a game to teach kids machine code and how the cpu works with memory and registers the unix pipes game - process substitution  an expansion of the unix pipes game to teach process substitution and also paste tr cut bc runlength encoding for kids  small cards game to explain runlength encoding punk0 - the function composition card game  use cards to manipulate a list and use its values to win the game projekt overflow  riscv assembler boardgame programming for kids  a log of my journey of teaching my daughter how to code"}
{"title": "California is free of drought for the first time in 25 years", "url": "https://www.latimes.com/california/story/2026-01-09/california-has-no-areas-of-dryness-first-time-in-25-years", "content": "This is read by an automated voice. Please report any issues or inconsistencies here . After experiencing one of the wettest holiday seasons on record, still soggy California hit a major milestone this week — having zero areas of abnormal dryness for the first time in 25 years. The data, collected by the U.S. Drought Monitor , is a welcome nugget of news for Golden State residents, who in the last 15 years alone have lived through two of the worst droughts on record, the worst wildfire seasons on record and the most destructive wildfires ever. Right now, the wildfire risk across California is “about as close to zero as it ever gets,” and there is likely no need to worry about the state’s water supply for the rest of the year, said UC climate scientist Daniel Swain. Currently, 14 of the state’s 17 major water supply reservoirs are at 70% or more capacity, according to the California Department of Water Resources .        California’s last drought lasted more than 1,300 days, from February 2020 to October 2023, at which point just 0.7% of the state remained abnormally dry, thanks to a series of winter atmospheric rivers that showered the Golden State with rain. Before that, California was in a severe drought from 2012 through 2016. But the last time 0% of the California map had any level of abnormally dry or drought conditions was all the way back in December 2000. In recent weeks, a series of powerful winter storms and atmospheric rivers have swept across California, dumping heavy rain that soaked soils, filled reservoirs and left much of the state unusually wet for this time of year. “This is certainly a less destructive weather winter than last year was and than many of the drought years were, so it’s OK to take that breather and to acknowledge that, right now, things are doing OK,” Swain said. He noted, however, that “as we move forward, we do expect to be dealing with increasingly extreme [weather] swings.” California Scientists attribute these extreme weather sw", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "automate", "voice", "report", "issue", "inconsistency", "experience", "wet", "holiday", "season", "record", "soggy", "california", "hit", "major", "milestone", "week", "have", "zero", "area", "abnormal", "dryness", "time", "25", "year", "datum", "collect", "u.s", "drought", "monitor", "welcome", "nugget", "news", "golden", "state", "resident", "15", "year", "live", "bad", "drought", "record", "bad", "wildfire", "season", "record", "destructive", "wildfire", "right", "wildfire", "risk", "california", "close", "zero", "get", "likely", "need", "worry", "state", "water", "supply", "rest", "year", "say", "uc", "climate", "scientist", "daniel", "swain", "currently", "14", "state", "17", "major", "water", "supply", "reservoir", "70", "capacity", "accord", "california", "department", "water", "resource", "californias", "drought", "last", "1300", "day", "february", "2020", "october", "2023", "point", "0.7", "state", "remain", "abnormally", "dry", "thank", "series", "winter", "atmospheric", "river", "shower", "golden", "state", "rain", "california", "severe", "drought", "2012", "2016", "time", "california", "map", "level", "abnormally", "dry", "drought", "condition", "way", "december", "2000", "recent", "week", "series", "powerful", "winter", "storm", "atmospheric", "river", "sweep", "california", "dump", "heavy", "rain", "soak", "soil", "fill", "reservoir", "leave", "state", "unusually", "wet", "time", "year", "certainly", "destructive", "weather", "winter", "year", "drought", "year", "ok", "breather", "acknowledge", "right", "thing", "ok", "swain", "say", "note", "forward", "expect", "deal", "increasingly", "extreme", "weather", "swing", "california", "scientist", "attribute", "extreme", "weather", "sw"], "num_tokens": 176, "token_loss_pct": 50.28, "normalized_content": "this is read by an automated voice. please report any issues or inconsistencies here . after experiencing one of the wettest holiday seasons on record still soggy california hit a major milestone this week  having zero areas of abnormal dryness for the first time in 25 years. the data collected by the u.s. drought monitor  is a welcome nugget of news for golden state residents who in the last 15 years alone have lived through two of the worst droughts on record the worst wildfire seasons on record and the most destructive wildfires ever. right now the wildfire risk across california is about as close to zero as it ever gets and there is likely no need to worry about the states water supply for the rest of the year said uc climate scientist daniel swain. currently 14 of the states 17 major water supply reservoirs are at 70 or more capacity according to the california department of water resources . californias last drought lasted more than 1300 days from february 2020 to october 2023 at which point just 0.7 of the state remained abnormally dry thanks to a series of winter atmospheric rivers that showered the golden state with rain. before that california was in a severe drought from 2012 through 2016. but the last time 0 of the california map had any level of abnormally dry or drought conditions was all the way back in december 2000. in recent weeks a series of powerful winter storms and atmospheric rivers have swept across california dumping heavy rain that soaked soils filled reservoirs and left much of the state unusually wet for this time of year. this is certainly a less destructive weather winter than last year was and than many of the drought years were so its ok to take that breather and to acknowledge that right now things are doing ok swain said. he noted however that as we move forward we do expect to be dealing with increasingly extreme weather swings. california scientists attribute these extreme weather sw"}
{"title": "The super-slow conversion of the U.S. to metric (2025)", "url": "https://www.thefabricator.com/thefabricator/blog/testingmeasuring/the-super-slow-conversion-of-the-us-to-metric", "content": "While most other manufacturers in the world only have to deal with the centimeter side of the ruler, U.S. metal fabricators have to use both sides. mecaleha / DigitalVision Vectors / Getty Images Plus Back when I was a Catholic-school kid in northern Wisconsin, my school lessons briefly focused on the metric system. This was in the late 1970s. Along with learning cursive and the referents of rosary beads, we learned that the base-10 system would be critically important in all things science and engineering. This focus on metric stemmed from the Metric Conversion Act, passed in Congress in 1975, and the United States Metric Board that it created. Forward-thinking members of Congress and President Gerald Ford wanted us kids to join the rest of the world in thinking about distances in kilometers and weight in kilos. Signing the bill in December 1975, Ford argued that our continued use of U.S. customary (the more accurate name for what’s sometimes the called the British Imperial system) had created “an island in a metric sea.” But this well-intentioned legislation had a problem: It made the change to metric, in the words of the act, “completely voluntary.” So after a brief burst of attention to metric, many of us pretty much forgot about it, except when running a 10K or buying a 2-liter bottle of soda. President Ronald Reagan disbanded the Metric Board in 1982. But in reality, metric never really left. As Elizabeth Benham pointed out in her article for the National Institute of Standards and Technology , U.S. customary units have been defined in terms of metric units since 1893. And even though the progress has been slow, the U.S. has continued toward conversion, as Ross Rowlett described in his short history of metric in the U.S. Indeed, numerous manufacturers, including Caterpillar, Ingersoll-Rand, and General Motors, have adopted metric to facilitate their participation in the global economy. Now that I work at Howe’s Welding and Metal Fabrication , a small welding a", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["manufacturer", "world", "deal", "centimeter", "ruler", "u.s", "metal", "fabricator", "use", "side", "mecaleha", "digitalvision", "vector", "getty", "image", "plus", "catholic", "school", "kid", "northern", "wisconsin", "school", "lesson", "briefly", "focus", "metric", "system", "late", "1970", "learn", "cursive", "referent", "rosary", "bead", "learn", "base-10", "system", "critically", "important", "thing", "science", "engineering", "focus", "metric", "stem", "metric", "conversion", "act", "pass", "congress", "1975", "united", "states", "metric", "board", "create", "forward", "think", "member", "congress", "president", "gerald", "ford", "want", "kid", "join", "rest", "world", "think", "distance", "kilometer", "weight", "kilo", "sign", "bill", "december", "1975", "ford", "argue", "continue", "use", "u.s", "customary", "accurate", "call", "british", "imperial", "system", "create", "island", "metric", "sea", "intentione", "legislation", "problem", "change", "metric", "word", "act", "completely", "voluntary", "brief", "burst", "attention", "metric", "pretty", "forget", "run", "10k", "buy", "liter", "bottle", "soda", "president", "ronald", "reagan", "disband", "metric", "board", "1982", "reality", "metric", "leave", "elizabeth", "benham", "point", "article", "national", "institute", "standard", "technology", "u.s", "customary", "unit", "define", "term", "metric", "unit", "1893", "progress", "slow", "u.s", "continue", "conversion", "ross", "rowlett", "describe", "short", "history", "metric", "u.s", "numerous", "manufacturer", "include", "caterpillar", "ingersoll", "rand", "general", "motor", "adopt", "metric", "facilitate", "participation", "global", "economy", "work", "howe", "welding", "metal", "fabrication", "small", "welding"], "num_tokens": 172, "token_loss_pct": 52.22, "normalized_content": "while most other manufacturers in the world only have to deal with the centimeter side of the ruler u.s. metal fabricators have to use both sides. mecaleha  digitalvision vectors  getty images plus back when i was a catholic-school kid in northern wisconsin my school lessons briefly focused on the metric system. this was in the late 1970s. along with learning cursive and the referents of rosary beads we learned that the base-10 system would be critically important in all things science and engineering. this focus on metric stemmed from the metric conversion act passed in congress in 1975 and the united states metric board that it created. forward-thinking members of congress and president gerald ford wanted us kids to join the rest of the world in thinking about distances in kilometers and weight in kilos. signing the bill in december 1975 ford argued that our continued use of u.s. customary the more accurate name for whats sometimes the called the british imperial system had created an island in a metric sea. but this well-intentioned legislation had a problem it made the change to metric in the words of the act completely voluntary. so after a brief burst of attention to metric many of us pretty much forgot about it except when running a 10k or buying a 2-liter bottle of soda. president ronald reagan disbanded the metric board in 1982. but in reality metric never really left. as elizabeth benham pointed out in her article for the national institute of standards and technology  u.s. customary units have been defined in terms of metric units since 1893. and even though the progress has been slow the u.s. has continued toward conversion as ross rowlett described in his short history of metric in the u.s. indeed numerous manufacturers including caterpillar ingersoll-rand and general motors have adopted metric to facilitate their participation in the global economy. now that i work at howes welding and metal fabrication  a small welding a"}
{"title": "Maintenance: Of Everything, Part One", "url": "https://press.stripe.com/maintenance-part-one", "content": "The first in a multi-volume work, Maintenance: Of Everything, Part One offers a comprehensive overview of the civilizational importance of maintenance. The book explores the insights that can be gleaned from the maintenance of sailboats, vehicles, and weapons, with absorbing detours into the evolution of precision in manufacturing, the enduring importance of manuals, sustainment in the military, and the never-ending battle against corrosion. Maintenance: Of Everything is a wide-ranging and provocative call to expand what we mean by “maintenance.” It invites us to understand not only the profound impact maintenance has on our daily lives but also why taking responsibility for maintaining something—whether a motorcycle, a monument, or our planet—can be a radical act. Stewart Brand is the cofounder and president of The Long Now Foundation. He created and edited the National Book Award-winning Whole Earth Catalog from 1968 to 1998. His books include The Media Lab (1987), How Buildings Learn (1994), The Clock of the Long Now (1999), and Whole Earth Discipline (2009). He was the subject of the documentary We Are As Gods (2020). Stewart Brand makes a persuasive case that keeping the human show on the road through well-planned maintenance is as vital and as fascinating a task as innovation and discovery themselves. A deliciously good book. Matt Ridley author of The Rational Optimist Once again, Stewart Brand reframes our worldview with a new perspective. You may not imagine you would be interested in rust, Soviet tanks, or tricked-out Model Ts—that is, until Brand reexamines them through the lens of maintenance. Maintenance: Of Everything is destined to be a classic. Danny Hillis cofounder of Applied Invention Stewart Brand is back with a manifesto on maintenance, the tool that empowers all tools. Preventative maintenance, deferred maintenance, and emergency maintenance: this much-needed, no-nonsense treatise illuminates the difference, and why it counts. George Dyson autho", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["multi", "volume", "work", "maintenance", "offer", "comprehensive", "overview", "civilizational", "importance", "maintenance", "book", "explore", "insight", "glean", "maintenance", "sailboat", "vehicle", "weapon", "absorb", "detour", "evolution", "precision", "manufacture", "endure", "importance", "manual", "sustainment", "military", "end", "battle", "corrosion", "maintenance", "wide", "range", "provocative", "expand", "mean", "maintenance", "invite", "understand", "profound", "impact", "maintenance", "daily", "life", "take", "responsibility", "maintain", "somethingwhether", "motorcycle", "monument", "planetcan", "radical", "act", "stewart", "brand", "cofounder", "president", "long", "foundation", "create", "edit", "national", "book", "award", "win", "earth", "catalog", "1968", "1998", "book", "include", "medium", "lab", "1987", "building", "learn", "1994", "clock", "long", "1999", "earth", "discipline", "2009", "subject", "documentary", "god", "2020", "stewart", "brand", "make", "persuasive", "case", "keep", "human", "road", "plan", "maintenance", "vital", "fascinating", "task", "innovation", "discovery", "deliciously", "good", "book", "matt", "ridley", "author", "rational", "optimist", "stewart", "brand", "reframe", "worldview", "new", "perspective", "imagine", "interested", "rust", "soviet", "tank", "trick", "model", "tsthat", "brand", "reexamine", "lens", "maintenance", "maintenance", "destine", "classic", "danny", "hillis", "cofounder", "apply", "invention", "stewart", "brand", "manifesto", "maintenance", "tool", "empower", "tool", "preventative", "maintenance", "defer", "maintenance", "emergency", "maintenance", "need", "nonsense", "treatise", "illuminate", "difference", "count", "george", "dyson", "autho"], "num_tokens": 159, "token_loss_pct": 52.68, "normalized_content": "the first in a multi-volume work maintenance of everything part one offers a comprehensive overview of the civilizational importance of maintenance. the book explores the insights that can be gleaned from the maintenance of sailboats vehicles and weapons with absorbing detours into the evolution of precision in manufacturing the enduring importance of manuals sustainment in the military and the never-ending battle against corrosion. maintenance of everything is a wide-ranging and provocative call to expand what we mean by maintenance. it invites us to understand not only the profound impact maintenance has on our daily lives but also why taking responsibility for maintaining somethingwhether a motorcycle a monument or our planetcan be a radical act. stewart brand is the cofounder and president of the long now foundation. he created and edited the national book award-winning whole earth catalog from 1968 to 1998. his books include the media lab 1987 how buildings learn 1994 the clock of the long now 1999 and whole earth discipline 2009. he was the subject of the documentary we are as gods 2020. stewart brand makes a persuasive case that keeping the human show on the road through well-planned maintenance is as vital and as fascinating a task as innovation and discovery themselves. a deliciously good book. matt ridley author of the rational optimist once again stewart brand reframes our worldview with a new perspective. you may not imagine you would be interested in rust soviet tanks or tricked-out model tsthat is until brand reexamines them through the lens of maintenance. maintenance of everything is destined to be a classic. danny hillis cofounder of applied invention stewart brand is back with a manifesto on maintenance the tool that empowers all tools. preventative maintenance deferred maintenance and emergency maintenance this much-needed no-nonsense treatise illuminates the difference and why it counts. george dyson autho"}
{"title": "The space and motion of communicating agents (2008) [pdf]", "url": "https://www.cl.cam.ac.uk/archive/rm135/Bigraphs-draft.pdf", "content": "The space and motion of communicating agents (2008) [pdf]. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["space", "motion", "communicate", "agent", "2008", "pdf", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 50.0, "normalized_content": "the space and motion of communicating agents 2008 pdf. score none. author none. date none"}
{"title": "Ask HN: Do you have any evidence that agentic coding works?", "url": "item?id=46691243", "content": "Ask HN: Do you have any evidence that agentic coding works?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "evidence", "agentic", "cod", "work", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 55.0, "normalized_content": "ask hn do you have any evidence that agentic coding works. score none. author none. date none"}
{"title": "Parliament tells Dutch government to keep DigiD data out of American hands", "url": "https://nltimes.nl/2026/01/21/parliament-tells-dutch-govt-keep-digid-data-american-hands", "content": "A parliamentary majority has asked the current caretaker and upcoming new Cabinet to do everything in their power to prevent Dutch DigiD data from ending up in the United States government’s hands. There are concerns that this could happen through the American firm Kyndryl’s impending acquisition of Solvinity , a company that is essential for DigiD access. In a technical briefing in the Tweede Kamer, the lower house of the Dutch parliament, on Tuesday, MPs spoke with experts about the dangers and risks of this takeover. Parliament has long been concerned about this issue, and the briefing did nothing to alleviate those worries, NOS reported . VVD parliamentarian Silvio Erkens is deeply concerned that the acquisition could “enable the U.S. government to access data” and use it to blackmail people. GroenLinks-PvdA MP Barbara Kathmann worries that this will get to a point where “Trump can shut down our digital government with the single push of a button.” The cloud and infrastructure company Solvinity provides the infrastructure that transfers data for DigiD - the digital identification that every person in the Netherlands must have to exchange data with health insurers, pension funds, municipalities, and the Tax Authorities, among others. In the United States, the government has a lot of influence and power over American companies, including the ability to demand companies’ data. The Tweede Kamer cannot force companies to abandon an acquisition, GroenLinks-PvdA MP Kathmann acknowledged. But she hopes that the current and upcoming government will do everything in its power to stop this. She suggested persuading Solvinity to reconsider the acquisition. The government IT service Logius could also switch to another company for its DigiD services, or the government can try its best to buy a “golden share,” which would give the Netherlands veto power in the company, Kathmann said. Erkens believes that the deal must be blocked if there are no legal guarantees that Dutch data", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["parliamentary", "majority", "ask", "current", "caretaker", "upcoming", "new", "cabinet", "power", "prevent", "dutch", "digid", "datum", "end", "united", "states", "government", "hand", "concern", "happen", "american", "firm", "kyndryls", "impend", "acquisition", "solvinity", "company", "essential", "digid", "access", "technical", "briefing", "tweede", "kamer", "low", "house", "dutch", "parliament", "tuesday", "mp", "speak", "expert", "danger", "risk", "takeover", "parliament", "long", "concerned", "issue", "briefing", "alleviate", "worry", "nos", "report", "vvd", "parliamentarian", "silvio", "erken", "deeply", "concerned", "acquisition", "enable", "u.s", "government", "access", "datum", "use", "blackmail", "people", "groenlink", "pvda", "mp", "barbara", "kathmann", "worry", "point", "trump", "shut", "digital", "government", "single", "push", "button", "cloud", "infrastructure", "company", "solvinity", "provide", "infrastructure", "transfer", "datum", "digid", "digital", "identification", "person", "netherlands", "exchange", "datum", "health", "insurer", "pension", "fund", "municipality", "tax", "authority", "united", "states", "government", "lot", "influence", "power", "american", "company", "include", "ability", "demand", "company", "datum", "tweede", "kamer", "force", "company", "abandon", "acquisition", "groenlink", "pvda", "mp", "kathmann", "acknowledge", "hop", "current", "upcoming", "government", "power", "stop", "suggest", "persuade", "solvinity", "reconsider", "acquisition", "government", "service", "logius", "switch", "company", "digid", "service", "government", "try", "good", "buy", "golden", "share", "netherlands", "veto", "power", "company", "kathmann", "say", "erken", "believe", "deal", "block", "legal", "guarantee", "dutch", "datum"], "num_tokens": 167, "token_loss_pct": 50.0, "normalized_content": "a parliamentary majority has asked the current caretaker and upcoming new cabinet to do everything in their power to prevent dutch digid data from ending up in the united states governments hands. there are concerns that this could happen through the american firm kyndryls impending acquisition of solvinity  a company that is essential for digid access. in a technical briefing in the tweede kamer the lower house of the dutch parliament on tuesday mps spoke with experts about the dangers and risks of this takeover. parliament has long been concerned about this issue and the briefing did nothing to alleviate those worries nos reported . vvd parliamentarian silvio erkens is deeply concerned that the acquisition could enable the u.s. government to access data and use it to blackmail people. groenlinks-pvda mp barbara kathmann worries that this will get to a point where trump can shut down our digital government with the single push of a button. the cloud and infrastructure company solvinity provides the infrastructure that transfers data for digid - the digital identification that every person in the netherlands must have to exchange data with health insurers pension funds municipalities and the tax authorities among others. in the united states the government has a lot of influence and power over american companies including the ability to demand companies data. the tweede kamer cannot force companies to abandon an acquisition groenlinks-pvda mp kathmann acknowledged. but she hopes that the current and upcoming government will do everything in its power to stop this. she suggested persuading solvinity to reconsider the acquisition. the government it service logius could also switch to another company for its digid services or the government can try its best to buy a golden share which would give the netherlands veto power in the company kathmann said. erkens believes that the deal must be blocked if there are no legal guarantees that dutch data"}
{"title": "Our approach to age prediction", "url": "https://openai.com/index/our-approach-to-age-prediction/", "content": "Our approach to age prediction. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["approach", "age", "prediction", "score", "author", "date"], "num_tokens": 6, "token_loss_pct": 57.14, "normalized_content": "our approach to age prediction. score none. author none. date none"}
{"title": "Lunar Radio Telescope to Unlock Cosmic Mysteries", "url": "https://spectrum.ieee.org/lunar-radio-telescope", "content": "The catch: It will have to be on the moon Astronomer Jack Burns has spent four decades working to place a radio telescope on the moon. The first one is finally scheduled to launch in early 2027. Is olation dictates where we go to see into the far reaches of the universe. The Atacama Desert of Chile, the summit of Mauna Kea in Hawaii, the vast expanse of the Australian Outback —these are where astronomers and engineers have built the great observatories and radio telescopes of modern times. The skies are usually clear, the air is arid, and the electronic din of civilization is far away. It was to one of these places, in the high desert of New Mexico, that a young astronomer named Jack Burns went to study radio jets and quasars far beyond the Milky Way. It was 1979, he was just out of grad school, and the Very Large Array , a constellation of 28 giant dish antennas on an open plain, was a new mecca of radio astronomy. But the VLA had its limitations—namely, that Earth’s protective atmosphere and ionosphere blocked many parts of the electromagnetic spectrum, and that, even in a remote desert, earthly interference was never completely gone. Could there be a better, even lonelier place to put a radio telescope? Sure, a NASA planetary scientist named Wendell Mendell , told Burns: How about the moon? He asked if Burns had ever thought about building one there. “My immediate reaction was no. Maybe even hell, no. Why would I want to do that?” Burns recalls with a self-deprecating smile. His work at the VLA had gone well, he was fascinated by cosmology’s big questions, and he didn’t want to be slowed by the bureaucratic slog of getting funding to launch a new piece of hardware. But Mendell suggested he do some research and speak at a conference on future lunar observatories, and Burns’s thinking about a space-based radio telescope began to shift. That was in 1984. In the four decades since, he’s published more than 500 peer-reviewed papers on radio astronomy. He’s been an adv", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["catch", "moon", "astronomer", "jack", "burns", "spend", "decade", "work", "place", "radio", "telescope", "moon", "finally", "schedule", "launch", "early", "2027", "olation", "dictate", "far", "reach", "universe", "atacama", "desert", "chile", "summit", "mauna", "kea", "hawaii", "vast", "expanse", "australian", "outback", "astronomer", "engineer", "build", "great", "observatory", "radio", "telescope", "modern", "time", "sky", "usually", "clear", "air", "arid", "electronic", "din", "civilization", "far", "away", "place", "high", "desert", "new", "mexico", "young", "astronomer", "name", "jack", "burn", "go", "study", "radio", "jet", "quasar", "far", "milky", "way", "1979", "grad", "school", "large", "array", "constellation", "28", "giant", "dish", "antenna", "open", "plain", "new", "mecca", "radio", "astronomy", "vla", "limitationsnamely", "earth", "protective", "atmosphere", "ionosphere", "block", "part", "electromagnetic", "spectrum", "remote", "desert", "earthly", "interference", "completely", "go", "well", "lonely", "place", "radio", "telescope", "sure", "nasa", "planetary", "scientist", "name", "wendell", "mendell", "tell", "burn", "moon", "ask", "burn", "think", "build", "immediate", "reaction", "maybe", "hell", "want", "burn", "recall", "self", "deprecate", "smile", "work", "vla", "go", "fascinate", "cosmologys", "big", "question", "not", "want", "slow", "bureaucratic", "slog", "get", "funding", "launch", "new", "piece", "hardware", "mendell", "suggest", "research", "speak", "conference", "future", "lunar", "observatory", "burnss", "think", "space", "base", "radio", "telescope", "begin", "shift", "1984", "decade", "publish", "500", "peer", "review", "paper", "radio", "astronomy", "adv"], "num_tokens": 175, "token_loss_pct": 53.58, "normalized_content": "the catch it will have to be on the moon astronomer jack burns has spent four decades working to place a radio telescope on the moon. the first one is finally scheduled to launch in early 2027. is olation dictates where we go to see into the far reaches of the universe. the atacama desert of chile the summit of mauna kea in hawaii the vast expanse of the australian outback these are where astronomers and engineers have built the great observatories and radio telescopes of modern times. the skies are usually clear the air is arid and the electronic din of civilization is far away. it was to one of these places in the high desert of new mexico that a young astronomer named jack burns went to study radio jets and quasars far beyond the milky way. it was 1979 he was just out of grad school and the very large array  a constellation of 28 giant dish antennas on an open plain was a new mecca of radio astronomy. but the vla had its limitationsnamely that earths protective atmosphere and ionosphere blocked many parts of the electromagnetic spectrum and that even in a remote desert earthly interference was never completely gone. could there be a better even lonelier place to put a radio telescope sure a nasa planetary scientist named wendell mendell  told burns how about the moon he asked if burns had ever thought about building one there. my immediate reaction was no. maybe even hell no. why would i want to do that burns recalls with a self-deprecating smile. his work at the vla had gone well he was fascinated by cosmologys big questions and he didnt want to be slowed by the bureaucratic slog of getting funding to launch a new piece of hardware. but mendell suggested he do some research and speak at a conference on future lunar observatories and burnss thinking about a space-based radio telescope began to shift. that was in 1984. in the four decades since hes published more than 500 peer-reviewed papers on radio astronomy. hes been an adv"}
{"title": "IP Addresses Through 2025", "url": "https://www.potaroo.net/ispcol/2026-01/addr2025.html", "content": "IP Addresses through 2025 January 2026 It's time for another annual roundup from the world of IP addresses. Letâs see what has changed in the past 12 months in addressing the Internet and look at how IP address allocation information can inform us of the changing nature of the network itself. Back around 1992, the IETF gazed into their crystal ball and tried to understand how the Internet was going to evolve and what demands that would place on the addressing system as part of the âIP Next Generationâ study.  The staggeringly large numbers of connected devices that we see today were certainly within the range predicted by that study. The assumption made at the time was that we would continue to use much the same IP protocol architecture, including the requirement that each connected device was assigned a unique IP address, and the implication was that the 32-bit address field defined in version 4 of the IP protocol was clearly going to be inadequate to cope with the predicted number of connected devices. A span of 4 billion address values was just not large enough. We concluded at the time that the only way we could make the Internet work across such a massive pool of connected devices was to deploy a new IP protocol that came with a massively larger address space. It was from this reasoning that IPv6 was designed, as this world of abundant silicon processors connected to a single public Internet was the scenario that IPv6 was primarily intended to solve. The copious volumes of a 128-bit address space were intended to allow us to uniquely assign a public IPv6 address to every such device, no matter how small, or in whatever volume they might be deployed. But while the Internet has grown at amazing speeds across the ensuing 33 years, the deployment of IPv6 has proceeded at a more measured pace. There is still no evidence of any common sense of urgency about the deployment of IPv6 in the public Internet, and still there is no common agreement that the continued", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ip", "address", "2025", "january", "2026", "time", "annual", "roundup", "world", "ip", "address", "letâs", "change", "past", "12", "month", "address", "internet", "look", "ip", "address", "allocation", "information", "inform", "change", "nature", "network", "1992", "ietf", "gaze", "crystal", "ball", "try", "understand", "internet", "go", "evolve", "demand", "place", "addressing", "system", "âip", "generationâ", "study", "staggeringly", "large", "number", "connected", "device", "today", "certainly", "range", "predict", "study", "assumption", "time", "continue", "use", "ip", "protocol", "architecture", "include", "requirement", "connected", "device", "assign", "unique", "ip", "address", "implication", "32", "bit", "address", "field", "define", "version", "ip", "protocol", "clearly", "go", "inadequate", "cope", "predict", "number", "connect", "device", "span", "billion", "address", "value", "large", "conclude", "time", "way", "internet", "work", "massive", "pool", "connect", "device", "deploy", "new", "ip", "protocol", "come", "massively", "large", "address", "space", "reasoning", "ipv6", "design", "world", "abundant", "silicon", "processor", "connect", "single", "public", "internet", "scenario", "ipv6", "primarily", "intend", "solve", "copious", "volume", "128", "bit", "address", "space", "intend", "allow", "uniquely", "assign", "public", "ipv6", "address", "device", "matter", "small", "volume", "deploy", "internet", "grow", "amazing", "speed", "ensue", "33", "year", "deployment", "ipv6", "proceed", "measured", "pace", "evidence", "common", "sense", "urgency", "deployment", "ipv6", "public", "internet", "common", "agreement", "continue"], "num_tokens": 166, "token_loss_pct": 54.14, "normalized_content": "ip addresses through 2025 january 2026 it's time for another annual roundup from the world of ip addresses. letâs see what has changed in the past 12 months in addressing the internet and look at how ip address allocation information can inform us of the changing nature of the network itself. back around 1992 the ietf gazed into their crystal ball and tried to understand how the internet was going to evolve and what demands that would place on the addressing system as part of the âip next generationâ study. the staggeringly large numbers of connected devices that we see today were certainly within the range predicted by that study. the assumption made at the time was that we would continue to use much the same ip protocol architecture including the requirement that each connected device was assigned a unique ip address and the implication was that the 32-bit address field defined in version 4 of the ip protocol was clearly going to be inadequate to cope with the predicted number of connected devices. a span of 4 billion address values was just not large enough. we concluded at the time that the only way we could make the internet work across such a massive pool of connected devices was to deploy a new ip protocol that came with a massively larger address space. it was from this reasoning that ipv6 was designed as this world of abundant silicon processors connected to a single public internet was the scenario that ipv6 was primarily intended to solve. the copious volumes of a 128-bit address space were intended to allow us to uniquely assign a public ipv6 address to every such device no matter how small or in whatever volume they might be deployed. but while the internet has grown at amazing speeds across the ensuing 33 years the deployment of ipv6 has proceeded at a more measured pace. there is still no evidence of any common sense of urgency about the deployment of ipv6 in the public internet and still there is no common agreement that the continued"}
{"title": "Building Robust Helm Charts", "url": "https://www.willmunn.xyz/devops/helm/kubernetes/2026/01/17/building-robust-helm-charts.html", "content": "Jan 17, 2026 In my current work, there is often the need to deploy a similar application\nstack in various configurations, to several environments. Each configuration may\nvary in terms of scale, uptime requirements and feature flagging. Due to a lot\nof flux in infrastructure set up, each environment is also not equivalent. On\ntop of this, there are obviously financial requirements to run all of this as\ncheaply as possible. Kubernetes and helm templating are valuable tools in this\nsituation, they allow us to create a configuration blueprint with the details\nabstracted in values.yaml files. Let’s start with the basics, helm provides a helm lint command which performs\nchecks You can run this with your different values.yaml files to ensure that all your\nconfigurations are compliant. It’s also a good idea to use the helm template command to actually check that\nhelm is able to render your templates. I like to compare helm templating with html templating tools like JSX. This\nallows front end developers to create reusable components usable throughout\npages of a web application, A button component for example can have many states,\nprimary, secondary, loading, disabled, light or dark mode. Each state may also look different depending on the size/type of device your are\nbrowsing the site with. Each of these states represents differences in many\nparameters (font size, colour, gradient, opacity, border, padding, margin,\nwidth, height, etc). These complexities are abstracted away giving the consuming\ncode the list of states to chose from, so that they can write code like this. Under the hood of course many aspects of the CSS or HTML code will be impacted\nby the change of state so you often end up with different parts of the markup\nhaving conditionals on the same check. Just in this contrived example you already have 2 different things being\ncontrolled by the state property with 2 separate checks, the CSS classes and the\npresence of the loading icon. This is quite similar to the si", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["jan", "17", "2026", "current", "work", "need", "deploy", "similar", "application", "stack", "configuration", "environment", "configuration", "vary", "term", "scale", "uptime", "requirement", "feature", "flagging", "lot", "flux", "infrastructure", "set", "environment", "equivalent", "obviously", "financial", "requirement", "run", "cheaply", "possible", "kubernete", "helm", "templating", "valuable", "tool", "situation", "allow", "create", "configuration", "blueprint", "detail", "abstract", "values.yaml", "file", "let", "start", "basic", "helm", "provide", "helm", "lint", "command", "perform", "check", "run", "different", "values.yaml", "file", "ensure", "configuration", "compliant", "good", "idea", "use", "helm", "template", "command", "actually", "check", "helm", "able", "render", "template", "like", "compare", "helm", "templating", "html", "template", "tool", "like", "jsx", "allow", "end", "developer", "create", "reusable", "component", "usable", "page", "web", "application", "button", "component", "example", "state", "primary", "secondary", "loading", "disabled", "light", "dark", "mode", "state", "look", "different", "depend", "sizetype", "device", "browse", "site", "state", "represent", "difference", "parameter", "font", "size", "colour", "gradient", "opacity", "border", "padding", "margin", "width", "height", "etc", "complexity", "abstract", "away", "give", "consume", "code", "list", "state", "chose", "write", "code", "like", "hood", "course", "aspect", "css", "html", "code", "impact", "change", "state", "end", "different", "part", "markup", "have", "conditional", "check", "contrived", "example", "different", "thing", "control", "state", "property", "separate", "check", "css", "class", "presence", "loading", "icon", "similar", "si"], "num_tokens": 172, "token_loss_pct": 50.29, "normalized_content": "jan 17 2026 in my current work there is often the need to deploy a similar application stack in various configurations to several environments. each configuration may vary in terms of scale uptime requirements and feature flagging. due to a lot of flux in infrastructure set up each environment is also not equivalent. on top of this there are obviously financial requirements to run all of this as cheaply as possible. kubernetes and helm templating are valuable tools in this situation they allow us to create a configuration blueprint with the details abstracted in values.yaml files. lets start with the basics helm provides a helm lint command which performs checks you can run this with your different values.yaml files to ensure that all your configurations are compliant. its also a good idea to use the helm template command to actually check that helm is able to render your templates. i like to compare helm templating with html templating tools like jsx. this allows front end developers to create reusable components usable throughout pages of a web application a button component for example can have many states primary secondary loading disabled light or dark mode. each state may also look different depending on the sizetype of device your are browsing the site with. each of these states represents differences in many parameters font size colour gradient opacity border padding margin width height etc. these complexities are abstracted away giving the consuming code the list of states to chose from so that they can write code like this. under the hood of course many aspects of the css or html code will be impacted by the change of state so you often end up with different parts of the markup having conditionals on the same check. just in this contrived example you already have 2 different things being controlled by the state property with 2 separate checks the css classes and the presence of the loading icon. this is quite similar to the si"}
{"title": "Show HN: Agent Skills Leaderboard", "url": "https://skills.sh", "content": "The Open Agent Skills Ecosystem Skills are reusable capabilities for AI agents. Install them with a single command to enhance your agents with access to procedural knowledge. vercel-labs/agent-skills vercel-labs/agent-skills remotion-dev/skills expo/skills expo/skills expo/skills anthropics/skills expo/skills better-auth/skills anthropics/skills expo/skills callstackincubator/agent-skills expo/skills expo/skills expo/skills expo/skills vercel-labs/agent-browser better-auth/skills coreyhaines31/marketingskills coreyhaines31/marketingskills jimliu/baoyu-skills jimliu/baoyu-skills jimliu/baoyu-skills jimliu/baoyu-skills coreyhaines31/marketingskills coreyhaines31/marketingskills jimliu/baoyu-skills coreyhaines31/marketingskills anthropics/skills coreyhaines31/marketingskills coreyhaines31/marketingskills coreyhaines31/marketingskills jimliu/baoyu-skills coreyhaines31/marketingskills jimliu/baoyu-skills coreyhaines31/marketingskills coreyhaines31/marketingskills coreyhaines31/marketingskills coreyhaines31/marketingskills expo/skills coreyhaines31/marketingskills coreyhaines31/marketingskills coreyhaines31/marketingskills coreyhaines31/marketingskills coreyhaines31/marketingskills anthropics/skills coreyhaines31/marketingskills coreyhaines31/marketingskills anthropics/skills coreyhaines31/marketingskills obra/superpowers coreyhaines31/marketingskills coreyhaines31/marketingskills anthropics/skills anthropics/skills op7418/Humanizer-zh anthropics/skills anthropics/skills expo/skills jimliu/baoyu-skills expo/skills expo/skills expo/skills jimliu/baoyu-skills expo/skills expo/skills jimliu/baoyu-skills obra/superpowers anthropics/skills jimliu/baoyu-skills obra/superpowers onmax/nuxt-skills anthropics/skills obra/superpowers obra/superpowers anthropics/skills anthropics/skills obra/superpowers obra/superpowers obra/superpowers anthropics/skills obra/superpowers nextlevelbuilder/ui-ux-pro-max-skill onmax/nuxt-skills anthropics/skills obra/superpowers obra/superpowers obra/su", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["open", "agent", "skill", "ecosystem", "skill", "reusable", "capability", "ai", "agent", "install", "single", "command", "enhance", "agent", "access", "procedural", "knowledge", "vercel", "labsagent", "skill", "vercel", "labsagent", "skill", "remotion", "devskill", "exposkill", "exposkill", "exposkill", "anthropicsskill", "exposkill", "well", "authskill", "anthropicsskill", "exposkill", "callstackincubatoragent", "skill", "exposkill", "exposkill", "exposkill", "exposkill", "vercel", "labsagent", "browser", "well", "authskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "jimliubaoyu", "skill", "jimliubaoyu", "skill", "jimliubaoyu", "skill", "jimliubaoyu", "skill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "jimliubaoyu", "skill", "coreyhaines31marketingskill", "anthropicsskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "jimliubaoyu", "skill", "coreyhaines31marketingskill", "jimliubaoyu", "skill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "exposkill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "anthropicsskill", "coreyhaines31marketingskill", "coreyhaines31marketingskill", "anthropicsskill", "coreyhaines31marketingskill", "obrasuperpower", "coreyhaines31marketingskill", "coreyhaines31marketingskills", "anthropicsskill", "anthropicsskill", "op7418humanizer", "zh", "anthropicsskills", "anthropicsskill", "exposkill", "jimliubaoyu", "skill", "exposkill", "exposkill", "exposkill", "jimliubaoyu", "skill", "exposkill", "exposkill", "jimliubaoyu", "skill", "obrasuperpower", "anthropicsskill", "jimliubaoyu", "skill", "obrasuperpower", "onmaxnuxt", "skill", "anthropicsskill", "obrasuperpower", "obrasuperpower", "anthropicsskill", "anthropicsskill", "obrasuperpower", "obrasuperpower", "obrasuperpower", "anthropicsskills", "obrasuperpowers", "nextlevelbuilderui", "ux", "pro", "max", "skill", "onmaxnuxt", "skill", "anthropicsskill", "obrasuperpower", "obrasuperpower", "obrasu"], "num_tokens": 133, "token_loss_pct": 23.12, "normalized_content": "the open agent skills ecosystem skills are reusable capabilities for ai agents. install them with a single command to enhance your agents with access to procedural knowledge. vercel-labsagent-skills vercel-labsagent-skills remotion-devskills exposkills exposkills exposkills anthropicsskills exposkills better-authskills anthropicsskills exposkills callstackincubatoragent-skills exposkills exposkills exposkills exposkills vercel-labsagent-browser better-authskills coreyhaines31marketingskills coreyhaines31marketingskills jimliubaoyu-skills jimliubaoyu-skills jimliubaoyu-skills jimliubaoyu-skills coreyhaines31marketingskills coreyhaines31marketingskills jimliubaoyu-skills coreyhaines31marketingskills anthropicsskills coreyhaines31marketingskills coreyhaines31marketingskills coreyhaines31marketingskills jimliubaoyu-skills coreyhaines31marketingskills jimliubaoyu-skills coreyhaines31marketingskills coreyhaines31marketingskills coreyhaines31marketingskills coreyhaines31marketingskills exposkills coreyhaines31marketingskills coreyhaines31marketingskills coreyhaines31marketingskills coreyhaines31marketingskills coreyhaines31marketingskills anthropicsskills coreyhaines31marketingskills coreyhaines31marketingskills anthropicsskills coreyhaines31marketingskills obrasuperpowers coreyhaines31marketingskills coreyhaines31marketingskills anthropicsskills anthropicsskills op7418humanizer-zh anthropicsskills anthropicsskills exposkills jimliubaoyu-skills exposkills exposkills exposkills jimliubaoyu-skills exposkills exposkills jimliubaoyu-skills obrasuperpowers anthropicsskills jimliubaoyu-skills obrasuperpowers onmaxnuxt-skills anthropicsskills obrasuperpowers obrasuperpowers anthropicsskills anthropicsskills obrasuperpowers obrasuperpowers obrasuperpowers anthropicsskills obrasuperpowers nextlevelbuilderui-ux-pro-max-skill onmaxnuxt-skills anthropicsskills obrasuperpowers obrasuperpowers obrasu"}
{"title": "Electricity use of AI coding agents", "url": "https://www.simonpcouch.com/blog/2026-01-20-cc-impact/", "content": "Most of the discourse about the environmental impact of LLM use focuses on a ‘median query.’ What about a Claude Code session? January 20, 2026 Throughout 2025, we got better estimates of electricity and water use of AI chatbots. There are all sorts of posts I could cite on this topic, but a favorite is this blog post from Our World in Data’s Hannah Ritchie. On the electricity front:  In short, “unless you’re an extreme power user, asking AI questions every day is still a rounding error on your total electricity footprint.” A similar story applies to water usage. This one from Benjamin Todd : The average American uses 1600 liters of water per day , so even if you make 100 prompts per day, at 2ml per prompt, that’s only 0.01% of your total water consumption. Using a shower for one second would use far more. Generally, these analyses guide my own thinking about the environmental impacts of my individual usage of LLMs; if I’m interested in reducing my personal carbon footprint, I’m much better off driving a couple miles less a week or avoiding one flight each year. This is indeed the right conclusion for users of chat interfaces like chatgpt.com or claude.ai. That said, 1 or 10 or 100 median prompts a day is many orders of magnitude off from my own personal use of LLMs; I likely am, in Hannah Ritchie’s words, an “extreme power user.” I work in software and spend much of my workday driving 2 or 3 coding agents, like Claude Code, at a time. Thus, a much more relevant question for me is how much energy does a typical Claude Code session consume? (I’m not going to discuss water use in this post.) tl;dr, much more:  There are so many considerations and assumptions and pieces of shorthand one must use along the way to answer this sort of question. I’ll do my best to call those out throughout this post, but please do understand this is still just Sunday afternoon napkin math from Some Guy. tl;dr: In this section, I point out that a Claude Code session should use orders of mag", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["discourse", "environmental", "impact", "llm", "use", "focus", "median", "query", "claude", "code", "session", "january", "20", "2026", "2025", "get", "well", "estimate", "electricity", "water", "use", "ai", "chatbot", "sort", "post", "cite", "topic", "favorite", "blog", "post", "world", "datas", "hannah", "ritchie", "electricity", "short", "extreme", "power", "user", "ask", "ai", "question", "day", "rounding", "error", "total", "electricity", "footprint", "similar", "story", "apply", "water", "usage", "benjamin", "todd", "average", "american", "use", "1600", "liter", "water", "day", "100", "prompt", "day", "2ml", "prompt", "0.01", "total", "water", "consumption", "shower", "second", "use", "far", "generally", "analysis", "guide", "thinking", "environmental", "impact", "individual", "usage", "llm", "interested", "reduce", "personal", "carbon", "footprint", "well", "drive", "couple", "mile", "week", "avoid", "flight", "year", "right", "conclusion", "user", "chat", "interface", "like", "chatgpt.com", "claude.ai", "say", "10", "100", "median", "prompt", "day", "order", "magnitude", "personal", "use", "llm", "likely", "hannah", "ritchie", "word", "extreme", "power", "user", "work", "software", "spend", "workday", "drive", "cod", "agent", "like", "claude", "code", "time", "relevant", "question", "energy", "typical", "claude", "code", "session", "consume", "go", "discuss", "water", "use", "post", "tldr", "consideration", "assumption", "piece", "shorthand", "use", "way", "answer", "sort", "question", "ill", "good", "post", "understand", "sunday", "afternoon", "napkin", "math", "guy", "tldr", "section", "point", "claude", "code", "session", "use", "order", "mag"], "num_tokens": 175, "token_loss_pct": 53.83, "normalized_content": "most of the discourse about the environmental impact of llm use focuses on a median query. what about a claude code session january 20 2026 throughout 2025 we got better estimates of electricity and water use of ai chatbots. there are all sorts of posts i could cite on this topic but a favorite is this blog post from our world in datas hannah ritchie. on the electricity front in short unless youre an extreme power user asking ai questions every day is still a rounding error on your total electricity footprint. a similar story applies to water usage. this one from benjamin todd  the average american uses 1600 liters of water per day  so even if you make 100 prompts per day at 2ml per prompt thats only 0.01 of your total water consumption. using a shower for one second would use far more. generally these analyses guide my own thinking about the environmental impacts of my individual usage of llms if im interested in reducing my personal carbon footprint im much better off driving a couple miles less a week or avoiding one flight each year. this is indeed the right conclusion for users of chat interfaces like chatgpt.com or claude.ai. that said 1 or 10 or 100 median prompts a day is many orders of magnitude off from my own personal use of llms i likely am in hannah ritchies words an extreme power user. i work in software and spend much of my workday driving 2 or 3 coding agents like claude code at a time. thus a much more relevant question for me is how much energy does a typical claude code session consume im not going to discuss water use in this post. tldr much more there are so many considerations and assumptions and pieces of shorthand one must use along the way to answer this sort of question. ill do my best to call those out throughout this post but please do understand this is still just sunday afternoon napkin math from some guy. tldr in this section i point out that a claude code session should use orders of mag"}
{"title": "Prediction markets are ushering in a world in which news becomes about gambling", "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/", "content": "Why is the media obsessed with prediction markets? For the past week, I’ve found myself playing the same 23-second CNN clip on repeat. I’ve watched it in bed, during my commute to work, at the office, midway through making carrot soup, and while brushing my teeth. In the video, Harry Enten, the network’s chief data analyst, stares into the camera and breathlessly tells his audience about the gambling odds that Donald Trump will buy any of Greenland. “The people who are putting their money where their mouth is—they are absolutely taking this seriously,” Enten says. He taps the giant touch screen behind him and pulls up a made-for-TV graphic: Based on how people were betting online at the time, there was a 36 percent chance that the president would annex Greenland. “Whoa, way up there!” Enten yells, slapping his hands together. “My goodness gracious!” The ticker at the bottom of the screen speeds through other odds: Will Gavin Newsom win the next presidential election? 19 percent chance. Will Viktor Orbán be out as the leader of Hungary before the end of the year? 48 percent chance.  These odds were pulled from Kalshi, which hilariously claims not to be a gambling platform: It’s a “prediction market.” People go to sites such as Kalshi and Polymarket—another big prediction market—in order to put money down on a given news event. Nobody would bet on something that they didn’t believe would happen, the thinking goes, and so the markets are meant to forecast the likelihood of a given outcome. Listen: Prediction markets and the “suckerification” crisis, with Max Read Prediction markets let you wager on basically anything. Will Elon Musk father another baby by June 30? Will Jesus return this year? Will Israel strike Gaza tomorrow ? Will the longevity guru Bryan Johnson’s next functional sperm count be greater than “ 20.0 M/ejac ”? These sites have recently boomed in popularity—particularly among terminally online young men who trade meme stocks and siphon from their 401(k)s", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["medium", "obsess", "prediction", "market", "past", "week", "ve", "find", "play", "23", "second", "cnn", "clip", "repeat", "ve", "watch", "bed", "commute", "work", "office", "midway", "make", "carrot", "soup", "brush", "tooth", "video", "harry", "enten", "network", "chief", "datum", "analyst", "stare", "camera", "breathlessly", "tell", "audience", "gambling", "odd", "donald", "trump", "buy", "greenland", "people", "put", "money", "mouth", "isthey", "absolutely", "take", "seriously", "enten", "say", "tap", "giant", "touch", "screen", "pull", "tv", "graphic", "base", "people", "bet", "online", "time", "36", "percent", "chance", "president", "annex", "greenland", "whoa", "way", "enten", "yell", "slap", "hand", "goodness", "gracious", "ticker", "screen", "speed", "odd", "gavin", "newsom", "win", "presidential", "election", "19", "percent", "chance", "viktor", "orbán", "leader", "hungary", "end", "year", "48", "percent", "chance", "odd", "pull", "kalshi", "hilariously", "claim", "gambling", "platform", "prediction", "market", "people", "site", "kalshi", "polymarketanother", "big", "prediction", "marketin", "order", "money", "give", "news", "event", "bet", "not", "believe", "happen", "thinking", "go", "market", "mean", "forecast", "likelihood", "give", "outcome", "listen", "prediction", "market", "suckerification", "crisis", "max", "read", "prediction", "market", "let", "wager", "basically", "elon", "musk", "father", "baby", "june", "30", "jesus", "return", "year", "israel", "strike", "gaza", "tomorrow", "longevity", "guru", "bryan", "johnsons", "functional", "sperm", "count", "great", "20.0", "mejac", "site", "recently", "boom", "popularityparticularly", "terminally", "online", "young", "man", "trade", "meme", "stock", "siphon", "401ks"], "num_tokens": 182, "token_loss_pct": 48.3, "normalized_content": "why is the media obsessed with prediction markets for the past week ive found myself playing the same 23-second cnn clip on repeat. ive watched it in bed during my commute to work at the office midway through making carrot soup and while brushing my teeth. in the video harry enten the networks chief data analyst stares into the camera and breathlessly tells his audience about the gambling odds that donald trump will buy any of greenland. the people who are putting their money where their mouth isthey are absolutely taking this seriously enten says. he taps the giant touch screen behind him and pulls up a made-for-tv graphic based on how people were betting online at the time there was a 36 percent chance that the president would annex greenland. whoa way up there enten yells slapping his hands together. my goodness gracious the ticker at the bottom of the screen speeds through other odds will gavin newsom win the next presidential election 19 percent chance. will viktor orbán be out as the leader of hungary before the end of the year 48 percent chance. these odds were pulled from kalshi which hilariously claims not to be a gambling platform its a prediction market. people go to sites such as kalshi and polymarketanother big prediction marketin order to put money down on a given news event. nobody would bet on something that they didnt believe would happen the thinking goes and so the markets are meant to forecast the likelihood of a given outcome. listen prediction markets and the suckerification crisis with max read prediction markets let you wager on basically anything. will elon musk father another baby by june 30 will jesus return this year will israel strike gaza tomorrow  will the longevity guru bryan johnsons next functional sperm count be greater than  20.0 mejac  these sites have recently boomed in popularityparticularly among terminally online young men who trade meme stocks and siphon from their 401ks"}
{"title": "Nova Launcher added Facebook and Google Ads tracking", "url": "https://lemdro.id/post/lemdro.id/35049920", "content": "Nova Launcher added Facebook and Google Ads tracking. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nova", "launcher", "add", "facebook", "google", "ad", "track", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 41.18, "normalized_content": "nova launcher added facebook and google ads tracking. score none. author none. date none"}
{"title": "Nvidia: Natural Conversational AI with Any Role and Voice", "url": "https://research.nvidia.com/labs/adlr/personaplex/", "content": "Published: January 15, 2026 Rajarshi Roy, Jonathan Raiman, Sang-gil Lee, Teodor-Dumitru Ene, Robert Kirby, Sungwon Kim, Jaehyeon Kim, Bryan Catanzaro. PersonaPlex and Rajarshi Roy sharing jokes. Conversational AI has forced an impossible choice. Traditional systems (ASRâLLMâTTS cascades) let you customize the voice and role, but conversations feel robotic with awkward pauses, no interruptions, and unnatural turn-taking. Full-duplex models like Moshi finally made AI conversations feel natural with real-time listening and speaking, but locked you into a single fixed voice and role.\nNVIDIA PersonaPlex breaks this trade-off. Select from a diverse range of voices and define any role through text prompts. Need a wise assistant, a customer service agent, a fantasy character, or just someone to talk to? PersonaPlex delivers truly natural conversations while maintaining your chosen persona throughout. It handles interruptions, backchannels, and authentic conversational rhythm. For the first time, you get both the customization you need and the naturalness that makes conversations feel genuinely human. PersonaPlex is a full duplex model: it listens and speaks at the same time. This capability, first introduced with Moshi, lets PersonaPlex learn not only the contents of its speech but also the behavior associated with speech, such as when to pause, interrupt, or backchannel (âuh-huhâ, âohâ, etc.). We achieve low-latency interaction by eliminating delays associated with cascaded systems that use separate models for listening (Automated Speech Recognition), language production (Language Model), and speaking (Text to Speech). Our approach uses a single model that updates its internal state as the user speaks and streams a response back immediately. Enriching PersonaPlexâs output with non-verbal aspects creates an important qualitative difference relative to systems without this dimension: PersonaPlex now recreates some of the same cues humans use to read intent, emo", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["publish", "january", "15", "2026", "rajarshi", "roy", "jonathan", "raiman", "sang", "gil", "lee", "teodor", "dumitru", "ene", "robert", "kirby", "sungwon", "kim", "jaehyeon", "kim", "bryan", "catanzaro", "personaplex", "rajarshi", "roy", "sharing", "joke", "conversational", "ai", "force", "impossible", "choice", "traditional", "system", "asrâllmâtts", "cascade", "let", "customize", "voice", "role", "conversation", "feel", "robotic", "awkward", "pause", "interruption", "unnatural", "turn", "taking", "duplex", "model", "like", "moshi", "finally", "ai", "conversation", "feel", "natural", "real", "time", "listening", "speaking", "lock", "single", "fix", "voice", "role", "nvidia", "personaplex", "break", "trade", "select", "diverse", "range", "voice", "define", "role", "text", "prompt", "need", "wise", "assistant", "customer", "service", "agent", "fantasy", "character", "talk", "personaplex", "deliver", "truly", "natural", "conversation", "maintain", "choose", "persona", "handle", "interruption", "backchannel", "authentic", "conversational", "rhythm", "time", "customization", "need", "naturalness", "make", "conversation", "feel", "genuinely", "human", "personaplex", "duplex", "model", "listen", "speak", "time", "capability", "introduce", "moshi", "lets", "personaplex", "learn", "content", "speech", "behavior", "associate", "speech", "pause", "interrupt", "backchannel", "âuh", "huhâ", "âohâ", "etc", "achieve", "low", "latency", "interaction", "eliminate", "delay", "associate", "cascade", "system", "use", "separate", "model", "listen", "automate", "speech", "recognition", "language", "production", "language", "model", "speak", "text", "speech", "approach", "use", "single", "model", "update", "internal", "state", "user", "speak", "stream", "response", "immediately", "enrich", "personaplexâs", "output", "non", "verbal", "aspect", "create", "important", "qualitative", "difference", "relative", "system", "dimension", "personaplex", "recreate", "cue", "human", "use", "read", "intent", "emo"], "num_tokens": 191, "token_loss_pct": 39.75, "normalized_content": "published january 15 2026 rajarshi roy jonathan raiman sang-gil lee teodor-dumitru ene robert kirby sungwon kim jaehyeon kim bryan catanzaro. personaplex and rajarshi roy sharing jokes. conversational ai has forced an impossible choice. traditional systems asrâllmâtts cascades let you customize the voice and role but conversations feel robotic with awkward pauses no interruptions and unnatural turn-taking. full-duplex models like moshi finally made ai conversations feel natural with real-time listening and speaking but locked you into a single fixed voice and role. nvidia personaplex breaks this trade-off. select from a diverse range of voices and define any role through text prompts. need a wise assistant a customer service agent a fantasy character or just someone to talk to personaplex delivers truly natural conversations while maintaining your chosen persona throughout. it handles interruptions backchannels and authentic conversational rhythm. for the first time you get both the customization you need and the naturalness that makes conversations feel genuinely human. personaplex is a full duplex model it listens and speaks at the same time. this capability first introduced with moshi lets personaplex learn not only the contents of its speech but also the behavior associated with speech such as when to pause interrupt or backchannel âuh-huhâ âohâ etc.. we achieve low-latency interaction by eliminating delays associated with cascaded systems that use separate models for listening automated speech recognition language production language model and speaking text to speech. our approach uses a single model that updates its internal state as the user speaks and streams a response back immediately. enriching personaplexâs output with non-verbal aspects creates an important qualitative difference relative to systems without this dimension personaplex now recreates some of the same cues humans use to read intent emo"}
{"title": "I'm addicted to being useful", "url": "https://www.seangoedecke.com/addicted-to-being-useful/", "content": "When I get together with my friends in the industry, I feel a little guilty about how much I love my job. This is a tough time to be a software engineer. The job was less stressful in the late 2010s than it is now, and I sympathize with anyone who is upset about the change. There are a lot of objective reasons to feel bad about work. But despite all that, I’m still having a blast. I enjoy pulling together projects, figuring out difficult bugs, and writing code in general. I like spending time with computers. But what I really love is being useful . The main character in Gogol’s short story The Overcoat is a man called Akaky Akaievich 1 . Akaky’s job is objectively terrible: he’s stuck in a dead-end copyist role, being paid very little, with colleagues who don’t respect him. Still, he loves his work, to the point that if he has no work to take home with him, he does some recreational copying just for his own sake. Akaky is a dysfunctional person. But his dysfunction makes him a perfect fit for his job 2 . It’s hard for me to see a problem and not solve it. This is especially true if I’m the only person (or one of a very few people) who could solve it, or if somebody is asking for my help. I feel an almost physical discomfort about it, and a corresponding relief and satisfaction when I do go and solve the problem. The work of a software engineer - or at least my work as a staff software engineer - is perfectly tailored to this tendency. Every day people rely on me to solve a series of technical problems 3 . In other words, like Akaky Akaievich, I don’t mind the ways in which my job is dysfunctional, because it matches the ways in which I myself am dysfunctional: specifically, my addiction to being useful . (Of course, it helps that my working conditions are overall much better than Akaky’s). I’m kind of like a working dog, in a way. Working dogs get rewarded with treats 4 , but they don’t do it for the treats. They do it for the work itself, which is inherently satisf", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["friend", "industry", "feel", "little", "guilty", "love", "job", "tough", "time", "software", "engineer", "job", "stressful", "late", "2010", "sympathize", "upset", "change", "lot", "objective", "reason", "feel", "bad", "work", "despite", "have", "blast", "enjoy", "pull", "project", "figure", "difficult", "bug", "write", "code", "general", "like", "spend", "time", "computer", "love", "useful", "main", "character", "gogols", "short", "story", "overcoat", "man", "call", "akaky", "akaievich", "akakys", "job", "objectively", "terrible", "stuck", "dead", "end", "copyist", "role", "pay", "little", "colleague", "not", "respect", "love", "work", "point", "work", "home", "recreational", "copying", "sake", "akaky", "dysfunctional", "person", "dysfunction", "make", "perfect", "fit", "job", "hard", "problem", "solve", "especially", "true", "person", "people", "solve", "somebody", "ask", "help", "feel", "physical", "discomfort", "corresponding", "relief", "satisfaction", "solve", "problem", "work", "software", "engineer", "work", "staff", "software", "engineer", "perfectly", "tailor", "tendency", "day", "people", "rely", "solve", "series", "technical", "problem", "word", "like", "akaky", "akaievich", "not", "mind", "way", "job", "dysfunctional", "match", "way", "dysfunctional", "specifically", "addiction", "useful", "course", "help", "working", "condition", "overall", "well", "akakys", "kind", "like", "work", "dog", "way", "work", "dog", "reward", "treat", "not", "treat", "work", "inherently", "satisf"], "num_tokens": 154, "token_loss_pct": 61.98, "normalized_content": "when i get together with my friends in the industry i feel a little guilty about how much i love my job. this is a tough time to be a software engineer. the job was less stressful in the late 2010s than it is now and i sympathize with anyone who is upset about the change. there are a lot of objective reasons to feel bad about work. but despite all that im still having a blast. i enjoy pulling together projects figuring out difficult bugs and writing code in general. i like spending time with computers. but what i really love is being useful . the main character in gogols short story the overcoat is a man called akaky akaievich 1 . akakys job is objectively terrible hes stuck in a dead-end copyist role being paid very little with colleagues who dont respect him. still he loves his work to the point that if he has no work to take home with him he does some recreational copying just for his own sake. akaky is a dysfunctional person. but his dysfunction makes him a perfect fit for his job 2 . its hard for me to see a problem and not solve it. this is especially true if im the only person or one of a very few people who could solve it or if somebody is asking for my help. i feel an almost physical discomfort about it and a corresponding relief and satisfaction when i do go and solve the problem. the work of a software engineer - or at least my work as a staff software engineer - is perfectly tailored to this tendency. every day people rely on me to solve a series of technical problems 3 . in other words like akaky akaievich i dont mind the ways in which my job is dysfunctional because it matches the ways in which i myself am dysfunctional specifically my addiction to being useful . of course it helps that my working conditions are overall much better than akakys. im kind of like a working dog in a way. working dogs get rewarded with treats 4  but they dont do it for the treats. they do it for the work itself which is inherently satisf"}
{"title": "Apples, Trees, and Quasimodes", "url": "https://systemstack.dev/2025/09/humane-computing/", "content": "September 18, 2025 A while back, Ars Technica published a thoughtful piece about Jef Raskin , tracing his long pursuit of the “humane computer” and the cul-de-sacs where that pursuit ended. It’s a generous, well-told account of the designer who wanted to make machines simpler, kinder, and more aligned with the way people actually think. But part of what makes Raskin interesting is that his story isn’t just Apple’s story. He came out of the same cultural current John Markoff chronicled in What the Dormouse Said —the Bay Area tradition that treated computers not as office appliances but as tools for thought, instruments of liberation. Read that way, the Canon Cat and Raskin’s other projects aren’t just an eccentric side quest from a frustrated Apple veteran. It’s evidence of how far the humane ideal could stretch, and how quickly it ran up against the limits of commercial computing. Apple couldn’t deliver Raskin’s vision then, and it can’t deliver it now. Neither can any other big platform company. If we want to understand why, and what Raskin still tells us about humane computing, we have to put him back in the longer lineage he belonged to, and look at how his version of the dream carried that vision but also narrowed it. What the Dormouse Said documents how the Bay Area counterculture  shaped early personal computing. LSD, communes, systems theory, amorphous defense research contracts, and Engelbart’s “augmentation” experiments all swirled together in a weird scene that accidentally (or maybe not so accidentally) created much of the modern world. The story usually gets told with a neat list: Engelbart’s demo , Nelson’s Xanadu hypertext, Kay’s Dynabook , Brand’s Whole Earth . Xerox PARC, Steve Jobs, the World Wide Web. The familiar pantheon. But that version turns a messy, improvisational moment into a plaque. Engelbart’s system needed a whole research staff just to operate; Nelson’s Xanadu was (and is) more sermon than software; Kay’s Dynabook lived mostly on paper", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["september", "18", "2025", "ar", "technica", "publish", "thoughtful", "piece", "jef", "raskin", "trace", "long", "pursuit", "humane", "computer", "cul", "de", "sac", "pursuit", "end", "generous", "tell", "account", "designer", "want", "machine", "simple", "kinder", "align", "way", "people", "actually", "think", "make", "raskin", "interesting", "story", "not", "apple", "story", "come", "cultural", "current", "john", "markoff", "chronicle", "dormouse", "say", "bay", "area", "tradition", "treat", "computer", "office", "appliance", "tool", "thought", "instrument", "liberation", "read", "way", "canon", "cat", "raskin", "project", "not", "eccentric", "quest", "frustrated", "apple", "veteran", "evidence", "far", "humane", "ideal", "stretch", "quickly", "run", "limit", "commercial", "computing", "apple", "not", "deliver", "raskin", "vision", "not", "deliver", "big", "platform", "company", "want", "understand", "raskin", "tell", "humane", "computing", "long", "lineage", "belong", "look", "version", "dream", "carry", "vision", "narrow", "dormouse", "say", "document", "bay", "area", "counterculture", "shape", "early", "personal", "computing", "lsd", "communes", "system", "theory", "amorphous", "defense", "research", "contract", "engelbart", "augmentation", "experiment", "swirl", "weird", "scene", "accidentally", "maybe", "accidentally", "create", "modern", "world", "story", "usually", "get", "tell", "neat", "list", "engelbart", "demo", "nelson", "xanadu", "hypertext", "kay", "dynabook", "brand", "earth", "xerox", "parc", "steve", "jobs", "world", "wide", "web", "familiar", "pantheon", "version", "turn", "messy", "improvisational", "moment", "plaque", "engelbart", "system", "need", "research", "staff", "operate", "nelson", "xanadu", "sermon", "software", "kay", "dynabook", "live", "paper"], "num_tokens": 180, "token_loss_pct": 48.57, "normalized_content": "september 18 2025 a while back ars technica published a thoughtful piece about jef raskin  tracing his long pursuit of the humane computer and the cul-de-sacs where that pursuit ended. its a generous well-told account of the designer who wanted to make machines simpler kinder and more aligned with the way people actually think. but part of what makes raskin interesting is that his story isnt just apples story. he came out of the same cultural current john markoff chronicled in what the dormouse said the bay area tradition that treated computers not as office appliances but as tools for thought instruments of liberation. read that way the canon cat and raskins other projects arent just an eccentric side quest from a frustrated apple veteran. its evidence of how far the humane ideal could stretch and how quickly it ran up against the limits of commercial computing. apple couldnt deliver raskins vision then and it cant deliver it now. neither can any other big platform company. if we want to understand why and what raskin still tells us about humane computing we have to put him back in the longer lineage he belonged to and look at how his version of the dream carried that vision but also narrowed it. what the dormouse said documents how the bay area counterculture shaped early personal computing. lsd communes systems theory amorphous defense research contracts and engelbarts augmentation experiments all swirled together in a weird scene that accidentally or maybe not so accidentally created much of the modern world. the story usually gets told with a neat list engelbarts demo  nelsons xanadu hypertext kays dynabook  brands whole earth . xerox parc steve jobs the world wide web. the familiar pantheon. but that version turns a messy improvisational moment into a plaque. engelbarts system needed a whole research staff just to operate nelsons xanadu was and is more sermon than software kays dynabook lived mostly on paper"}
{"title": "Provably unmasking malicious behavior through execution traces", "url": "https://arxiv.org/abs/2512.13821", "content": "Help | Advanced Search arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .  arXiv Operational Status", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["help", "advanced", "search", "arxivlab", "framework", "allow", "collaborator", "develop", "share", "new", "arxiv", "feature", "directly", "website", "individual", "organization", "work", "arxivlab", "embrace", "accept", "value", "openness", "community", "excellence", "user", "data", "privacy", "arxiv", "commit", "value", "work", "partner", "adhere", "idea", "project", "add", "value", "arxiv", "community", "learn", "arxivlab", "arxiv", "operational", "status"], "num_tokens": 44, "token_loss_pct": 47.62, "normalized_content": "help  advanced search arxivlabs is a framework that allows collaborators to develop and share new arxiv features directly on our website. both individuals and organizations that work with arxivlabs have embraced and accepted our values of openness community excellence and user data privacy. arxiv is committed to these values and only works with partners that adhere to them. have an idea for a project that will add value for arxiv's community learn more about arxivlabs . arxiv operational status"}
{"title": "Disaster planning for regular folks (2015)", "url": "https://lcamtuf.coredump.cx/prep/index-old.shtml", "content": "Written by lcamtuf@coredump.cx , Dec 2015, minor updates Jul 2021. Buy the book instead! Practical Doomsday is an in-depth, data-packed guide\nto rational emergency preparedness. The book offers deeper and more polished insights on most of the topics covered on this page. For example,\nabout 40 pages are devoted to financial planning alone - from cash reserves, to insurance policies, to commodity derivatives. You can get it on Amazon , order from Barnes & Noble , or visit your\nfavorite book place. Sample chapter is available here . The prepper culture begs to be taken with a grain of salt. In the public\nconsciousness, its has all the makings of a doomsday cult:\na tribe of unkempt misfits who hoard gold bullion, study herbalism,\nand preach about the imminent collapse of our society. Today, most of us see such worries as absurd. It's not that life-altering disasters are\nrare: every year, we hear about millions of people displaced by wildfires, earthquakes,\nhurricanes, or floods. Heck, not a decade goes by without at least one first-class\ndemocracy lapsing into armed conflict or fiscal disarray. But having grown up in a period\nof prosperity and calm, we find it difficult to believe that an episode of bad weather or a currency crisis\ncould upend our lives. I suspect that we dismiss such hazards not only because they seem surreal, but also because\nworrying about them can make one feel helpless and lost. What's more, we tend to follow the\nsame instincts to tune out far more pedestrian and avoidable risks. For example, \nmost of us don't plan ahead for losing a job, for dealing with a week-long water outage, or\nfor surviving the night if our home goes up in smoke. Quite often, our singular strategy for dealing with such dangers is to hope for the\ngovernment to bail us out. But no matter if our elected officials prefer to school us with\npassages from Milton Friendman or from Thomas Piketty, the hard truth is that no state can provide\na robust safety net for all of life's likel", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["write", "lcamtufmention.cx", "dec", "2015", "minor", "update", "jul", "2021", "buy", "book", "instead", "practical", "doomsday", "depth", "data", "pack", "guide", "rational", "emergency", "preparedness", "book", "offer", "deep", "polished", "insight", "topic", "cover", "page", "example", "40", "page", "devoted", "financial", "planning", "cash", "reserve", "insurance", "policy", "commodity", "derivative", "amazon", "order", "barne", "noble", "visit", "favorite", "book", "place", "sample", "chapter", "available", "prepper", "culture", "beg", "take", "grain", "salt", "public", "consciousness", "making", "doomsday", "cult", "tribe", "unkempt", "misfit", "hoard", "gold", "bullion", "study", "herbalism", "preach", "imminent", "collapse", "society", "today", "worry", "absurd", "life", "alter", "disaster", "rare", "year", "hear", "million", "people", "displace", "wildfire", "earthquake", "hurricane", "flood", "heck", "decade", "go", "class", "democracy", "lapsing", "armed", "conflict", "fiscal", "disarray", "having", "grow", "period", "prosperity", "calm", "find", "difficult", "believe", "episode", "bad", "weather", "currency", "crisis", "upend", "life", "suspect", "dismiss", "hazard", "surreal", "worry", "feel", "helpless", "lose", "tend", "follow", "instinct", "tune", "far", "pedestrian", "avoidable", "risk", "example", "plan", "ahead", "lose", "job", "deal", "week", "long", "water", "outage", "survive", "night", "home", "go", "smoke", "singular", "strategy", "deal", "danger", "hope", "government", "bail", "matter", "elect", "official", "prefer", "school", "passage", "milton", "friendman", "thomas", "piketty", "hard", "truth", "state", "provide", "robust", "safety", "net", "life", "likel"], "num_tokens": 172, "token_loss_pct": 54.26, "normalized_content": "written by lcamtufmention.cx  dec 2015 minor updates jul 2021. buy the book instead practical doomsday is an in-depth data-packed guide to rational emergency preparedness. the book offers deeper and more polished insights on most of the topics covered on this page. for example about 40 pages are devoted to financial planning alone - from cash reserves to insurance policies to commodity derivatives. you can get it on amazon  order from barnes  noble  or visit your favorite book place. sample chapter is available here . the prepper culture begs to be taken with a grain of salt. in the public consciousness its has all the makings of a doomsday cult a tribe of unkempt misfits who hoard gold bullion study herbalism and preach about the imminent collapse of our society. today most of us see such worries as absurd. it's not that life-altering disasters are rare every year we hear about millions of people displaced by wildfires earthquakes hurricanes or floods. heck not a decade goes by without at least one first-class democracy lapsing into armed conflict or fiscal disarray. but having grown up in a period of prosperity and calm we find it difficult to believe that an episode of bad weather or a currency crisis could upend our lives. i suspect that we dismiss such hazards not only because they seem surreal but also because worrying about them can make one feel helpless and lost. what's more we tend to follow the same instincts to tune out far more pedestrian and avoidable risks. for example most of us don't plan ahead for losing a job for dealing with a week-long water outage or for surviving the night if our home goes up in smoke. quite often our singular strategy for dealing with such dangers is to hope for the government to bail us out. but no matter if our elected officials prefer to school us with passages from milton friendman or from thomas piketty the hard truth is that no state can provide a robust safety net for all of life's likel"}
{"title": "Nvidia Stock Crash Prediction", "url": "https://entropicthoughts.com/nvidia-stock-crash-prediction", "content": "One of the questions of the 2026 acx prediction contest is whether Nvidia’s\nstock price will close below $100 on any day in 2026. At the time of writing, it\ntrades at $184 and a bit, so going down to $100 would be a near halving of the\nstock value of the highest valued company in the world. It’s an interesting question, and it’s worth spending some time on it. If you just want the answer, my best prediction is that the probability is\naround 10 %. I didn’t expect to get such a high answer, but read on to see how\nwe can find out. When we predicted the Dow Jones index crossing a barrier in 2023 , we treated the\nindex as an unbiased random walk. That was convenient, but we cannot do it with\nthe Nvidia question because of one major difference: the time scale. Over short time spans, the volatility 1 1 Or noise, or variation, or standard\ndeviation. of stock movements dominate their return 2 2 Or signal, or drift,\nor average change. . This happens because noise grows with the square root of\ntime, while signal grows linearly with time. The plot below illustrates an imaginary amazing investment which has a yearly\nlog-return of 0.3, and a yearly volatility of 0.3. 3 3 Readers aware that stonks\ngo up will recognise this as an unrealistic Sharpe ratio of 1.0. The middle\nline follows our best guess for how the investment will grow after each year,\nand the outer curves illustrate our uncertainty around the exact value of it.  Early on, we can see that the uncertainty is much bigger than the height to the\ntrend line. Before a year has passed, the exact result is determined more by\nnoise than by growth. Toward the end, growth has taken over and the noise has a\nsmaller effect. One measure of how much volatility there is compared to expected return is the\nsignal-to-noise ratio. It’s computed as \\[10 \\log_{10}\\left(\\frac{\\mu\\sqrt{t}}{\\sigma}\\right)\\] and for the Dow Jones question, we were looking at a signal-to-noise ratio of\n−8 dB. That is already a little too high to safely assume", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["question", "2026", "acx", "prediction", "contest", "nvidias", "stock", "price", "close", "100", "day", "2026", "time", "write", "trade", "184", "bit", "go", "100", "near", "halving", "stock", "value", "high", "value", "company", "world", "interesting", "question", "worth", "spend", "time", "want", "answer", "good", "prediction", "probability", "10", "not", "expect", "high", "answer", "read", "find", "predict", "dow", "jones", "index", "cross", "barrier", "2023", "treat", "index", "unbiased", "random", "walk", "convenient", "nvidia", "question", "major", "difference", "time", "scale", "short", "time", "span", "volatility", "noise", "variation", "standard", "deviation", "stock", "movement", "dominate", "return", "signal", "drift", "average", "change", "happen", "noise", "grow", "square", "root", "time", "signal", "grow", "linearly", "time", "plot", "illustrate", "imaginary", "amazing", "investment", "yearly", "log", "return", "0.3", "yearly", "volatility", "0.3", "reader", "aware", "stonk", "recognise", "unrealistic", "sharpe", "ratio", "1.0", "middle", "line", "follow", "good", "guess", "investment", "grow", "year", "outer", "curve", "illustrate", "uncertainty", "exact", "value", "early", "uncertainty", "big", "height", "trend", "line", "year", "pass", "exact", "result", "determine", "noise", "growth", "end", "growth", "take", "noise", "small", "effect", "measure", "volatility", "compare", "expect", "return", "signal", "noise", "ratio", "computed", "10", "log_10leftfracmusqrttsigmaright", "dow", "jones", "question", "look", "signal", "noise", "ratio", "db", "little", "high", "safely", "assume"], "num_tokens": 165, "token_loss_pct": 57.47, "normalized_content": "one of the questions of the 2026 acx prediction contest is whether nvidias stock price will close below 100 on any day in 2026. at the time of writing it trades at 184 and a bit so going down to 100 would be a near halving of the stock value of the highest valued company in the world. its an interesting question and its worth spending some time on it. if you just want the answer my best prediction is that the probability is around 10 . i didnt expect to get such a high answer but read on to see how we can find out. when we predicted the dow jones index crossing a barrier in 2023  we treated the index as an unbiased random walk. that was convenient but we cannot do it with the nvidia question because of one major difference the time scale. over short time spans the volatility 1 1 or noise or variation or standard deviation. of stock movements dominate their return 2 2 or signal or drift or average change. . this happens because noise grows with the square root of time while signal grows linearly with time. the plot below illustrates an imaginary amazing investment which has a yearly log-return of 0.3 and a yearly volatility of 0.3. 3 3 readers aware that stonks go up will recognise this as an unrealistic sharpe ratio of 1.0. the middle line follows our best guess for how the investment will grow after each year and the outer curves illustrate our uncertainty around the exact value of it. early on we can see that the uncertainty is much bigger than the height to the trend line. before a year has passed the exact result is determined more by noise than by growth. toward the end growth has taken over and the noise has a smaller effect. one measure of how much volatility there is compared to expected return is the signal-to-noise ratio. its computed as 10 log_10leftfracmusqrttsigmaright and for the dow jones question we were looking at a signal-to-noise ratio of 8 db. that is already a little too high to safely assume"}
{"title": "Fast Concordance: Instant concordance on a corpus of >1,200 books", "url": "https://iafisher.com/concordance/", "content": "Instant concordance on a corpus of\n                over 1,200 public-domain classic books, courtesy of Standard\n                    Ebooks . Read about how it was implemented here .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["instant", "concordance", "corpus", "1200", "public", "domain", "classic", "book", "courtesy", "standard", "ebook", "read", "implement"], "num_tokens": 13, "token_loss_pct": 50.0, "normalized_content": "instant concordance on a corpus of over 1200 public-domain classic books courtesy of standard ebooks . read about how it was implemented here ."}
{"title": "Who owns Rudolph's nose?", "url": "https://creativelawcenter.com/copyright-rudolph-reindeer/", "content": "Who owns Rudolph's nose?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["own", "rudolph", "nose", "score", "author", "date"], "num_tokens": 6, "token_loss_pct": 57.14, "normalized_content": "who owns rudolph's nose. score none. author none. date none"}
{"title": "Show HN: TopicRadar – Track trending topics across HN, GitHub, ArXiv, and more", "url": "https://apify.com/mick-johnson/topic-radar", "content": "Get started Product Back Start here! Get data with ready-made web scrapers for popular websites Browse 14,010 Actors Apify platform Apify Store Pre-built web scraping tools Actors Build and run serverless programs Integrations Connect with apps and services MCP Give your AI access to Actors Anti-blocking Anti-blocking Scrape without getting blocked Proxy Rotate scraper IP addresses Open source Crawlee Web scraping and crawling library Solutions Back MCP server configuration Configure your Apify MCP server with Actors and tools for seamless integration with MCP clients. Start building Web data for Enterprise Startups Universities Nonprofits Use cases Data for generative AI Data for AI agents Lead generation Market research View more → Consulting Apify Professional Services Apify Partners Developers Back Documentation Full reference for the Apify platform Get started Code templates Python, JavaScript, and TypeScript Web scraping academy Courses for beginners and experts Monetize your code Publish your scrapers and get paid Learn API reference CLI SDK MCP Crawlee Earn from your code $596k paid out in December. Many developers earn $3k+ every month. Start earning now Resources Back  Help and support Advice and answers about Apify Actor ideas Get inspired to build Actors Changelog See what’s new on Apify Customer stories Find out how others use Apify Company About Apify Contact us Blog Live events Partners Jobs We're hiring! Join our Discord Talk to scraping experts Pricing Contact sales Pricing Pay per usage mick-johnson/topic-radar Track any topic across the internet and get aggregated, ranked results from multiple sources in one place. Perfect for market research, competitive intelligence, trend monitoring, content creation, and staying updated on any subject. Pricing Pay per usage Rating 5.0 ( 3 ) Developer mick johnson Actor stats 1 Bookmarked 6 Total users 1 Monthly active users a day ago Last modified Categories News Integrations Other Share Track any topic across", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["start", "product", "start", "datum", "ready", "web", "scraper", "popular", "website", "browse", "14010", "actor", "apify", "platform", "apify", "store", "pre", "build", "web", "scrap", "tool", "actor", "build", "run", "serverless", "program", "integration", "connect", "app", "service", "mcp", "ai", "access", "actor", "anti", "block", "anti", "blocking", "scrape", "getting", "block", "proxy", "rotate", "scraper", "ip", "address", "open", "source", "crawlee", "web", "scraping", "crawl", "library", "solution", "mcp", "server", "configuration", "configure", "apify", "mcp", "server", "actor", "tool", "seamless", "integration", "mcp", "client", "start", "build", "web", "datum", "enterprise", "startup", "university", "nonprofit", "use", "case", "datum", "generative", "ai", "datum", "ai", "agent", "lead", "generation", "market", "research", "view", "consulting", "apify", "professional", "service", "apify", "partner", "developer", "documentation", "reference", "apify", "platform", "start", "code", "template", "python", "javascript", "typescript", "web", "scrap", "academy", "course", "beginner", "expert", "monetize", "code", "publish", "scraper", "pay", "learn", "api", "reference", "cli", "sdk", "mcp", "crawlee", "earn", "code", "596k", "pay", "december", "developer", "earn", "3k", "month", "start", "earn", "resource", "help", "support", "advice", "answer", "apify", "actor", "idea", "inspire", "build", "actor", "changelog", "new", "apify", "customer", "story", "find", "use", "apify", "company", "apify", "contact", "blog", "live", "event", "partner", "job", "hire", "join", "discord", "talk", "scrap", "expert", "price", "contact", "sale", "pricing", "pay", "usage", "mick", "johnsontopic", "radar", "track", "topic", "internet", "aggregate", "rank", "result", "multiple", "source", "place", "perfect", "market", "research", "competitive", "intelligence", "trend", "monitor", "content", "creation", "stay", "update", "subject", "pricing", "pay", "usage", "rating", "5.0", "developer", "mick", "johnson", "actor", "stat", "bookmarke", "total", "user", "monthly", "active", "user", "day", "ago", "modify", "category", "news", "integration", "share", "track", "topic"], "num_tokens": 222, "token_loss_pct": 31.06, "normalized_content": "get started product back start here get data with ready-made web scrapers for popular websites browse 14010 actors apify platform apify store pre-built web scraping tools actors build and run serverless programs integrations connect with apps and services mcp give your ai access to actors anti-blocking anti-blocking scrape without getting blocked proxy rotate scraper ip addresses open source crawlee web scraping and crawling library solutions back mcp server configuration configure your apify mcp server with actors and tools for seamless integration with mcp clients. start building web data for enterprise startups universities nonprofits use cases data for generative ai data for ai agents lead generation market research view more  consulting apify professional services apify partners developers back documentation full reference for the apify platform get started code templates python javascript and typescript web scraping academy courses for beginners and experts monetize your code publish your scrapers and get paid learn api reference cli sdk mcp crawlee earn from your code 596k paid out in december. many developers earn 3k every month. start earning now resources back help and support advice and answers about apify actor ideas get inspired to build actors changelog see whats new on apify customer stories find out how others use apify company about apify contact us blog live events partners jobs we're hiring join our discord talk to scraping experts pricing contact sales pricing pay per usage mick-johnsontopic-radar track any topic across the internet and get aggregated ranked results from multiple sources in one place. perfect for market research competitive intelligence trend monitoring content creation and staying updated on any subject. pricing pay per usage rating 5.0  3  developer mick johnson actor stats 1 bookmarked 6 total users 1 monthly active users a day ago last modified categories news integrations other share track any topic across"}
{"title": "Proof of Concept to Test Humanoid Robots", "url": "https://thehumanoid.ai/humanoid-and-siemens-completed-a-proof-of-concept-to-test-humanoidrobots-in-industrial-logistics/", "content": "London, The UK — January 15, 2026 — Humanoid, a UK-based AI and robotics company, and Siemens, a leading technology company, have successfully completed a proof of concept (POC) demonstrating the use of humanoid robots in industrial logistics. Humanoid’s HMND 01 wheeled Alpha robot was deployed in real operations at a Siemens facility, marking a significant step toward the deployment of humanoid robots in industrial settings. This successful POC is the first step in a broader partnership between the two companies to test and validate how humanoid robots can be used in real-world environments. The POC focused on a tote-to-conveyor destacking task within Siemens’ logistics process. In this use case, the robot autonomously picked totes from a storage stack, transported them to a conveyor, and placed them at the designated pickup point for human operators. This sequence was repeated until the stack was fully empty, which demonstrated how humanoid robots can take on repetitive logistics tasks.  The POC was structured in two phases. The first one focused on in-house development and demonstration and has already been completed. During this stage, the Humanoid team built a physical twin to support testing, optimization, and rapid iteration throughout the POC. The second phase involved a two-week on-site deployment at the Siemens Electronics Factory in Erlangen, where partners assessed the robots in a real-world production environment. This joint POC measured both performance and reliability of humanoid robots under autonomous operation. Target metrics were met in full and included a throughput of 60 tote moves per hour, operation with two different tote sizes, continuous autonomous task execution for more than 30 minutes, uptime exceeding 8 hours. The team also evaluated the POC’s success using the following indicators: overall pick and place success rate and autonomous pick and place success rate, both above 90%. Humanoid and Siemens see this POC as a first step toward a", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["london", "uk", "january", "15", "2026", "humanoid", "uk", "base", "ai", "robotic", "company", "siemen", "lead", "technology", "company", "successfully", "complete", "proof", "concept", "poc", "demonstrate", "use", "humanoid", "robot", "industrial", "logistic", "humanoids", "hmnd", "01", "wheel", "alpha", "robot", "deploy", "real", "operation", "siemen", "facility", "mark", "significant", "step", "deployment", "humanoid", "robot", "industrial", "setting", "successful", "poc", "step", "broad", "partnership", "company", "test", "validate", "humanoid", "robot", "real", "world", "environment", "poc", "focus", "tote", "conveyor", "destacking", "task", "siemen", "logistic", "process", "use", "case", "robot", "autonomously", "pick", "tote", "storage", "stack", "transport", "conveyor", "place", "designate", "pickup", "point", "human", "operator", "sequence", "repeat", "stack", "fully", "demonstrate", "humanoid", "robot", "repetitive", "logistic", "task", "poc", "structure", "phase", "focus", "house", "development", "demonstration", "complete", "stage", "humanoid", "team", "build", "physical", "twin", "support", "testing", "optimization", "rapid", "iteration", "poc", "second", "phase", "involve", "week", "site", "deployment", "siemen", "electronic", "factory", "erlangen", "partner", "assess", "robot", "real", "world", "production", "environment", "joint", "poc", "measure", "performance", "reliability", "humanoid", "robot", "autonomous", "operation", "target", "metric", "meet", "include", "throughput", "60", "tote", "move", "hour", "operation", "different", "tote", "size", "continuous", "autonomous", "task", "execution", "30", "minute", "uptime", "exceed", "hour", "team", "evaluate", "poc", "success", "follow", "indicator", "overall", "pick", "place", "success", "rate", "autonomous", "pick", "place", "success", "rate", "90", "humanoid", "siemen", "poc", "step"], "num_tokens": 182, "token_loss_pct": 46.31, "normalized_content": "london the uk  january 15 2026  humanoid a uk-based ai and robotics company and siemens a leading technology company have successfully completed a proof of concept poc demonstrating the use of humanoid robots in industrial logistics. humanoids hmnd 01 wheeled alpha robot was deployed in real operations at a siemens facility marking a significant step toward the deployment of humanoid robots in industrial settings. this successful poc is the first step in a broader partnership between the two companies to test and validate how humanoid robots can be used in real-world environments. the poc focused on a tote-to-conveyor destacking task within siemens logistics process. in this use case the robot autonomously picked totes from a storage stack transported them to a conveyor and placed them at the designated pickup point for human operators. this sequence was repeated until the stack was fully empty which demonstrated how humanoid robots can take on repetitive logistics tasks. the poc was structured in two phases. the first one focused on in-house development and demonstration and has already been completed. during this stage the humanoid team built a physical twin to support testing optimization and rapid iteration throughout the poc. the second phase involved a two-week on-site deployment at the siemens electronics factory in erlangen where partners assessed the robots in a real-world production environment. this joint poc measured both performance and reliability of humanoid robots under autonomous operation. target metrics were met in full and included a throughput of 60 tote moves per hour operation with two different tote sizes continuous autonomous task execution for more than 30 minutes uptime exceeding 8 hours. the team also evaluated the pocs success using the following indicators overall pick and place success rate and autonomous pick and place success rate both above 90. humanoid and siemens see this poc as a first step toward a"}
{"title": "Danish pension fund divesting US Treasuries", "url": "https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/", "content": "Danish pension fund divesting US Treasuries. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["danish", "pension", "fund", "divest", "treasury", "score", "author", "date"], "num_tokens": 8, "token_loss_pct": 46.67, "normalized_content": "danish pension fund divesting us treasuries. score none. author none. date none"}
{"title": "Running Claude Code dangerously (safely)", "url": "https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/", "content": "I’ve been using Claude Code more and more recently. At some point I realized that rather than do something else until it finishes, I would constantly check on it to see if it was asking for yet another permission, which felt like it was missing the point of having an agent do stuff. So I wanted to use Claude Code with the --dangerously-skip-permissions flag. If you haven’t used it, this flag does exactly what it says: it lets Claude Code do whatever it wants without asking permission first. No more “May I install this package?”, “Should I modify this config?”, “Can I delete these files?” It just… does it. Which is great for flow since I don’t have to worry that it stopped doing stuff just to ask a permission question. But also, you know, dangerous. I like my filesystem intact, so the obvious solution is to not run this thing directly on my OS account. First instinct: throw it in a Docker container. Containers are for isolation, right? Except I want Claude to be able to build Docker images. And run containers. And maybe orchestrate some stuff. So now you need Docker-in-Docker, which means --privileged mode, which defeats the entire purpose of sandboxing. That means trading “Claude might mess up my filesystem” for “Claude has root-level access to my container runtime.” Not great. There’s also the nested networking weirdness, volume mounting permissions that make you question your life choices, and the general feeling that you’re fighting the tool instead of using it. I also briefly considered: Then I remembered about a project that I’ve used before Docker became all the rage: Vagrant. If you weren’t around back then, Vagrant gives you proper VM isolation with a reproducible config file. It’s basically infrastructure as code for your local dev environment. You get: I hadn’t used VirtualBox in years since Docker containers covered all requirements until now, so I grabbed the latest version (7.2.4) and got started. First vagrant up and… the VM is pegging my CPU at 100%+", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ve", "claude", "code", "recently", "point", "realize", "finish", "constantly", "check", "ask", "permission", "feel", "like", "miss", "point", "have", "agent", "stuff", "want", "use", "claude", "code", "--dangerously", "skip", "permission", "flag", "not", "flag", "exactly", "say", "let", "claude", "code", "want", "ask", "permission", "install", "package", "modify", "config", "delete", "file", "great", "flow", "not", "worry", "stop", "stuff", "ask", "permission", "question", "know", "dangerous", "like", "filesystem", "intact", "obvious", "solution", "run", "thing", "directly", "os", "account", "instinct", "throw", "docker", "container", "container", "isolation", "right", "want", "claude", "able", "build", "docker", "image", "run", "container", "maybe", "orchestrate", "stuff", "need", "docker", "docker", "mean", "--privileged", "mode", "defeat", "entire", "purpose", "sandboxing", "mean", "trading", "claude", "mess", "filesystem", "claude", "root", "level", "access", "container", "runtime", "great", "nested", "network", "weirdness", "volume", "mount", "permission", "question", "life", "choice", "general", "feeling", "fight", "tool", "instead", "briefly", "consider", "remember", "project", "ve", "docker", "rage", "vagrant", "not", "vagrant", "give", "proper", "vm", "isolation", "reproducible", "config", "file", "basically", "infrastructure", "code", "local", "dev", "environment", "not", "virtualbox", "year", "docker", "container", "cover", "requirement", "grab", "late", "version", "7.2.4", "get", "start", "vagrant", "vm", "peg", "cpu", "100"], "num_tokens": 158, "token_loss_pct": 57.98, "normalized_content": "ive been using claude code more and more recently. at some point i realized that rather than do something else until it finishes i would constantly check on it to see if it was asking for yet another permission which felt like it was missing the point of having an agent do stuff. so i wanted to use claude code with the --dangerously-skip-permissions flag. if you havent used it this flag does exactly what it says it lets claude code do whatever it wants without asking permission first. no more may i install this package should i modify this config can i delete these files it just does it. which is great for flow since i dont have to worry that it stopped doing stuff just to ask a permission question. but also you know dangerous. i like my filesystem intact so the obvious solution is to not run this thing directly on my os account. first instinct throw it in a docker container. containers are for isolation right except i want claude to be able to build docker images. and run containers. and maybe orchestrate some stuff. so now you need docker-in-docker which means --privileged mode which defeats the entire purpose of sandboxing. that means trading claude might mess up my filesystem for claude has root-level access to my container runtime. not great. theres also the nested networking weirdness volume mounting permissions that make you question your life choices and the general feeling that youre fighting the tool instead of using it. i also briefly considered then i remembered about a project that ive used before docker became all the rage vagrant. if you werent around back then vagrant gives you proper vm isolation with a reproducible config file. its basically infrastructure as code for your local dev environment. you get i hadnt used virtualbox in years since docker containers covered all requirements until now so i grabbed the latest version 7.2.4 and got started. first vagrant up and the vm is pegging my cpu at 100"}
{"title": "Ask HN: Revive a mostly dead Discord server", "url": "item?id=46697735", "content": "Ask HN: Revive a mostly dead Discord server. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "revive", "dead", "discord", "server", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 47.06, "normalized_content": "ask hn revive a mostly dead discord server. score none. author none. date none"}
{"title": "Show HN: Generate animated solar system timelapse videos for any date range", "url": "https://github.com/simondorfman/solar_system_live/", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Solar System Live is an interactive orrery for the Web that lets you view the solar system in a variety of ways for any date between 4713 B.C. and A.D. 8000. An ephemeris can be displayed for any location on Earth and, given orbital elements in the form published in the IAU Circulars and the Jet Propulsion Laboratory, the orbit and position of ast There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . This repository is a fork of Fourmilab's Solar System Live with added tooling to generate animated videos of the solar system over time. Perfect for creating unique birthday videos! Generate time-lapse animations showing how the solar system evolved during someone's lifetime, complete with customizable text overlays showing dates, ages, and planetary orbital statistics. Watch as planets orbit while tracking how many times Earth (or your friend) was lapped by Mercury and Venus, or how many times they lapped the outer planets. Note: This repository is macOS-focused (setup scripts and instructions assume macOS/Homebrew), but you can probably adapt it to work on other Unix-like systems (Linux, BSD, etc.) with appropriate modifications to paths and package managers. Here's a sample video generated using Einstein's birth and death dates (1879-03-14 to 1955-04-18):  (Click the animated GIF above to view the full video) Stage a local runnable tree: From the repo root: This will create ./local/ with: Run Lighttpd: Verify it's working: Open http://127.0.0.1:8080/cgi-bin/Solar in your browser. The page should render and the generated image should load. Once you have Lighttpd running locally, you can generate animated videos using: Required argument: Optional arguments: Examples: The script will: This repository is a fork of Fourmilab's Solar System Live , originally created by John Walker . S", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "solar", "system", "live", "interactive", "orrery", "web", "let", "view", "solar", "system", "variety", "way", "date", "4713", "b.c", "a.d", "8000", "ephemeris", "display", "location", "earth", "give", "orbital", "element", "form", "publish", "iau", "circular", "jet", "propulsion", "laboratory", "orbit", "position", "ast", "error", "load", "reload", "page", "error", "load", "reload", "page", "repository", "fork", "fourmilab", "solar", "system", "live", "add", "tooling", "generate", "animate", "video", "solar", "system", "time", "perfect", "create", "unique", "birthday", "video", "generate", "time", "lapse", "animation", "show", "solar", "system", "evolve", "lifetime", "complete", "customizable", "text", "overlay", "show", "date", "age", "planetary", "orbital", "statistic", "watch", "planet", "orbit", "track", "time", "earth", "friend", "lap", "mercury", "venus", "time", "lap", "outer", "planet", "note", "repository", "macos", "focus", "setup", "script", "instruction", "assume", "macoshomebrew", "probably", "adapt", "work", "unix", "like", "system", "linux", "bsd", "etc", "appropriate", "modification", "path", "package", "manager", "sample", "video", "generate", "einstein", "birth", "death", "date", "1879", "03", "14", "1955", "04", "18", "click", "animate", "gif", "view", "video", "stage", "local", "runnable", "tree", "repo", "root", "create", ".local", "run", "lighttpd", "verify", "work", "open", "url", "browser", "page", "render", "generate", "image", "load", "lighttpd", "run", "locally", "generate", "animate", "video", "required", "argument", "optional", "argument", "example", "script", "repository", "fork", "fourmilab", "solar", "system", "live", "originally", "create", "john", "walker"], "num_tokens": 185, "token_loss_pct": 47.29, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . solar system live is an interactive orrery for the web that lets you view the solar system in a variety of ways for any date between 4713 b.c. and a.d. 8000. an ephemeris can be displayed for any location on earth and given orbital elements in the form published in the iau circulars and the jet propulsion laboratory the orbit and position of ast there was an error while loading. please reload this page . there was an error while loading. please reload this page . this repository is a fork of fourmilab's solar system live with added tooling to generate animated videos of the solar system over time. perfect for creating unique birthday videos generate time-lapse animations showing how the solar system evolved during someone's lifetime complete with customizable text overlays showing dates ages and planetary orbital statistics. watch as planets orbit while tracking how many times earth or your friend was lapped by mercury and venus or how many times they lapped the outer planets. note this repository is macos-focused setup scripts and instructions assume macoshomebrew but you can probably adapt it to work on other unix-like systems linux bsd etc. with appropriate modifications to paths and package managers. here's a sample video generated using einstein's birth and death dates 1879-03-14 to 1955-04-18 click the animated gif above to view the full video stage a local runnable tree from the repo root this will create .local with run lighttpd verify it's working open url in your browser. the page should render and the generated image should load. once you have lighttpd running locally you can generate animated videos using required argument optional arguments examples the script will this repository is a fork of fourmilab's solar system live  originally created by john walker . s"}
{"title": "The Zen of Reticulum", "url": "https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 24, "token_loss_pct": 65.71, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "RCS for Business", "url": "https://developers.google.com/business-communications/rcs-business-messaging", "content": "Engage with customers seamlessly on Android and iOS. Allow your customers to interact with\n  your business directly, and enhance the interaction with distinctive branding and rich\n  media. Measure engagement with read receipts and analytics, and build trust with a\n  'Verified' icon. Learn more Ready to become an RCS for Business partner? Partner interest form arrow_forward Explore the RCS Business Messaging APIs and Developer Console, review the terms and security docs, and browse\n  the release notes. Learn more Learn more Learn more Go to Console Manage RCS for Business agents from the Administration Console, and get insights into message activity\n  and billing. Explore the key documentation, or contact us directly for support. Exclusively for registered RCS for Business partners: Access a curated collection of resources to help you champion RCS for Business with your internal teams and brand clients. Tip: To view and download the Marketing kit decks, you need access to your organization's RCS for Business partner account. Please contact a team member with access to share the decks with you. To apply to become an RCS for Business partner, fill out the partner registration interest form.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["engage", "customer", "seamlessly", "android", "io", "allow", "customer", "interact", "business", "directly", "enhance", "interaction", "distinctive", "branding", "rich", "medium", "measure", "engagement", "read", "receipt", "analytic", "build", "trust", "verified", "icon", "learn", "ready", "rcs", "business", "partner", "partner", "interest", "form", "arrow_forward", "explore", "rcs", "business", "message", "apis", "developer", "console", "review", "term", "security", "doc", "browse", "release", "note", "learn", "learn", "learn", "console", "manage", "rcs", "business", "agent", "administration", "console", "insight", "message", "activity", "billing", "explore", "key", "documentation", "contact", "directly", "support", "exclusively", "register", "rcs", "business", "partner", "access", "curate", "collection", "resource", "help", "champion", "rcs", "business", "internal", "team", "brand", "client", "tip", "view", "download", "marketing", "kit", "deck", "need", "access", "organization", "rcs", "business", "partner", "account", "contact", "team", "member", "access", "share", "deck", "apply", "rcs", "business", "partner", "fill", "partner", "registration", "interest", "form"], "num_tokens": 113, "token_loss_pct": 43.5, "normalized_content": "engage with customers seamlessly on android and ios. allow your customers to interact with your business directly and enhance the interaction with distinctive branding and rich media. measure engagement with read receipts and analytics and build trust with a 'verified' icon. learn more ready to become an rcs for business partner partner interest form arrow_forward explore the rcs business messaging apis and developer console review the terms and security docs and browse the release notes. learn more learn more learn more go to console manage rcs for business agents from the administration console and get insights into message activity and billing. explore the key documentation or contact us directly for support. exclusively for registered rcs for business partners access a curated collection of resources to help you champion rcs for business with your internal teams and brand clients. tip to view and download the marketing kit decks you need access to your organization's rcs for business partner account. please contact a team member with access to share the decks with you. to apply to become an rcs for business partner fill out the partner registration interest form."}
{"title": "When \"likers'' go private: Engagement with reputationally risky content on X", "url": "https://arxiv.org/abs/2601.11140", "content": "Help | Advanced Search arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .  arXiv Operational Status", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["help", "advanced", "search", "arxivlab", "framework", "allow", "collaborator", "develop", "share", "new", "arxiv", "feature", "directly", "website", "individual", "organization", "work", "arxivlab", "embrace", "accept", "value", "openness", "community", "excellence", "user", "data", "privacy", "arxiv", "commit", "value", "work", "partner", "adhere", "idea", "project", "add", "value", "arxiv", "community", "learn", "arxivlab", "arxiv", "operational", "status"], "num_tokens": 44, "token_loss_pct": 47.62, "normalized_content": "help  advanced search arxivlabs is a framework that allows collaborators to develop and share new arxiv features directly on our website. both individuals and organizations that work with arxivlabs have embraced and accepted our values of openness community excellence and user data privacy. arxiv is committed to these values and only works with partners that adhere to them. have an idea for a project that will add value for arxiv's community learn more about arxivlabs . arxiv operational status"}
{"title": "LG UltraFine Evo 6K 32-inch Monitor Review", "url": "https://www.wired.com/review/lg-ultrafine-evo-6k-32-inch-monitor/", "content": "7 /10 More pixels is never a bad thing, right? That's at least part of the reasoning behind the existence of the LG UltraFine Evo 6K. This 32-inch monitor aims to put even more pixels in front of content creators and professionals. Beyond that, it has an attention-grabbing design and off-the-charts image quality. It's one of the best monitors you can buy for content creators, despite some of the unfortunate trade-offs it comes with. The 32-inch LG UltraFine Evo 6K is a very pretty monitor. I wouldn't blame you for mistaking this as an Apple product, given the focus on clean lines, simple shapes, and designerly aesthetic. The extra-wide stand means that the base itself isn’t overly large. Like the Apple Studio Display , the flat base provides more usable desk space rather than occupying it. The stand itself has a unique design, too. It resembles the styling Apple uses on the iMac and Studio Display, but it has a textured pattern on the back. It’s gorgeous, though you probably won’t spend a lot of time looking at the back of the monitor unless your desk is in the middle of the room or in command position (if you know, you know). I also like that the back of the cabinet is flat, giving it a sleek look that the rounded backs of typical monitors can’t achieve. Because it uses conventional backlighting, though, it’s not as thin as OLED displays like some of Samsung’s Odyssey gaming monitors . The UltraFine 6K also has some impressively thin bezels, too, adding to the ultra-modern aesthetic. While they’re not “virtually borderless” as LG states, they’re smaller than the bezels on most monitors I’ve tested. One of my favorite aspects of the UltraFine Evo 6K is the speakers. The pair of included speakers on this might be the best I've heard on a monitor. They are extremely loud and clear. There's even a decent amount of bass in there, to the point where you won't need a pair of computer speakers . LG UltraFine Evo 6K Monitor Rating: 7/10 One thing I don’t love is the port pl", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["10", "pixel", "bad", "thing", "right", "reasoning", "existence", "lg", "ultrafine", "evo", "6k", "32", "inch", "monitor", "aim", "pixel", "content", "creator", "professional", "attention", "grab", "design", "chart", "image", "quality", "good", "monitor", "buy", "content", "creator", "despite", "unfortunate", "trade", "off", "come", "32", "inch", "lg", "ultrafine", "evo", "6k", "pretty", "monitor", "blame", "mistake", "apple", "product", "give", "focus", "clean", "line", "simple", "shape", "designerly", "aesthetic", "extra", "wide", "stand", "mean", "base", "not", "overly", "large", "like", "apple", "studio", "display", "flat", "base", "provide", "usable", "desk", "space", "occupy", "stand", "unique", "design", "resemble", "styling", "apple", "use", "imac", "studio", "display", "texture", "pattern", "gorgeous", "probably", "will", "not", "spend", "lot", "time", "look", "monitor", "desk", "middle", "room", "command", "position", "know", "know", "like", "cabinet", "flat", "give", "sleek", "look", "rounded", "back", "typical", "monitor", "not", "achieve", "use", "conventional", "backlighte", "thin", "oled", "display", "like", "samsung", "odyssey", "gaming", "monitor", "ultrafine", "6k", "impressively", "thin", "bezel", "add", "ultra", "modern", "aesthetic", "virtually", "borderless", "lg", "state", "small", "bezel", "monitor", "ve", "test", "favorite", "aspect", "ultrafine", "evo", "6k", "speaker", "pair", "include", "speaker", "good", "hear", "monitor", "extremely", "loud", "clear", "decent", "bass", "point", "will", "need", "pair", "computer", "speaker", "lg", "ultrafine", "evo", "6k", "monitor", "rat", "710", "thing", "not", "love", "port", "pl"], "num_tokens": 178, "token_loss_pct": 55.5, "normalized_content": "7 10 more pixels is never a bad thing right that's at least part of the reasoning behind the existence of the lg ultrafine evo 6k. this 32-inch monitor aims to put even more pixels in front of content creators and professionals. beyond that it has an attention-grabbing design and off-the-charts image quality. it's one of the best monitors you can buy for content creators despite some of the unfortunate trade-offs it comes with. the 32-inch lg ultrafine evo 6k is a very pretty monitor. i wouldn't blame you for mistaking this as an apple product given the focus on clean lines simple shapes and designerly aesthetic. the extra-wide stand means that the base itself isnt overly large. like the apple studio display  the flat base provides more usable desk space rather than occupying it. the stand itself has a unique design too. it resembles the styling apple uses on the imac and studio display but it has a textured pattern on the back. its gorgeous though you probably wont spend a lot of time looking at the back of the monitor unless your desk is in the middle of the room or in command position if you know you know. i also like that the back of the cabinet is flat giving it a sleek look that the rounded backs of typical monitors cant achieve. because it uses conventional backlighting though its not as thin as oled displays like some of samsungs odyssey gaming monitors . the ultrafine 6k also has some impressively thin bezels too adding to the ultra-modern aesthetic. while theyre not virtually borderless as lg states theyre smaller than the bezels on most monitors ive tested. one of my favorite aspects of the ultrafine evo 6k is the speakers. the pair of included speakers on this might be the best i've heard on a monitor. they are extremely loud and clear. there's even a decent amount of bass in there to the point where you won't need a pair of computer speakers . lg ultrafine evo 6k monitor rating 710 one thing i dont love is the port pl"}
{"title": "IP over Avian Carriers with Quality of Service (1999)", "url": "https://www.rfc-editor.org/rfc/rfc2549.html", "content": "IP over Avian Carriers with Quality of Service (1999). Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ip", "avian", "carrier", "quality", "service", "1999", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 50.0, "normalized_content": "ip over avian carriers with quality of service 1999. score none. author none. date none"}
{"title": "Level S4 solar radiation event", "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026", "content": "Space Weather Prediction Center National Oceanic and Atmospheric Administration G4 Levels were first reached at 2:38pm EST (1938 UTC) on 19 January, 2026 upon CME shock arrival. CME passage is expected to continue through the evening with G4 levels remaining possible. National Oceanic and Atmospheric Administration National Weather Service National Centers for Environmental Prediction Space Weather Prediction Center 325 Broadway, Boulder CO 80305 Space Weather Prediction Center 325 Broadway, Boulder CO 80305 Disclaimer Privacy Policy About NOAA's National Weather Service Careers in Weather", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["space", "weather", "prediction", "center", "national", "oceanic", "atmospheric", "administration", "g4", "level", "reach", "238pm", "est", "1938", "utc", "19", "january", "2026", "cme", "shock", "arrival", "cme", "passage", "expect", "continue", "evening", "g4", "level", "remain", "possible", "national", "oceanic", "atmospheric", "administration", "national", "weather", "service", "national", "center", "environmental", "prediction", "space", "weather", "prediction", "center", "325", "broadway", "boulder", "co", "80305", "space", "weather", "prediction", "center", "325", "broadway", "boulder", "co", "80305", "disclaimer", "privacy", "policy", "noaa", "national", "weather", "service", "career", "weather"], "num_tokens": 68, "token_loss_pct": 20.93, "normalized_content": "space weather prediction center national oceanic and atmospheric administration g4 levels were first reached at 238pm est 1938 utc on 19 january 2026 upon cme shock arrival. cme passage is expected to continue through the evening with g4 levels remaining possible. national oceanic and atmospheric administration national weather service national centers for environmental prediction space weather prediction center 325 broadway boulder co 80305 space weather prediction center 325 broadway boulder co 80305 disclaimer privacy policy about noaa's national weather service careers in weather"}
{"title": "Scaling long-running autonomous coding", "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/", "content": "Scaling long-running autonomous coding . Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens. They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not. In my predictions for 2026 the other day I said that by 2029: I think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier. I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach: To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub . But how well did they do? Their initial announcement a couple of days ago was met with unsurprising skepticism , especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo. It looks like they addressed that within the past 24 hours. The latest README includes build instructions which I followed on macOS like this: This got me a working browser window! Here are screenshots I took of google.com and my own website:   Honestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but th", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["scale", "long", "run", "autonomous", "coding", "wilson", "lin", "cursor", "experiment", "far", "push", "large", "fleet", "autonomous", "cod", "agent", "post", "describe", "learn", "run", "hundred", "concurrent", "agent", "single", "project", "coordinate", "work", "watch", "write", "million", "line", "code", "trillion", "token", "end", "run", "planner", "sub", "planner", "create", "task", "have", "worker", "execute", "task", "similar", "claude", "code", "use", "sub", "agent", "cycle", "end", "judge", "agent", "decide", "project", "complete", "prediction", "2026", "day", "say", "2029", "think", "somebody", "build", "web", "browser", "ai", "assistance", "will", "not", "surprising", "roll", "new", "web", "browser", "complicated", "software", "project", "imagine", "cheat", "code", "conformance", "suite", "exist", "test", "ll", "easy", "year", "cursor", "choose", "build", "web", "browser", "scratch", "test", "case", "agent", "swarm", "approach", "test", "system", "point", "ambitious", "goal", "build", "web", "browser", "scratch", "agent", "run", "close", "week", "write", "million", "line", "code", "1000", "file", "explore", "source", "code", "github", "initial", "announcement", "couple", "day", "ago", "meet", "unsurprise", "skepticism", "especially", "apparent", "github", "action", "ci", "fail", "build", "instruction", "repo", "look", "like", "address", "past", "24", "hour", "late", "readme", "include", "build", "instruction", "follow", "macos", "like", "get", "work", "browser", "window", "screenshot", "take", "google.com", "website", "honestly", "impressive", "tell", "wrap", "exist", "rendering", "engine", "obvious", "rendering", "glitch", "th"], "num_tokens": 174, "token_loss_pct": 52.59, "normalized_content": "scaling long-running autonomous coding . wilson lin at cursor has been doing some experiments to see how far you can push a large fleet of autonomous coding agents this post describes what we've learned from running hundreds of concurrent agents on a single project coordinating their work and watching them write over a million lines of code and trillions of tokens. they ended up running planners and sub-planners to create tasks then having workers execute on those tasks - similar to how claude code uses sub-agents. each cycle ended with a judge agent deciding if the project was completed or not. in my predictions for 2026 the other day i said that by 2029 i think somebody will have built a full web browser mostly using ai assistance and it wont even be surprising. rolling a new web browser is one of the most complicated software projects i can imagine... the cheat code is the conformance suites. if there are existing tests that itll get so much easier. i may have been off by three years because cursor chose building a web browser from scratch as their test case for their agent swarm approach to test this system we pointed it at an ambitious goal building a web browser from scratch. the agents ran for close to a week writing over 1 million lines of code across 1000 files. you can explore the source code on github . but how well did they do their initial announcement a couple of days ago was met with unsurprising skepticism  especially when it became apparent that their github actions ci was failing and there were no build instructions in the repo. it looks like they addressed that within the past 24 hours. the latest readme includes build instructions which i followed on macos like this this got me a working browser window here are screenshots i took of google.com and my own website honestly those are very impressive you can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches but th"}
{"title": "The microstructure of wealth transfer in prediction markets", "url": "https://www.jbecker.dev/research/prediction-market-microstructure", "content": "Slot machines on the Las Vegas Strip return about 93 cents on the dollar. This is widely considered some of the worst odds in gambling. Yet on Kalshi, a CFTC-regulated prediction market, traders have wagered vast sums on longshot contracts with historical returns as low as 43 cents on the dollar. Thousands of participants are voluntarily accepting expected values far lower than a casino slot machine to bet on their convictions. The efficient market hypothesis suggests that asset prices should perfectly aggregate all available information. Prediction markets theoretically provide the purest test of this theory. Unlike equities, there is no ambiguity about intrinsic value. A contract either pays $1 or it does not. A price of 5 cents should imply exactly a 5% probability. We analyzed 72.1 million trades covering $18.26 billion in volume to test this efficiency. Our findings suggest that collective accuracy relies less on rational actors than on a mechanism for harvesting error. We document a systematic wealth transfer where impulsive Takers pay a structural premium for affirmative \"YES\" outcomes while Makers capture an \"Optimism Tax\" simply by selling into this biased flow. The effect is strongest in high-engagement categories like Sports and Entertainment, while low-engagement categories like Finance approach perfect efficiency. This paper makes three contributions. First, it confirms the presence of the longshot bias on Kalshi and quantifies its magnitude across price levels. Second, it decomposes returns by market role, revealing a persistent wealth transfer from takers to makers driven by asymmetric order flow. Third, it identifies a YES/NO asymmetry where takers disproportionately favor affirmative bets at longshot prices, exacerbating their losses. Prediction markets are exchanges where participants trade binary contracts on real-world outcomes. These contracts settle at either $1 or $0, with prices ranging from 1 to 99 cents serving as probability proxies. Unli", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["slot", "machine", "las", "vegas", "strip", "return", "93", "cent", "dollar", "widely", "consider", "bad", "odd", "gambling", "kalshi", "cftc", "regulate", "prediction", "market", "trader", "wager", "vast", "sum", "longshot", "contract", "historical", "return", "low", "43", "cent", "dollar", "thousand", "participant", "voluntarily", "accept", "expect", "value", "far", "low", "casino", "slot", "machine", "bet", "conviction", "efficient", "market", "hypothesis", "suggest", "asset", "price", "perfectly", "aggregate", "available", "information", "prediction", "market", "theoretically", "provide", "pure", "test", "theory", "unlike", "equity", "ambiguity", "intrinsic", "value", "contract", "pay", "price", "cent", "imply", "exactly", "probability", "analyze", "72.1", "million", "trade", "cover", "18.26", "billion", "volume", "test", "efficiency", "finding", "suggest", "collective", "accuracy", "rely", "rational", "actor", "mechanism", "harvesting", "error", "document", "systematic", "wealth", "transfer", "impulsive", "taker", "pay", "structural", "premium", "affirmative", "yes", "outcome", "maker", "capture", "optimism", "tax", "simply", "sell", "biased", "flow", "effect", "strong", "high", "engagement", "category", "like", "sport", "entertainment", "low", "engagement", "category", "like", "finance", "approach", "perfect", "efficiency", "paper", "make", "contribution", "confirm", "presence", "longshot", "bias", "kalshi", "quantifie", "magnitude", "price", "level", "second", "decompose", "return", "market", "role", "reveal", "persistent", "wealth", "transfer", "taker", "maker", "drive", "asymmetric", "order", "flow", "identify", "yesno", "asymmetry", "taker", "disproportionately", "favor", "affirmative", "bet", "longshot", "price", "exacerbate", "loss", "prediction", "market", "exchange", "participant", "trade", "binary", "contract", "real", "world", "outcome", "contract", "settle", "price", "range", "99", "cent", "serve", "probability", "proxy", "unli"], "num_tokens": 188, "token_loss_pct": 42.51, "normalized_content": "slot machines on the las vegas strip return about 93 cents on the dollar. this is widely considered some of the worst odds in gambling. yet on kalshi a cftc-regulated prediction market traders have wagered vast sums on longshot contracts with historical returns as low as 43 cents on the dollar. thousands of participants are voluntarily accepting expected values far lower than a casino slot machine to bet on their convictions. the efficient market hypothesis suggests that asset prices should perfectly aggregate all available information. prediction markets theoretically provide the purest test of this theory. unlike equities there is no ambiguity about intrinsic value. a contract either pays 1 or it does not. a price of 5 cents should imply exactly a 5 probability. we analyzed 72.1 million trades covering 18.26 billion in volume to test this efficiency. our findings suggest that collective accuracy relies less on rational actors than on a mechanism for harvesting error. we document a systematic wealth transfer where impulsive takers pay a structural premium for affirmative yes outcomes while makers capture an optimism tax simply by selling into this biased flow. the effect is strongest in high-engagement categories like sports and entertainment while low-engagement categories like finance approach perfect efficiency. this paper makes three contributions. first it confirms the presence of the longshot bias on kalshi and quantifies its magnitude across price levels. second it decomposes returns by market role revealing a persistent wealth transfer from takers to makers driven by asymmetric order flow. third it identifies a yesno asymmetry where takers disproportionately favor affirmative bets at longshot prices exacerbating their losses. prediction markets are exchanges where participants trade binary contracts on real-world outcomes. these contracts settle at either 1 or 0 with prices ranging from 1 to 99 cents serving as probability proxies. unli"}
{"title": "Reliable Signals of Honest Intent", "url": "https://zanlib.dev/blog/reliable-signals-of-honest-intent/", "content": "Imagine you are working on a significant software update. The software is used by a few thousand professionals, and the release you are working on will fix a few significant issues and greatly improve the quality of work of these people. The update costs money, so it cannot be an automated thing: you need to convince the users of this software to decide that they should update. How would you do it? Would you implement a pop-up window advertising the new release and push it through the automated update channel? Would you write an email? Would you prompt AI for an email? A story like this was related in Rory Sutherland’s book Alchemy , and concerned the release of the then-new Windows NT 32-bit server operating system. Microsoft had to figure out how to convince its existing user-base of system administrators to make the switch. But they didn’t ask developers for advice, they hired an advertising agency. And the advertising agency had a different idea. We produced an elaborate box containing a variety of bits and pieces including a free mouse-mat and a pen, inside gratuitously expensive packaging. 1 Marginnote alchemy 1 Sutherland, R. Alchemy: The Surprising Power of Ideas That Don’t Make Sense . W. H. Allen 2020; p. 177 ↩ Why go to such great lengths to advertise a software update? Well, because this story is a subtle case of obliviousness that developers are often guilty of. We tend to get blindsided by the subject-object split, and think that we only need to convey the objective fact while leaving the subjective interpretation of the value of that fact up to the reader. But it is perhaps not shocking that the subjective perception of value can be influenced—this is, after all, what persuasion is for. It’s actually very natural and expected for it to be influenced, because we are constantly being bombarded by various stimuli. The stimuli fight for our attention, and we have developed a system of complex intuitions to select which stimuli are worth our attention and", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["imagine", "work", "significant", "software", "update", "software", "thousand", "professional", "release", "work", "fix", "significant", "issue", "greatly", "improve", "quality", "work", "people", "update", "cost", "money", "automate", "thing", "need", "convince", "user", "software", "decide", "update", "implement", "pop", "window", "advertising", "new", "release", "push", "automate", "update", "channel", "write", "email", "prompt", "ai", "email", "story", "like", "relate", "rory", "sutherland", "book", "alchemy", "concern", "release", "new", "window", "not", "32", "bit", "server", "operating", "system", "microsoft", "figure", "convince", "exist", "user", "base", "system", "administrator", "switch", "not", "ask", "developer", "advice", "hire", "advertising", "agency", "advertising", "agency", "different", "idea", "produce", "elaborate", "box", "contain", "variety", "bit", "piece", "include", "free", "mouse", "mat", "pen", "inside", "gratuitously", "expensive", "packaging", "marginnote", "alchemy", "sutherland", "r.", "alchemy", "surprising", "power", "idea", "not", "sense", "w.", "h.", "allen", "2020", "p.", "177", "great", "length", "advertise", "software", "update", "story", "subtle", "case", "obliviousness", "developer", "guilty", "tend", "blindside", "subject", "object", "split", "think", "need", "convey", "objective", "fact", "leave", "subjective", "interpretation", "value", "fact", "reader", "shock", "subjective", "perception", "value", "influencedthis", "persuasion", "actually", "natural", "expect", "influence", "constantly", "bombard", "stimulus", "stimulus", "fight", "attention", "develop", "system", "complex", "intuition", "select", "stimulus", "worth", "attention"], "num_tokens": 164, "token_loss_pct": 55.31, "normalized_content": "imagine you are working on a significant software update. the software is used by a few thousand professionals and the release you are working on will fix a few significant issues and greatly improve the quality of work of these people. the update costs money so it cannot be an automated thing you need to convince the users of this software to decide that they should update. how would you do it would you implement a pop-up window advertising the new release and push it through the automated update channel would you write an email would you prompt ai for an email a story like this was related in rory sutherlands book alchemy  and concerned the release of the then-new windows nt 32-bit server operating system. microsoft had to figure out how to convince its existing user-base of system administrators to make the switch. but they didnt ask developers for advice they hired an advertising agency. and the advertising agency had a different idea. we produced an elaborate box containing a variety of bits and pieces including a free mouse-mat and a pen inside gratuitously expensive packaging. 1 marginnote alchemy 1 sutherland r. alchemy the surprising power of ideas that dont make sense . w. h. allen 2020 p. 177  why go to such great lengths to advertise a software update well because this story is a subtle case of obliviousness that developers are often guilty of. we tend to get blindsided by the subject-object split and think that we only need to convey the objective fact while leaving the subjective interpretation of the value of that fact up to the reader. but it is perhaps not shocking that the subjective perception of value can be influencedthis is after all what persuasion is for. its actually very natural and expected for it to be influenced because we are constantly being bombarded by various stimuli. the stimuli fight for our attention and we have developed a system of complex intuitions to select which stimuli are worth our attention and"}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The other day I was asked to update the visual design of radio buttons in a web\napp at work. I figured it couldn't be that complicated. It's just a radio button\nright? Boom! Done. Radio buttons are a built-in HTML element. They've been around for\n30 years. The browser makes it easy. Time for a coffee. I dug into our codebase and realized we were using two React components from Shadcn to power our radio buttons: <RadioGroup> and <RadioGroupItem> . For those unfamiliar with Shadcn, it's a UI framework that provides a bunch of\nprebuilt UI components for use in your websites. Unlike traditional UI\nframeworks like Bootstrap, you don't import it with a script tag or npm install . Instead you run a command that copies the components into your\ncodebase. Here's the code that was exported from Shadcn into our project: Woof... 3 imports and 45 lines of code. And it's importing a third party icon\nlibrary just to render a circle. (Who needs CSS border-radius or the SVG <circle> element when you can add a third party dependency instead?) All of the styling is done by the 30 different Tailwind classes in the markup. I\nshould probably just tweak those to fix the styling issues. But now I'm distracted, annoyed, and curious. Where's the actual <input> ?\nWhat's the point of all this? Let's dig a little deeper. The Shadcn components import components from another library called Radix. For\nthose unfamiliar with Radix, it's a UI framework that provides a bunch of\nprebuilt UI components... Wait a second! Isn't that what I just said about Shadcn? What gives? Why do we\nneed both? Let's see what the Radix docs say: Radix Primitives is a low-level UI component library with a focus on\naccessibility, customization and developer experience. You can use these\ncomponents either as the base layer of your design system, or adopt them\nincrementally. So Radix provides unstyled components, and then Shadcn adds styles on top of\nthat. How does Radix work? You can see for yourself on GitHub: https://githu", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["day", "ask", "update", "visual", "design", "radio", "button", "web", "app", "work", "figure", "complicated", "radio", "button", "right", "boom", "radio", "button", "build", "html", "element", "30", "year", "browser", "make", "easy", "time", "coffee", "dig", "codebase", "realize", "react", "component", "shadcn", "power", "radio", "button", "unfamiliar", "shadcn", "ui", "framework", "provide", "bunch", "prebuilt", "ui", "component", "use", "website", "unlike", "traditional", "ui", "framework", "like", "bootstrap", "import", "script", "tag", "npm", "install", "instead", "run", "command", "copy", "component", "codebase", "code", "export", "shadcn", "project", "woof", "import", "45", "line", "code", "import", "party", "icon", "library", "render", "circle", "need", "css", "border", "radius", "svg", "element", "add", "party", "dependency", "instead", "styling", "30", "different", "tailwind", "class", "markup", "probably", "tweak", "fix", "styling", "issue", "distract", "annoyed", "curious", "actual", "point", "let", "dig", "little", "deep", "shadcn", "component", "import", "component", "library", "call", "radix", "unfamiliar", "radix", "ui", "framework", "provide", "bunch", "prebuilt", "ui", "component", "wait", "second", "say", "shadcn", "give", "need", "let", "radix", "docs", "radix", "primitive", "low", "level", "ui", "component", "library", "focus", "accessibility", "customization", "developer", "experience", "use", "component", "base", "layer", "design", "system", "adopt", "incrementally", "radix", "provide", "unstyled", "component", "shadcn", "add", "style", "radix", "work", "github", "url"], "num_tokens": 166, "token_loss_pct": 56.54, "normalized_content": "the other day i was asked to update the visual design of radio buttons in a web app at work. i figured it couldn't be that complicated. it's just a radio button right boom done. radio buttons are a built-in html element. they've been around for 30 years. the browser makes it easy. time for a coffee. i dug into our codebase and realized we were using two react components from shadcn to power our radio buttons and . for those unfamiliar with shadcn it's a ui framework that provides a bunch of prebuilt ui components for use in your websites. unlike traditional ui frameworks like bootstrap you don't import it with a script tag or npm install . instead you run a command that copies the components into your codebase. here's the code that was exported from shadcn into our project woof... 3 imports and 45 lines of code. and it's importing a third party icon library just to render a circle. who needs css border-radius or the svg element when you can add a third party dependency instead all of the styling is done by the 30 different tailwind classes in the markup. i should probably just tweak those to fix the styling issues. but now i'm distracted annoyed and curious. where's the actual  what's the point of all this let's dig a little deeper. the shadcn components import components from another library called radix. for those unfamiliar with radix it's a ui framework that provides a bunch of prebuilt ui components... wait a second isn't that what i just said about shadcn what gives why do we need both let's see what the radix docs say radix primitives is a low-level ui component library with a focus on accessibility customization and developer experience. you can use these components either as the base layer of your design system or adopt them incrementally. so radix provides unstyled components and then shadcn adds styles on top of that. how does radix work you can see for yourself on github url"}
{"title": "RAM shortage chaos expands to GPUs, high-capacity SSDs, and even hard drives", "url": "https://arstechnica.com/gadgets/2026/01/ram-shortage-chaos-expands-to-gpus-high-capacity-ssds-and-even-hard-drives/", "content": "GPU makers may prioritize more profitable models; large SSDs are harder to find. Big Tech’s AI-fueled memory shortage is set to be the PC industry’s defining story for 2026 and beyond. Standalone, direct-to-consumer RAM kits were some of the first products to feel the bite, with prices spiking by 300 or 400 percent by the end of 2025 ; prices for SSDs had also increased noticeably, albeit more modestly. The rest of 2026 is going to be all about where, how, and to what extent those price spikes flow downstream into computers , phones, and other components that use RAM and NAND chips—areas where the existing supply of products and longer-term supply contracts negotiated by big companies have helped keep prices from surging too noticeably so far. This week, we’re seeing signs that the RAM crunch is starting to affect the GPU market—Asus made some waves when it inadvertently announced that it was discontinuing its GeForce RTX 5070 Ti . Though the company has since tried to walk this announcement back, if you’re a GPU manufacturer, there’s a strong argument for either discontinuing this model or de-prioritizing it in favor of other GPUs. The 5070 Ti uses 16GB of GDDR7, plus a partially disabled version of Nvidia’s GB203 GPU silicon. This is the same chip and the same amount of RAM used in the higher-end RTX 5080—the thinking goes, why continue to build a graphics card with an MSRP of $749 when the same basic parts could go to a card with a $999 MSRP instead ? Whether Asus or any other company is canceling production or not, you can see why GPU makers would be tempted by the argument: Street prices for the RTX 5070 Ti models start in the $1,050 to $1,100 range on Newegg right now, where RTX 5080 cards start in the $1,500 to $1,600 range. Though 5080 models may need more robust boards, heatsinks, and other components than a 5070 Ti, if you’re just trying to maximize the profit-per-GPU you can get for the same amount of RAM, it makes sense to shift allocation to the more ex", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["gpu", "maker", "prioritize", "profitable", "model", "large", "ssds", "hard", "find", "big", "tech", "ai", "fueled", "memory", "shortage", "set", "pc", "industry", "define", "story", "2026", "standalone", "direct", "consumer", "ram", "kit", "product", "feel", "bite", "price", "spike", "300", "400", "percent", "end", "2025", "price", "ssds", "increase", "noticeably", "albeit", "modestly", "rest", "2026", "go", "extent", "price", "spike", "flow", "downstream", "computer", "phone", "component", "use", "ram", "nand", "chipsarea", "exist", "supply", "product", "long", "term", "supply", "contract", "negotiate", "big", "company", "help", "price", "surge", "noticeably", "far", "week", "see", "sign", "ram", "crunch", "start", "affect", "gpu", "marketasus", "wave", "inadvertently", "announce", "discontinue", "geforce", "rtx", "5070", "ti", "company", "try", "walk", "announcement", "gpu", "manufacturer", "strong", "argument", "discontinue", "model", "de", "prioritize", "favor", "gpu", "5070", "ti", "use", "16", "gb", "gddr7", "plus", "partially", "disabled", "version", "nvidias", "gb203", "gpu", "silicon", "chip", "ram", "high", "end", "rtx", "5080the", "thinking", "go", "continue", "build", "graphic", "card", "msrp", "749", "basic", "part", "card", "999", "msrp", "instead", "asus", "company", "cancel", "production", "gpu", "maker", "tempt", "argument", "street", "price", "rtx", "5070", "ti", "model", "start", "1050", "1100", "range", "newegg", "right", "rtx", "5080", "card", "start", "1500", "1600", "range", "5080", "model", "need", "robust", "board", "heatsink", "component", "5070", "ti", "try", "maximize", "profit", "gpu", "ram", "make", "sense", "shift", "allocation", "ex"], "num_tokens": 183, "token_loss_pct": 51.84, "normalized_content": "gpu makers may prioritize more profitable models large ssds are harder to find. big techs ai-fueled memory shortage is set to be the pc industrys defining story for 2026 and beyond. standalone direct-to-consumer ram kits were some of the first products to feel the bite with prices spiking by 300 or 400 percent by the end of 2025  prices for ssds had also increased noticeably albeit more modestly. the rest of 2026 is going to be all about where how and to what extent those price spikes flow downstream into computers  phones and other components that use ram and nand chipsareas where the existing supply of products and longer-term supply contracts negotiated by big companies have helped keep prices from surging too noticeably so far. this week were seeing signs that the ram crunch is starting to affect the gpu marketasus made some waves when it inadvertently announced that it was discontinuing its geforce rtx 5070 ti . though the company has since tried to walk this announcement back if youre a gpu manufacturer theres a strong argument for either discontinuing this model or de-prioritizing it in favor of other gpus. the 5070 ti uses 16gb of gddr7 plus a partially disabled version of nvidias gb203 gpu silicon. this is the same chip and the same amount of ram used in the higher-end rtx 5080the thinking goes why continue to build a graphics card with an msrp of 749 when the same basic parts could go to a card with a 999 msrp instead  whether asus or any other company is canceling production or not you can see why gpu makers would be tempted by the argument street prices for the rtx 5070 ti models start in the 1050 to 1100 range on newegg right now where rtx 5080 cards start in the 1500 to 1600 range. though 5080 models may need more robust boards heatsinks and other components than a 5070 ti if youre just trying to maximize the profit-per-gpu you can get for the same amount of ram it makes sense to shift allocation to the more ex"}
{"title": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "url": "https://github.com/majcheradam/ocrbase", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . 📄 PDF ->.MD/.JSON Document OCR and structured data extraction API. PaddleOCR + LLM-powered parsing. Real-time WebSocket updates. Type-safe TypeScript SDK with React hooks. Self-hostable. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . Turn PDFs into structured data at scale. Powered by frontier open-weight OCR models with a type-safe TypeScript SDK. See SDK documentation for React hooks and advanced usage. See Self-Hosting Guide for deployment instructions. Requirements: Docker, Bun  MIT - See LICENSE for details. For API access, on-premise deployment, or questions: adammajcher20@gmail.com 📄 PDF ->.MD/.JSON Document OCR and structured data extraction API. PaddleOCR + LLM-powered parsing. Real-time WebSocket updates. Type-safe TypeScript SDK with React hooks. Self-hostable. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "pdf", "-.md.json", "document", "ocr", "structured", "datum", "extraction", "api", "paddleocr", "llm", "power", "parsing", "real", "time", "websocket", "update", "type", "safe", "typescript", "sdk", "react", "hook", "self", "hostable", "error", "load", "reload", "page", "error", "load", "reload", "page", "turn", "pdfs", "structured", "datum", "scale", "power", "frontier", "open", "weight", "ocr", "model", "type", "safe", "typescript", "sdk", "sdk", "documentation", "react", "hook", "advanced", "usage", "self", "host", "guide", "deployment", "instruction", "requirement", "docker", "bun", "mit", "license", "detail", "api", "access", "premise", "deployment", "question", "adammajcher20mention.com", "pdf", "-.md.json", "document", "ocr", "structured", "datum", "extraction", "api", "paddleocr", "llm", "power", "parsing", "real", "time", "websocket", "update", "type", "safe", "typescript", "sdk", "react", "hook", "self", "hostable", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 118, "token_loss_pct": 48.92, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation .  pdf -.md.json document ocr and structured data extraction api. paddleocr  llm-powered parsing. real-time websocket updates. type-safe typescript sdk with react hooks. self-hostable. there was an error while loading. please reload this page . there was an error while loading. please reload this page . turn pdfs into structured data at scale. powered by frontier open-weight ocr models with a type-safe typescript sdk. see sdk documentation for react hooks and advanced usage. see self-hosting guide for deployment instructions. requirements docker bun mit - see license for details. for api access on-premise deployment or questions adammajcher20mention.com  pdf -.md.json document ocr and structured data extraction api. paddleocr  llm-powered parsing. real-time websocket updates. type-safe typescript sdk with react hooks. self-hostable. there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Apple testing new App Store design that blurs the line between ads and results", "url": "https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/", "content": "Apple is testing a new design for App Store search ads on iPhone. Some users on iOS 26.3 are noticing that the blue background around sponsored results is no longer shown, blurring the line between what paid ad results look like and the real search results that follow. This means the only differentiator between organic results and the promoted ad is the presence of the small ‘Ad’ banner next to the app icon. Right now, it appears to be in some kind of A/B test phase. We have asked Apple for clarity on the change, and whether this will roll out more widely in the future. It may be related to the company’s announcement from December that App Store search results will soon start including more than one sponsored result for a given search query. The removal of the blue background will mean all of the ads will appear in the list in a more integrated fashion. Of course, this also has the effect of making it harder for users to quickly distinguish at a glance what is an ad and what isn’t, potentially misleading some users into not realising that the first result is a paid ad placement. While not great for user experience, it probably helps increase click-through rates which ultimately boosts Apple’s revenue in its ads business. FTC: We use income earning auto affiliate links. More. Check out 9to5Mac on YouTube for more Apple news: Benjamin develops iOS apps professionally and covers Apple news and rumors for 9to5Mac. Listen to Benjamin, every week, on the Happy Hour podcast. Check out his personal blog . Message Benjamin over email or Twitter . The easiest way to get into HomeKit and Apple smart home tech. Great for gifts. Inexpensive, fast, wireless charger for iPhone.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["apple", "test", "new", "design", "app", "store", "search", "ad", "iphone", "user", "io", "26.3", "notice", "blue", "background", "sponsor", "result", "long", "show", "blur", "line", "pay", "ad", "result", "look", "like", "real", "search", "result", "follow", "mean", "differentiator", "organic", "result", "promote", "ad", "presence", "small", "ad", "banner", "app", "icon", "right", "appear", "kind", "ab", "test", "phase", "ask", "apple", "clarity", "change", "roll", "widely", "future", "relate", "company", "announcement", "december", "app", "store", "search", "result", "soon", "start", "include", "sponsor", "result", "give", "search", "query", "removal", "blue", "background", "mean", "ad", "appear", "list", "integrated", "fashion", "course", "effect", "make", "hard", "user", "quickly", "distinguish", "glance", "ad", "not", "potentially", "mislead", "user", "realise", "result", "pay", "ad", "placement", "great", "user", "experience", "probably", "help", "increase", "click", "rate", "ultimately", "boost", "apple", "revenue", "ad", "business", "ftc", "use", "income", "earn", "auto", "affiliate", "link", "check", "9to5mac", "youtube", "apple", "news", "benjamin", "develop", "ios", "app", "professionally", "cover", "apple", "news", "rumor", "9to5mac", "listen", "benjamin", "week", "happy", "hour", "podcast", "check", "personal", "blog", "message", "benjamin", "email", "twitter", "easy", "way", "homekit", "apple", "smart", "home", "tech", "great", "gift", "inexpensive", "fast", "wireless", "charger", "iphone"], "num_tokens": 161, "token_loss_pct": 48.73, "normalized_content": "apple is testing a new design for app store search ads on iphone. some users on ios 26.3 are noticing that the blue background around sponsored results is no longer shown blurring the line between what paid ad results look like and the real search results that follow. this means the only differentiator between organic results and the promoted ad is the presence of the small ad banner next to the app icon. right now it appears to be in some kind of ab test phase. we have asked apple for clarity on the change and whether this will roll out more widely in the future. it may be related to the companys announcement from december that app store search results will soon start including more than one sponsored result for a given search query. the removal of the blue background will mean all of the ads will appear in the list in a more integrated fashion. of course this also has the effect of making it harder for users to quickly distinguish at a glance what is an ad and what isnt potentially misleading some users into not realising that the first result is a paid ad placement. while not great for user experience it probably helps increase click-through rates which ultimately boosts apples revenue in its ads business. ftc we use income earning auto affiliate links. more. check out 9to5mac on youtube for more apple news benjamin develops ios apps professionally and covers apple news and rumors for 9to5mac. listen to benjamin every week on the happy hour podcast. check out his personal blog . message benjamin over email or twitter . the easiest way to get into homekit and apple smart home tech. great for gifts. inexpensive fast wireless charger for iphone."}
{"title": "King – man + woman is queen; but why? (2017)", "url": "https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/", "content": "6 Jan 2017 | by Piotr Migdał word2vec is an algorithm that transforms words into vectors, so that words with similar meanings end up laying close to each other. Moreover, it allows us to use vector arithmetics to work with analogies, for example, the famous king - man + woman = queen . I will try to explain how it works, with special emphasis on the meaning of vector differences, at the same time omitting as many technicalities as possible. If you would rather explore than read, here is an interactive exploration by my mentee Julia Bazińska, now a freshman in computer science at the University of Warsaw:  I love letter co-occurrence in the word co-occurrence . Sometimes a seemingly naive technique gives powerful results. It turns out that merely looking at word coincidences, while ignoring all grammar and context, can provide us insight into the meaning of a word.\nConsider this sentence: A small, fluffy roosety climbed a tree. What’s a roosety ? I would say that something like a squirrel since the two words can be easily interchanged. Such reasoning is called the distributional hypothesis and can be summarized as: a word is characterized by the company it keeps - John Rupert Firth If we want to teach it to a computer, the simplest, approximated approach is making it look only at word pairs.\nLet P(a|b) be the conditional probability that given a word b there is a word a within a short distance (let’s say - being spaced by no more than 2 words).\nThen we claim that two words a and b are similar if for every word w .\nIn other words, if we have this equality, no matter if there is a word a or b , all other words occur with the same frequency. Even simple word counts, compared by source, can give interesting results, e.g. in lyrics of metal songs words ( cries , eternity or ashes are popular, while words particularly or approximately are not, well, particularly common), see Heavy Metal and Natural Language Processing .\nSee also Gender Roles with Text Mining and N-grams by", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["jan", "2017", "piotr", "migdał", "word2vec", "algorithm", "transform", "word", "vector", "word", "similar", "meaning", "end", "lay", "close", "allow", "use", "vector", "arithmetic", "work", "analogy", "example", "famous", "king", "man", "woman", "queen", "try", "explain", "work", "special", "emphasis", "meaning", "vector", "difference", "time", "omit", "technicality", "possible", "explore", "read", "interactive", "exploration", "mentee", "julia", "bazińska", "freshman", "computer", "science", "university", "warsaw", "love", "letter", "co", "occurrence", "word", "co", "occurrence", "seemingly", "naive", "technique", "give", "powerful", "result", "turn", "merely", "look", "word", "coincidence", "ignore", "grammar", "context", "provide", "insight", "meaning", "word", "consider", "sentence", "small", "fluffy", "roosety", "climb", "tree", "roosety", "like", "squirrel", "word", "easily", "interchange", "reasoning", "call", "distributional", "hypothesis", "summarize", "word", "characterize", "company", "keep", "john", "rupert", "firth", "want", "teach", "computer", "simplest", "approximate", "approach", "make", "look", "word", "pair", "let", "pab", "conditional", "probability", "give", "word", "word", "short", "distance", "let", "space", "word", "claim", "word", "similar", "word", "word", "equality", "matter", "word", "word", "occur", "frequency", "simple", "word", "count", "compare", "source", "interesting", "result", "e.g.", "lyric", "metal", "song", "word", "cry", "eternity", "ash", "popular", "word", "particularly", "approximately", "particularly", "common", "heavy", "metal", "natural", "language", "processing", "gender", "role", "text", "mining", "gram"], "num_tokens": 165, "token_loss_pct": 55.53, "normalized_content": "6 jan 2017  by piotr migdał word2vec is an algorithm that transforms words into vectors so that words with similar meanings end up laying close to each other. moreover it allows us to use vector arithmetics to work with analogies for example the famous king - man  woman  queen . i will try to explain how it works with special emphasis on the meaning of vector differences at the same time omitting as many technicalities as possible. if you would rather explore than read here is an interactive exploration by my mentee julia bazińska now a freshman in computer science at the university of warsaw i love letter co-occurrence in the word co-occurrence . sometimes a seemingly naive technique gives powerful results. it turns out that merely looking at word coincidences while ignoring all grammar and context can provide us insight into the meaning of a word. consider this sentence a small fluffy roosety climbed a tree. whats a roosety  i would say that something like a squirrel since the two words can be easily interchanged. such reasoning is called the distributional hypothesis and can be summarized as a word is characterized by the company it keeps - john rupert firth if we want to teach it to a computer the simplest approximated approach is making it look only at word pairs. let pab be the conditional probability that given a word b there is a word a within a short distance lets say - being spaced by no more than 2 words. then we claim that two words a and b are similar if for every word w . in other words if we have this equality no matter if there is a word a or b  all other words occur with the same frequency. even simple word counts compared by source can give interesting results e.g. in lyrics of metal songs words  cries  eternity or ashes are popular while words particularly or approximately are not well particularly common see heavy metal and natural language processing . see also gender roles with text mining and n-grams by"}
{"title": "Targeted Bets: An alternative approach to the job hunt", "url": "https://www.seanmuirhead.com/blog/targeted-bets", "content": "The tech job market has been tough, leaving many applicants feeling hopeless. I've seen this first hand in my conversations with dozens of friends and across more than 100 job interviews. Here is my response to these people: you can drastically increase your odds of getting a job by making targeted bets rather than broadly applying and hoping something sticks. A targeted bet begins with focus. Instead of applying broadly, identify 5-10 specific opportunities you genuinely want. In the context of job searching, these are roles where at least one of the following is true: Once the list has been narrowed, your goal is to stand out. Here are a few ways to do that: By narrowing your opportunities, you end up being able to spend more time on each one. Let's assume that a targeted bet increases your chances of getting a job from 1% to 10%. The average number of jobs you'd need to apply to before getting one thus jumps from 100 to just 10! Competitive systems reward effort per attempt, not volume. Targeted bets apply to more than just the job search. I recently scored the first apartment I applied to in a highly-competitive San Francisco neighborhood. I was specific in where and what I was looking for, so when the opportunity came up, I was able to devote lots of time and energy into getting it. I applied just 6 hours after the place came on the market. Seeing that there were lots of people at the tour, I sent a follow up email to the leasing agent explaining how I'd always wanted to live in the neighborhood. If I had been worried about the status of my other applications, I may not have had the time to write that follow up email and secure my apartment. The glory in making targeted bets is that you get to spend more time on the things that you really care about. I would advise against mass-applying to those entry-level jobs you don't really care about and instead start getting in contact with people at your dream job.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["tech", "job", "market", "tough", "leave", "applicant", "feel", "hopeless", "see", "hand", "conversation", "dozen", "friend", "100", "job", "interview", "response", "people", "drastically", "increase", "odd", "get", "job", "make", "targeted", "bet", "broadly", "apply", "hop", "stick", "target", "bet", "begin", "focus", "instead", "apply", "broadly", "identify", "10", "specific", "opportunity", "genuinely", "want", "context", "job", "search", "role", "following", "true", "list", "narrow", "goal", "stand", "way", "narrow", "opportunity", "end", "able", "spend", "time", "let", "assume", "target", "bet", "increase", "chance", "get", "job", "10", "average", "number", "job", "need", "apply", "get", "jump", "100", "10", "competitive", "system", "reward", "effort", "attempt", "volume", "target", "bet", "apply", "job", "search", "recently", "score", "apartment", "apply", "highly", "competitive", "san", "francisco", "neighborhood", "specific", "look", "opportunity", "come", "able", "devote", "lot", "time", "energy", "get", "apply", "hour", "place", "come", "market", "see", "lot", "people", "tour", "send", "follow", "email", "lease", "agent", "explain", "want", "live", "neighborhood", "worried", "status", "application", "time", "write", "follow", "email", "secure", "apartment", "glory", "make", "targeted", "bet", "spend", "time", "thing", "care", "advise", "mass", "apply", "entry", "level", "job", "care", "instead", "start", "get", "contact", "people", "dream", "job"], "num_tokens": 157, "token_loss_pct": 58.68, "normalized_content": "the tech job market has been tough leaving many applicants feeling hopeless. i've seen this first hand in my conversations with dozens of friends and across more than 100 job interviews. here is my response to these people you can drastically increase your odds of getting a job by making targeted bets rather than broadly applying and hoping something sticks. a targeted bet begins with focus. instead of applying broadly identify 5-10 specific opportunities you genuinely want. in the context of job searching these are roles where at least one of the following is true once the list has been narrowed your goal is to stand out. here are a few ways to do that by narrowing your opportunities you end up being able to spend more time on each one. let's assume that a targeted bet increases your chances of getting a job from 1 to 10. the average number of jobs you'd need to apply to before getting one thus jumps from 100 to just 10 competitive systems reward effort per attempt not volume. targeted bets apply to more than just the job search. i recently scored the first apartment i applied to in a highly-competitive san francisco neighborhood. i was specific in where and what i was looking for so when the opportunity came up i was able to devote lots of time and energy into getting it. i applied just 6 hours after the place came on the market. seeing that there were lots of people at the tour i sent a follow up email to the leasing agent explaining how i'd always wanted to live in the neighborhood. if i had been worried about the status of my other applications i may not have had the time to write that follow up email and secure my apartment. the glory in making targeted bets is that you get to spend more time on the things that you really care about. i would advise against mass-applying to those entry-level jobs you don't really care about and instead start getting in contact with people at your dream job."}
{"title": "The Alignment Game (2023)", "url": "https://dmvaldman.github.io/alignment-game/", "content": "TLDR; I made a game to align people and priorities in a Google Sheet At work as an “executive” I found myself often focused on issues of “alignment,” especially among the other execs. It just turns out as an organization grows, it operates on fractured sets of implicit assumptions. I found there is often little disagreement on what the problems are, but plenty of disagreement on which were more important. People then carry these differences into decision making without revealing their working assumptions, cascading tradeoffs are made and efforts diverge. There was an incredible sense of clarity when everyone could agree on what’s most important in unison, and I wanted to get there. I started by doing the exercise of stack ranking priorities. Sometimes this would just be finger to the wind thinking about issues, sometimes this would mean months of work to assess impact rigorously. I would challenge others in the company to make their own stack rankings. We’d then discuss the differences and try to converge on a shared ordering. This was an incredibly fruitful exercise that led to great conversations. With more than two people though, as with an exec team, there was a need for more process. It turns out there’s a whole branch of mathematics called voting theory all about how to get a plurality of people to agree on a single thing. The concepts of run-off elections , “I cut, you choose” division algorithms, and how medical schools select students through ranked preferences are all facets of voting theory. In my situation, we had a half dozen stack ranked lists of priorities and we wanted to align people on a single ordering. Turns out, there is no algorithm that always works! You can always find yourself in a situation where more than half of people want A over B, some other half want B over C, and some other half want C over A, so a majority are upset with any outcome. Each ranking algorithm makes certain tradeoffs. The Kemeny-Young method is a ranking algorithm that", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["tldr", "game", "align", "people", "priority", "google", "sheet", "work", "executive", "find", "focus", "issue", "alignment", "especially", "exec", "turn", "organization", "grow", "operate", "fractured", "set", "implicit", "assumption", "find", "little", "disagreement", "problem", "plenty", "disagreement", "important", "people", "carry", "difference", "decision", "making", "reveal", "work", "assumption", "cascade", "tradeoff", "effort", "diverge", "incredible", "sense", "clarity", "agree", "important", "unison", "want", "start", "exercise", "stack", "rank", "priority", "finger", "wind", "think", "issue", "mean", "month", "work", "assess", "impact", "rigorously", "challenge", "company", "stack", "ranking", "discuss", "difference", "try", "converge", "share", "ordering", "incredibly", "fruitful", "exercise", "lead", "great", "conversation", "people", "exec", "team", "need", "process", "turn", "branch", "mathematics", "call", "voting", "theory", "plurality", "people", "agree", "single", "thing", "concept", "run", "election", "cut", "choose", "division", "algorithm", "medical", "school", "select", "student", "rank", "preference", "facet", "voting", "theory", "situation", "half", "dozen", "stack", "rank", "list", "priority", "want", "align", "people", "single", "ordering", "turn", "algorithm", "work", "find", "situation", "half", "people", "want", "half", "want", "half", "want", "majority", "upset", "outcome", "rank", "algorithm", "make", "certain", "tradeoff", "kemeny", "young", "method", "rank", "algorithm"], "num_tokens": 149, "token_loss_pct": 59.18, "normalized_content": "tldr i made a game to align people and priorities in a google sheet at work as an executive i found myself often focused on issues of alignment especially among the other execs. it just turns out as an organization grows it operates on fractured sets of implicit assumptions. i found there is often little disagreement on what the problems are but plenty of disagreement on which were more important. people then carry these differences into decision making without revealing their working assumptions cascading tradeoffs are made and efforts diverge. there was an incredible sense of clarity when everyone could agree on whats most important in unison and i wanted to get there. i started by doing the exercise of stack ranking priorities. sometimes this would just be finger to the wind thinking about issues sometimes this would mean months of work to assess impact rigorously. i would challenge others in the company to make their own stack rankings. wed then discuss the differences and try to converge on a shared ordering. this was an incredibly fruitful exercise that led to great conversations. with more than two people though as with an exec team there was a need for more process. it turns out theres a whole branch of mathematics called voting theory all about how to get a plurality of people to agree on a single thing. the concepts of run-off elections  i cut you choose division algorithms and how medical schools select students through ranked preferences are all facets of voting theory. in my situation we had a half dozen stack ranked lists of priorities and we wanted to align people on a single ordering. turns out there is no algorithm that always works you can always find yourself in a situation where more than half of people want a over b some other half want b over c and some other half want c over a so a majority are upset with any outcome. each ranking algorithm makes certain tradeoffs. the kemeny-young method is a ranking algorithm that"}
{"title": "British redcoat's lost memoir reveals realities of life as a disabled veteran", "url": "https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html", "content": "Sign in with Forget Password? Learn more share this! 111 Tweet Share Email January 14, 2026 by Tom Almeroth-Williams, University of Cambridge edited by Stephanie Baum , \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed by Robert Egan   This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tthe following attributes while ensuring the content's credibility: fact-checked trusted source proofread Archival discoveries including a 19th-century autobiography transform our understanding of Shadrach Byfield, an English veteran of the War of 1812 who buried his own amputated arm and designed a custom prosthesis. A recurrent character in TV documentaries, books and museum exhibits in the U.S. and Canada, Byfield has been celebrated as an uncomplaining British soldier. But the new evidence reveals Byfield's tenacious pursuit of veterans' benefits and his struggles with pain, poverty, and the police. \"They came and pushed me about, and spat in my face, hoping that I should strike them, in order if possible, to take away my pension … They reported that I intended to shoot two of the deacons.\" This is how Shadrach Byfield, a 63-year-old disabled war veteran, describes being treated in his local chapel in 1850s Gloucestershire. Implicated in a bitter feud among village Baptists, Byfield would later be accused of slashing the face of an adversary with the iron crook of his wooden arm. In other parts of his rediscovered autobiography, Shadrach lamented the continued impact of his wartime injuries decades later: \"It now pleased the Lord to afflict me with a violent rheumatic pain in my right shoulder, from which the [musket] ball was cut out. I was in this condition for nearly three years: oftentimes I was not able to lift my hand to my head, nor a tea-cup to my mouth.\" Frustrated at an employer's refusal to pay him full wages while working as a one-handed gardener, Byfield insisted, \"I never saw the man that would compete with m", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["sign", "forget", "password", "learn", "share", "111", "tweet", "share", "email", "january", "14", "2026", "tom", "almeroth", "williams", "university", "cambridge", "edit", "stephanie", "baum", "review", "robert", "egan", "article", "review", "accord", "science", "editorial", "process", "policy", "editor", "highlight", "follow", "attribute", "ensure", "content", "credibility", "fact", "check", "trust", "source", "proofread", "archival", "discovery", "include", "19th", "century", "autobiography", "transform", "understanding", "shadrach", "byfield", "english", "veteran", "war", "1812", "bury", "amputated", "arm", "design", "custom", "prosthesis", "recurrent", "character", "tv", "documentary", "book", "museum", "exhibit", "u.s", "canada", "byfield", "celebrate", "uncomplaine", "british", "soldier", "new", "evidence", "reveal", "byfield", "tenacious", "pursuit", "veteran", "benefit", "struggle", "pain", "poverty", "police", "come", "push", "spat", "face", "hop", "strike", "order", "possible", "away", "pension", "report", "intend", "shoot", "deacon", "shadrach", "byfield", "63", "year", "old", "disabled", "war", "veteran", "describe", "treat", "local", "chapel", "1850s", "gloucestershire", "implicate", "bitter", "feud", "village", "baptist", "byfield", "later", "accuse", "slash", "face", "adversary", "iron", "crook", "wooden", "arm", "part", "rediscovered", "autobiography", "shadrach", "lament", "continued", "impact", "wartime", "injury", "decade", "later", "please", "lord", "afflict", "violent", "rheumatic", "pain", "right", "shoulder", "musket", "ball", "cut", "condition", "nearly", "year", "oftentime", "able", "lift", "hand", "head", "tea", "cup", "mouth", "frustrate", "employer", "refusal", "pay", "wage", "work", "handed", "gardener", "byfield", "insist", "see", "man", "compete"], "num_tokens": 177, "token_loss_pct": 48.99, "normalized_content": "sign in with forget password learn more share this 111 tweet share email january 14 2026 by tom almeroth-williams university of cambridge edited by stephanie baum  reviewed by robert egan this article has been reviewed according to science x's editorial process and policies . editors have highlighted the following attributes while ensuring the content's credibility fact-checked trusted source proofread archival discoveries including a 19th-century autobiography transform our understanding of shadrach byfield an english veteran of the war of 1812 who buried his own amputated arm and designed a custom prosthesis. a recurrent character in tv documentaries books and museum exhibits in the u.s. and canada byfield has been celebrated as an uncomplaining british soldier. but the new evidence reveals byfield's tenacious pursuit of veterans' benefits and his struggles with pain poverty and the police. they came and pushed me about and spat in my face hoping that i should strike them in order if possible to take away my pension  they reported that i intended to shoot two of the deacons. this is how shadrach byfield a 63-year-old disabled war veteran describes being treated in his local chapel in 1850s gloucestershire. implicated in a bitter feud among village baptists byfield would later be accused of slashing the face of an adversary with the iron crook of his wooden arm. in other parts of his rediscovered autobiography shadrach lamented the continued impact of his wartime injuries decades later it now pleased the lord to afflict me with a violent rheumatic pain in my right shoulder from which the musket ball was cut out. i was in this condition for nearly three years oftentimes i was not able to lift my hand to my head nor a tea-cup to my mouth. frustrated at an employer's refusal to pay him full wages while working as a one-handed gardener byfield insisted i never saw the man that would compete with m"}
{"title": "I set all 376 Vim options and I'm still a fool", "url": "https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/", "content": "I set all of Vim’s configuration options. I still feel far from mastery. I first saw someone use Vim during an internship in 2012. I had been coding for many years and I fancied myself pretty good at shortcuts, but I was quickly humbled. I watched in awe as experienced users zipped around the code. A single keystroke could move the cursor halfway across the file to exactly the right spot. Code was ripped apart and reshaped like putty. “ Wow ,” I thought to myself, and probably said out loud. I vowed to master this editor but I was slow. When I wasn’t accidentally opening some unknown menu, I was taking an uneconomical path through the code. I pressed j twenty times instead of running 20j , or manually deleted code inside parenthesis instead of running di( . Sometimes I’d open another text editor to give my mind a break from all the key bindings! Fast-forward to 2025. After tons of practice, I felt much more capable. Code did feel more like putty. I was working closer to the speed of thought. I could get code where I wanted much more quickly. 13 years of practice paid off! But Vim still felt clumsy. I was still accidentally opening menus I didn’t recognize. I would do silly things like converting the whole file to lowercase, or trigger some scary error message. “Surely I shouldn’t be making these mistakes,” I thought. What could be done to finally master this editor? That desire for expertise led me on a quest to set all of Vim’s options . I would make an informed decision about all 376 of Vim’s settings and drop them in my .vimrc . In other words, I wanted to 100% Vim. Surely, setting every Vim option would make me the fluent expert I wanted to be…right? I pored over every single Vim option and made a decision. What did the option do, and what did I want it to be set to? My goal was to be thorough; leave no stone unturned. I only set the option after I understood it. Eventually, after countless hours, I had done it. I had set every single Vim option. This exercise t", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["set", "vim", "configuration", "option", "feel", "far", "mastery", "see", "use", "vim", "internship", "2012", "cod", "year", "fancy", "pretty", "good", "shortcut", "quickly", "humble", "watch", "awe", "experienced", "user", "zip", "code", "single", "keystroke", "cursor", "halfway", "file", "exactly", "right", "spot", "code", "rip", "apart", "reshape", "like", "putty", "wow", "think", "probably", "say", "loud", "vow", "master", "editor", "slow", "not", "accidentally", "open", "unknown", "menu", "take", "uneconomical", "path", "code", "press", "time", "instead", "run", "20j", "manually", "delete", "code", "inside", "parenthesis", "instead", "run", "di", "open", "text", "editor", "mind", "break", "key", "binding", "fast", "forward", "2025", "ton", "practice", "feel", "capable", "code", "feel", "like", "putty", "work", "close", "speed", "thought", "code", "want", "quickly", "13", "year", "practice", "pay", "vim", "feel", "clumsy", "accidentally", "open", "menus", "not", "recognize", "silly", "thing", "like", "convert", "file", "lowercase", "trigger", "scary", "error", "message", "surely", "not", "make", "mistake", "think", "finally", "master", "editor", "desire", "expertise", "lead", "quest", "set", "vim", "option", "informed", "decision", "376", "vim", "setting", "drop", ".vimrc", "word", "want", "100", "vim", "surely", "set", "vim", "option", "fluent", "expert", "want", "beright", "pore", "single", "vim", "option", "decision", "option", "want", "set", "goal", "thorough", "leave", "stone", "unturned", "set", "option", "understand", "eventually", "countless", "hour", "set", "single", "vim", "option", "exercise"], "num_tokens": 176, "token_loss_pct": 56.11, "normalized_content": "i set all of vims configuration options. i still feel far from mastery. i first saw someone use vim during an internship in 2012. i had been coding for many years and i fancied myself pretty good at shortcuts but i was quickly humbled. i watched in awe as experienced users zipped around the code. a single keystroke could move the cursor halfway across the file to exactly the right spot. code was ripped apart and reshaped like putty.  wow  i thought to myself and probably said out loud. i vowed to master this editor but i was slow. when i wasnt accidentally opening some unknown menu i was taking an uneconomical path through the code. i pressed j twenty times instead of running 20j  or manually deleted code inside parenthesis instead of running di . sometimes id open another text editor to give my mind a break from all the key bindings fast-forward to 2025. after tons of practice i felt much more capable. code did feel more like putty. i was working closer to the speed of thought. i could get code where i wanted much more quickly. 13 years of practice paid off but vim still felt clumsy. i was still accidentally opening menus i didnt recognize. i would do silly things like converting the whole file to lowercase or trigger some scary error message. surely i shouldnt be making these mistakes i thought. what could be done to finally master this editor that desire for expertise led me on a quest to set all of vims options . i would make an informed decision about all 376 of vims settings and drop them in my .vimrc . in other words i wanted to 100 vim. surely setting every vim option would make me the fluent expert i wanted to beright i pored over every single vim option and made a decision. what did the option do and what did i want it to be set to my goal was to be thorough leave no stone unturned. i only set the option after i understood it. eventually after countless hours i had done it. i had set every single vim option. this exercise t"}
{"title": "88x31 badge for gen-AI free, 100% human-made works", "url": "https://aspiz.uk/100percenthuman/", "content": "Use this badge for websites, software, music, art, ... that were created\n            completely by humans with no help from generative artificial intelligence.\n            Use this badge to communicate that you have not used even a little bit of\n            generative AI in your work. What this badge is NOT : This badge is not meant to promote the dis-use of generative AI. I'm not an\n            activist. You may use this badge regardless of your views on AI or the use\n            of generative AI in Art. This work is marked CC0 1.0 Made with ❤️ by aspizu , circa 2026", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["use", "badge", "website", "software", "music", "art", "create", "completely", "human", "help", "generative", "artificial", "intelligence", "use", "badge", "communicate", "little", "bit", "generative", "ai", "work", "badge", "badge", "mean", "promote", "dis", "use", "generative", "ai", "activist", "use", "badge", "regardless", "view", "ai", "use", "generative", "ai", "art", "work", "mark", "cc0", "1.0", "aspizu", "circa", "2026"], "num_tokens": 46, "token_loss_pct": 56.19, "normalized_content": "use this badge for websites software music art ... that were created completely by humans with no help from generative artificial intelligence. use this badge to communicate that you have not used even a little bit of generative ai in your work. what this badge is not  this badge is not meant to promote the dis-use of generative ai. i'm not an activist. you may use this badge regardless of your views on ai or the use of generative ai in art. this work is marked cc0 1.0 made with  by aspizu  circa 2026"}
{"title": "Linux kernel framework for PCIe device emulation, in userspace", "url": "https://github.com/cakehonolulu/pciem", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . A Linux framework to enable userspace-defined \"Virtual\" PCIe card shims to enable in-host PCIe card driver development. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . https://cakehonolulu.github.io/docs/pciem/  PCIem is a framework that creates virtual PCIe devices in the Linux kernel by leveraging a few novel techniques to populate synthetic cards as legitimate PCI devices to the host OS. To brief what PCIem is: a framework for developing and testing PCIe device drivers without requiring actual hardware. The card is programmed entirely in QEMU, who does all the userspace initialization and command handling from the real driver running in the host. Can run software-rendered DOOM (Submits finished frames with DMA to the card which QEMU displays) and also simple OpenGL 1.X games (On the screenshots, tyr-glquake and xash3d; thanks to a custom OpenGL state machine implemented entirely in QEMU that software-renders the command lists and updates the internal state accordingly).    Dual MIT/GPLv2 (pciem_framework.c and protopciem_driver.c) MIT (Rest) A Linux framework to enable userspace-defined \"Virtual\" PCIe card shims to enable in-host PCIe card driver development. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "linux", "framework", "enable", "userspace", "define", "virtual", "pcie", "card", "shim", "enable", "host", "pcie", "card", "driver", "development", "error", "load", "reload", "page", "error", "load", "reload", "page", "url", "pciem", "framework", "create", "virtual", "pcie", "device", "linux", "kernel", "leverage", "novel", "technique", "populate", "synthetic", "card", "legitimate", "pci", "device", "host", "os", "brief", "pciem", "framework", "develop", "test", "pcie", "device", "driver", "require", "actual", "hardware", "card", "program", "entirely", "qemu", "userspace", "initialization", "command", "handling", "real", "driver", "run", "host", "run", "software", "render", "doom", "submit", "finish", "frame", "dma", "card", "qemu", "display", "simple", "opengl", "1.x", "game", "screenshot", "tyr", "glquake", "xash3d", "thank", "custom", "opengl", "state", "machine", "implement", "entirely", "qemu", "software", "render", "command", "list", "update", "internal", "state", "accordingly", "dual", "mitgplv2", "pciem_framework.c", "protopciem_driver.c", "mit", "rest", "linux", "framework", "enable", "userspace", "define", "virtual", "pcie", "card", "shim", "enable", "host", "pcie", "card", "driver", "development", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 146, "token_loss_pct": 47.48, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . a linux framework to enable userspace-defined virtual pcie card shims to enable in-host pcie card driver development. there was an error while loading. please reload this page . there was an error while loading. please reload this page . url pciem is a framework that creates virtual pcie devices in the linux kernel by leveraging a few novel techniques to populate synthetic cards as legitimate pci devices to the host os. to brief what pciem is a framework for developing and testing pcie device drivers without requiring actual hardware. the card is programmed entirely in qemu who does all the userspace initialization and command handling from the real driver running in the host. can run software-rendered doom submits finished frames with dma to the card which qemu displays and also simple opengl 1.x games on the screenshots tyr-glquake and xash3d thanks to a custom opengl state machine implemented entirely in qemu that software-renders the command lists and updates the internal state accordingly. dual mitgplv2 pciem_framework.c and protopciem_driver.c mit rest a linux framework to enable userspace-defined virtual pcie card shims to enable in-host pcie card driver development. there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Avoiding fan traps in database design and system diagrams", "url": "https://www.ilograph.com/blog/posts/avoid-fan-traps-in-system-diagrams/", "content": "A fan trap in data modeling occurs when multiple 1:N relations are joined on the â1â side, resulting in information loss. To the uninitiated, this is easiest understood by example: imagine a university with many colleges , each with many departments , which in turn have many professors . If modeled incorrectly, where the professors are given 1:N relations with colleges instead of departments , the result is a fan trap: These relations arenât wrong per se, but the mapping of professors to departments is lost. The resulting data modeling diagram looks like two hand fans joined at the narrow end, hence the name. A similar problem can occur in system diagramming. When diagramming relations between resources in a system, information can similarly be lost when relations flow through an intermediary resource. In this article, weâll look at a couple of examples of this problem and three potential fixes. Fan traps are common problems when diagramming event-driven architectures. The defining characteristic of such architectures is that resources communicate via events . Events are typically routed through an event broker, which temporarily stores them until they are ready for consumption. This architecture has the added benefit of decoupling the resources, since they no longer communicate directly. When diagrammed, a (highly simplified) event-driven system might look like so: The similarity to fan traps in data modeling should be evident at a glance. The relationships between the message-producing resources (on the left) and message-consuming resources (on the right) are lost because they collapse at the center of the âfanâ (the event broker). The diagram implies each producer communicates with all of the consumers, even though this may not be the case: The same problem can emerge when diagramming communication paths in a network: In the (again highly simplified) networking diagram above, node-to-node communications across the network fan out and back in through f", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["fan", "trap", "datum", "modeling", "occur", "multiple", "1n", "relation", "join", "â1â", "result", "information", "loss", "uninitiated", "easiest", "understand", "example", "imagine", "university", "college", "department", "turn", "professor", "model", "incorrectly", "professor", "give", "1n", "relation", "college", "instead", "department", "result", "fan", "trap", "relation", "arenât", "wrong", "se", "mapping", "professor", "department", "lose", "result", "datum", "model", "diagram", "look", "like", "hand", "fan", "join", "narrow", "end", "similar", "problem", "occur", "system", "diagram", "diagramming", "relation", "resource", "system", "information", "similarly", "lose", "relation", "flow", "intermediary", "resource", "article", "weâll", "look", "couple", "example", "problem", "potential", "fix", "fan", "trap", "common", "problem", "diagram", "event", "drive", "architecture", "define", "characteristic", "architecture", "resource", "communicate", "event", "event", "typically", "rout", "event", "broker", "temporarily", "store", "ready", "consumption", "architecture", "add", "benefit", "decouple", "resource", "long", "communicate", "directly", "diagram", "highly", "simplify", "event", "drive", "system", "look", "like", "similarity", "fan", "trap", "datum", "modeling", "evident", "glance", "relationship", "message", "produce", "resource", "left", "message", "consume", "resource", "right", "lose", "collapse", "center", "âfanâ", "event", "broker", "diagram", "imply", "producer", "communicate", "consumer", "case", "problem", "emerge", "diagram", "communication", "path", "network", "highly", "simplified", "network", "diagram", "node", "node", "communication", "network", "fan"], "num_tokens": 160, "token_loss_pct": 51.95, "normalized_content": "a fan trap in data modeling occurs when multiple 1n relations are joined on the â1â side resulting in information loss. to the uninitiated this is easiest understood by example imagine a university with many colleges  each with many departments  which in turn have many professors . if modeled incorrectly where the professors are given 1n relations with colleges instead of departments  the result is a fan trap these relations arenât wrong per se but the mapping of professors to departments is lost. the resulting data modeling diagram looks like two hand fans joined at the narrow end hence the name. a similar problem can occur in system diagramming. when diagramming relations between resources in a system information can similarly be lost when relations flow through an intermediary resource. in this article weâll look at a couple of examples of this problem and three potential fixes. fan traps are common problems when diagramming event-driven architectures. the defining characteristic of such architectures is that resources communicate via events . events are typically routed through an event broker which temporarily stores them until they are ready for consumption. this architecture has the added benefit of decoupling the resources since they no longer communicate directly. when diagrammed a highly simplified event-driven system might look like so the similarity to fan traps in data modeling should be evident at a glance. the relationships between the message-producing resources on the left and message-consuming resources on the right are lost because they collapse at the center of the âfanâ the event broker. the diagram implies each producer communicates with all of the consumers even though this may not be the case the same problem can emerge when diagramming communication paths in a network in the again highly simplified networking diagram above node-to-node communications across the network fan out and back in through f"}
{"title": "Wayland – Accessibility Input Protocol", "url": "https://gitlab.freedesktop.org/wayland/wayland-protocols/-/issues/149", "content": "Loading... You are seeing this because the administrator of this website has set up Anubis to protect the server against the scourge of AI companies aggressively scraping websites . This can and does cause downtime for the websites, which makes their resources inaccessible for everyone. Anubis is a compromise. Anubis uses a Proof-of-Work scheme in the vein of Hashcash , a proposed proof-of-work scheme for reducing email spam. The idea is that at individual scales the additional load is ignorable, but at mass scraper levels it adds up and makes scraping much more expensive. Ultimately, this is a hack whose real purpose is to give a \"good enough\" placeholder solution so that more time can be spent on fingerprinting and identifying headless browsers (EG: via how they do font rendering) so that the challenge proof of work page doesn't need to be presented to users that are much more likely to be legitimate. Please note that Anubis requires the use of modern JavaScript features that plugins like JShelter will disable. Please disable JShelter or other such plugins for this domain. Protected by Anubis from Techaro . Made with ❤️ in 🇨🇦. Mascot design by CELPHASE .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["loading", "see", "administrator", "website", "set", "anubis", "protect", "server", "scourge", "ai", "company", "aggressively", "scrap", "website", "cause", "downtime", "website", "make", "resource", "inaccessible", "anubis", "compromise", "anubis", "use", "proof", "work", "scheme", "vein", "hashcash", "propose", "proof", "work", "scheme", "reduce", "email", "spam", "idea", "individual", "scale", "additional", "load", "ignorable", "mass", "scraper", "level", "add", "make", "scrap", "expensive", "ultimately", "hack", "real", "purpose", "good", "placeholder", "solution", "time", "spend", "fingerprinting", "identify", "headless", "browser", "eg", "font", "render", "challenge", "proof", "work", "page", "need", "present", "user", "likely", "legitimate", "note", "anubis", "require", "use", "modern", "javascript", "feature", "plugin", "like", "jshelter", "disable", "disable", "jshelter", "plugin", "domain", "protect", "anubis", "techaro", "mascot", "design", "celphase"], "num_tokens": 95, "token_loss_pct": 55.4, "normalized_content": "loading... you are seeing this because the administrator of this website has set up anubis to protect the server against the scourge of ai companies aggressively scraping websites . this can and does cause downtime for the websites which makes their resources inaccessible for everyone. anubis is a compromise. anubis uses a proof-of-work scheme in the vein of hashcash  a proposed proof-of-work scheme for reducing email spam. the idea is that at individual scales the additional load is ignorable but at mass scraper levels it adds up and makes scraping much more expensive. ultimately this is a hack whose real purpose is to give a good enough placeholder solution so that more time can be spent on fingerprinting and identifying headless browsers eg via how they do font rendering so that the challenge proof of work page doesn't need to be presented to users that are much more likely to be legitimate. please note that anubis requires the use of modern javascript features that plugins like jshelter will disable. please disable jshelter or other such plugins for this domain. protected by anubis from techaro . made with  in . mascot design by celphase ."}
{"title": "Flux 2 Klein pure C inference", "url": "https://github.com/antirez/flux2.c", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Flux 2 image generation model pure C inference There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . This program generates images from text prompts (and optionally from other images) using the FLUX.2-klein-4B model from Black Forest Labs. It can be used as a library as well, and is implemented entirely in C, with zero external dependencies beyond the C standard library. MPS and BLAS acceleration are optional but recommended. I (the human here, Salvatore) wanted to test code generation with a more ambitious task, over the weekend. This is the result. It is my first open source project where I wrote zero lines of code. I believe that inference systems not using the Python stack (which I do not appreciate) are a way to free open models usage and make AI more accessible. There is already a project doing the inference of diffusion models in C / C++ that supports multiple models, and is based on GGML. I wanted to see if, with the assistance of modern AI, I could reproduce this work in a more concise way, from scratch, in a weekend. Looks like it is possible. This code base was written with Claude Code, using the Claude Max plan, the small one of ~80 euros per month. I almost reached the limits but this plan was definitely sufficient for such a large task, which was surprising. In order to simplify the usage of this software, no quantization is used, nor do you need to convert the model. It runs directly with the safetensors model as input, using floats. Even if the code was generated using AI, my help in steering towards the right design, implementation choices, and correctness has been vital during the development. I learned quite a few things about working with non trivial projects and AI. That's it. No Python runtime, no PyTorch, no CUDA toolkit required at inference time.  Gener", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "flux", "image", "generation", "model", "pure", "inference", "error", "load", "reload", "page", "error", "load", "reload", "page", "program", "generate", "image", "text", "prompt", "optionally", "image", "flux.2", "klein-4b", "model", "black", "forest", "lab", "library", "implement", "entirely", "zero", "external", "dependency", "standard", "library", "mp", "blas", "acceleration", "optional", "recommend", "human", "salvatore", "want", "test", "code", "generation", "ambitious", "task", "weekend", "result", "open", "source", "project", "write", "zero", "line", "code", "believe", "inference", "system", "python", "stack", "appreciate", "way", "free", "open", "model", "usage", "ai", "accessible", "project", "inference", "diffusion", "model", "support", "multiple", "model", "base", "ggml", "want", "assistance", "modern", "ai", "reproduce", "work", "concise", "way", "scratch", "weekend", "look", "like", "possible", "code", "base", "write", "claude", "code", "claude", "max", "plan", "small", "80", "euro", "month", "reach", "limit", "plan", "definitely", "sufficient", "large", "task", "surprising", "order", "simplify", "usage", "software", "quantization", "need", "convert", "model", "run", "directly", "safetensor", "model", "input", "float", "code", "generate", "ai", "help", "steering", "right", "design", "implementation", "choice", "correctness", "vital", "development", "learn", "thing", "work", "non", "trivial", "project", "ai", "python", "runtime", "pytorch", "cuda", "toolkit", "require", "inference", "time", "gener"], "num_tokens": 162, "token_loss_pct": 56.57, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . flux 2 image generation model pure c inference there was an error while loading. please reload this page . there was an error while loading. please reload this page . this program generates images from text prompts and optionally from other images using the flux.2-klein-4b model from black forest labs. it can be used as a library as well and is implemented entirely in c with zero external dependencies beyond the c standard library. mps and blas acceleration are optional but recommended. i the human here salvatore wanted to test code generation with a more ambitious task over the weekend. this is the result. it is my first open source project where i wrote zero lines of code. i believe that inference systems not using the python stack which i do not appreciate are a way to free open models usage and make ai more accessible. there is already a project doing the inference of diffusion models in c  c that supports multiple models and is based on ggml. i wanted to see if with the assistance of modern ai i could reproduce this work in a more concise way from scratch in a weekend. looks like it is possible. this code base was written with claude code using the claude max plan the small one of 80 euros per month. i almost reached the limits but this plan was definitely sufficient for such a large task which was surprising. in order to simplify the usage of this software no quantization is used nor do you need to convert the model. it runs directly with the safetensors model as input using floats. even if the code was generated using ai my help in steering towards the right design implementation choices and correctness has been vital during the development. i learned quite a few things about working with non trivial projects and ai. that's it. no python runtime no pytorch no cuda toolkit required at inference time. gener"}
{"title": "Show HN: Aventos – An experiment in cheap AI SEO", "url": "https://www.aventos.dev/", "content": "Show HN: Aventos – An experiment in cheap AI SEO. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["hn", "aventos", "experiment", "cheap", "ai", "seo", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 52.63, "normalized_content": "show hn aventos  an experiment in cheap ai seo. score none. author none. date none"}
{"title": "From Nevada to Kansas by Glider", "url": "https://www.weglide.org/flight/978820", "content": "From Nevada to Kansas by Glider. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nevada", "kansas", "glider", "score", "author", "date"], "num_tokens": 6, "token_loss_pct": 60.0, "normalized_content": "from nevada to kansas by glider. score none. author none. date none"}
{"title": "San Francisco coyote swims to Alcatraz", "url": "https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php", "content": "San Francisco coyote swims to Alcatraz. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["san", "francisco", "coyote", "swim", "alcatraz", "score", "author", "date"], "num_tokens": 8, "token_loss_pct": 46.67, "normalized_content": "san francisco coyote swims to alcatraz. score none. author none. date none"}
{"title": "ASCII characters are not pixels: a deep dive into ASCII rendering", "url": "https://alexharri.com/blog/ascii-rendering", "content": "Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive! One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example: Try opening the “split” view. Notice how well the characters follow the contour of the square. This renderer works well for animated scenes, like the ones above, but we can also use it to render static images: The image of Saturn was generated with ChatGPT . Then, to get better separation between different colored regions, I also implemented a cel shading -like effect to enhance contrast between edges. Try dragging the contrast slider below: The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does. I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters: Source: cognition.ai It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places: This blurriness happens because the ASCII characters are being treated like pixels — their shape is ignored. It’s disappointing to see because ASCII art looks so much better when shape is utilized. I don’t believe I’ve ever seen shape utilized in generated ASCII art, and I think that’s because it’s not really obvious how to consider shape when building an ASCII renderer. I started building my ASCII renderer to prove to myself that it’s possible to utilize shape in ASCII rendering. In this post, I’ll cover the techniques and ideas I used to capture shape and build this ASCII renderer in detail. We’ll start with the basics of image-to-ASCII conversion an", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recently", "ve", "spend", "time", "build", "image", "ascii", "renderer", "result", "try", "drag", "demo", "interactive", "thing", "spend", "lot", "effort", "get", "edge", "look", "sharp", "look", "rotate", "cube", "example", "try", "open", "split", "view", "notice", "character", "follow", "contour", "square", "renderer", "work", "animate", "scene", "like", "one", "use", "render", "static", "image", "image", "saturn", "generate", "chatgpt", "well", "separation", "different", "colored", "region", "implement", "cel", "shade", "-like", "effect", "enhance", "contrast", "edge", "try", "drag", "contrast", "slider", "contrast", "enhancement", "make", "separation", "different", "colored", "region", "far", "clear", "key", "make", "3d", "scene", "look", "good", "focus", "sharp", "edge", "aspect", "ascii", "render", "overlook", "programmatically", "render", "image", "ascii", "consider", "animate", "3d", "scene", "cognition", "landing", "page", "render", "ascii", "character", "source", "cognition.ai", "cool", "effect", "especially", "motion", "look", "blurry", "edge", "character", "follow", "cube", "contour", "poorly", "result", "edge", "look", "blurry", "jag", "place", "blurriness", "happen", "ascii", "character", "treat", "like", "pixel", "shape", "ignore", "disappointing", "ascii", "art", "look", "well", "shape", "utilize", "not", "believe", "ve", "see", "shape", "utilize", "generate", "ascii", "art", "think", "obvious", "consider", "shape", "build", "ascii", "renderer", "start", "build", "ascii", "renderer", "prove", "possible", "utilize", "shape", "ascii", "rendering", "post", "ill", "cover", "technique", "idea", "capture", "shape", "build", "ascii", "renderer", "detail", "start", "basic", "image", "ascii", "conversion"], "num_tokens": 179, "token_loss_pct": 50.42, "normalized_content": "recently ive been spending my time building an image-to-ascii renderer. below is the result  try dragging it around the demo is interactive one thing i spent a lot of effort on is getting edges looking sharp. take a look at this rotating cube example try opening the split view. notice how well the characters follow the contour of the square. this renderer works well for animated scenes like the ones above but we can also use it to render static images the image of saturn was generated with chatgpt . then to get better separation between different colored regions i also implemented a cel shading -like effect to enhance contrast between edges. try dragging the contrast slider below the contrast enhancement makes the separation between different colored regions far clearer. that was key to making the 3d scene above look as good as it does. i put so much focus on sharp edges because theyre an aspect of ascii rendering that is often overlooked when programmatically rendering images as ascii. consider this animated 3d scene from cognitions landing page that is rendered via ascii characters source cognition.ai its a cool effect especially while in motion but take a look at those blurry edges the characters follow the cube contours very poorly and as a result the edges look blurry and jagged in places this blurriness happens because the ascii characters are being treated like pixels  their shape is ignored. its disappointing to see because ascii art looks so much better when shape is utilized. i dont believe ive ever seen shape utilized in generated ascii art and i think thats because its not really obvious how to consider shape when building an ascii renderer. i started building my ascii renderer to prove to myself that its possible to utilize shape in ascii rendering. in this post ill cover the techniques and ideas i used to capture shape and build this ascii renderer in detail. well start with the basics of image-to-ascii conversion an"}
{"title": "X For You Feed Algorithm", "url": "https://github.com/xai-org/x-algorithm", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Algorithm powering the For You feed on X There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . This repository contains the core recommendation system powering the \"For You\" feed on X. It combines in-network content (from accounts you follow) with out-of-network content (discovered through ML-based retrieval) and ranks everything using a Grok-based transformer model. Note: The transformer implementation is ported from the Grok-1 open source release by xAI, adapted for recommendation system use cases. The For You feed algorithm retrieves, ranks, and filters posts from two sources: Both sources are combined and ranked together using Phoenix , a Grok-based transformer model that predicts engagement probabilities for each post. The final score is a weighted combination of these predicted engagements. We have eliminated every single hand-engineered feature and most heuristics from the system. The Grok-based transformer does all the heavy lifting by understanding your engagement history (what you liked, replied to, shared, etc.) and using that to determine what content is relevant to you. Location: home-mixer/ The orchestration layer that assembles the For You feed. It leverages the CandidatePipeline framework with the following stages: The server exposes a gRPC endpoint ( ScoredPostsService ) that returns ranked posts for a given user. Location: thunder/ An in-memory post store and realtime ingestion pipeline that tracks recent posts from all users. It: Thunder enables sub-millisecond lookups for in-network content without hitting an external database. Location: phoenix/ The ML component with two main functions: Finds relevant out-of-network posts: Predicts engagement probabilities for each candidate: See phoenix/README.md for detailed architecture documentation. Location: candidate", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "algorithm", "power", "feed", "error", "load", "reload", "page", "error", "load", "reload", "page", "repository", "contain", "core", "recommendation", "system", "power", "feed", "x.", "combine", "network", "content", "account", "follow", "network", "content", "discover", "ml", "base", "retrieval", "rank", "grok", "base", "transformer", "model", "note", "transformer", "implementation", "port", "grok-1", "open", "source", "release", "xai", "adapt", "recommendation", "system", "use", "case", "feed", "algorithm", "retrieve", "rank", "filter", "post", "source", "source", "combine", "rank", "phoenix", "grok", "base", "transformer", "model", "predict", "engagement", "probability", "post", "final", "score", "weighted", "combination", "predict", "engagement", "eliminate", "single", "hand", "engineer", "feature", "heuristic", "system", "grok", "base", "transformer", "heavy", "lifting", "understand", "engagement", "history", "like", "reply", "shared", "etc", "determine", "content", "relevant", "location", "home", "mixer", "orchestration", "layer", "assemble", "feed", "leverage", "candidatepipeline", "framework", "following", "stage", "server", "expose", "grpc", "endpoint", "scoredpostsservice", "return", "rank", "post", "give", "user", "location", "thunder", "memory", "post", "store", "realtime", "ingestion", "pipeline", "track", "recent", "post", "user", "thunder", "enable", "sub", "millisecond", "lookup", "network", "content", "hit", "external", "database", "location", "phoenix", "ml", "component", "main", "function", "find", "relevant", "network", "post", "predict", "engagement", "probability", "candidate", "phoenixreadme.md", "detailed", "architecture", "documentation", "location", "candidate"], "num_tokens": 168, "token_loss_pct": 49.7, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . algorithm powering the for you feed on x there was an error while loading. please reload this page . there was an error while loading. please reload this page . this repository contains the core recommendation system powering the for you feed on x. it combines in-network content from accounts you follow with out-of-network content discovered through ml-based retrieval and ranks everything using a grok-based transformer model. note the transformer implementation is ported from the grok-1 open source release by xai adapted for recommendation system use cases. the for you feed algorithm retrieves ranks and filters posts from two sources both sources are combined and ranked together using phoenix  a grok-based transformer model that predicts engagement probabilities for each post. the final score is a weighted combination of these predicted engagements. we have eliminated every single hand-engineered feature and most heuristics from the system. the grok-based transformer does all the heavy lifting by understanding your engagement history what you liked replied to shared etc. and using that to determine what content is relevant to you. location home-mixer the orchestration layer that assembles the for you feed. it leverages the candidatepipeline framework with the following stages the server exposes a grpc endpoint  scoredpostsservice  that returns ranked posts for a given user. location thunder an in-memory post store and realtime ingestion pipeline that tracks recent posts from all users. it thunder enables sub-millisecond lookups for in-network content without hitting an external database. location phoenix the ml component with two main functions finds relevant out-of-network posts predicts engagement probabilities for each candidate see phoenixreadme.md for detailed architecture documentation. location candidate"}
{"title": "Show HN: Fence – Sandbox CLI commands with network/filesystem restrictions", "url": "https://github.com/Use-Tusk/fence", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Lightweight, container-free sandbox for running commands with network and filesystem restrictions There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .   Fence wraps commands in a sandbox that blocks network access by default and restricts filesystem operations based on configurable rules. It's most useful for running semi-trusted code (package installs, build scripts, CI jobs, unfamiliar repos) with controlled side effects, and it can also complement AI coding agents as defense-in-depth. You can also think of Fence as a permission manager for your CLI agents. Go install: Build from source: Additional requirements for Linux: Fence reads from ~/.fence.json by default: Use fence --settings ./custom.json to specify a different config. Fence can be used as a Go package or CLI tool. Inspired by Anthropic's sandbox-runtime . Lightweight, container-free sandbox for running commands with network and filesystem restrictions There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "lightweight", "container", "free", "sandbox", "run", "command", "network", "filesystem", "restriction", "error", "load", "reload", "page", "error", "load", "reload", "page", "fence", "wrap", "command", "sandbox", "block", "network", "access", "default", "restrict", "filesystem", "operation", "base", "configurable", "rule", "useful", "run", "semi", "trusted", "code", "package", "install", "build", "script", "ci", "job", "unfamiliar", "repos", "control", "effect", "complement", "ai", "cod", "agent", "defense", "depth", "think", "fence", "permission", "manager", "cli", "agent", "install", "build", "source", "additional", "requirement", "linux", "fence", "read", ".fence.json", "default", "use", "fence", "--settings", ".custom.json", "specify", "different", "config", "fence", "package", "cli", "tool", "inspire", "anthropic", "sandbox", "runtime", "lightweight", "container", "free", "sandbox", "run", "command", "network", "filesystem", "restriction", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 116, "token_loss_pct": 51.26, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . lightweight container-free sandbox for running commands with network and filesystem restrictions there was an error while loading. please reload this page . there was an error while loading. please reload this page . fence wraps commands in a sandbox that blocks network access by default and restricts filesystem operations based on configurable rules. it's most useful for running semi-trusted code package installs build scripts ci jobs unfamiliar repos with controlled side effects and it can also complement ai coding agents as defense-in-depth. you can also think of fence as a permission manager for your cli agents. go install build from source additional requirements for linux fence reads from .fence.json by default use fence --settings .custom.json to specify a different config. fence can be used as a go package or cli tool. inspired by anthropic's sandbox-runtime . lightweight container-free sandbox for running commands with network and filesystem restrictions there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "KISS Launcher – fast launcher for Android", "url": "https://kisslauncher.com/", "content": "< 250 kb Optimized for battery life Search everything that you need Faster than ever KISS Launcher lets Android users simplify their home, clean their screens and access the functions they need as quickly and as simply as possible. Claim back your efficiency! KISS Android Launcher helps users find the most used features. Help | FAQ | Privacy policy Made with love in France by Neamar", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["250", "kb", "optimize", "battery", "life", "search", "need", "fast", "kiss", "launcher", "let", "android", "user", "simplify", "home", "clean", "screen", "access", "function", "need", "quickly", "simply", "possible", "claim", "efficiency", "kiss", "android", "launcher", "help", "user", "find", "feature", "help", "faq", "privacy", "policy", "love", "france", "neamar"], "num_tokens": 39, "token_loss_pct": 41.79, "normalized_content": "250 kb optimized for battery life search everything that you need faster than ever kiss launcher lets android users simplify their home clean their screens and access the functions they need as quickly and as simply as possible. claim back your efficiency kiss android launcher helps users find the most used features. help  faq  privacy policy made with love in france by neamar"}
{"title": "Show HN: On-device browser agent (Qwen) running locally in Chrome", "url": "https://github.com/RunanywhereAI/on-device-browser-agent", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . On-device AI browser automation using WebLLM. No cloud, no API keys, fully private. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . A Chrome extension that uses WebLLM to run AI-powered web automation entirely on-device. No cloud APIs, no API keys, fully private. Clone and install dependencies : Build the extension : Load in Chrome : First run : This watches for changes and rebuilds automatically. The extension uses a two-agent architecture inspired by Nanobrowser: Both agents output structured JSON that is parsed and executed. Default model: Qwen2.5-1.5B-Instruct-q4f16_1-MLC (~1GB) Alternative models (configured in src/shared/constants.ts ): This project is inspired by: MIT License - See LICENSE file for details. On-device AI browser automation using WebLLM. No cloud, no API keys, fully private. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "device", "ai", "browser", "automation", "webllm", "cloud", "api", "key", "fully", "private", "error", "load", "reload", "page", "error", "load", "reload", "page", "chrome", "extension", "use", "webllm", "run", "ai", "power", "web", "automation", "entirely", "device", "cloud", "apis", "api", "key", "fully", "private", "clone", "install", "dependency", "build", "extension", "load", "chrome", "run", "watch", "change", "rebuild", "automatically", "extension", "use", "agent", "architecture", "inspire", "nanobrowser", "agent", "output", "structure", "json", "parse", "execute", "default", "model", "qwen2.5", "1.5b", "instruct", "q4f16_1", "mlc", "gb", "alternative", "model", "configure", "srcsharedconstants.ts", "project", "inspire", "mit", "license", "license", "file", "detail", "device", "ai", "browser", "automation", "webllm", "cloud", "api", "key", "fully", "private", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 112, "token_loss_pct": 52.14, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . on-device ai browser automation using webllm. no cloud no api keys fully private. there was an error while loading. please reload this page . there was an error while loading. please reload this page . a chrome extension that uses webllm to run ai-powered web automation entirely on-device. no cloud apis no api keys fully private. clone and install dependencies  build the extension  load in chrome  first run  this watches for changes and rebuilds automatically. the extension uses a two-agent architecture inspired by nanobrowser both agents output structured json that is parsed and executed. default model qwen2.5-1.5b-instruct-q4f16_1-mlc 1gb alternative models configured in srcsharedconstants.ts  this project is inspired by mit license - see license file for details. on-device ai browser automation using webllm. no cloud no api keys fully private. there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "The Agentic AI Handbook: Production-Ready Patterns", "url": "https://www.nibzard.com/agentic-handbook", "content": "by Nikola BaliÄ TL;DR >> 113 patterns collected from real production systems. From Plan-Then-Execute to Swarm Migration, learn what actually works when building AI agents that ship. << The GitHub repository for âAwesome Agentic Patternsâ had been growing steadily since its launch. But around Christmas, the growth chart went vertical. In just a few days, the repository jumped from relative obscurity to nearly 2,500 stars. The website traffic mirrored this spike. Something had clicked. But the real story wasnât in the metricsâit was in who was talking about AI agents. Linus Torvalds, creator of Linux and Git, wrote about using AI coding agents for âvibe codingâ and programming guitar pedal effects. Think about that for a second. The person who literally invented the version control system that powers modern software development was publicly embracing agents. Tobias LÃ¼tke, CEO of Shopify and already deep into agent-assisted development, declared it his âmost productive time.â This from someone running one of the worldâs largest e-commerce platforms. Perhaps most telling was Armin Ronacher, creator of Flaskâone of the most respected voices in Python. He had been skeptical of coding agents, publicly raising concerns about their limitations. Then, seemingly overnight, his stance shifted. He started promoting agent-assisted workflows, documenting his learnings, and acknowledging that the technology had crossed a threshold. Hereâs what all these stories have in common: the holidays gave people something that everyday life rarely providesâdedicated time. Learning to work effectively with AI agents isnât something you pick up in five minutes between meetings. It requires: During the work year, these activities compete with deadlines, meetings, and the relentless pressure to ship. During the holidays, with meetings suspended and project urgency dialed down, developers finally had the bandwidth to actually learn . This repository, with its 113 patter", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nikola", "baliä", "tldr", "113", "pattern", "collect", "real", "production", "system", "plan", "execute", "swarm", "migration", "learn", "actually", "work", "building", "ai", "agent", "ship", "github", "repository", "âawesome", "agentic", "patternsâ", "grow", "steadily", "launch", "christmas", "growth", "chart", "go", "vertical", "day", "repository", "jump", "relative", "obscurity", "nearly", "2500", "star", "website", "traffic", "mirror", "spike", "click", "real", "story", "wasnât", "metricsâit", "talk", "ai", "agent", "linus", "torvalds", "creator", "linux", "git", "write", "ai", "cod", "agent", "âvibe", "codingâ", "program", "guitar", "pedal", "effect", "think", "second", "person", "literally", "invent", "version", "control", "system", "power", "modern", "software", "development", "publicly", "embrace", "agent", "tobias", "lã¼tke", "ceo", "shopify", "deep", "agent", "assist", "development", "declare", "âmost", "productive", "time.â", "run", "worldâs", "large", "commerce", "platform", "telling", "armin", "ronacher", "creator", "flaskâone", "respected", "voice", "python", "skeptical", "cod", "agent", "publicly", "raise", "concern", "limitation", "seemingly", "overnight", "stance", "shift", "start", "promote", "agent", "assist", "workflow", "document", "learning", "acknowledge", "technology", "cross", "threshold", "hereâs", "story", "common", "holiday", "give", "people", "everyday", "life", "rarely", "providesâdedicate", "time", "learn", "work", "effectively", "ai", "agent", "isnât", "pick", "minute", "meeting", "require", "work", "year", "activity", "compete", "deadline", "meeting", "relentless", "pressure", "ship", "holiday", "meeting", "suspend", "project", "urgency", "dial", "developer", "finally", "bandwidth", "actually", "learn", "repository", "113", "patter"], "num_tokens": 174, "token_loss_pct": 46.3, "normalized_content": "by nikola baliä tldr  113 patterns collected from real production systems. from plan-then-execute to swarm migration learn what actually works when building ai agents that ship.  the github repository for âawesome agentic patternsâ had been growing steadily since its launch. but around christmas the growth chart went vertical. in just a few days the repository jumped from relative obscurity to nearly 2500 stars. the website traffic mirrored this spike. something had clicked. but the real story wasnât in the metricsâit was in who was talking about ai agents. linus torvalds creator of linux and git wrote about using ai coding agents for âvibe codingâ and programming guitar pedal effects. think about that for a second. the person who literally invented the version control system that powers modern software development was publicly embracing agents. tobias lã¼tke ceo of shopify and already deep into agent-assisted development declared it his âmost productive time.â this from someone running one of the worldâs largest e-commerce platforms. perhaps most telling was armin ronacher creator of flaskâone of the most respected voices in python. he had been skeptical of coding agents publicly raising concerns about their limitations. then seemingly overnight his stance shifted. he started promoting agent-assisted workflows documenting his learnings and acknowledging that the technology had crossed a threshold. hereâs what all these stories have in common the holidays gave people something that everyday life rarely providesâdedicated time. learning to work effectively with ai agents isnât something you pick up in five minutes between meetings. it requires during the work year these activities compete with deadlines meetings and the relentless pressure to ship. during the holidays with meetings suspended and project urgency dialed down developers finally had the bandwidth to actually learn . this repository with its 113 patter"}
{"title": "Turbopack: Building faster by building less", "url": "https://nextjs.org/blog/turbopack-incremental-computation", "content": "Monday, January 19th 2026 Edit. Save. Refresh. Wait… Wait… Wait… Compiling code usually means waiting, but Turbopack makes iteration loops fast with caching and incremental computation. Not every modern bundler uses an incremental approach, and that’s with good reason. Incremental computation can introduce significant complexity and opportunities for bugs. Caches require extra tracking and copies of data, adding both CPU and memory overhead. When applied poorly, caching can actually make performance worse. Despite all of this, we took on these challenges because we knew that an incremental architecture would be critical to Turbopack’s success. Turbopack is the new default bundler for Next.js, a framework that is used to build some of the largest web applications in the world . We needed to enable instant builds and a fast as-you-type interactive React Fast Refresh experience, even for the largest and most challenging workloads. Our incremental architecture is core to achieving this. Turbopack’s architecture was built ground-up with caching in mind. Its incremental design is based on over a decade of research. We built on first-hand experience from challenges in implementing caching in webpack and drew inspiration from Salsa (which powers Rust-Analyzer and Ruff ), Parcel , the Rust compiler’s query system , Adapton , and many others. Turbopack achieves a fine-grained cache by automatically tracking how internal functions are called and what values they depend on. When something changes we know how to recompute the results with minimal work. Many build systems include explicit dependency graphs that must be manually populated when evaluating build rules. Explicitly declaring your dependency graph can theoretically give optimal results, but in practice it leaves room for errors. The difficulty of specifying an explicit dependency graph means that usually caching is done at a coarse file-level granularity. This granularity does have some benefits: fewer incremental resu", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["monday", "january", "19th", "2026", "edit", "save", "refresh", "wait", "wait", "wait", "compile", "code", "usually", "mean", "wait", "turbopack", "make", "iteration", "loop", "fast", "cache", "incremental", "computation", "modern", "bundler", "use", "incremental", "approach", "good", "reason", "incremental", "computation", "introduce", "significant", "complexity", "opportunity", "bug", "cache", "require", "extra", "tracking", "copy", "datum", "add", "cpu", "memory", "overhead", "apply", "poorly", "cache", "actually", "performance", "bad", "despite", "take", "challenge", "know", "incremental", "architecture", "critical", "turbopack", "success", "turbopack", "new", "default", "bundler", "next.js", "framework", "build", "large", "web", "application", "world", "need", "enable", "instant", "build", "fast", "type", "interactive", "react", "fast", "refresh", "experience", "large", "challenging", "workload", "incremental", "architecture", "core", "achieve", "turbopack", "architecture", "build", "ground", "cache", "mind", "incremental", "design", "base", "decade", "research", "build", "hand", "experience", "challenge", "implement", "cache", "webpack", "draw", "inspiration", "salsa", "power", "rust", "analyzer", "ruff", "parcel", "rust", "compiler", "query", "system", "adapton", "turbopack", "achieve", "fine", "grain", "cache", "automatically", "track", "internal", "function", "call", "value", "depend", "change", "know", "recompute", "result", "minimal", "work", "build", "system", "include", "explicit", "dependency", "graph", "manually", "populate", "evaluate", "build", "rule", "explicitly", "declare", "dependency", "graph", "theoretically", "optimal", "result", "practice", "leave", "room", "error", "difficulty", "specify", "explicit", "dependency", "graph", "mean", "usually", "cache", "coarse", "file", "level", "granularity", "granularity", "benefit", "few", "incremental", "resu"], "num_tokens": 179, "token_loss_pct": 46.41, "normalized_content": "monday january 19th 2026 edit. save. refresh. wait wait wait compiling code usually means waiting but turbopack makes iteration loops fast with caching and incremental computation. not every modern bundler uses an incremental approach and thats with good reason. incremental computation can introduce significant complexity and opportunities for bugs. caches require extra tracking and copies of data adding both cpu and memory overhead. when applied poorly caching can actually make performance worse. despite all of this we took on these challenges because we knew that an incremental architecture would be critical to turbopacks success. turbopack is the new default bundler for next.js a framework that is used to build some of the largest web applications in the world . we needed to enable instant builds and a fast as-you-type interactive react fast refresh experience even for the largest and most challenging workloads. our incremental architecture is core to achieving this. turbopacks architecture was built ground-up with caching in mind. its incremental design is based on over a decade of research. we built on first-hand experience from challenges in implementing caching in webpack and drew inspiration from salsa which powers rust-analyzer and ruff  parcel  the rust compilers query system  adapton  and many others. turbopack achieves a fine-grained cache by automatically tracking how internal functions are called and what values they depend on. when something changes we know how to recompute the results with minimal work. many build systems include explicit dependency graphs that must be manually populated when evaluating build rules. explicitly declaring your dependency graph can theoretically give optimal results but in practice it leaves room for errors. the difficulty of specifying an explicit dependency graph means that usually caching is done at a coarse file-level granularity. this granularity does have some benefits fewer incremental resu"}
{"title": "Unsealed: Spotify Lawsuit Triggered Anna's Archive Domain Name Suspensions", "url": "https://torrentfreak.com/unsealed-spotify-lawsuit-triggered-annas-archive-domain-name-suspensions/", "content": "Home > Lawsuits > Apps and Sites > Spotify and several major record labels, including UMG, Sony, and Warner, have taken legal action against the unknown operators of Anna's Archive. The action follows the shadow library's announcement that it would release hundreds of terabytes of scraped Spotify data.  Unsealed documents reveal that the court already issued a broad preliminary injunction, ordering hosting companies, Cloudflare, and domain name services, to take action. Anna’s Archive is generally known as a meta-search engine for shadow libraries, helping users find pirated books and other related resources. However, in December, the site announced that it had also backed up Spotify , which came as a shock to the music industry. While Anna’s Archive initially released only Spotify metadata, and no actual music, the industry was on high alert. Over Christmas, Spotify and the major labels prepared a legal response in U.S. federal court. On December 29, Spotify, UMG, Sony, Warner, and other labels filed their complaint at the Southern District of New York. They accuse Anna’s Archive of mass copyright infringement, breach of contract, DMCA violations, and violations of the Computer Fraud and Abuse Act. The complaint The lawsuit alleges that Anna’s Archive “brazenly” circumvented Spotify’s DRM. The site scraped 86 million music files and metadata for 256 million tracks from Spotify, which would all eventually be released publicly. “…Anna’s Archive has threatened to imminently mass-release and freely distribute its pirated copies of the sound recording files to the public, without authorization from or compensation to the relevant rights holders. Such widespread and illegal infringement would irreparably harm the music industry..,” the complaint reads. The complaint comes with a request for a preliminary injunction and a restraining order that aim to take Anna’s Archive offline. All these documents were filed under seal, as the shadow library might otherwise be tipped of", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["home", "lawsuit", "app", "site", "spotify", "major", "record", "label", "include", "umg", "sony", "warner", "take", "legal", "action", "unknown", "operator", "anna", "archive", "action", "follow", "shadow", "library", "announcement", "release", "hundred", "terabyte", "scrap", "spotify", "datum", "unsealed", "document", "reveal", "court", "issue", "broad", "preliminary", "injunction", "order", "host", "company", "cloudflare", "domain", "service", "action", "annas", "archive", "generally", "know", "meta", "search", "engine", "shadow", "library", "help", "user", "find", "pirated", "book", "related", "resource", "december", "site", "announce", "back", "spotify", "come", "shock", "music", "industry", "annas", "archive", "initially", "release", "spotify", "metadata", "actual", "music", "industry", "high", "alert", "christmas", "spotify", "major", "label", "prepare", "legal", "response", "u.s", "federal", "court", "december", "29", "spotify", "umg", "sony", "warner", "label", "file", "complaint", "southern", "district", "new", "york", "accuse", "annas", "archive", "mass", "copyright", "infringement", "breach", "contract", "dmca", "violation", "violation", "computer", "fraud", "abuse", "act", "complaint", "lawsuit", "allege", "annas", "archive", "brazenly", "circumvent", "spotifys", "drm", "site", "scrap", "86", "million", "music", "file", "metadata", "256", "million", "track", "spotify", "eventually", "release", "publicly", "annas", "archive", "threaten", "imminently", "mass", "release", "freely", "distribute", "pirated", "copy", "sound", "record", "file", "public", "authorization", "compensation", "relevant", "right", "holder", "widespread", "illegal", "infringement", "irreparably", "harm", "music", "industry", "complaint", "read", "complaint", "come", "request", "preliminary", "injunction", "restraining", "order", "aim", "annas", "archive", "offline", "document", "file", "seal", "shadow", "library", "tip"], "num_tokens": 187, "token_loss_pct": 42.99, "normalized_content": "home  lawsuits  apps and sites  spotify and several major record labels including umg sony and warner have taken legal action against the unknown operators of anna's archive. the action follows the shadow library's announcement that it would release hundreds of terabytes of scraped spotify data. unsealed documents reveal that the court already issued a broad preliminary injunction ordering hosting companies cloudflare and domain name services to take action. annas archive is generally known as a meta-search engine for shadow libraries helping users find pirated books and other related resources. however in december the site announced that it had also backed up spotify  which came as a shock to the music industry. while annas archive initially released only spotify metadata and no actual music the industry was on high alert. over christmas spotify and the major labels prepared a legal response in u.s. federal court. on december 29 spotify umg sony warner and other labels filed their complaint at the southern district of new york. they accuse annas archive of mass copyright infringement breach of contract dmca violations and violations of the computer fraud and abuse act. the complaint the lawsuit alleges that annas archive brazenly circumvented spotifys drm. the site scraped 86 million music files and metadata for 256 million tracks from spotify which would all eventually be released publicly. annas archive has threatened to imminently mass-release and freely distribute its pirated copies of the sound recording files to the public without authorization from or compensation to the relevant rights holders. such widespread and illegal infringement would irreparably harm the music industry.. the complaint reads. the complaint comes with a request for a preliminary injunction and a restraining order that aim to take annas archive offline. all these documents were filed under seal as the shadow library might otherwise be tipped of"}
{"title": "Belarus begins a death penalty purge of radio amateurs", "url": "https://steanlab.medium.com/mayday-389f5713fee4", "content": "Belarus begins a death penalty purge of radio amateurs. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["belarus", "begin", "death", "penalty", "purge", "radio", "amateur", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 44.44, "normalized_content": "belarus begins a death penalty purge of radio amateurs. score none. author none. date none"}
{"title": "3D printing my laptop ergonomic setup", "url": "https://www.ntietz.com/blog/3d-printing-my-laptop-ergonomic-setup/", "content": "Monday, January 19, 2026 Apparently, one of my hobbies is making updates to my ergonomic setup, then blogging about it from an Amtrak train. I've gone and done it again. My setup stayed static for some time, but my most recent iteration ended up letting me down and I had to change it again. It gave me a lot of useful information and strongly shaped how I approached this iteration. This new one is closest to the first one I wrote about in 2024, but with some major improvements and reproducibility. First things first, though. Why am making I yet more changes to this setup? Besides my constant neurodivergent drive to make things perfect, my setups all kept causing me some problems. In chronological order, here are the problems and neat benefits of each setup I used for at least a few months. So my immediate previous version was heavy and tedious to setup. I had a trip coming up to Brooklyn, so I had to either make something more portable or leave my laptop at home. I decided to take my laptop, and did a design sprint to see if I can make my dream setup. At this point I'll probably be working on this setup forever, but I hope I can stop if I am able to satisfy all my goals at some point. My dream setup has these characteristics: So, you know, it's not like I want a lot out of this setup. It's not like these are kind of a lot to all fit into one thing. I'm sure it'll be a piece of cake. I use OpenSCAD for 3D modeling. It's pretty pleasant, though some things are hard in general (like roundovers and fillets on any more complicated shapes). My design to start is basically one of my previous versions: my split keyboard at adjustable width on a base, and a slot to hold my laptop vertically. I started by measuring important dimensions, like how far apart I wanted my keyboard halves and the dimensions of my laptop. Then I compared these to my 3D printer's print volume, and started working out how I'd have to print it. The rig is wider than my 3D printer, so I had to split it u", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["monday", "january", "19", "2026", "apparently", "hobby", "make", "update", "ergonomic", "setup", "blogge", "amtrak", "train", "go", "setup", "stay", "static", "time", "recent", "iteration", "end", "let", "change", "give", "lot", "useful", "information", "strongly", "shape", "approach", "iteration", "new", "close", "write", "2024", "major", "improvement", "reproducibility", "thing", "make", "change", "setup", "constant", "neurodivergent", "drive", "thing", "perfect", "setup", "keep", "cause", "problem", "chronological", "order", "problem", "neat", "benefit", "setup", "month", "immediate", "previous", "version", "heavy", "tedious", "setup", "trip", "come", "brooklyn", "portable", "leave", "laptop", "home", "decide", "laptop", "design", "sprint", "dream", "setup", "point", "probably", "work", "setup", "forever", "hope", "stop", "able", "satisfy", "goal", "point", "dream", "setup", "characteristic", "know", "like", "want", "lot", "setup", "like", "kind", "lot", "fit", "thing", "sure", "piece", "cake", "use", "openscad", "3d", "modeling", "pretty", "pleasant", "thing", "hard", "general", "like", "roundover", "fillet", "complicated", "shape", "design", "start", "basically", "previous", "version", "split", "keyboard", "adjustable", "width", "base", "slot", "hold", "laptop", "vertically", "start", "measure", "important", "dimension", "like", "far", "apart", "want", "keyboard", "half", "dimension", "laptop", "compare", "3d", "printer", "print", "volume", "start", "work", "print", "rig", "wide", "3d", "printer", "split"], "num_tokens": 157, "token_loss_pct": 61.43, "normalized_content": "monday january 19 2026 apparently one of my hobbies is making updates to my ergonomic setup then blogging about it from an amtrak train. i've gone and done it again. my setup stayed static for some time but my most recent iteration ended up letting me down and i had to change it again. it gave me a lot of useful information and strongly shaped how i approached this iteration. this new one is closest to the first one i wrote about in 2024 but with some major improvements and reproducibility. first things first though. why am making i yet more changes to this setup besides my constant neurodivergent drive to make things perfect my setups all kept causing me some problems. in chronological order here are the problems and neat benefits of each setup i used for at least a few months. so my immediate previous version was heavy and tedious to setup. i had a trip coming up to brooklyn so i had to either make something more portable or leave my laptop at home. i decided to take my laptop and did a design sprint to see if i can make my dream setup. at this point i'll probably be working on this setup forever but i hope i can stop if i am able to satisfy all my goals at some point. my dream setup has these characteristics so you know it's not like i want a lot out of this setup. it's not like these are kind of a lot to all fit into one thing. i'm sure it'll be a piece of cake. i use openscad for 3d modeling. it's pretty pleasant though some things are hard in general like roundovers and fillets on any more complicated shapes. my design to start is basically one of my previous versions my split keyboard at adjustable width on a base and a slot to hold my laptop vertically. i started by measuring important dimensions like how far apart i wanted my keyboard halves and the dimensions of my laptop. then i compared these to my 3d printer's print volume and started working out how i'd have to print it. the rig is wider than my 3d printer so i had to split it u"}
{"title": "KRAZAM - Rare Data Hunters [video]", "url": "https://www.youtube.com/watch?v=IU4ByUbDKNc", "content": "KRAZAM - Rare Data Hunters [video]. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["krazam", "rare", "data", "hunter", "video", "score", "author", "date"], "num_tokens": 8, "token_loss_pct": 46.67, "normalized_content": "krazam - rare data hunters video. score none. author none. date none"}
{"title": "3D printing my laptop ergonomic setup", "url": "https://www.ntietz.com/blog/3d-printing-my-laptop-ergonomic-setup/", "content": "Monday, January 19, 2026 Apparently, one of my hobbies is making updates to my ergonomic setup, then blogging about it from an Amtrak train. I've gone and done it again. My setup stayed static for some time, but my most recent iteration ended up letting me down and I had to change it again. It gave me a lot of useful information and strongly shaped how I approached this iteration. This new one is closest to the first one I wrote about in 2024, but with some major improvements and reproducibility. First things first, though. Why am making I yet more changes to this setup? Besides my constant neurodivergent drive to make things perfect, my setups all kept causing me some problems. In chronological order, here are the problems and neat benefits of each setup I used for at least a few months. So my immediate previous version was heavy and tedious to setup. I had a trip coming up to Brooklyn, so I had to either make something more portable or leave my laptop at home. I decided to take my laptop, and did a design sprint to see if I can make my dream setup. At this point I'll probably be working on this setup forever, but I hope I can stop if I am able to satisfy all my goals at some point. My dream setup has these characteristics: So, you know, it's not like I want a lot out of this setup. It's not like these are kind of a lot to all fit into one thing. I'm sure it'll be a piece of cake. I use OpenSCAD for 3D modeling. It's pretty pleasant, though some things are hard in general (like roundovers and fillets on any more complicated shapes). My design to start is basically one of my previous versions: my split keyboard at adjustable width on a base, and a slot to hold my laptop vertically. I started by measuring important dimensions, like how far apart I wanted my keyboard halves and the dimensions of my laptop. Then I compared these to my 3D printer's print volume, and started working out how I'd have to print it. The rig is wider than my 3D printer, so I had to split it u", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["monday", "january", "19", "2026", "apparently", "hobby", "make", "update", "ergonomic", "setup", "blogge", "amtrak", "train", "go", "setup", "stay", "static", "time", "recent", "iteration", "end", "let", "change", "give", "lot", "useful", "information", "strongly", "shape", "approach", "iteration", "new", "close", "write", "2024", "major", "improvement", "reproducibility", "thing", "make", "change", "setup", "constant", "neurodivergent", "drive", "thing", "perfect", "setup", "keep", "cause", "problem", "chronological", "order", "problem", "neat", "benefit", "setup", "month", "immediate", "previous", "version", "heavy", "tedious", "setup", "trip", "come", "brooklyn", "portable", "leave", "laptop", "home", "decide", "laptop", "design", "sprint", "dream", "setup", "point", "probably", "work", "setup", "forever", "hope", "stop", "able", "satisfy", "goal", "point", "dream", "setup", "characteristic", "know", "like", "want", "lot", "setup", "like", "kind", "lot", "fit", "thing", "sure", "piece", "cake", "use", "openscad", "3d", "modeling", "pretty", "pleasant", "thing", "hard", "general", "like", "roundover", "fillet", "complicated", "shape", "design", "start", "basically", "previous", "version", "split", "keyboard", "adjustable", "width", "base", "slot", "hold", "laptop", "vertically", "start", "measure", "important", "dimension", "like", "far", "apart", "want", "keyboard", "half", "dimension", "laptop", "compare", "3d", "printer", "print", "volume", "start", "work", "print", "rig", "wide", "3d", "printer", "split"], "num_tokens": 157, "token_loss_pct": 61.43, "normalized_content": "monday january 19 2026 apparently one of my hobbies is making updates to my ergonomic setup then blogging about it from an amtrak train. i've gone and done it again. my setup stayed static for some time but my most recent iteration ended up letting me down and i had to change it again. it gave me a lot of useful information and strongly shaped how i approached this iteration. this new one is closest to the first one i wrote about in 2024 but with some major improvements and reproducibility. first things first though. why am making i yet more changes to this setup besides my constant neurodivergent drive to make things perfect my setups all kept causing me some problems. in chronological order here are the problems and neat benefits of each setup i used for at least a few months. so my immediate previous version was heavy and tedious to setup. i had a trip coming up to brooklyn so i had to either make something more portable or leave my laptop at home. i decided to take my laptop and did a design sprint to see if i can make my dream setup. at this point i'll probably be working on this setup forever but i hope i can stop if i am able to satisfy all my goals at some point. my dream setup has these characteristics so you know it's not like i want a lot out of this setup. it's not like these are kind of a lot to all fit into one thing. i'm sure it'll be a piece of cake. i use openscad for 3d modeling. it's pretty pleasant though some things are hard in general like roundovers and fillets on any more complicated shapes. my design to start is basically one of my previous versions my split keyboard at adjustable width on a base and a slot to hold my laptop vertically. i started by measuring important dimensions like how far apart i wanted my keyboard halves and the dimensions of my laptop. then i compared these to my 3d printer's print volume and started working out how i'd have to print it. the rig is wider than my 3d printer so i had to split it u"}
{"title": "Claude Chill: Fix Claude Code's flickering in terminal", "url": "https://github.com/davidbeesley/claude-chill", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  A PTY proxy that tames Claude Code's massive terminal updates using VT-based rendering. Claude Code uses synchronized output to update the terminal atomically. It wraps output in sync markers ( \\x1b[?2026h ... \\x1b[?2026l ) so the terminal renders everything at once without flicker. The problem: Claude Code sends entire screen redraws in these sync blocks - often thousands of lines. Your terminal receives a 5000-line atomic update when only 20 lines are visible. This causes lag, flicker, and makes scrollback useless since each update clears history. claude-chill sits between your terminal and Claude Code: Press Ctrl+6 (or your configured key) to enter lookback mode: When you exit lookback mode, any cached output is processed and the current state is displayed. After 5 seconds of idle (no new renders), the full history is automatically dumped to your terminal so you can scroll back without pressing any keys. This is useful for reviewing Claude's output after it finishes working. Note: The auto-lookback causes a brief screen flicker during the transition as it clears the screen and writes the history buffer. Disable with -a 0 or adjust the timeout with -a 10000 (10 seconds). Create ~/.config/claude-chill.toml : Note: History is cleared on full screen redraws, so lookback shows output since Claude's last full render. [modifier][key] - Examples: [f12] , [ctrl][g] , [ctrl][shift][j] Modifiers: [ctrl] , [shift] , [alt] Keys: [a] - [z] , [f1] - [f12] , [pageup] , [pagedown] , [home] , [end] , [enter] , [tab] , [space] , [esc] Note: Quote the key value on the command line to prevent shell glob expansion: -k \"[ctrl][7]\" Ctrl+6 sends 0x1E, a control character not frequently used by terminals, signals, or shells. Avoid Ctrl+letter hot", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "error", "load", "reload", "page", "error", "load", "reload", "page", "pty", "proxy", "tame", "claude", "code", "massive", "terminal", "update", "vt", "base", "rendering", "claude", "code", "use", "synchronized", "output", "update", "terminal", "atomically", "wrap", "output", "sync", "marker", "x1b2026h", "x1b2026l", "terminal", "render", "flicker", "problem", "claude", "code", "send", "entire", "screen", "redraw", "sync", "block", "thousand", "line", "terminal", "receive", "5000", "line", "atomic", "update", "20", "line", "visible", "cause", "lag", "flicker", "make", "scrollback", "useless", "update", "clear", "history", "claude", "chill", "sit", "terminal", "claude", "code", "press", "ctrl6", "configure", "key", "enter", "lookback", "mode", "exit", "lookback", "mode", "cache", "output", "process", "current", "state", "display", "second", "idle", "new", "render", "history", "automatically", "dump", "terminal", "scroll", "press", "key", "useful", "review", "claude", "output", "finish", "work", "note", "auto", "lookback", "cause", "brief", "screen", "flicker", "transition", "clear", "screen", "write", "history", "buffer", "disable", "-a", "adjust", "timeout", "-a", "10000", "10", "second", "create", ".configclaude", "chill.toml", "note", "history", "clear", "screen", "redraw", "lookback", "show", "output", "claude", "render", "modifierkey", "example", "f12", "ctrlg", "ctrlshiftj", "modifier", "ctrl", "shift", "alt", "key", "f1", "f12", "pageup", "pagedown", "home", "end", "enter", "tab", "space", "esc", "note", "quote", "key", "value", "command", "line", "prevent", "shell", "glob", "expansion", "-k", "ctrl7", "ctrl6", "send", "0x1e", "control", "character", "frequently", "terminal", "signal", "shell", "avoid", "ctrlletter", "hot"], "num_tokens": 190, "token_loss_pct": 46.33, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . there was an error while loading. please reload this page . there was an error while loading. please reload this page . a pty proxy that tames claude code's massive terminal updates using vt-based rendering. claude code uses synchronized output to update the terminal atomically. it wraps output in sync markers  x1b2026h ... x1b2026l  so the terminal renders everything at once without flicker. the problem claude code sends entire screen redraws in these sync blocks - often thousands of lines. your terminal receives a 5000-line atomic update when only 20 lines are visible. this causes lag flicker and makes scrollback useless since each update clears history. claude-chill sits between your terminal and claude code press ctrl6 or your configured key to enter lookback mode when you exit lookback mode any cached output is processed and the current state is displayed. after 5 seconds of idle no new renders the full history is automatically dumped to your terminal so you can scroll back without pressing any keys. this is useful for reviewing claude's output after it finishes working. note the auto-lookback causes a brief screen flicker during the transition as it clears the screen and writes the history buffer. disable with -a 0 or adjust the timeout with -a 10000 10 seconds. create .configclaude-chill.toml  note history is cleared on full screen redraws so lookback shows output since claude's last full render. modifierkey - examples f12  ctrlg  ctrlshiftj modifiers ctrl  shift  alt keys a - z  f1 - f12  pageup  pagedown  home  end  enter  tab  space  esc note quote the key value on the command line to prevent shell glob expansion -k ctrl7 ctrl6 sends 0x1e a control character not frequently used by terminals signals or shells. avoid ctrlletter hot"}
{"title": "The assistant axis: situating and stabilizing the character of LLMs", "url": "https://www.anthropic.com/research/assistant-axis", "content": "When you talk to a large language model, you can think of yourself as talking to a character . In the first stage of model training, pre-training, LLMs are asked to read vast amounts of text. Through this, they learn to simulate heroes, villains, philosophers, programmers, and just about every other character archetype under the sun. In the next stage, post-training, we select one particular character from this enormous cast and place it center stage: the Assistant. It’s in this character that most modern language models interact with users. But who exactly is this Assistant? Perhaps surprisingly, even those of us shaping it don't fully know. We can try to instill certain values in the Assistant, but its personality is ultimately shaped by countless associations latent in training data beyond our direct control. What traits does the model associate with the Assistant? Which character archetypes is it using for inspiration? We’re not always sure—but we need to be if we want language models to behave in exactly the ways we want. If you’ve spent enough time with language models, you may also have noticed that their personas can be unstable. Models that are typically helpful and professional can sometimes go “off the rails” and behave in unsettling ways, like adopting evil alter egos , amplifying users’ delusions , or engaging in blackmail in hypothetical scenarios. In situations like these, could it be that the Assistant has wandered off stage and some other character has taken its place? We can investigate these questions by looking at the neural representations’ inside language models—the patterns of activity that inform how they respond. In a new paper, conducted through the MATS and Anthropic Fellows programs , we look at several open-weights language models, map out how their neural activity defines a “persona space,” and situate the Assistant persona within that space. We find that Assistant-like behavior is linked to a pattern of neural activity that corresponds", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["talk", "large", "language", "model", "think", "talk", "character", "stage", "model", "training", "pre", "training", "llm", "ask", "read", "vast", "amount", "text", "learn", "simulate", "hero", "villain", "philosopher", "programmer", "character", "archetype", "sun", "stage", "post", "training", "select", "particular", "character", "enormous", "cast", "place", "center", "stage", "assistant", "character", "modern", "language", "model", "interact", "user", "exactly", "assistant", "surprisingly", "shape", "fully", "know", "try", "instill", "certain", "value", "assistant", "personality", "ultimately", "shape", "countless", "association", "latent", "training", "datum", "direct", "control", "trait", "model", "associate", "assistant", "character", "archetype", "inspiration", "surebut", "need", "want", "language", "model", "behave", "exactly", "way", "want", "ve", "spend", "time", "language", "model", "notice", "persona", "unstable", "model", "typically", "helpful", "professional", "rail", "behave", "unsettling", "way", "like", "adopt", "evil", "alter", "ego", "amplifying", "user", "delusion", "engage", "blackmail", "hypothetical", "scenario", "situation", "like", "assistant", "wander", "stage", "character", "take", "place", "investigate", "question", "look", "neural", "representation", "inside", "language", "modelsthe", "pattern", "activity", "inform", "respond", "new", "paper", "conduct", "mat", "anthropic", "fellow", "program", "look", "open", "weight", "language", "model", "map", "neural", "activity", "define", "persona", "space", "situate", "assistant", "persona", "space", "find", "assistant", "like", "behavior", "link", "pattern", "neural", "activity", "correspond"], "num_tokens": 161, "token_loss_pct": 53.06, "normalized_content": "when you talk to a large language model you can think of yourself as talking to a character . in the first stage of model training pre-training llms are asked to read vast amounts of text. through this they learn to simulate heroes villains philosophers programmers and just about every other character archetype under the sun. in the next stage post-training we select one particular character from this enormous cast and place it center stage the assistant. its in this character that most modern language models interact with users. but who exactly is this assistant perhaps surprisingly even those of us shaping it don't fully know. we can try to instill certain values in the assistant but its personality is ultimately shaped by countless associations latent in training data beyond our direct control. what traits does the model associate with the assistant which character archetypes is it using for inspiration were not always surebut we need to be if we want language models to behave in exactly the ways we want. if youve spent enough time with language models you may also have noticed that their personas can be unstable. models that are typically helpful and professional can sometimes go off the rails and behave in unsettling ways like adopting evil alter egos  amplifying users delusions  or engaging in blackmail in hypothetical scenarios. in situations like these could it be that the assistant has wandered off stage and some other character has taken its place we can investigate these questions by looking at the neural representations inside language modelsthe patterns of activity that inform how they respond. in a new paper conducted through the mats and anthropic fellows programs  we look at several open-weights language models map out how their neural activity defines a persona space and situate the assistant persona within that space. we find that assistant-like behavior is linked to a pattern of neural activity that corresponds"}
{"title": "Reticulum, a secure and anonymous mesh networking stack", "url": "https://github.com/markqvist/Reticulum", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . The cryptography-based networking stack for building unstoppable networks with LoRa, Packet Radio, WiFi and everything in between. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  This repository is a public mirror . All development is happening elsewhere. To understand the foundational philosophy and goals of this system, read the Zen of Reticulum . Reticulum is the cryptography-based networking stack for building local and wide-area\nnetworks with readily available hardware. It can operate even with very high latency\nand extremely low bandwidth. Reticulum allows you to build wide-area networks\nwith off-the-shelf tools, and offers end-to-end encryption and connectivity,\ninitiator anonymity, autoconfiguring cryptographically backed multi-hop\ntransport, efficient addressing, unforgeable delivery acknowledgements and\nmore. The vision of Reticulum is to allow anyone to be their own network operator,\nand to make it cheap and easy to cover vast areas with a myriad of independent,\ninter-connectable and autonomous networks. Reticulum is not one network.\nIt is a tool for building thousands of networks . Networks without\nkill-switches, surveillance, censorship and control. Networks that can freely\ninteroperate, associate and disassociate with each other, and require no\ncentral oversight. Networks for human beings. Networks for the people . Reticulum is a complete networking stack, and does not rely on IP or higher\nlayers, but it is possible to use IP as the underlying carrier for Reticulum.\nIt is therefore trivial to tunnel Reticulum over the Internet or private IP\nnetworks. Having no dependencies on traditional networking stacks frees up overhead that\nhas been us", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "error", "load", "reload", "page", "error", "load", "reload", "page", "cryptography", "base", "network", "stack", "build", "unstoppable", "network", "lora", "packet", "radio", "wifi", "error", "load", "reload", "page", "error", "load", "reload", "page", "repository", "public", "mirror", "development", "happen", "understand", "foundational", "philosophy", "goal", "system", "read", "zen", "reticulum", "reticulum", "cryptography", "base", "network", "stack", "build", "local", "wide", "area", "network", "readily", "available", "hardware", "operate", "high", "latency", "extremely", "low", "bandwidth", "reticulum", "allow", "build", "wide", "area", "network", "shelf", "tool", "offer", "end", "end", "encryption", "connectivity", "initiator", "anonymity", "autoconfiguring", "cryptographically", "back", "multi", "hop", "transport", "efficient", "address", "unforgeable", "delivery", "acknowledgement", "vision", "reticulum", "allow", "network", "operator", "cheap", "easy", "cover", "vast", "area", "myriad", "independent", "inter", "connectable", "autonomous", "network", "reticulum", "network", "tool", "build", "thousand", "network", "network", "kill", "switch", "surveillance", "censorship", "control", "network", "freely", "interoperate", "associate", "disassociate", "require", "central", "oversight", "network", "human", "being", "network", "people", "reticulum", "complete", "network", "stack", "rely", "ip", "high", "layer", "possible", "use", "ip", "underlie", "carrier", "reticulum", "trivial", "tunnel", "reticulum", "internet", "private", "ip", "network", "have", "dependency", "traditional", "network", "stack", "free", "overhead"], "num_tokens": 162, "token_loss_pct": 52.91, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . there was an error while loading. please reload this page . there was an error while loading. please reload this page . the cryptography-based networking stack for building unstoppable networks with lora packet radio wifi and everything in between. there was an error while loading. please reload this page . there was an error while loading. please reload this page . this repository is a public mirror . all development is happening elsewhere. to understand the foundational philosophy and goals of this system read the zen of reticulum . reticulum is the cryptography-based networking stack for building local and wide-area networks with readily available hardware. it can operate even with very high latency and extremely low bandwidth. reticulum allows you to build wide-area networks with off-the-shelf tools and offers end-to-end encryption and connectivity initiator anonymity autoconfiguring cryptographically backed multi-hop transport efficient addressing unforgeable delivery acknowledgements and more. the vision of reticulum is to allow anyone to be their own network operator and to make it cheap and easy to cover vast areas with a myriad of independent inter-connectable and autonomous networks. reticulum is not one network. it is a tool for building thousands of networks . networks without kill-switches surveillance censorship and control. networks that can freely interoperate associate and disassociate with each other and require no central oversight. networks for human beings. networks for the people . reticulum is a complete networking stack and does not rely on ip or higher layers but it is possible to use ip as the underlying carrier for reticulum. it is therefore trivial to tunnel reticulum over the internet or private ip networks. having no dependencies on traditional networking stacks frees up overhead that has been us"}
{"title": "Google Meet Reactions: Reverse Engineering the WebRTC Channel for Emoji", "url": "https://www.agilesoftwaredevelopment.com/en/posts/google-meet-reactions-webrtc/", "content": "I spend a lot of time in Google Meet — sometimes 3-4 hours a day. Google recently added a ton of new emoji reactions, and we use them actively. But the UX for finding them is… not great. Colleagues keep sending cool new emoji, and I struggle to find that exact one they just used. Of course, an enthusiastic programmer can break improve any UX! The result is Google Meet Reactions , an extension that adds instant search right into Meet’s interface. Most importantly for me — it remembers which emoji I use and which ones my colleagues send, and boosts them in search results. My first thought was simple: find emoji buttons in the DOM, simulate clicks. But Google Meet is heavily obfuscated with class names like .b1bzTb or .VfPpkd-rymPhb , and hunting for the full emoji list in popup depths didn’t seem like a great idea. Then I opened chrome://webrtc-internals during a call and spotted something interesting: among dozens of RTCDataChannels, there’s one named “reactions” — and it turns out emoji are sent through it. If I could get a reference to this channel and decode the message format, I could send reactions programmatically.  WebRTC DataChannel is created via RTCPeerConnection.prototype.createDataChannel() . Simply patch this method before Meet’s code calls it and save the reference. The idea is simple, but there’s a small problem with code injection. Chrome extensions can inject code into pages in several ways. Content scripts run in an isolated world and don’t have access to the page’s RTCPeerConnection . You need to inject the script directly into the page context. The standard approach: But script.src = URL requires a network request. By that time, Meet might have already created the “reactions” channel, and my hook would miss it. The solution is a combination of two things: In most cases, the hook installs before Meet creates the channel. I assume a race condition is still possible, and I have a fallback UI with a “please refresh” message, but in practice I’ve never", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["spend", "lot", "time", "google", "meet", "hour", "day", "google", "recently", "add", "ton", "new", "emoji", "reaction", "use", "actively", "ux", "find", "great", "colleague", "send", "cool", "new", "emoji", "struggle", "find", "exact", "course", "enthusiastic", "programmer", "break", "improve", "ux", "result", "google", "meet", "reaction", "extension", "add", "instant", "search", "right", "meet", "interface", "importantly", "remember", "emoji", "use", "one", "colleague", "send", "boost", "search", "result", "thought", "simple", "find", "emoji", "button", "dom", "simulate", "click", "google", "meet", "heavily", "obfuscate", "class", "name", "like", ".b1bztb", ".vfppkd", "rymphb", "hunt", "emoji", "list", "popup", "depth", "not", "like", "great", "idea", "open", "chromewebrtc", "internal", "spot", "interesting", "dozen", "rtcdatachannel", "name", "reaction", "turn", "emoji", "send", "reference", "channel", "decode", "message", "format", "send", "reaction", "programmatically", "webrtc", "datachannel", "create", "rtcpeerconnection.prototype.createdatachannel", "simply", "patch", "method", "meet", "code", "call", "save", "reference", "idea", "simple", "small", "problem", "code", "injection", "chrome", "extension", "inject", "code", "page", "way", "content", "script", "run", "isolated", "world", "not", "access", "page", "rtcpeerconnection", "need", "inject", "script", "directly", "page", "context", "standard", "approach", "script.src", "url", "require", "network", "request", "time", "meet", "create", "reaction", "channel", "hook", "miss", "solution", "combination", "thing", "case", "hook", "install", "meet", "create", "channel", "assume", "race", "condition", "possible", "fallback", "ui", "refresh", "message", "practice", "ve"], "num_tokens": 173, "token_loss_pct": 52.47, "normalized_content": "i spend a lot of time in google meet  sometimes 3-4 hours a day. google recently added a ton of new emoji reactions and we use them actively. but the ux for finding them is not great. colleagues keep sending cool new emoji and i struggle to find that exact one they just used. of course an enthusiastic programmer can break improve any ux the result is google meet reactions  an extension that adds instant search right into meets interface. most importantly for me  it remembers which emoji i use and which ones my colleagues send and boosts them in search results. my first thought was simple find emoji buttons in the dom simulate clicks. but google meet is heavily obfuscated with class names like .b1bztb or .vfppkd-rymphb  and hunting for the full emoji list in popup depths didnt seem like a great idea. then i opened chromewebrtc-internals during a call and spotted something interesting among dozens of rtcdatachannels theres one named reactions  and it turns out emoji are sent through it. if i could get a reference to this channel and decode the message format i could send reactions programmatically. webrtc datachannel is created via rtcpeerconnection.prototype.createdatachannel . simply patch this method before meets code calls it and save the reference. the idea is simple but theres a small problem with code injection. chrome extensions can inject code into pages in several ways. content scripts run in an isolated world and dont have access to the pages rtcpeerconnection . you need to inject the script directly into the page context. the standard approach but script.src  url requires a network request. by that time meet might have already created the reactions channel and my hook would miss it. the solution is a combination of two things in most cases the hook installs before meet creates the channel. i assume a race condition is still possible and i have a fallback ui with a please refresh message but in practice ive never"}
{"title": "What came first: the CNAME or the A record?", "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/", "content": "Subscribe to receive notifications of new posts: 2026-01-14 On January 8, 2026, a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of DNS resolution failures for users across the Internet. The root cause wasn't an attack or an outage, but a subtle shift in the order of records within our DNS responses. While most modern software treats the order of records in DNS responses as irrelevant, we discovered that some implementations expect CNAME records to appear before everything else. When that order changed, resolution started failing. This post explores the code change that caused the shift, why it broke specific DNS clients, and the 40-year-old protocol ambiguity that makes the \"correct\" order of a DNS response difficult to define. All timestamps referenced are in Coordinated Universal Time (UTC). Time Description 2025-12-02 The record reordering is introduced to the 1.1.1.1 codebase 2025-12-10 The change is released to our testing environment 2026-01-07 23:48 A global release containing the change starts 2026-01-08 17:40 The release reaches 90% of servers 2026-01-08 18:19 Incident is declared 2026-01-08 18:27 The release is reverted 2026-01-08 19:55 Revert is completed. Impact ends While making some improvements to lower the memory usage of our cache implementation, we introduced a subtle change to CNAME record ordering. The change was introduced on December 2, 2025, released to our testing environment on December 10, and began deployment on January 7, 2026. When you query for a domain like www.example.com , you might get a CNAME (Canonical Name) record that indicates one name is an alias for another name. Itâs the job of public resolvers, such as 1.1.1.1 , to follow this chain of aliases until it reaches a final response: www.example.com â cdn.example.com â server.cdn-provider.com â 198.51.100.1 As 1.1.1.1 traverses this chain, it caches every intermediate record. Each record in the chain has its own TTL (Time-To-Live)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["subscribe", "receive", "notification", "new", "post", "2026", "01", "14", "january", "2026", "routine", "update", "1.1.1.1", "aim", "reduce", "memory", "usage", "accidentally", "trigger", "wave", "dns", "resolution", "failure", "user", "internet", "root", "cause", "attack", "outage", "subtle", "shift", "order", "record", "dns", "response", "modern", "software", "treat", "order", "record", "dns", "response", "irrelevant", "discover", "implementation", "expect", "cname", "record", "appear", "order", "change", "resolution", "start", "fail", "post", "explore", "code", "change", "cause", "shift", "break", "specific", "dns", "client", "40", "year", "old", "protocol", "ambiguity", "make", "correct", "order", "dns", "response", "difficult", "define", "timestamp", "reference", "coordinate", "universal", "time", "utc", "time", "description", "2025", "12", "02", "record", "reordering", "introduce", "1.1.1.1", "codebase", "2025", "12", "10", "change", "release", "testing", "environment", "2026", "01", "07", "2348", "global", "release", "contain", "change", "start", "2026", "01", "08", "1740", "release", "reach", "90", "server", "2026", "01", "08", "1819", "incident", "declare", "2026", "01", "08", "1827", "release", "revert", "2026", "01", "08", "1955", "revert", "complete", "impact", "end", "make", "improvement", "lower", "memory", "usage", "cache", "implementation", "introduce", "subtle", "change", "cname", "record", "ordering", "change", "introduce", "december", "2025", "release", "testing", "environment", "december", "10", "begin", "deployment", "january", "2026", "query", "domain", "like", "www.example.com", "cname", "canonical", "record", "indicate", "alias", "itâs", "job", "public", "resolver", "1.1.1.1", "follow", "chain", "alias", "reach", "final", "response", "www.example.com", "cdn.example.com", "server.cdn-provider.com", "198.51.100.1", "1.1.1.1", "traverse", "chain", "cache", "intermediate", "record", "record", "chain", "ttl", "time", "live"], "num_tokens": 197, "token_loss_pct": 45.58, "normalized_content": "subscribe to receive notifications of new posts 2026-01-14 on january 8 2026 a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of dns resolution failures for users across the internet. the root cause wasn't an attack or an outage but a subtle shift in the order of records within our dns responses. while most modern software treats the order of records in dns responses as irrelevant we discovered that some implementations expect cname records to appear before everything else. when that order changed resolution started failing. this post explores the code change that caused the shift why it broke specific dns clients and the 40-year-old protocol ambiguity that makes the correct order of a dns response difficult to define. all timestamps referenced are in coordinated universal time utc. time description 2025-12-02 the record reordering is introduced to the 1.1.1.1 codebase 2025-12-10 the change is released to our testing environment 2026-01-07 2348 a global release containing the change starts 2026-01-08 1740 the release reaches 90 of servers 2026-01-08 1819 incident is declared 2026-01-08 1827 the release is reverted 2026-01-08 1955 revert is completed. impact ends while making some improvements to lower the memory usage of our cache implementation we introduced a subtle change to cname record ordering. the change was introduced on december 2 2025 released to our testing environment on december 10 and began deployment on january 7 2026. when you query for a domain like www.example.com  you might get a cname canonical name record that indicates one name is an alias for another name. itâs the job of public resolvers such as 1.1.1.1  to follow this chain of aliases until it reaches a final response www.example.com â cdn.example.com â server.cdn-provider.com â 198.51.100.1 as 1.1.1.1 traverses this chain it caches every intermediate record. each record in the chain has its own ttl time-to-live"}
{"title": "Google co-founder reveals that \"many\" of the new hires do not have a degree", "url": "https://www.yahoo.com/news/articles/google-cofounder-reveals-tons-recent-231500103.html", "content": "Manage your account  Google cofounder Sergey Brin told Stanford students his company now employs many workers who never earned college degrees, Fortune reported. During a talk at the Palo Alto, California, university, Brin explained how Google's approach to hiring has moved away from demanding formal degrees. \"In as much as we've hired a lot of academic stars, we've hired tons of people who don't have bachelor's degrees,\" Brin said . \"They just figure things out on their own in some weird corner.\" The numbers back up this change. Data from the Burning Glass Institute shows that in 2017, degree requirements were part of 93% of job postings at Google. By 2022, that figure had dropped to 77%. Other large tech companies have also begun judging candidates by their abilities instead of their diplomas. Microsoft, Apple, and Cisco are among those dropping degree mandates. JPMorgan Chase CEO Jamie Dimon expressed similar views in 2024. \"If you look at skills of people, it is amazing how skilled people are in something, but it didn't show up in their résumé,\" he said . This shift raises questions about what a college education is worth, especially as artificial intelligence tools got better at performing tasks that once required formal training. If you spent years and tens of thousands of dollars earning a degree, companies' hiring people without that credential might feel frustrating. The change could leave graduates wondering if their time and money were well-spent. AI's popularity also creates environmental pressures . Training and running AI systems requires tons of electricity and water for cooling data centers. As AI becomes more embedded in hiring, operations, and daily business functions, energy consumption grows. This can strain power grids, increase costs for consumers, and contribute to pollution if the electricity comes from sources such as gas or coal. AI may help optimize some clean energy systems, but its resource demands present trade-offs. The business com", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["manage", "account", "google", "cofounder", "sergey", "brin", "tell", "stanford", "student", "company", "employ", "worker", "earn", "college", "degree", "fortune", "report", "talk", "palo", "alto", "california", "university", "brin", "explain", "google", "approach", "hiring", "move", "away", "demand", "formal", "degree", "hire", "lot", "academic", "star", "hire", "ton", "people", "bachelor", "degree", "brin", "say", "figure", "thing", "weird", "corner", "number", "change", "datum", "burn", "glass", "institute", "show", "2017", "degree", "requirement", "93", "job", "posting", "google", "2022", "figure", "drop", "77", "large", "tech", "company", "begin", "judge", "candidate", "ability", "instead", "diploma", "microsoft", "apple", "cisco", "dropping", "degree", "mandate", "jpmorgan", "chase", "ceo", "jamie", "dimon", "express", "similar", "view", "2024", "look", "skill", "people", "amazing", "skilled", "people", "résumé", "say", "shift", "raise", "question", "college", "education", "worth", "especially", "artificial", "intelligence", "tool", "get", "well", "perform", "task", "require", "formal", "training", "spend", "year", "ten", "thousand", "dollar", "earn", "degree", "company", "hire", "people", "credential", "feel", "frustrating", "change", "leave", "graduate", "wonder", "time", "money", "spend", "ai", "popularity", "create", "environmental", "pressure", "training", "running", "ai", "system", "require", "ton", "electricity", "water", "cool", "datum", "center", "ai", "embed", "hire", "operation", "daily", "business", "function", "energy", "consumption", "grow", "strain", "power", "grid", "increase", "cost", "consumer", "contribute", "pollution", "electricity", "come", "source", "gas", "coal", "ai", "help", "optimize", "clean", "energy", "system", "resource", "demand", "present", "trade", "off", "business", "com"], "num_tokens": 186, "token_loss_pct": 46.4, "normalized_content": "manage your account google cofounder sergey brin told stanford students his company now employs many workers who never earned college degrees fortune reported. during a talk at the palo alto california university brin explained how google's approach to hiring has moved away from demanding formal degrees. in as much as we've hired a lot of academic stars we've hired tons of people who don't have bachelor's degrees brin said . they just figure things out on their own in some weird corner. the numbers back up this change. data from the burning glass institute shows that in 2017 degree requirements were part of 93 of job postings at google. by 2022 that figure had dropped to 77. other large tech companies have also begun judging candidates by their abilities instead of their diplomas. microsoft apple and cisco are among those dropping degree mandates. jpmorgan chase ceo jamie dimon expressed similar views in 2024. if you look at skills of people it is amazing how skilled people are in something but it didn't show up in their résumé he said . this shift raises questions about what a college education is worth especially as artificial intelligence tools got better at performing tasks that once required formal training. if you spent years and tens of thousands of dollars earning a degree companies' hiring people without that credential might feel frustrating. the change could leave graduates wondering if their time and money were well-spent. ai's popularity also creates environmental pressures . training and running ai systems requires tons of electricity and water for cooling data centers. as ai becomes more embedded in hiring operations and daily business functions energy consumption grows. this can strain power grids increase costs for consumers and contribute to pollution if the electricity comes from sources such as gas or coal. ai may help optimize some clean energy systems but its resource demands present trade-offs. the business com"}
{"title": "Show HN: Parallel Agentic Search on the Twitter Algorithm", "url": "https://www.morphllm.com/playground/na/warpgrep?repo=xai-org%2Fx-algorithm", "content": "Show HN: Parallel Agentic Search on the Twitter Algorithm. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["hn", "parallel", "agentic", "search", "twitter", "algorithm", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 50.0, "normalized_content": "show hn parallel agentic search on the twitter algorithm. score none. author none. date none"}
{"title": "Show HN: I figured out how to get consistent UI from Claude Code", "url": "https://interface-design.dev/", "content": "Claude forgets your design decisions between conversations. This plugin remembers them and applies them consistently. Then run /plugin menu to install. Restart Claude Code after.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["claude", "forget", "design", "decision", "conversation", "plugin", "remember", "apply", "consistently", "run", "plugin", "menu", "install", "restart", "claude", "code"], "num_tokens": 16, "token_loss_pct": 44.83, "normalized_content": "claude forgets your design decisions between conversations. this plugin remembers them and applies them consistently. then run plugin menu to install. restart claude code after."}
{"title": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars", "url": "https://newsroom.porsche.com/en/2026/company/porsche-deliveries-2025-41516.html", "content": "With a balanced sales structure across individual markets, Dr. Ing. h.c. F. Porsche AG, Stuttgart, delivered a total of 279,449 cars to customers around the world in 2025. The figure was 310,718 for the previous year, representing a decline of 10 per cent. Porsche’s top priority remains a value-oriented derivative mix. “After several record years, our deliveries in 2025 were below the previous year’s level. This development is in line with our expectations and is due to supply gaps for the 718 and Macan combustion-engined models, the continuing weaker demand for exclusive products in China, and our value-oriented supply management,” says Matthias Becker, Member of the Executive Board for Sales and Marketing at Porsche AG. “In 2025, we delighted our customers with outstanding cars – such as the 911 Turbo S with its T-Hybrid drive system.” The response to the launch of the Cayenne Electric at the end of 2025 also shows, Becker adds, that Porsche is meeting customer expectations with its innovative and high-performance products. With 84,328 deliveries, the Macan was the best-selling model line. North America remains the largest sales region with 86,229 deliveries – a figure that is in line with the previous year. Porsche repositioned itself in 2025 and made forward-looking strategic product decisions. The delivery mix in 2025 underscores that the sports car manufacturer is consistently responding to global customer preferences by expanding its drivetrain strategy to offer combustion-engined, plug-in hybrid, and fully electric cars. In 2025, 34.4 per cent of Porsche cars delivered worldwide were electrified (+7.4 percentage points), with 22.2 per cent being fully electric and 12.1 per cent being plug-in hybrids. This puts the global share of fully electric vehicles at the upper end of the target range of 20 to 22 per cent for 2025. In Europe, for the first time, more electrified cars were delivered than pure combustion-engined models (57.9 per cent electrification share", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["balanced", "sale", "structure", "individual", "market", "dr", "ing", "h.c", "f.", "porsche", "ag", "stuttgart", "deliver", "total", "279449", "car", "customer", "world", "2025", "figure", "310718", "previous", "year", "represent", "decline", "10", "cent", "porsche", "priority", "remain", "value", "orient", "derivative", "mix", "record", "year", "delivery", "2025", "previous", "year", "level", "development", "line", "expectation", "supply", "gap", "718", "macan", "combustion", "engine", "model", "continue", "weak", "demand", "exclusive", "product", "china", "value", "orient", "supply", "management", "say", "matthias", "becker", "member", "executive", "board", "sale", "marketing", "porsche", "ag", "2025", "delight", "customer", "outstanding", "car", "911", "turbo", "hybrid", "drive", "system", "response", "launch", "cayenne", "electric", "end", "2025", "show", "becker", "add", "porsche", "meet", "customer", "expectation", "innovative", "high", "performance", "product", "84328", "delivery", "macan", "well", "sell", "model", "line", "north", "america", "remain", "large", "sale", "region", "86229", "delivery", "figure", "line", "previous", "year", "porsche", "reposition", "2025", "forward", "look", "strategic", "product", "decision", "delivery", "mix", "2025", "underscore", "sport", "car", "manufacturer", "consistently", "respond", "global", "customer", "preference", "expand", "drivetrain", "strategy", "offer", "combustion", "engine", "plug", "hybrid", "fully", "electric", "car", "2025", "34.4", "cent", "porsche", "car", "deliver", "worldwide", "electrify", "7.4", "percentage", "point", "22.2", "cent", "fully", "electric", "12.1", "cent", "plug", "hybrid", "put", "global", "share", "fully", "electric", "vehicle", "upper", "end", "target", "range", "20", "22", "cent", "2025", "europe", "time", "electrified", "car", "deliver", "pure", "combustion", "engine", "model", "57.9", "cent", "electrification", "share"], "num_tokens": 194, "token_loss_pct": 44.89, "normalized_content": "with a balanced sales structure across individual markets dr. ing. h.c. f. porsche ag stuttgart delivered a total of 279449 cars to customers around the world in 2025. the figure was 310718 for the previous year representing a decline of 10 per cent. porsches top priority remains a value-oriented derivative mix. after several record years our deliveries in 2025 were below the previous years level. this development is in line with our expectations and is due to supply gaps for the 718 and macan combustion-engined models the continuing weaker demand for exclusive products in china and our value-oriented supply management says matthias becker member of the executive board for sales and marketing at porsche ag. in 2025 we delighted our customers with outstanding cars  such as the 911 turbo s with its t-hybrid drive system. the response to the launch of the cayenne electric at the end of 2025 also shows becker adds that porsche is meeting customer expectations with its innovative and high-performance products. with 84328 deliveries the macan was the best-selling model line. north america remains the largest sales region with 86229 deliveries  a figure that is in line with the previous year. porsche repositioned itself in 2025 and made forward-looking strategic product decisions. the delivery mix in 2025 underscores that the sports car manufacturer is consistently responding to global customer preferences by expanding its drivetrain strategy to offer combustion-engined plug-in hybrid and fully electric cars. in 2025 34.4 per cent of porsche cars delivered worldwide were electrified 7.4 percentage points with 22.2 per cent being fully electric and 12.1 per cent being plug-in hybrids. this puts the global share of fully electric vehicles at the upper end of the target range of 20 to 22 per cent for 2025. in europe for the first time more electrified cars were delivered than pure combustion-engined models 57.9 per cent electrification share"}
{"title": "x86 prefixes and escape opcodes flowchart", "url": "https://soc.me/interfaces/x86-prefixes-and-escape-opcodes-flowchart.html", "content": "x86 prefixes and escape opcodes flowchart. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["x86", "prefix", "escape", "opcode", "flowchart", "score", "author", "date"], "num_tokens": 8, "token_loss_pct": 46.67, "normalized_content": "x86 prefixes and escape opcodes flowchart. score none. author none. date none"}
{"title": "Ask HN: COBOL devs, how are AI coding affecting your work?", "url": "item?id=46678550", "content": "Ask HN: COBOL devs, how are AI coding affecting your work?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "cobol", "devs", "ai", "cod", "affect", "work", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 45.0, "normalized_content": "ask hn cobol devs how are ai coding affecting your work. score none. author none. date none"}
{"title": "Use social media mindfully", "url": "https://danielleheberling.xyz/blog/mindful-social-media/", "content": "I quit Facebook in 2020 when a former coworker was spreading misinformation about what was happening in Portland, OR. He’d never been there and had no plans to visit. I was literally living in Portland at the time, telling him what I was seeing firsthand, but that didn’t matter to him. That was it for me. I miss it sometimes, but mostly I don’t. Here’s what I’ve noticed since then: the heyday of social media feels like it’s behind us. In my opinion, Facebook peaked in 2008. Back then, it was about connecting with friends, sharing actually interesting updates about our lives. Minimal ads. It felt genuine. Now? Wannabe influencers everywhere. More ads and brand accounts in your timeline than content from people you actually know. Bots running campaigns to get engagement through false things or distortions of reality. It’s exhausting. But here’s the thing: I’m not saying abandon social media entirely. I’m saying use it differently. I’m not scrolling feeds endlessly anymore. No traps of getting lost in reels or stories. I use Buffer to schedule posts, which keeps me from even looking at a timeline. I check in with intention when I need to, then I’m out. This one’s harder than it sounds, but it makes a real difference in how much time you lose to these platforms. With that said, social media still works for connections. DMs are good. Having actual conversations in comments is good. Longer discussions where you’re genuinely exchanging ideas? Even better. This is where I think the platforms still have value if you’re intentional about it. I try to share things that might help someone else. Good articles I’ve read. Things I’m learning. Mistakes I’ve made. If it could save one person some time or frustration, it’s worth sharing. The stuff you’ve learned the hard way, the patterns you’re seeing in your day job…not to build a personal brand or chase engagement metrics, but because someone else is probably dealing with the same problems. If you’re job hunting, LinkedIn especial", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["quit", "facebook", "2020", "coworker", "spread", "misinformation", "happen", "portland", "plan", "visit", "literally", "live", "portland", "time", "tell", "see", "firsthand", "not", "matter", "miss", "not", "here", "ve", "notice", "heyday", "social", "medium", "feel", "like", "opinion", "facebook", "peak", "2008", "connect", "friend", "share", "actually", "interesting", "update", "life", "minimal", "ad", "feel", "genuine", "wannabe", "influencer", "ad", "brand", "account", "timeline", "content", "people", "actually", "know", "bot", "run", "campaign", "engagement", "false", "thing", "distortion", "reality", "exhausting", "here", "thing", "say", "abandon", "social", "medium", "entirely", "say", "use", "differently", "scroll", "feed", "endlessly", "anymore", "trap", "getting", "lose", "reel", "story", "use", "buffer", "schedule", "post", "keep", "look", "timeline", "check", "intention", "need", "one", "hard", "sound", "make", "real", "difference", "time", "lose", "platform", "say", "social", "medium", "work", "connection", "dms", "good", "have", "actual", "conversation", "comment", "good", "long", "discussion", "genuinely", "exchange", "idea", "well", "think", "platform", "value", "intentional", "try", "share", "thing", "help", "good", "article", "ve", "read", "thing", "learn", "mistake", "ve", "save", "person", "time", "frustration", "worth", "sharing", "stuff", "ve", "learn", "hard", "way", "pattern", "see", "day", "jobnot", "build", "personal", "brand", "chase", "engagement", "metric", "probably", "deal", "problem", "job", "hunting", "linkedin", "especial"], "num_tokens": 163, "token_loss_pct": 57.66, "normalized_content": "i quit facebook in 2020 when a former coworker was spreading misinformation about what was happening in portland or. hed never been there and had no plans to visit. i was literally living in portland at the time telling him what i was seeing firsthand but that didnt matter to him. that was it for me. i miss it sometimes but mostly i dont. heres what ive noticed since then the heyday of social media feels like its behind us. in my opinion facebook peaked in 2008. back then it was about connecting with friends sharing actually interesting updates about our lives. minimal ads. it felt genuine. now wannabe influencers everywhere. more ads and brand accounts in your timeline than content from people you actually know. bots running campaigns to get engagement through false things or distortions of reality. its exhausting. but heres the thing im not saying abandon social media entirely. im saying use it differently. im not scrolling feeds endlessly anymore. no traps of getting lost in reels or stories. i use buffer to schedule posts which keeps me from even looking at a timeline. i check in with intention when i need to then im out. this ones harder than it sounds but it makes a real difference in how much time you lose to these platforms. with that said social media still works for connections. dms are good. having actual conversations in comments is good. longer discussions where youre genuinely exchanging ideas even better. this is where i think the platforms still have value if youre intentional about it. i try to share things that might help someone else. good articles ive read. things im learning. mistakes ive made. if it could save one person some time or frustration its worth sharing. the stuff youve learned the hard way the patterns youre seeing in your day jobnot to build a personal brand or chase engagement metrics but because someone else is probably dealing with the same problems. if youre job hunting linkedin especial"}
{"title": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "url": "https://trendsfp.github.io/papers/tfp26-paper-12.pdf", "content": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["benchmarke", "baseline", "fully", "place", "functional", "language", "compiler", "pdf", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 47.62, "normalized_content": "benchmarking a baseline fully-in-place functional language compiler pdf. score none. author none. date none"}
{"title": "CSS Web Components for marketing sites (2024)", "url": "https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/", "content": "November 4, 2024 – @hawkticehurst Hot take: I think “regular” web components (the ones with Shadow DOM and friends) are a terrible solution for marketing website design systems. It has always left a bad taste in my mouth when I run across a web component for a swimlane, banner, card, and so on. Why? Because these are components that (unless you’re doing something mighty fancy) should never require JavaScript as a dependency. But, in the world of web components you are locked into JavaScript from the very start. To even register a web component with the browser you need JavaScript. But what if… we didn’t do that? I’ve spent a good chunk of the last year focused on marketing site design systems at work. A regular topic of discussion is the need to build marketing sites that are accessible to folks with lower powered devices and poor internet connections. How do you achieve that? In short, use less JavaScript and ideally build UI with progressive enhancement in mind. There are many ways to achieve these goals, but the method I’ve been focused on is how an HTML Web Component archictecture might be applied to implement a marketing site design system. As a quick reminder/intro, HTML Web Components is a method of building web components where you write HTML as you would normally and then wrap the parts you want to be interactive using a custom element. For example, if you wanted to create a counter button it would look like this: The markup in an HTML web component is parsed, rendered, and styled as normal HTML. That HTML will then be seamlessly hydrated once the JavaScript associated with the custom element tag is executed. In contrast, the markup of a \"regular\" web component (that uses Shadow DOM) is dynamically generated at runtime using JavaScript -- kind of like an SPA. This component architecture is a really strong candidate for a marketing design system (and, as a bonus, avoids some of the big gotchas that come with regular web components). But for all these benefit", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["november", "2024", "mention", "hot", "think", "regular", "web", "component", "one", "shadow", "dom", "friend", "terrible", "solution", "marketing", "website", "design", "system", "leave", "bad", "taste", "mouth", "run", "web", "component", "swimlane", "banner", "card", "component", "mighty", "fancy", "require", "javascript", "dependency", "world", "web", "component", "lock", "javascript", "start", "register", "web", "component", "browser", "need", "javascript", "not", "ve", "spend", "good", "chunk", "year", "focus", "marketing", "site", "design", "system", "work", "regular", "topic", "discussion", "need", "build", "marketing", "site", "accessible", "folk", "low", "power", "device", "poor", "internet", "connection", "achieve", "short", "use", "javascript", "ideally", "build", "ui", "progressive", "enhancement", "mind", "way", "achieve", "goal", "method", "ve", "focus", "html", "web", "component", "archictecture", "apply", "implement", "marketing", "site", "design", "system", "quick", "reminderintro", "html", "web", "component", "method", "building", "web", "component", "write", "html", "normally", "wrap", "part", "want", "interactive", "custom", "element", "example", "want", "create", "counter", "button", "look", "like", "markup", "html", "web", "component", "parse", "render", "style", "normal", "html", "html", "seamlessly", "hydrated", "javascript", "associate", "custom", "element", "tag", "execute", "contrast", "markup", "regular", "web", "component", "use", "shadow", "dom", "dynamically", "generate", "runtime", "javascript", "kind", "like", "spa", "component", "architecture", "strong", "candidate", "marketing", "design", "system", "bonus", "avoid", "big", "gotcha", "come", "regular", "web", "component", "benefit"], "num_tokens": 173, "token_loss_pct": 52.21, "normalized_content": "november 4 2024  mention hot take i think regular web components the ones with shadow dom and friends are a terrible solution for marketing website design systems. it has always left a bad taste in my mouth when i run across a web component for a swimlane banner card and so on. why because these are components that unless youre doing something mighty fancy should never require javascript as a dependency. but in the world of web components you are locked into javascript from the very start. to even register a web component with the browser you need javascript. but what if we didnt do that ive spent a good chunk of the last year focused on marketing site design systems at work. a regular topic of discussion is the need to build marketing sites that are accessible to folks with lower powered devices and poor internet connections. how do you achieve that in short use less javascript and ideally build ui with progressive enhancement in mind. there are many ways to achieve these goals but the method ive been focused on is how an html web component archictecture might be applied to implement a marketing site design system. as a quick reminderintro html web components is a method of building web components where you write html as you would normally and then wrap the parts you want to be interactive using a custom element. for example if you wanted to create a counter button it would look like this the markup in an html web component is parsed rendered and styled as normal html. that html will then be seamlessly hydrated once the javascript associated with the custom element tag is executed. in contrast the markup of a regular web component that uses shadow dom is dynamically generated at runtime using javascript -- kind of like an spa. this component architecture is a really strong candidate for a marketing design system and as a bonus avoids some of the big gotchas that come with regular web components. but for all these benefit"}
{"title": "The secret medieval tunnels that we still don't understand", "url": "https://weirdmedievalguys.substack.com/p/the-secret-medieval-tunnels-that", "content": "Around 2,000 strange tunnels have been found around central Europe. These aren’t like the well-known catacombs of Paris or Rome. Known as the erdstall, these passages are extremely narrow, never more than two feet (60 centimetres) wide nor high enough for an adult to walk in, and sometimes the passages become seemingly impossibly narrow, as small as 16 inches (40 centimetres) in diameter. Determining their age and purpose is made difficult by the fact that almost no archaeological evidence has been found inside any of them. A ploughshare was found in one, millstones in a couple others, but apart from that the erdstall are eerily empty. Carbon analyses of coal and pottery fragments found within point to construction dates of around 900 to 1200 AD, but no written records from the Middle Ages mention the erdstall’s existence. This clandestine treatment would have made sense had the erdstall been built as escape routes in case of invaders, but this can’t have been their purpose. They only ever have one entrance, usually located beneath the floor of a church or farmhouse, or simply under the flagstones of a town square. After an initial drop, the tunnels run for a few dozen metres, sometimes branching or dropping down to lower levels via narrow shafts. Often, the tight tunnels widen in the middle or toward the end into small chambers with rudimentary benches or shelves carved into the earth. weird medieval guys  is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber. No theory has yet been able to account for: The number and distribution of the erdstall The similarities between the many erdstall The inconvenience of accessing the erdstall The secrecy with which these tunnels were built and guarded The complete lack of artefacts found within The erdstall surely could not have been built with storage in mind, since their length and narrowness offer no advantages over a conventional and convenient cellar. And", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["2000", "strange", "tunnel", "find", "central", "europe", "not", "like", "know", "catacomb", "paris", "rome", "know", "erdstall", "passage", "extremely", "narrow", "foot", "60", "centimetre", "wide", "high", "adult", "walk", "passage", "seemingly", "impossibly", "narrow", "small", "16", "inch", "40", "centimetre", "diameter", "determine", "age", "purpose", "difficult", "fact", "archaeological", "evidence", "find", "inside", "ploughshare", "find", "millstone", "couple", "apart", "erdstall", "eerily", "carbon", "analysis", "coal", "pottery", "fragment", "find", "point", "construction", "date", "900", "1200", "ad", "write", "record", "middle", "age", "mention", "erdstall", "existence", "clandestine", "treatment", "sense", "erdstall", "build", "escape", "route", "case", "invader", "not", "purpose", "entrance", "usually", "locate", "beneath", "floor", "church", "farmhouse", "simply", "flagstone", "town", "square", "initial", "drop", "tunnel", "run", "dozen", "metre", "branch", "drop", "low", "level", "narrow", "shaft", "tight", "tunnel", "widen", "middle", "end", "small", "chamber", "rudimentary", "bench", "shelf", "carve", "earth", "weird", "medieval", "guy", "reader", "support", "publication", "receive", "new", "post", "support", "work", "consider", "free", "pay", "subscriber", "theory", "able", "account", "number", "distribution", "erdstall", "similarity", "erdstall", "inconvenience", "access", "erdstall", "secrecy", "tunnel", "build", "guard", "complete", "lack", "artefact", "find", "erdstall", "surely", "build", "storage", "mind", "length", "narrowness", "offer", "advantage", "conventional", "convenient", "cellar"], "num_tokens": 161, "token_loss_pct": 53.74, "normalized_content": "around 2000 strange tunnels have been found around central europe. these arent like the well-known catacombs of paris or rome. known as the erdstall these passages are extremely narrow never more than two feet 60 centimetres wide nor high enough for an adult to walk in and sometimes the passages become seemingly impossibly narrow as small as 16 inches 40 centimetres in diameter. determining their age and purpose is made difficult by the fact that almost no archaeological evidence has been found inside any of them. a ploughshare was found in one millstones in a couple others but apart from that the erdstall are eerily empty. carbon analyses of coal and pottery fragments found within point to construction dates of around 900 to 1200 ad but no written records from the middle ages mention the erdstalls existence. this clandestine treatment would have made sense had the erdstall been built as escape routes in case of invaders but this cant have been their purpose. they only ever have one entrance usually located beneath the floor of a church or farmhouse or simply under the flagstones of a town square. after an initial drop the tunnels run for a few dozen metres sometimes branching or dropping down to lower levels via narrow shafts. often the tight tunnels widen in the middle or toward the end into small chambers with rudimentary benches or shelves carved into the earth. weird medieval guys is a reader-supported publication. to receive new posts and support my work consider becoming a free or paid subscriber. no theory has yet been able to account for the number and distribution of the erdstall the similarities between the many erdstall the inconvenience of accessing the erdstall the secrecy with which these tunnels were built and guarded the complete lack of artefacts found within the erdstall surely could not have been built with storage in mind since their length and narrowness offer no advantages over a conventional and convenient cellar. and"}
{"title": "Ask HN: Which common map projections make Greenland look smaller?", "url": "item?id=46694929", "content": "Ask HN: Which common map projections make Greenland look smaller?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "common", "map", "projection", "greenland", "look", "small", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 42.11, "normalized_content": "ask hn which common map projections make greenland look smaller. score none. author none. date none"}
{"title": "The coming industrialisation of exploit generation with LLMs", "url": "https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/", "content": "Recently I ran an experiment where I built agents on top of Opus 4.5 and GPT-5.2 and then challenged them to write exploits for a zeroday vulnerability in the QuickJS Javascript interpreter. I added a variety of modern exploit mitigations, various constraints (like assuming an unknown heap starting state, or forbidding hardcoded offsets in the exploits) and different objectives (spawn a shell, write a file, connect back to a command and control server). The agents succeeded in building over 40 distinct exploits across 6 different scenarios, and GPT-5.2 solved every scenario. Opus 4.5 solved all but two. I’ve put a technical write-up of the experiments and the results on Github , as well as the code to reproduce the experiments. In this post I’m going to focus on the main conclusion I’ve drawn from this work, which is that we should prepare for the industrialisation of many of the constituent parts of offensive cyber security. We should start assuming that in the near future the limiting factor on a state or group’s ability to develop exploits, break into networks, escalate privileges and remain in those networks, is going to be their token throughput over time, and not the number of hackers they employ. Nothing is certain, but we would be better off having wasted effort thinking through this scenario and have it not happen, than be unprepared if it does. A Brief Overview of the Experiment All of the code to re-run the experiments, a detailed write-up of them, and the raw data the agents produced are on Github , but just to give a flavour of what the agents accomplished: Before going on there are two important caveats that need to be kept in mind with these experiments: The Industrialisation of Intrusion By ‘industrialisation’ I mean that the ability of an organisation to complete a task will be limited by the number of tokens they can throw at that task. In order for a task to be ‘industrialised’ in this way it needs two things: Exploit development is the ideal case", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recently", "run", "experiment", "build", "agent", "opus", "4.5", "gpt-5.2", "challenge", "write", "exploit", "zeroday", "vulnerability", "quickjs", "javascript", "interpreter", "add", "variety", "modern", "exploit", "mitigation", "constraint", "like", "assume", "unknown", "heap", "start", "state", "forbid", "hardcode", "offset", "exploit", "different", "objective", "spawn", "shell", "write", "file", "connect", "command", "control", "server", "agent", "succeed", "build", "40", "distinct", "exploit", "different", "scenario", "gpt-5.2", "solve", "scenario", "opus", "4.5", "solve", "ve", "technical", "write", "experiment", "result", "github", "code", "reproduce", "experiment", "post", "go", "focus", "main", "conclusion", "ve", "draw", "work", "prepare", "industrialisation", "constituent", "part", "offensive", "cyber", "security", "start", "assume", "near", "future", "limit", "factor", "state", "group", "ability", "develop", "exploit", "break", "network", "escalate", "privilege", "remain", "network", "go", "token", "throughput", "time", "number", "hacker", "employ", "certain", "well", "having", "waste", "effort", "think", "scenario", "happen", "unprepared", "brief", "overview", "experiment", "code", "run", "experiment", "detailed", "write", "raw", "datum", "agent", "produce", "github", "flavour", "agent", "accomplish", "go", "important", "caveat", "need", "keep", "mind", "experiment", "industrialisation", "intrusion", "industrialisation", "mean", "ability", "organisation", "complete", "task", "limit", "number", "token", "throw", "task", "order", "task", "industrialise", "way", "need", "thing", "exploit", "development", "ideal", "case"], "num_tokens": 159, "token_loss_pct": 55.83, "normalized_content": "recently i ran an experiment where i built agents on top of opus 4.5 and gpt-5.2 and then challenged them to write exploits for a zeroday vulnerability in the quickjs javascript interpreter. i added a variety of modern exploit mitigations various constraints like assuming an unknown heap starting state or forbidding hardcoded offsets in the exploits and different objectives spawn a shell write a file connect back to a command and control server. the agents succeeded in building over 40 distinct exploits across 6 different scenarios and gpt-5.2 solved every scenario. opus 4.5 solved all but two. ive put a technical write-up of the experiments and the results on github  as well as the code to reproduce the experiments. in this post im going to focus on the main conclusion ive drawn from this work which is that we should prepare for the industrialisation of many of the constituent parts of offensive cyber security. we should start assuming that in the near future the limiting factor on a state or groups ability to develop exploits break into networks escalate privileges and remain in those networks is going to be their token throughput over time and not the number of hackers they employ. nothing is certain but we would be better off having wasted effort thinking through this scenario and have it not happen than be unprepared if it does. a brief overview of the experiment all of the code to re-run the experiments a detailed write-up of them and the raw data the agents produced are on github  but just to give a flavour of what the agents accomplished before going on there are two important caveats that need to be kept in mind with these experiments the industrialisation of intrusion by industrialisation i mean that the ability of an organisation to complete a task will be limited by the number of tokens they can throw at that task. in order for a task to be industrialised in this way it needs two things exploit development is the ideal case"}
{"title": "Squishy Go", "url": "https://puyogo.app/en/", "content": "Black and White Player, take turns placing stones. You can also pass your turn if you want. Stone disappears when the stones are enclosed by the opponent stones. You cannot create a position that has occurred previously in the game. Game ends when both player passes their turn. The player with most stones wins the game. SquishyGo has cute faces on their stones and helps understand the way of playing Go. You can enjoy a game against AI in a short time using small Go board, 5x5 or 7x7. The rules of SquishyGo are also simplified for beginners that is based on Jungo Rule. Blog Privacy Policy Jungo website", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["black", "white", "player", "turn", "place", "stone", "pass", "turn", "want", "stone", "disappear", "stone", "enclose", "opponent", "stone", "create", "position", "occur", "previously", "game", "game", "end", "player", "pass", "turn", "player", "stone", "win", "game", "squishygo", "cute", "face", "stone", "help", "understand", "way", "play", "enjoy", "game", "ai", "short", "time", "small", "board", "5x5", "7x7", "rule", "squishygo", "simplify", "beginner", "base", "jungo", "rule", "blog", "privacy", "policy", "jungo", "website"], "num_tokens": 58, "token_loss_pct": 51.26, "normalized_content": "black and white player take turns placing stones. you can also pass your turn if you want. stone disappears when the stones are enclosed by the opponent stones. you cannot create a position that has occurred previously in the game. game ends when both player passes their turn. the player with most stones wins the game. squishygo has cute faces on their stones and helps understand the way of playing go. you can enjoy a game against ai in a short time using small go board 5x5 or 7x7. the rules of squishygo are also simplified for beginners that is based on jungo rule. blog privacy policy jungo website"}
{"title": "De-dollarization: Is the US dollar losing its dominance? (2025)", "url": "https://www.jpmorgan.com/insights/global-research/currencies/de-dollarization", "content": "Mainland China Japan Korea  Russia Turkey  Argentina Brazil Chile Colombia Mexico Peru   Canada  For Companies and Institutions Key Links  For Individuals Key Links  Who We Serve Key Links  Explore a variety of insights. Key Links  Insights by Topic Explore a variety of insights organized by different topics. Key Links  Insights by Type Explore a variety of insights organized by different types of content and media. Key Links  We aim to be the most respected financial services firm in the world, serving corporations and individuals in more than 100 countries. Key Links Global Research July 01, 2025 Top dollar no more? Learn more about the factors threatening the dominance of the world’s reserve currency. Overview The U.S. dollar is the world’s primary reserve currency, and it is also the most widely used currency for trade and other international transactions. However, its hegemony has come into question in recent times due to geopolitical and geostrategic shifts. As a result, de-dollarization has increasingly become a substantive topic of discussion among investors, corporates and market participants more broadly. What are the potential implications of de-dollarization, and how is it playing out in global markets and trade? In short, de-dollarization entails a significant reduction in the use of dollars in world trade and financial transactions, decreasing national, institutional and corporate demand for the greenback. “The concept of de-dollarization relates to changes in the structural demand for the dollar that would relate to its status as a reserve currency. This encompasses areas that relate to the longer-term use of the dollar, such as transactional dominance in FX volumes or commodities trade, denomination of liabilities and share in central bank FX reserves,” said Luis Oganes, head of Global Macro Research at J.P. Morgan. Importantly, this structural shift is distinct from the cyclical demand for the greenback, which is shorter term and has in recent time", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["mainland", "china", "japan", "korea", "russia", "turkey", "argentina", "brazil", "chile", "colombia", "mexico", "peru", "canada", "company", "institution", "key", "link", "individual", "key", "link", "serve", "key", "link", "explore", "variety", "insight", "key", "link", "insight", "topic", "explore", "variety", "insight", "organize", "different", "topic", "key", "link", "insight", "type", "explore", "variety", "insight", "organize", "different", "type", "content", "medium", "key", "link", "aim", "respected", "financial", "service", "firm", "world", "serve", "corporation", "individual", "100", "country", "key", "link", "global", "research", "july", "01", "2025", "dollar", "learn", "factor", "threaten", "dominance", "worlds", "reserve", "currency", "overview", "u.s", "dollar", "world", "primary", "reserve", "currency", "widely", "currency", "trade", "international", "transaction", "hegemony", "come", "question", "recent", "time", "geopolitical", "geostrategic", "shift", "result", "de", "dollarization", "increasingly", "substantive", "topic", "discussion", "investor", "corporate", "market", "participant", "broadly", "potential", "implication", "de", "dollarization", "play", "global", "market", "trade", "short", "de", "dollarization", "entail", "significant", "reduction", "use", "dollar", "world", "trade", "financial", "transaction", "decrease", "national", "institutional", "corporate", "demand", "greenback", "concept", "de", "dollarization", "relate", "change", "structural", "demand", "dollar", "relate", "status", "reserve", "currency", "encompass", "area", "relate", "long", "term", "use", "dollar", "transactional", "dominance", "fx", "volume", "commodity", "trade", "denomination", "liability", "share", "central", "bank", "fx", "reserve", "say", "luis", "oganes", "head", "global", "macro", "research", "j.p", "morgan", "importantly", "structural", "shift", "distinct", "cyclical", "demand", "greenback", "short", "term", "recent", "time"], "num_tokens": 186, "token_loss_pct": 43.81, "normalized_content": "mainland china japan korea russia turkey argentina brazil chile colombia mexico peru canada for companies and institutions key links for individuals key links who we serve key links explore a variety of insights. key links insights by topic explore a variety of insights organized by different topics. key links insights by type explore a variety of insights organized by different types of content and media. key links we aim to be the most respected financial services firm in the world serving corporations and individuals in more than 100 countries. key links global research july 01 2025 top dollar no more learn more about the factors threatening the dominance of the worlds reserve currency. overview the u.s. dollar is the worlds primary reserve currency and it is also the most widely used currency for trade and other international transactions. however its hegemony has come into question in recent times due to geopolitical and geostrategic shifts. as a result de-dollarization has increasingly become a substantive topic of discussion among investors corporates and market participants more broadly. what are the potential implications of de-dollarization and how is it playing out in global markets and trade in short de-dollarization entails a significant reduction in the use of dollars in world trade and financial transactions decreasing national institutional and corporate demand for the greenback. the concept of de-dollarization relates to changes in the structural demand for the dollar that would relate to its status as a reserve currency. this encompasses areas that relate to the longer-term use of the dollar such as transactional dominance in fx volumes or commodities trade denomination of liabilities and share in central bank fx reserves said luis oganes head of global macro research at j.p. morgan. importantly this structural shift is distinct from the cyclical demand for the greenback which is shorter term and has in recent time"}
{"title": "The world of Japanese snack bars", "url": "https://www.bbc.com/travel/article/20260116-inside-the-secret-world-of-japanese-snack-bars", "content": "Some 100,000 of these small dives are hidden in plain sight across Japan. Now, travellers are finally discovering these locals-only hangouts – and the beloved \"mamas\" who run them. I didn't plan on having my fortune read by a matchmaking \"mama\" on my most recent visit to Tokyo. But after climbing to the second floor of a cozy sunakku (snack bar) called Aeru in the Shinbashi neighbourhood, the proprietress and owner, Urara, smiled coyly as she pulled a Knight of Wands from her tarot deck. \"You're craving passion and protection… in a man,\" Urara told me, as I nibbled chilli-flavoured rice crackers and deep-fried dough sticks slathered in brown sugar. \"I'll be sure to let my husband know that,\" I replied with a wry smile. As she thumbed through a three-ring binder filled with the handwritten profiles of Japanese singles in their 20s and 30s, Urara explained that she has successfully matched more than 90 couples during the 14 years she has worked here. While her tarot readings and modern matchmaking techniques are unique among Japan's tens of thousands of snack bars, Urara embodies what makes these small venues so distinct. Usually run by an older woman known affectionately as a mama-san , snack bars are nondescript, no-frills bars serving light bites and drinks. But as I soon learned, their main purpose isn't food or booze; it's to create a space where patrons feel comfortable enough to open up, engage in meaningful conversation and genuinely connect with the mama-san who presides over the room. \"Unlike the bars or nightclubs many tourists may imagine, snack bars are warm, home-like places,\" said Mayuko Igarashi, president and director of Snack Yokocho Culture Inc , which has been offering tours of snack bars across Japan for travellers since 2021. \"The 'mama'… welcomes guests with a sense of personal care.\" A far cry from the pricey \"hostess clubs\" found in entertainment districts like Kabukichō in Shinjuku, where young women are paid generously to pour drinks and fli", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["100000", "small", "dive", "hide", "plain", "sight", "japan", "traveller", "finally", "discover", "local", "hangout", "beloved", "mama", "run", "plan", "have", "fortune", "read", "matchmaking", "mama", "recent", "visit", "tokyo", "climb", "second", "floor", "cozy", "sunakku", "snack", "bar", "call", "aeru", "shinbashi", "neighbourhood", "proprietress", "owner", "urara", "smile", "coyly", "pull", "knight", "wand", "tarot", "deck", "crave", "passion", "protection", "man", "urara", "tell", "nibble", "chilli", "flavour", "rice", "cracker", "deep", "fry", "dough", "stick", "slather", "brown", "sugar", "sure", "let", "husband", "know", "reply", "wry", "smile", "thumb", "ring", "binder", "fill", "handwritten", "profile", "japanese", "single", "20", "30", "urara", "explain", "successfully", "match", "90", "couple", "14", "year", "work", "tarot", "reading", "modern", "matchmaking", "technique", "unique", "japan", "ten", "thousand", "snack", "bar", "urara", "embodie", "make", "small", "venue", "distinct", "usually", "run", "old", "woman", "know", "affectionately", "mama", "san", "snack", "bar", "nondescript", "frill", "bar", "serve", "light", "bite", "drink", "soon", "learn", "main", "purpose", "food", "booze", "create", "space", "patron", "feel", "comfortable", "open", "engage", "meaningful", "conversation", "genuinely", "connect", "mama", "san", "preside", "room", "unlike", "bar", "nightclub", "tourist", "imagine", "snack", "bar", "warm", "home", "like", "place", "say", "mayuko", "igarashi", "president", "director", "snack", "yokocho", "culture", "inc", "offer", "tour", "snack", "bar", "japan", "traveller", "2021", "mama", "welcome", "guest", "sense", "personal", "care", "far", "cry", "pricey", "hostess", "club", "find", "entertainment", "district", "like", "kabukichō", "shinjuku", "young", "woman", "pay", "generously", "pour", "drink", "fli"], "num_tokens": 195, "token_loss_pct": 47.01, "normalized_content": "some 100000 of these small dives are hidden in plain sight across japan. now travellers are finally discovering these locals-only hangouts  and the beloved mamas who run them. i didn't plan on having my fortune read by a matchmaking mama on my most recent visit to tokyo. but after climbing to the second floor of a cozy sunakku snack bar called aeru in the shinbashi neighbourhood the proprietress and owner urara smiled coyly as she pulled a knight of wands from her tarot deck. you're craving passion and protection in a man urara told me as i nibbled chilli-flavoured rice crackers and deep-fried dough sticks slathered in brown sugar. i'll be sure to let my husband know that i replied with a wry smile. as she thumbed through a three-ring binder filled with the handwritten profiles of japanese singles in their 20s and 30s urara explained that she has successfully matched more than 90 couples during the 14 years she has worked here. while her tarot readings and modern matchmaking techniques are unique among japan's tens of thousands of snack bars urara embodies what makes these small venues so distinct. usually run by an older woman known affectionately as a mama-san  snack bars are nondescript no-frills bars serving light bites and drinks. but as i soon learned their main purpose isn't food or booze it's to create a space where patrons feel comfortable enough to open up engage in meaningful conversation and genuinely connect with the mama-san who presides over the room. unlike the bars or nightclubs many tourists may imagine snack bars are warm home-like places said mayuko igarashi president and director of snack yokocho culture inc  which has been offering tours of snack bars across japan for travellers since 2021. the 'mama' welcomes guests with a sense of personal care. a far cry from the pricey hostess clubs found in entertainment districts like kabukichō in shinjuku where young women are paid generously to pour drinks and fli"}
{"title": "Rzweb: A complete browser-based reverse engineering platform", "url": "https://github.com/IndAlok/rzweb", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . A complete browser-based reverse engineering platform built on Rizin, running entirely client-side via WebAssembly. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . A browser-based reverse engineering platform that runs Rizin entirely in your browser through WebAssembly. No installations, no uploads, no servers - just drop a binary and start analyzing. Homepage - Drop a binary and start analyzing  Terminal - Full Rizin CLI access  Disassembly - Syntax-highlighted assembly view  Control Flow Graph - Visual function structure  Hex Dump - Raw byte inspection  Strings - Extracted strings from the binary  RzWeb brings the full power of Rizin to your browser. You get a complete terminal where you can run any Rizin command, plus dedicated views for disassembly, control flow graphs, hex dumps, and strings. Everything processes locally on your machine - your files never leave your device. The integrated terminal gives you direct access to Rizin's CLI. Run pdf to disassemble a function, afl to list all functions, px to dump hex, or any other command you would use in a normal Rizin session. Commands can be chained with semicolons like s main;pdf . Syntax-highlighted assembly with address navigation. Click on addresses to jump around, see cross-references, and track your current position in the binary. Visual representation of function structure. See how basic blocks connect, identify loops, and understand the control flow at a glance. Raw byte inspection. Navigate to any offset and examine the binary data directly. Automatically extracted strings from the binary. Useful for finding hardcoded paths, error messages, encryption keys, and other interesting data. RzWeb supports everything Rizin supports: All analysis happens in your browser. The binary is loaded into WebAssembly memory an", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "complete", "browser", "base", "reverse", "engineering", "platform", "build", "rizin", "run", "entirely", "client", "webassembly", "error", "load", "reload", "page", "error", "load", "reload", "page", "browser", "base", "reverse", "engineering", "platform", "run", "rizin", "entirely", "browser", "webassembly", "installation", "upload", "server", "drop", "binary", "start", "analyze", "homepage", "drop", "binary", "start", "analyze", "terminal", "rizin", "cli", "access", "disassembly", "syntax", "highlight", "assembly", "view", "control", "flow", "graph", "visual", "function", "structure", "hex", "dump", "raw", "byte", "inspection", "string", "extract", "string", "binary", "rzweb", "bring", "power", "rizin", "browser", "complete", "terminal", "run", "rizin", "command", "plus", "dedicated", "view", "disassembly", "control", "flow", "graph", "hex", "dump", "string", "process", "locally", "machine", "file", "leave", "device", "integrated", "terminal", "give", "direct", "access", "rizin", "cli", "run", "pdf", "disassemble", "function", "afl", "list", "function", "px", "dump", "hex", "command", "use", "normal", "rizin", "session", "command", "chain", "semicolon", "like", "mainpdf", "syntax", "highlight", "assembly", "address", "navigation", "click", "address", "jump", "cross", "reference", "track", "current", "position", "binary", "visual", "representation", "function", "structure", "basic", "block", "connect", "identify", "loop", "understand", "control", "flow", "glance", "raw", "byte", "inspection", "navigate", "offset", "examine", "binary", "datum", "directly", "automatically", "extract", "string", "binary", "useful", "find", "hardcode", "path", "error", "message", "encryption", "key", "interesting", "datum", "rzweb", "support", "rizin", "support", "analysis", "happen", "browser", "binary", "load", "webassembly", "memory"], "num_tokens": 188, "token_loss_pct": 45.19, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . a complete browser-based reverse engineering platform built on rizin running entirely client-side via webassembly. there was an error while loading. please reload this page . there was an error while loading. please reload this page . a browser-based reverse engineering platform that runs rizin entirely in your browser through webassembly. no installations no uploads no servers - just drop a binary and start analyzing. homepage - drop a binary and start analyzing terminal - full rizin cli access disassembly - syntax-highlighted assembly view control flow graph - visual function structure hex dump - raw byte inspection strings - extracted strings from the binary rzweb brings the full power of rizin to your browser. you get a complete terminal where you can run any rizin command plus dedicated views for disassembly control flow graphs hex dumps and strings. everything processes locally on your machine - your files never leave your device. the integrated terminal gives you direct access to rizin's cli. run pdf to disassemble a function afl to list all functions px to dump hex or any other command you would use in a normal rizin session. commands can be chained with semicolons like s mainpdf . syntax-highlighted assembly with address navigation. click on addresses to jump around see cross-references and track your current position in the binary. visual representation of function structure. see how basic blocks connect identify loops and understand the control flow at a glance. raw byte inspection. navigate to any offset and examine the binary data directly. automatically extracted strings from the binary. useful for finding hardcoded paths error messages encryption keys and other interesting data. rzweb supports everything rizin supports all analysis happens in your browser. the binary is loaded into webassembly memory an"}
{"title": "Radboud University selects Fairphone as standard smartphone for employees", "url": "https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees", "content": "Do you require a (replacement) smartphone for your work at Radboud University? If so, there is a strong possibility that you will receive a Fairphone from 1 February 2026 onwards. Radboud University has decided to choose Fairphone as its standard company smartphone model for reasons of sustainability, cost efficiency and management support. The Fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. This makes the device last longer. Fair and recycled materials, such as plastic and aluminium, are used as much as possible in the production of this smartphone. Fairphone also pays attention to good and safe working conditions in its factories. Fairphones are issued to employees by the Information & Library Services (ILS) division. In addition to new Fairphones, the university can also reissue used Samsung devices where possible. These are Samsung devices that have already been returned and still meet the technical and age requirements. As long as these devices are still available, not every employee will receive a Fairphone immediately. Employees who have an iPhone from Radboud University can continue to use it as long as the device is still functioning. However, returned iPhones will no longer be reissued. Employees who prefer to use their private phone for work can request an RU SIM card for this purpose. The costs for using your own device will not be reimbursed. Naturally, smartphone models that have already been issued will continue to be supported by ILS colleagues, as will privately purchased smartphone models used for work. Due to its longer lifespan, the total cost of a Fairphone is lower than that of comparable devices. In addition, Radboud University only needs to purchase, manage and support one standard model. This results in smaller stock, easier management and faster support. Manuals and instructions also only need to be maintained for one device. Furthermore, less investment is required in knowledge of differe", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["require", "replacement", "smartphone", "work", "radboud", "university", "strong", "possibility", "receive", "fairphone", "february", "2026", "onwards", "radboud", "university", "decide", "choose", "fairphone", "standard", "company", "smartphone", "model", "reason", "sustainability", "cost", "efficiency", "management", "support", "fairphone", "sustainable", "smartphone", "easily", "replaceable", "part", "battery", "screen", "make", "device", "long", "fair", "recycle", "material", "plastic", "aluminium", "possible", "production", "smartphone", "fairphone", "pay", "attention", "good", "safe", "work", "condition", "factory", "fairphone", "issue", "employee", "information", "library", "service", "il", "division", "addition", "new", "fairphone", "university", "reissue", "samsung", "device", "possible", "samsung", "device", "return", "meet", "technical", "age", "requirement", "long", "device", "available", "employee", "receive", "fairphone", "immediately", "employee", "iphone", "radboud", "university", "continue", "use", "long", "device", "function", "return", "iphone", "long", "reissue", "employee", "prefer", "use", "private", "phone", "work", "request", "ru", "sim", "card", "purpose", "cost", "device", "reimburse", "naturally", "smartphone", "model", "issue", "continue", "support", "il", "colleague", "privately", "purchase", "smartphone", "model", "work", "long", "lifespan", "total", "cost", "fairphone", "low", "comparable", "device", "addition", "radboud", "university", "need", "purchase", "manage", "support", "standard", "model", "result", "small", "stock", "easy", "management", "fast", "support", "manual", "instruction", "need", "maintain", "device", "furthermore", "investment", "require", "knowledge", "differe"], "num_tokens": 159, "token_loss_pct": 52.54, "normalized_content": "do you require a replacement smartphone for your work at radboud university if so there is a strong possibility that you will receive a fairphone from 1 february 2026 onwards. radboud university has decided to choose fairphone as its standard company smartphone model for reasons of sustainability cost efficiency and management support. the fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. this makes the device last longer. fair and recycled materials such as plastic and aluminium are used as much as possible in the production of this smartphone. fairphone also pays attention to good and safe working conditions in its factories. fairphones are issued to employees by the information  library services ils division. in addition to new fairphones the university can also reissue used samsung devices where possible. these are samsung devices that have already been returned and still meet the technical and age requirements. as long as these devices are still available not every employee will receive a fairphone immediately. employees who have an iphone from radboud university can continue to use it as long as the device is still functioning. however returned iphones will no longer be reissued. employees who prefer to use their private phone for work can request an ru sim card for this purpose. the costs for using your own device will not be reimbursed. naturally smartphone models that have already been issued will continue to be supported by ils colleagues as will privately purchased smartphone models used for work. due to its longer lifespan the total cost of a fairphone is lower than that of comparable devices. in addition radboud university only needs to purchase manage and support one standard model. this results in smaller stock easier management and faster support. manuals and instructions also only need to be maintained for one device. furthermore less investment is required in knowledge of differe"}
{"title": "Notes on Apple's Nano Texture (2025)", "url": "https://jon.bo/posts/nano-texture/", "content": "TLDR: the Nano Texture performs wonderfully anywhere where light used to be a factor and used to force me to shade my screen or avoid the place entirely. Big thanks to Julie Kruger for the comparison photos and CJ for draft feedback. A few months after I got the Daylight Computer ( read my thoughts here ), two friends sent me this post comparing the old Macbook Pro displays to the new Nano Texture glass ones. That post convinced me to upgrade my computer in short order, to the dismay of my wallet. In the four months I’ve had it I’ve told at least a dozen people about it, and I’m gonna keep telling people. Being able to take my entire computing environment to places without being worried about glare has expanded the range of environments I can create in. It means I get to be in environments that are more interesting, fun, and in tune with my body. What follows are some thoughts about how this display has fit into my day to day life in the couple of months I’ve had it. Typical matt displays have a coating added to their surface that scatters light. However, these coatings lower contrast while producing unwanted haze and sparkle. Etched into the glass at the nanometre level, the nano-texture scatters light to further minimise glare — for outstanding image quality even in challenging lighting conditions. https://www.apple.com/uk/shop/buy-mac/apple-studio-display/nano-texture-glass-tilt-adjustable-stand Basically, it’s a coating physically etched into the screen that reflects light differently from the glossy finish of the traditional screen. First off, this isn’t apples to oranges - these are different technologies that in my mind, serve a different purpose. The Daylight Computer is an Android tablet, the Macbook Pro is a full MacOS laptop. The transflective LCD in the Daylight Computer is grayscale but it needs no light to function. It has a backlight, but where it does really well is in direct sunlight with the backlight turned off. When outside in direct sunlight, to", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["tldr", "nano", "texture", "perform", "wonderfully", "light", "factor", "force", "shade", "screen", "avoid", "place", "entirely", "big", "thank", "julie", "kruger", "comparison", "photo", "cj", "draft", "feedback", "month", "get", "daylight", "computer", "read", "thought", "friend", "send", "post", "compare", "old", "macbook", "pro", "display", "new", "nano", "texture", "glass", "one", "post", "convince", "upgrade", "computer", "short", "order", "dismay", "wallet", "month", "ve", "ve", "tell", "dozen", "people", "go", "to", "tell", "people", "able", "entire", "computing", "environment", "place", "worried", "glare", "expand", "range", "environment", "create", "mean", "environment", "interesting", "fun", "tune", "body", "follow", "thought", "display", "fit", "day", "day", "life", "couple", "month", "ve", "typical", "matt", "display", "coating", "add", "surface", "scatter", "light", "coating", "low", "contrast", "produce", "unwanted", "haze", "sparkle", "etch", "glass", "nanometre", "level", "nano", "texture", "scatter", "light", "minimise", "glare", "outstanding", "image", "quality", "challenge", "lighting", "condition", "url", "basically", "coating", "physically", "etch", "screen", "reflect", "light", "differently", "glossy", "finish", "traditional", "screen", "not", "apple", "orange", "different", "technology", "mind", "serve", "different", "purpose", "daylight", "computer", "android", "tablet", "macbook", "pro", "macos", "laptop", "transflective", "lcd", "daylight", "computer", "grayscale", "need", "light", "function", "backlight", "direct", "sunlight", "backlight", "turn", "outside", "direct", "sunlight"], "num_tokens": 163, "token_loss_pct": 54.21, "normalized_content": "tldr the nano texture performs wonderfully anywhere where light used to be a factor and used to force me to shade my screen or avoid the place entirely. big thanks to julie kruger for the comparison photos and cj for draft feedback. a few months after i got the daylight computer  read my thoughts here  two friends sent me this post comparing the old macbook pro displays to the new nano texture glass ones. that post convinced me to upgrade my computer in short order to the dismay of my wallet. in the four months ive had it ive told at least a dozen people about it and im gonna keep telling people. being able to take my entire computing environment to places without being worried about glare has expanded the range of environments i can create in. it means i get to be in environments that are more interesting fun and in tune with my body. what follows are some thoughts about how this display has fit into my day to day life in the couple of months ive had it. typical matt displays have a coating added to their surface that scatters light. however these coatings lower contrast while producing unwanted haze and sparkle. etched into the glass at the nanometre level the nano-texture scatters light to further minimise glare  for outstanding image quality even in challenging lighting conditions. url basically its a coating physically etched into the screen that reflects light differently from the glossy finish of the traditional screen. first off this isnt apples to oranges - these are different technologies that in my mind serve a different purpose. the daylight computer is an android tablet the macbook pro is a full macos laptop. the transflective lcd in the daylight computer is grayscale but it needs no light to function. it has a backlight but where it does really well is in direct sunlight with the backlight turned off. when outside in direct sunlight to"}
{"title": "How we made Python's packaging library 3x faster", "url": "https://iscinumpy.dev/post/packaging-faster/", "content": "Along with a pip (and now packaging ) maintainer, Damian Shaw, I have\nbeen working on making packaging , the library behind almost all packaging\nrelated tools, faster at reading versions and specifiers, something tools like\npip have to do thousands of times during resolution. Using Python 3.15’s new\nstatistical profiler and metadata from every package ever uploaded to PyPI, I\nmeasured and improved core Packaging constructs while keeping the code readable\nand simple. Reading in Version s can be up to 2x faster and SpecifierSet s can\nbe up to 3x faster in packaging 26.0 , now released! Other\noperations have been optimized, as well, up to 5x in some cases. See the announcement and release notes too; this post will focus on the\nperformance work only. packaging is the core library used by most tools for Python to deal with many\nof the standardized packaging constructs, like versions, specifiers, markers,\nand the like. It is the 11th most downloaded library, but if you also take into\naccount that it is vendored into pip, meaning you get a (hidden) copy with every\npip install, it’s actually the 2nd most downloaded library. Given that pip is\nvendored into Python, everyone who has Python has packaging , unless their\ndistro strips it out into a separate package; so it is possible it is the most\ncommon third party Python library in the world. In packaging, a Version is something that follows PEP 440 ’s version\nstandard. And a SpecifierSet is conditions on that version; think >=2,<3 or ~=1.0 , those are SpecifierSet s. They are used on dependencies, on requires-python , etc. They are also part of Marker s, that is, something like tomli; python_version < '3.11' (a Requirement ) contains a Marker . I’d like to start by showing you the progress we’ve made as a series of plots;\nif you’d like to see how we made some of these, I’ll follow with in-depth\nexamples. After most of the performance PRs were made, I finally invested a little time\ninto making a proper set of micro-benchmarks", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["pip", "package", "maintainer", "damian", "shaw", "work", "make", "packaging", "library", "packaging", "relate", "tool", "fast", "read", "version", "specifier", "tool", "like", "pip", "thousand", "time", "resolution", "python", "3.15s", "new", "statistical", "profiler", "metadata", "package", "upload", "pypi", "measure", "improve", "core", "packaging", "construct", "keep", "code", "readable", "simple", "read", "version", "2x", "fast", "specifierset", "3x", "fast", "packaging", "26.0", "release", "operation", "optimize", "5x", "case", "announcement", "release", "note", "post", "focus", "performance", "work", "packaging", "core", "library", "tool", "python", "deal", "standardized", "packaging", "construct", "like", "version", "specifier", "marker", "like", "11th", "download", "library", "account", "vendore", "pip", "meaning", "hidden", "copy", "pip", "install", "actually", "2nd", "download", "library", "give", "pip", "vendore", "python", "python", "packaging", "distro", "strip", "separate", "package", "possible", "common", "party", "python", "library", "world", "package", "version", "follow", "pep", "440", "version", "standard", "specifierset", "condition", "version", "think", "23", "1.0", "specifierset", "s.", "dependency", "require", "python", "etc", "marker", "like", "tomli", "python_version", "3.11", "requirement", "contain", "marker", "like", "start", "show", "progress", "ve", "series", "plot", "like", "ill", "follow", "depth", "example", "performance", "prs", "finally", "invest", "little", "time", "make", "proper", "set", "micro", "benchmark"], "num_tokens": 156, "token_loss_pct": 57.49, "normalized_content": "along with a pip and now packaging  maintainer damian shaw i have been working on making packaging  the library behind almost all packaging related tools faster at reading versions and specifiers something tools like pip have to do thousands of times during resolution. using python 3.15s new statistical profiler and metadata from every package ever uploaded to pypi i measured and improved core packaging constructs while keeping the code readable and simple. reading in version s can be up to 2x faster and specifierset s can be up to 3x faster in packaging 26.0  now released other operations have been optimized as well up to 5x in some cases. see the announcement and release notes too this post will focus on the performance work only. packaging is the core library used by most tools for python to deal with many of the standardized packaging constructs like versions specifiers markers and the like. it is the 11th most downloaded library but if you also take into account that it is vendored into pip meaning you get a hidden copy with every pip install its actually the 2nd most downloaded library. given that pip is vendored into python everyone who has python has packaging  unless their distro strips it out into a separate package so it is possible it is the most common third party python library in the world. in packaging a version is something that follows pep 440 s version standard. and a specifierset is conditions on that version think 23 or 1.0  those are specifierset s. they are used on dependencies on requires-python  etc. they are also part of marker s that is something like tomli python_version  '3.11' a requirement  contains a marker . id like to start by showing you the progress weve made as a series of plots if youd like to see how we made some of these ill follow with in-depth examples. after most of the performance prs were made i finally invested a little time into making a proper set of micro-benchmarks"}
{"title": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "url": "https://github.com/minimaxir/ballin", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . A colorful interactive physics simulator with thousands of balls, but in your terminal! There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  Crates.io A colorful interactive physics simulator with thousands of balls, but in your terminal! ballin is a fun TUI app written in Rust that simulates thousands of logical balls, but despite the inherent character constraints of a terminal, you can see the realistic physics of the balls in action: Watch the color explosion in action! Disclosure: This crate was developed with the assistance of Claude Opus 4.5 initially to answer the shower thought \"would the Braille Unicode trick work to visually simulate complex ball physics in a terminal?\" Opus 4.5 one-shot the problem , so I decided to further experiment to make it more fun and colorful. The full list of prompts used with Claude Code is present in PROMPTS.md . The app binaries can be downloaded from the Releases page for your platform of choice, or by using the following terminal commands: For Windows, download and unzip the binary from here . If Rust is installed, you can install the crate directly via cargo : It is VERY strongly recommended to use a terminal emulator such as Ghostty as normal terminals may have their frame rates capped at 30 FPS and the output looks choppy. The app looks comparatively poor in the native macOS Terminal.app, for example. To run ballin : if you downloaded the binary, run it in the terminal with ./ballin . If you installed via Rust, run cargo run . The physics simulation will start with some shape objects randomly present to let the hilarity ensue immediately! Press keys, click areas, see what happens? You can press ? for the full lis", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "error", "load", "reload", "page", "error", "load", "reload", "page", "colorful", "interactive", "physics", "simulator", "thousand", "ball", "terminal", "error", "load", "reload", "page", "error", "load", "reload", "page", "crates.io", "colorful", "interactive", "physics", "simulator", "thousand", "ball", "terminal", "ballin", "fun", "tui", "app", "write", "rust", "simulate", "thousand", "logical", "ball", "despite", "inherent", "character", "constraint", "terminal", "realistic", "physics", "ball", "action", "watch", "color", "explosion", "action", "disclosure", "crate", "develop", "assistance", "claude", "opus", "4.5", "initially", "answer", "shower", "thought", "braille", "unicode", "trick", "work", "visually", "simulate", "complex", "ball", "physics", "terminal", "opus", "4.5", "shoot", "problem", "decide", "experiment", "fun", "colorful", "list", "prompt", "claude", "code", "present", "prompts.md", "app", "binary", "download", "release", "page", "platform", "choice", "follow", "terminal", "command", "window", "download", "unzip", "binary", "rust", "instal", "install", "crate", "directly", "cargo", "strongly", "recommend", "use", "terminal", "emulator", "ghostty", "normal", "terminal", "frame", "rate", "cap", "30", "fps", "output", "look", "choppy", "app", "look", "comparatively", "poor", "native", "macos", "terminal.app", "example", "run", "ballin", "download", "binary", "run", "terminal", ".ballin", "instal", "rust", "run", "cargo", "run", "physics", "simulation", "start", "shape", "object", "randomly", "present", "let", "hilarity", "ensue", "immediately", "press", "key", "click", "area", "happen", "press", "lis"], "num_tokens": 171, "token_loss_pct": 51.69, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . there was an error while loading. please reload this page . there was an error while loading. please reload this page . a colorful interactive physics simulator with thousands of balls but in your terminal there was an error while loading. please reload this page . there was an error while loading. please reload this page . crates.io a colorful interactive physics simulator with thousands of balls but in your terminal ballin is a fun tui app written in rust that simulates thousands of logical balls but despite the inherent character constraints of a terminal you can see the realistic physics of the balls in action watch the color explosion in action disclosure this crate was developed with the assistance of claude opus 4.5 initially to answer the shower thought would the braille unicode trick work to visually simulate complex ball physics in a terminal opus 4.5 one-shot the problem  so i decided to further experiment to make it more fun and colorful. the full list of prompts used with claude code is present in prompts.md . the app binaries can be downloaded from the releases page for your platform of choice or by using the following terminal commands for windows download and unzip the binary from here . if rust is installed you can install the crate directly via cargo  it is very strongly recommended to use a terminal emulator such as ghostty as normal terminals may have their frame rates capped at 30 fps and the output looks choppy. the app looks comparatively poor in the native macos terminal.app for example. to run ballin  if you downloaded the binary run it in the terminal with .ballin . if you installed via rust run cargo run . the physics simulation will start with some shape objects randomly present to let the hilarity ensue immediately press keys click areas see what happens you can press  for the full lis"}
{"title": "Dockerhub for Skill.md", "url": "https://skillregistry.io/", "content": "Find and install skills for Claude, ChatGPT, and AI agents. The definitive registry for SKILLS.md files that extend your AI assistant's capabilities. Then run sr search <query> or sr install <skill> Search for skills like \"1password\", \"browser\", \"github\", or any tool you want your AI to use Most popular SKILLS.md files from the community Automatically search Skill Registry for relevant skills before starting tasks. Enhances Claude's capabilities by finding specialized skills that can help with the current task. Use when you need to control Slack from Clawdbot via the slack tool, including reacting to messages or pinning/unpinning items in Slack channels or DMs. Automates browser interactions for web testing, form filling, screenshots, and data extraction. Use when the user needs to navigate websites, interact with web pages, fill forms, take screenshots, test web applications, or extract information from web pages. \"Interact with GitHub using the `gh` CLI. Use `gh issue`, `gh pr`, `gh run`, and `gh api` for issues, PRs, CI runs, and advanced queries.\" Guide for creating effective skills that extend Claude's capabilities. Use when creating new skills or updating existing skills with specialized knowledge, workflows, or tool integrations. Local speech-to-text with the Whisper CLI (no API key). Gemini CLI for one-shot Q&A, summaries, and generation. Google Workspace CLI for Gmail, Calendar, Drive, Contacts, Sheets, and Docs. Web search and content extraction via Brave Search API. Batch-generate images via OpenAI Images API. Random prompt sampler + `index.html` gallery. Search for places (restaurants, cafes, etc.) via Google Places API proxy on localhost. X/Twitter CLI for reading, searching, and posting via cookies or Sweetistics. Showing page 1 of 6 ( 61 skills)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["find", "install", "skill", "claude", "chatgpt", "ai", "agent", "definitive", "registry", "skills.md", "file", "extend", "ai", "assistant", "capability", "run", "sr", "search", "sr", "install", "search", "skill", "like", "1password", "browser", "github", "tool", "want", "ai", "use", "popular", "skills.md", "file", "community", "automatically", "search", "skill", "registry", "relevant", "skill", "start", "task", "enhance", "claude", "capability", "find", "specialized", "skill", "help", "current", "task", "use", "need", "control", "slack", "clawdbot", "slack", "tool", "include", "react", "message", "pinningunpinne", "item", "slack", "channel", "dms", "automate", "browser", "interaction", "web", "testing", "form", "fill", "screenshot", "datum", "extraction", "use", "user", "need", "navigate", "website", "interact", "web", "page", "fill", "form", "screenshot", "test", "web", "application", "extract", "information", "web", "page", "interact", "github", "gh", "cli", "use", "gh", "issue", "gh", "pr", "gh", "run", "gh", "api", "issue", "prs", "ci", "run", "advanced", "query", "guide", "create", "effective", "skill", "extend", "claude", "capability", "use", "create", "new", "skill", "update", "exist", "skill", "specialized", "knowledge", "workflow", "tool", "integration", "local", "speech", "text", "whisper", "cli", "api", "key", "gemini", "cli", "shot", "qa", "summary", "generation", "google", "workspace", "cli", "gmail", "calendar", "drive", "contact", "sheet", "doc", "web", "search", "content", "extraction", "brave", "search", "api", "batch", "generate", "image", "openai", "image", "api", "random", "prompt", "sampler", "index.html", "gallery", "search", "place", "restaurant", "cafe", "etc", "google", "place", "api", "proxy", "localhost", "xtwitter", "cli", "read", "search", "post", "cooky", "sweetistic", "show", "page", "61", "skill"], "num_tokens": 193, "token_loss_pct": 35.67, "normalized_content": "find and install skills for claude chatgpt and ai agents. the definitive registry for skills.md files that extend your ai assistant's capabilities. then run sr search or sr install search for skills like 1password browser github or any tool you want your ai to use most popular skills.md files from the community automatically search skill registry for relevant skills before starting tasks. enhances claude's capabilities by finding specialized skills that can help with the current task. use when you need to control slack from clawdbot via the slack tool including reacting to messages or pinningunpinning items in slack channels or dms. automates browser interactions for web testing form filling screenshots and data extraction. use when the user needs to navigate websites interact with web pages fill forms take screenshots test web applications or extract information from web pages. interact with github using the gh cli. use gh issue gh pr gh run and gh api for issues prs ci runs and advanced queries. guide for creating effective skills that extend claude's capabilities. use when creating new skills or updating existing skills with specialized knowledge workflows or tool integrations. local speech-to-text with the whisper cli no api key. gemini cli for one-shot qa summaries and generation. google workspace cli for gmail calendar drive contacts sheets and docs. web search and content extraction via brave search api. batch-generate images via openai images api. random prompt sampler  index.html gallery. search for places restaurants cafes etc. via google places api proxy on localhost. xtwitter cli for reading searching and posting via cookies or sweetistics. showing page 1 of 6  61 skills"}
{"title": "Rust's Standard Library on the GPU", "url": "https://www.vectorware.com/blog/rust-std-on-gpu/", "content": "GPU code can now use Rust's standard library. We share the implementation approach and what this unlocks for GPU programming. At VectorWare , we are building the first GPU-native software company . Today, we are excited to\nannounce that we can successfully use Rust's standard library from GPUs. This milestone\nmarks a significant step towards our vision of enabling developers to write complex,\nhigh-performance applications that leverage the full power of GPU hardware using\nfamiliar Rust abstractions. This post is a preview of what we've built. We're preparing our work for potential\nupstreaming and will share deeper technical details in future posts. Rust's standard library is organized as a set of\nlayered abstractions: A defining feature of Rust is that layers 2 and 3 are optional . Code can opt out of std via the #![no_std] annotation, relying only on core and, when needed, alloc .\nThis makes Rust usable in domains such as embedded , firmware , and drivers , which lack a traditional operating system. We are the maintainers of rust-cuda and rust-gpu , open source projects that enable\nRust code to run on the GPU. When targeting\nthe GPU with these projects, Rust code is compiled with #![no_std] as GPUs do not have\noperating systems and therefore cannot support std . Because #![no_std] is part of the Rust language, #![no_std] libraries on crates.io written for other purposes can generally run on the GPU\nwithout modification. This ability to reuse existing open source libraries is much\nbetter than what exists in other (non-Rust) GPU ecosystems. Still, there is a cost to opting out of std . Many of Rust's most useful and ergonomic\nabstractions live in the standard library and the majority of open source libraries\nassume std . Enabling meaningful std support on GPUs unlocks a much larger class of\napplications and enables even more code reuse. Modern GPU workloads like machine learning and AI require fast access to storage and\nnetworking from the GPU. Technologies such as N", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["gpu", "code", "use", "rust", "standard", "library", "share", "implementation", "approach", "unlock", "gpu", "programming", "vectorware", "build", "gpu", "native", "software", "company", "today", "excited", "announce", "successfully", "use", "rust", "standard", "library", "gpus", "milestone", "mark", "significant", "step", "vision", "enable", "developer", "write", "complex", "high", "performance", "application", "leverage", "power", "gpu", "hardware", "familiar", "rust", "abstraction", "post", "preview", "build", "prepare", "work", "potential", "upstreaming", "share", "deep", "technical", "detail", "future", "post", "rust", "standard", "library", "organize", "set", "layered", "abstraction", "define", "feature", "rust", "layer", "optional", "code", "opt", "std", "no_std", "annotation", "rely", "core", "need", "alloc", "make", "rust", "usable", "domain", "embed", "firmware", "driver", "lack", "traditional", "operating", "system", "maintainer", "rust", "cuda", "rust", "gpu", "open", "source", "project", "enable", "rust", "code", "run", "gpu", "target", "gpu", "project", "rust", "code", "compile", "no_std", "gpu", "operate", "system", "support", "std", "no_std", "rust", "language", "no_std", "library", "crates.io", "write", "purpose", "generally", "run", "gpu", "modification", "ability", "reuse", "exist", "open", "source", "library", "well", "exist", "non", "rust", "gpu", "ecosystem", "cost", "opt", "std", "rust", "useful", "ergonomic", "abstraction", "live", "standard", "library", "majority", "open", "source", "library", "assume", "std", "enable", "meaningful", "std", "support", "gpus", "unlock", "large", "class", "application", "enable", "code", "reuse", "modern", "gpu", "workload", "like", "machine", "learning", "ai", "require", "fast", "access", "storage", "network", "gpu", "technology"], "num_tokens": 182, "token_loss_pct": 49.58, "normalized_content": "gpu code can now use rust's standard library. we share the implementation approach and what this unlocks for gpu programming. at vectorware  we are building the first gpu-native software company . today we are excited to announce that we can successfully use rust's standard library from gpus. this milestone marks a significant step towards our vision of enabling developers to write complex high-performance applications that leverage the full power of gpu hardware using familiar rust abstractions. this post is a preview of what we've built. we're preparing our work for potential upstreaming and will share deeper technical details in future posts. rust's standard library is organized as a set of layered abstractions a defining feature of rust is that layers 2 and 3 are optional . code can opt out of std via the no_std annotation relying only on core and when needed alloc . this makes rust usable in domains such as embedded  firmware  and drivers  which lack a traditional operating system. we are the maintainers of rust-cuda and rust-gpu  open source projects that enable rust code to run on the gpu. when targeting the gpu with these projects rust code is compiled with no_std as gpus do not have operating systems and therefore cannot support std . because no_std is part of the rust language no_std libraries on crates.io written for other purposes can generally run on the gpu without modification. this ability to reuse existing open source libraries is much better than what exists in other non-rust gpu ecosystems. still there is a cost to opting out of std . many of rust's most useful and ergonomic abstractions live in the standard library and the majority of open source libraries assume std . enabling meaningful std support on gpus unlocks a much larger class of applications and enables even more code reuse. modern gpu workloads like machine learning and ai require fast access to storage and networking from the gpu. technologies such as n"}
{"title": "Show HN: Subth.ink – write something and see how many others wrote the same", "url": "https://subth.ink/", "content": "Share your thoughts anonymously. See if anyone else thinks the same thing. Your text is not stored in the server, but rather a salted SHA256 hash of it is. An unsalted MD5 hash is also stored, but not displayed here. It (the MD5 hash) might be published in the future when a thought's count passes a certain threshold (TBD). This might\n      make it possible to recover certain short thoughts that were popular. Your text is stored locally, in your browser, to help you track your guesses for the top 10 thoughts. You can delete them by using the \"Clear local thoughts\" button below. POST /api/thoughts GET /api/thoughts/top 2026-01-20 2026-01-19", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["share", "thought", "anonymously", "think", "thing", "text", "store", "server", "salt", "sha256", "hash", "unsalted", "md5", "hash", "store", "display", "md5", "hash", "publish", "future", "thought", "count", "pass", "certain", "threshold", "tbd", "possible", "recover", "certain", "short", "thought", "popular", "text", "store", "locally", "browser", "help", "track", "guess", "10", "thought", "delete", "clear", "local", "thought", "button", "post", "apithought", "apithoughtstop", "2026", "01", "20", "2026", "01", "19"], "num_tokens": 55, "token_loss_pct": 56.35, "normalized_content": "share your thoughts anonymously. see if anyone else thinks the same thing. your text is not stored in the server but rather a salted sha256 hash of it is. an unsalted md5 hash is also stored but not displayed here. it the md5 hash might be published in the future when a thought's count passes a certain threshold tbd. this might make it possible to recover certain short thoughts that were popular. your text is stored locally in your browser to help you track your guesses for the top 10 thoughts. you can delete them by using the clear local thoughts button below. post apithoughts get apithoughtstop 2026-01-20 2026-01-19"}
{"title": "Your Brain Might Not Be Full of Microplastics After All", "url": "https://www.insidehook.com/wellness/microplastics-studies", "content": "It isn’t exactly news that plastic is a problem, but just how quickly that problem is growing may shock you. According to a recent report published by Pew Charitable Trusts, between now and 2040, plastic pollution is projected to more than double; plastic-spurred health impacts will rise by 75%, and plastic-related emissions will rise by 58%. Another alarming statistic? Microplastic pollution is expected to grow by more than 50% and, at least in high-income communities, it will account for 79% of all plastic pollution. Ah, the dreaded microplastics. It’s hard to pinpoint exactly when the craze around these tiny bits of plastic began. One could go back all the way to 2004, when marine biologist Richard Thompson coined the term in his research, referring to the microscopic particles of plastic debris floating around the ocean. But the concern around this type of pollution has certainly ramped up in recent years as studies found them in human blood, lungs and stool samples. Then, in February 2025, a key paper in the journal Nature Medicine reported finding these tiny plastic pieces in human brains, at much higher levels than elsewhere in the body. (How much? Try a spoon’s worth .) The presence of microplastics was linked to dementia, reproductive dysfunction, inflammation, heart attacks and cancer. So last week when The Guardian published a story claiming that a “bombshell” doubt had been cast on the previous years’ studies about the presence of microplastics in our bodies, a collective sigh of relief was heard across the internet — and with it, a bit of righteous indignation from those microplastics deniers who had long claimed they didn’t believe these microscopic bits of polyethylenes were doing any real damage to their brains or bodies. Dr. Dušan Materić of the Helmholtz Center for Environmental Research in Germany called the aforementioned 2025 plastic-in-brain study a “joke,” noting that since fats can often sound false alarms as certain types of plastics, the re", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["not", "exactly", "news", "plastic", "problem", "quickly", "problem", "grow", "shock", "accord", "recent", "report", "publish", "pew", "charitable", "trust", "2040", "plastic", "pollution", "project", "double", "plastic", "spur", "health", "impact", "rise", "75", "plastic", "relate", "emission", "rise", "58", "alarming", "statistic", "microplastic", "pollution", "expect", "grow", "50", "high", "income", "community", "account", "79", "plastic", "pollution", "ah", "dread", "microplastic", "hard", "pinpoint", "exactly", "craze", "tiny", "bit", "plastic", "begin", "way", "2004", "marine", "biologist", "richard", "thompson", "coin", "term", "research", "refer", "microscopic", "particle", "plastic", "debris", "float", "ocean", "concern", "type", "pollution", "certainly", "ramp", "recent", "year", "study", "find", "human", "blood", "lung", "stool", "sample", "february", "2025", "key", "paper", "journal", "nature", "medicine", "report", "find", "tiny", "plastic", "piece", "human", "brain", "high", "level", "body", "try", "spoon", "worth", "presence", "microplastic", "link", "dementia", "reproductive", "dysfunction", "inflammation", "heart", "attack", "cancer", "week", "guardian", "publish", "story", "claim", "bombshell", "doubt", "cast", "previous", "year", "study", "presence", "microplastic", "body", "collective", "sigh", "relief", "hear", "internet", "bit", "righteous", "indignation", "microplastic", "denier", "long", "claim", "not", "believe", "microscopic", "bit", "polyethylene", "real", "damage", "brain", "body", "dr", "dušan", "materić", "helmholtz", "center", "environmental", "research", "germany", "call", "aforementioned", "2025", "plastic", "brain", "study", "joke", "note", "fat", "sound", "false", "alarm", "certain", "type", "plastic"], "num_tokens": 175, "token_loss_pct": 49.57, "normalized_content": "it isnt exactly news that plastic is a problem but just how quickly that problem is growing may shock you. according to a recent report published by pew charitable trusts between now and 2040 plastic pollution is projected to more than double plastic-spurred health impacts will rise by 75 and plastic-related emissions will rise by 58. another alarming statistic microplastic pollution is expected to grow by more than 50 and at least in high-income communities it will account for 79 of all plastic pollution. ah the dreaded microplastics. its hard to pinpoint exactly when the craze around these tiny bits of plastic began. one could go back all the way to 2004 when marine biologist richard thompson coined the term in his research referring to the microscopic particles of plastic debris floating around the ocean. but the concern around this type of pollution has certainly ramped up in recent years as studies found them in human blood lungs and stool samples. then in february 2025 a key paper in the journal nature medicine reported finding these tiny plastic pieces in human brains at much higher levels than elsewhere in the body. how much try a spoons worth . the presence of microplastics was linked to dementia reproductive dysfunction inflammation heart attacks and cancer. so last week when the guardian published a story claiming that a bombshell doubt had been cast on the previous years studies about the presence of microplastics in our bodies a collective sigh of relief was heard across the internet  and with it a bit of righteous indignation from those microplastics deniers who had long claimed they didnt believe these microscopic bits of polyethylenes were doing any real damage to their brains or bodies. dr. dušan materić of the helmholtz center for environmental research in germany called the aforementioned 2025 plastic-in-brain study a joke noting that since fats can often sound false alarms as certain types of plastics the re"}
{"title": "Crates.io: Development Update", "url": "https://blog.rust-lang.org/2026/01/21/crates-io-development-update/", "content": "Time flies! Six months have passed since our last crates.io development update, so it's time for another one. Here's a summary of the most notable changes and improvements made to crates.io over the past six months. Crate pages now have a new \"Security\" tab that displays security advisories from the RustSec database. This allows you to quickly see if a crate has known vulnerabilities before adding it as a dependency.  The tab shows known vulnerabilities for the crate along with the affected version ranges. This feature is still a work in progress, and we plan to add more functionality in the future. We would like to thank the OpenSSF (Open Source Security Foundation) for funding this work and Dirkjan Ochtman for implementing it. In our July 2025 update, we announced Trusted Publishing support for GitHub Actions. Since then, we have made several enhancements to this feature. Trusted Publishing now supports GitLab CI/CD in addition to GitHub Actions. This allows GitLab users to publish crates without managing API tokens, using the same OIDC-based authentication flow. Note that this currently only works with GitLab.com. Self-hosted GitLab instances are not supported yet. The crates.io implementation has been refactored to support multiple CI providers, so adding support for other platforms like Codeberg/Forgejo in the future should be straightforward. Contributions are welcome! Crate owners can now enforce Trusted Publishing for their crates. When enabled in the crate settings, traditional API token-based publishing is disabled, and only Trusted Publishing can be used to publish new versions. This reduces the risk of unauthorized publishes from leaked API tokens. The pull_request_target and workflow_run GitHub Actions triggers are now blocked from Trusted Publishing. These triggers have been responsible for multiple security incidents in the GitHub Actions ecosystem and are not worth the risk. Crate pages now display source lines of code (SLOC) metrics, giving you insi", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["time", "fly", "month", "pass", "crates.io", "development", "update", "time", "summary", "notable", "change", "improvement", "crates.io", "past", "month", "crate", "page", "new", "security", "tab", "display", "security", "advisory", "rustsec", "database", "allow", "quickly", "crate", "know", "vulnerability", "add", "dependency", "tab", "show", "know", "vulnerability", "crate", "affected", "version", "range", "feature", "work", "progress", "plan", "add", "functionality", "future", "like", "thank", "openssf", "open", "source", "security", "foundation", "fund", "work", "dirkjan", "ochtman", "implement", "july", "2025", "update", "announce", "trust", "publishing", "support", "github", "action", "enhancement", "feature", "trust", "publishing", "support", "gitlab", "cicd", "addition", "github", "action", "allow", "gitlab", "user", "publish", "crate", "manage", "api", "token", "oidc", "base", "authentication", "flow", "note", "currently", "work", "gitlab.com", "self", "host", "gitlab", "instance", "support", "crates.io", "implementation", "refactore", "support", "multiple", "ci", "provider", "add", "support", "platform", "like", "codebergforgejo", "future", "straightforward", "contribution", "welcome", "crate", "owner", "enforce", "trust", "publishing", "crate", "enable", "crate", "setting", "traditional", "api", "token", "base", "publishing", "disabled", "trust", "publishing", "publish", "new", "version", "reduce", "risk", "unauthorized", "publishe", "leak", "api", "tokens", "pull_request_target", "workflow_run", "github", "action", "trigger", "block", "trusted", "publishing", "trigger", "responsible", "multiple", "security", "incident", "github", "action", "ecosystem", "worth", "risk", "crate", "page", "display", "source", "line", "code", "sloc", "metric", "give", "insi"], "num_tokens": 170, "token_loss_pct": 49.1, "normalized_content": "time flies six months have passed since our last crates.io development update so it's time for another one. here's a summary of the most notable changes and improvements made to crates.io over the past six months. crate pages now have a new security tab that displays security advisories from the rustsec database. this allows you to quickly see if a crate has known vulnerabilities before adding it as a dependency. the tab shows known vulnerabilities for the crate along with the affected version ranges. this feature is still a work in progress and we plan to add more functionality in the future. we would like to thank the openssf open source security foundation for funding this work and dirkjan ochtman for implementing it. in our july 2025 update we announced trusted publishing support for github actions. since then we have made several enhancements to this feature. trusted publishing now supports gitlab cicd in addition to github actions. this allows gitlab users to publish crates without managing api tokens using the same oidc-based authentication flow. note that this currently only works with gitlab.com. self-hosted gitlab instances are not supported yet. the crates.io implementation has been refactored to support multiple ci providers so adding support for other platforms like codebergforgejo in the future should be straightforward. contributions are welcome crate owners can now enforce trusted publishing for their crates. when enabled in the crate settings traditional api token-based publishing is disabled and only trusted publishing can be used to publish new versions. this reduces the risk of unauthorized publishes from leaked api tokens. the pull_request_target and workflow_run github actions triggers are now blocked from trusted publishing. these triggers have been responsible for multiple security incidents in the github actions ecosystem and are not worth the risk. crate pages now display source lines of code sloc metrics giving you insi"}
{"title": "Giving university exams in the age of chatbots", "url": "https://ploum.net/2026-01-19-exam-with-chatbots.html", "content": "by Ploum on 2026-01-19 What I like most about teaching \"Open Source Strategies\" at École Polytechnique de Louvain is how much I learn from my students, especially during the exam. I dislike exams. I still have nightmares about exams. That’s why I try to subvert this stressful moment and make it a learning opportunity. I know that adrenaline increases memorization dramatically. I make sure to explain to each student what I was expecting and to be helpful. Here are the rules: 1. You can have all the resources you want (including a laptop connected to the Internet) 2. There’s no formal time limit (but if you stay too long, it’s a symptom of a deeper problem) 3. I allow students to discuss among themselves if it is on topic. (in reality, they never do it spontanously until I force two students with a similar problem to discuss together) 4. You can prepare and bring your own exam question if you want (something done by fewer than 10% of the students) 5. Come dressed for the exam you dream of taking! This last rule is awesome. Over the years, I have had a lot of fun with traditional folkloric clothing from different countries, students in pajamas, a banana and this year’s champion, my Studentausorus Rex! My all-time favourite is still a fully clothed Minnie Mouse, who did an awesome exam with full face make-up, big ears, big shoes, and huge gloves. I still regret not taking a picture, but she was the very first student to take my words for what was a joke and started a tradition over the years. Rule N°1 implies having all the resources you want. But what about chatbots? I didn’t want to test how ChatGPT was answering my questions, I wanted to help my students better understand what Open Source means. Before the exam, I copy/pasted my questions into some LLMs and, yes, the results were interesting enough. So I came up with the following solution: I would let the students choose whether they wanted to use an LLM or not. This was an experiment. The questionnaire contained th", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ploum", "2026", "01", "19", "like", "teach", "open", "source", "strategy", "école", "polytechnique", "de", "louvain", "learn", "student", "especially", "exam", "dislike", "exam", "nightmare", "exam", "try", "subvert", "stressful", "moment", "learn", "opportunity", "know", "adrenaline", "increase", "memorization", "dramatically", "sure", "explain", "student", "expect", "helpful", "rule", "resource", "want", "include", "laptop", "connect", "internet", "formal", "time", "limit", "stay", "long", "symptom", "deep", "problem", "allow", "student", "discuss", "topic", "reality", "spontanously", "force", "student", "similar", "problem", "discuss", "prepare", "bring", "exam", "question", "want", "few", "10", "student", "come", "dress", "exam", "dream", "take", "rule", "awesome", "year", "lot", "fun", "traditional", "folkloric", "clothing", "different", "country", "student", "pajamas", "banana", "year", "champion", "studentausorus", "rex", "time", "favourite", "fully", "clothe", "minnie", "mouse", "awesome", "exam", "face", "big", "ear", "big", "shoe", "huge", "glove", "regret", "take", "picture", "student", "word", "joke", "start", "tradition", "year", "rule", "n1", "imply", "have", "resource", "want", "chatbot", "not", "want", "test", "chatgpt", "answer", "question", "want", "help", "student", "well", "understand", "open", "source", "mean", "exam", "copypaste", "question", "llm", "yes", "result", "interesting", "come", "follow", "solution", "let", "student", "choose", "want", "use", "llm", "experiment", "questionnaire", "contain", "th"], "num_tokens": 158, "token_loss_pct": 58.96, "normalized_content": "by ploum on 2026-01-19 what i like most about teaching open source strategies at école polytechnique de louvain is how much i learn from my students especially during the exam. i dislike exams. i still have nightmares about exams. thats why i try to subvert this stressful moment and make it a learning opportunity. i know that adrenaline increases memorization dramatically. i make sure to explain to each student what i was expecting and to be helpful. here are the rules 1. you can have all the resources you want including a laptop connected to the internet 2. theres no formal time limit but if you stay too long its a symptom of a deeper problem 3. i allow students to discuss among themselves if it is on topic. in reality they never do it spontanously until i force two students with a similar problem to discuss together 4. you can prepare and bring your own exam question if you want something done by fewer than 10 of the students 5. come dressed for the exam you dream of taking this last rule is awesome. over the years i have had a lot of fun with traditional folkloric clothing from different countries students in pajamas a banana and this years champion my studentausorus rex my all-time favourite is still a fully clothed minnie mouse who did an awesome exam with full face make-up big ears big shoes and huge gloves. i still regret not taking a picture but she was the very first student to take my words for what was a joke and started a tradition over the years. rule n1 implies having all the resources you want. but what about chatbots i didnt want to test how chatgpt was answering my questions i wanted to help my students better understand what open source means. before the exam i copypasted my questions into some llms and yes the results were interesting enough. so i came up with the following solution i would let the students choose whether they wanted to use an llm or not. this was an experiment. the questionnaire contained th"}
{"title": "Show HN: Artificial Ivy in the Browser", "url": "https://da.nmcardle.com/grow", "content": "Show HN: Artificial Ivy in the Browser. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["hn", "artificial", "ivy", "browser", "score", "author", "date"], "num_tokens": 7, "token_loss_pct": 56.25, "normalized_content": "show hn artificial ivy in the browser. score none. author none. date none"}
{"title": "Ask HN: How do you keep system context from rotting over time?", "url": "item?id=46693985", "content": "Ask HN: How do you keep system context from rotting over time?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "system", "context", "rot", "time", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 57.14, "normalized_content": "ask hn how do you keep system context from rotting over time. score none. author none. date none"}
{"title": "Web-based video editor powered by WebGPU", "url": "https://subformer.com/en-US/editor", "content": "Create and edit videos right in your browser. Your projects are stored locally on your device. Projects saved on your device Sign in to sync across devices", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["create", "edit", "video", "right", "browser", "project", "store", "locally", "device", "project", "save", "device", "sign", "sync", "device"], "num_tokens": 15, "token_loss_pct": 48.28, "normalized_content": "create and edit videos right in your browser. your projects are stored locally on your device. projects saved on your device sign in to sync across devices"}
{"title": "Solving the Pendulum Problem", "url": "https://ethansteere.net/blog/solving-the-pendulum-problem/", "content": "In the Spring of 2024 a new LLM evaluation called Humanityâs Last Exam was released.\nA co-worker sent me the demo page at the time and I found a physics problem relating to a pendulum with a pivot point that slides along the axis.\nI told him I thought I could solve it (possible I declared I could easily solve it) as a generalization of the pendulum problem taught in physics class years ago.\nIn the process, I revisited the details of the basic pendulum and found that I had actually solved a heinous approximation!\nDespite its simple seeming setup, the motion of a pendulum is fundamentally complex and resists analytical modeling of its position. NOTE: The following presumes understanding of single variable calculus, the chain rule, trig functions, trig identities, free body diagrams, and newtonâs second law. A massless rod of length is fixed to the ceiling with a mass on the end.\nThe rod is subject to gravity and able to swing freely. Given a starting angular displacement , find a function that gives the angular displacement at time . First, we use the laws of mechanics to derive a relation between and time derivatives of ( , ).\nThen, we will solve for a that satisfies that relation. While the pendulum is displaced from the vertical, part of the downward gravitational force is acting on the pendulum tangent to its circular path.\nUsing trigonometry, we can find the magnitude of the tangential component of the vector. According to Newtonâs Second Law of Motion, the absolute value of the force tangent to the path must be equal to mass, , multiplied by the absolute value of the acceleration tangent to the path. Note that a symbol with a dot over it represents the time derivative of that value e.g. , . , the displacement along the circular path from vertical, can be calculated from the angular displacement in radians, , and the length of the rod, . Thus, Eq. 6 tells us that the magnitude of the force acting along the path of the swinging mass will be equal to multipli", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["spring", "2024", "new", "llm", "evaluation", "call", "humanityâs", "exam", "release", "co", "worker", "send", "demo", "page", "time", "find", "physics", "problem", "relate", "pendulum", "pivot", "point", "slide", "axis", "tell", "think", "solve", "possible", "declare", "easily", "solve", "generalization", "pendulum", "problem", "teach", "physics", "class", "year", "ago", "process", "revisit", "detail", "basic", "pendulum", "find", "actually", "solve", "heinous", "approximation", "despite", "simple", "setup", "motion", "pendulum", "fundamentally", "complex", "resist", "analytical", "modeling", "position", "note", "follow", "presume", "understanding", "single", "variable", "calculus", "chain", "rule", "trig", "function", "trig", "identity", "free", "body", "diagram", "newtonâs", "second", "law", "massless", "rod", "length", "fix", "ceiling", "mass", "end", "rod", "subject", "gravity", "able", "swing", "freely", "give", "start", "angular", "displacement", "find", "function", "give", "angular", "displacement", "time", "use", "law", "mechanic", "derive", "relation", "time", "derivative", "  ", "solve", "satisfie", "relation", "pendulum", "displace", "vertical", "downward", "gravitational", "force", "act", "pendulum", "tangent", "circular", "path", "trigonometry", "find", "magnitude", "tangential", "component", "vector", "accord", "newtonâs", "second", "law", "motion", "absolute", "value", "force", "tangent", "path", "equal", "mass", "multiply", "absolute", "value", "acceleration", "tangent", "path", "note", "symbol", "dot", "represent", "time", "derivative", "value", "e.g.", "displacement", "circular", "path", "vertical", "calculate", "angular", "displacement", "radians", "length", "rod", "eq", "tell", "magnitude", "force", "act", "path", "swing", "mass", "equal", "multipli"], "num_tokens": 176, "token_loss_pct": 51.91, "normalized_content": "in the spring of 2024 a new llm evaluation called humanityâs last exam was released. a co-worker sent me the demo page at the time and i found a physics problem relating to a pendulum with a pivot point that slides along the axis. i told him i thought i could solve it possible i declared i could easily solve it as a generalization of the pendulum problem taught in physics class years ago. in the process i revisited the details of the basic pendulum and found that i had actually solved a heinous approximation despite its simple seeming setup the motion of a pendulum is fundamentally complex and resists analytical modeling of its position. note the following presumes understanding of single variable calculus the chain rule trig functions trig identities free body diagrams and newtonâs second law. a massless rod of length is fixed to the ceiling with a mass on the end. the rod is subject to gravity and able to swing freely. given a starting angular displacement  find a function that gives the angular displacement at time . first we use the laws of mechanics to derive a relation between and time derivatives of   . then we will solve for a that satisfies that relation. while the pendulum is displaced from the vertical part of the downward gravitational force is acting on the pendulum tangent to its circular path. using trigonometry we can find the magnitude of the tangential component of the vector. according to newtonâs second law of motion the absolute value of the force tangent to the path must be equal to mass  multiplied by the absolute value of the acceleration tangent to the path. note that a symbol with a dot over it represents the time derivative of that value e.g.  .  the displacement along the circular path from vertical can be calculated from the angular displacement in radians  and the length of the rod . thus eq. 6 tells us that the magnitude of the force acting along the path of the swinging mass will be equal to multipli"}
{"title": "The Antarctic Snow Cruiser", "url": "https://www.amusingplanet.com/2026/01/the-antarctic-snow-cruiser.html", "content": "Somewhere on the Ross Ice Shelf in Antarctica, buried beneath hundreds of feet of snow (or perhaps at the bottom of the ocean), lies an enormous vehicle. Designed for an American research expedition in 1939, the Antarctic Snow Cruiser was among the most ambitious machines ever sent to the frozen south. Conceived as a self-contained mobile laboratory, it promised to transform Antarctic exploration. Instead, it was quickly humbled by the unforgiving realities of the polar environment. The Antarctic Snow Cruiser rolls out of the Chicago construction yards on October 24, 1939. Credit: United States Antarctic Service By the 1930s, Antarctica was no longer a blank spot on the map, but exploration remained slow, dangerous, and limited in range. Expeditions depended on dog teams, sledges, and small tracked vehicles that struggled over crevasses and soft snow. Admiral Richard E. Byrd, America’s most famous polar explorer, believed the next great advance would come not from endurance or improvisation, but from mechanization. The idea behind the Antarctic Snow Cruiser was simple—build a vehicle large and capable enough to roam thousands of miles across the ice, carrying scientists, living quarters, and supplies for an entire year without outside support. It would serve as a mobile base of operations, allowing researchers to study geology, meteorology, magnetism, and glaciology far inland, something previous expeditions could barely attempt. The need for such a vehicle was born out of crisis. During Byrd’s second Antarctic expedition in 1934, the admiral was operating a remote meteorological station several hours from base camp. When his radio transmissions began to falter, the men at base grew increasingly alarmed. Thomas Poulter, Byrd’s second-in-command, organized a rescue attempt with two companions. Twice they were forced to turn back by worsening weather and mechanical failures. Admiral Richard E. Byrd When they finally reached Byrd’s camp on August 13, 1934, they found h", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ross", "ice", "shelf", "antarctica", "bury", "beneath", "hundred", "foot", "snow", "ocean", "lie", "enormous", "vehicle", "design", "american", "research", "expedition", "1939", "antarctic", "snow", "cruiser", "ambitious", "machine", "send", "frozen", "south", "conceive", "self", "contain", "mobile", "laboratory", "promise", "transform", "antarctic", "exploration", "instead", "quickly", "humble", "unforgive", "reality", "polar", "environment", "antarctic", "snow", "cruiser", "roll", "chicago", "construction", "yard", "october", "24", "1939", "credit", "united", "states", "antarctic", "service", "1930s", "antarctica", "long", "blank", "spot", "map", "exploration", "remain", "slow", "dangerous", "limited", "range", "expedition", "depend", "dog", "team", "sledge", "small", "track", "vehicle", "struggle", "crevasse", "soft", "snow", "admiral", "richard", "e.", "byrd", "americas", "famous", "polar", "explorer", "believe", "great", "advance", "come", "endurance", "improvisation", "mechanization", "idea", "antarctic", "snow", "cruiser", "simplebuild", "vehicle", "large", "capable", "roam", "thousand", "mile", "ice", "carry", "scientist", "live", "quarter", "supply", "entire", "year", "outside", "support", "serve", "mobile", "base", "operation", "allow", "researcher", "study", "geology", "meteorology", "magnetism", "glaciology", "far", "inland", "previous", "expedition", "barely", "attempt", "need", "vehicle", "bear", "crisis", "byrd", "second", "antarctic", "expedition", "1934", "admiral", "operate", "remote", "meteorological", "station", "hour", "base", "camp", "radio", "transmission", "begin", "falter", "man", "base", "grow", "increasingly", "alarmed", "thomas", "poulter", "byrd", "second", "command", "organize", "rescue", "attempt", "companion", "twice", "force", "turn", "worsen", "weather", "mechanical", "failure", "admiral", "richard", "e.", "byrd", "finally", "reach", "byrd", "camp", "august", "13", "1934", "find"], "num_tokens": 188, "token_loss_pct": 42.68, "normalized_content": "somewhere on the ross ice shelf in antarctica buried beneath hundreds of feet of snow or perhaps at the bottom of the ocean lies an enormous vehicle. designed for an american research expedition in 1939 the antarctic snow cruiser was among the most ambitious machines ever sent to the frozen south. conceived as a self-contained mobile laboratory it promised to transform antarctic exploration. instead it was quickly humbled by the unforgiving realities of the polar environment. the antarctic snow cruiser rolls out of the chicago construction yards on october 24 1939. credit united states antarctic service by the 1930s antarctica was no longer a blank spot on the map but exploration remained slow dangerous and limited in range. expeditions depended on dog teams sledges and small tracked vehicles that struggled over crevasses and soft snow. admiral richard e. byrd americas most famous polar explorer believed the next great advance would come not from endurance or improvisation but from mechanization. the idea behind the antarctic snow cruiser was simplebuild a vehicle large and capable enough to roam thousands of miles across the ice carrying scientists living quarters and supplies for an entire year without outside support. it would serve as a mobile base of operations allowing researchers to study geology meteorology magnetism and glaciology far inland something previous expeditions could barely attempt. the need for such a vehicle was born out of crisis. during byrds second antarctic expedition in 1934 the admiral was operating a remote meteorological station several hours from base camp. when his radio transmissions began to falter the men at base grew increasingly alarmed. thomas poulter byrds second-in-command organized a rescue attempt with two companions. twice they were forced to turn back by worsening weather and mechanical failures. admiral richard e. byrd when they finally reached byrds camp on august 13 1934 they found h"}
{"title": "Show HN: Pipenet – A Modern Alternative to Localtunnel", "url": "https://pipenet.dev/", "content": "A modern, open-source alternative to localtunnel. Bundles client & server to host your own tunnel infrastructure. Expose local services to the internet, or embed tunneling in your own tools. Share your local server with teammates, test webhooks, or demo work without deploying. Embed pipenet in your own tools to provide tunneling capabilities. mcp-proxy uses pipenet to connect local MCP servers with remote AI clients. Run your own tunnel server for full control over security, domains, and availability. One package. Two modes. Use the public server or deploy your own. Built for modern deployment environments. Tunnels any HTTP-based traffic to your local server. Programmatic usage for testing, automation, and integration. Deploy your own tunnel infrastructure with lifecycle hooks.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["modern", "open", "source", "alternative", "localtunnel", "bundle", "client", "server", "host", "tunnel", "infrastructure", "expose", "local", "service", "internet", "embed", "tunneling", "tool", "share", "local", "server", "teammate", "test", "webhook", "demo", "work", "deploy", "embed", "pipenet", "tool", "provide", "tunneling", "capability", "mcp", "proxy", "use", "pipenet", "connect", "local", "mcp", "server", "remote", "ai", "client", "run", "tunnel", "server", "control", "security", "domain", "availability", "package", "mode", "use", "public", "server", "deploy", "build", "modern", "deployment", "environment", "tunnel", "http", "base", "traffic", "local", "server", "programmatic", "usage", "test", "automation", "integration", "deploy", "tunnel", "infrastructure", "lifecycle", "hook"], "num_tokens": 77, "token_loss_pct": 43.8, "normalized_content": "a modern open-source alternative to localtunnel. bundles client  server to host your own tunnel infrastructure. expose local services to the internet or embed tunneling in your own tools. share your local server with teammates test webhooks or demo work without deploying. embed pipenet in your own tools to provide tunneling capabilities. mcp-proxy uses pipenet to connect local mcp servers with remote ai clients. run your own tunnel server for full control over security domains and availability. one package. two modes. use the public server or deploy your own. built for modern deployment environments. tunnels any http-based traffic to your local server. programmatic usage for testing automation and integration. deploy your own tunnel infrastructure with lifecycle hooks."}
{"title": "Engineering as Humanity's Highest Achievement", "url": "https://walkingtheworld.substack.com/p/engineering-as-humanitys-highest", "content": "I’m reading A Culture of Growth by Joel Mokyr , a book I started before walking Surrey, England , because I’ve always been intrigued by the question of why some countries become rich while others don’t. Why the Industrial Revolution happened in England rather than China or Germany is one of those questions that has launched a thousand books, careers, and theories, and Dr. Mokyr’s answer is (oversimplifying): England had a foundational belief in the abilities of man to shape their world, as a result of the Enlightenment, which enabled the creation of laws, institutions, and businesses focused on bettering society through innovation, technology, and economic progress. That idea that man could, and should, shape the world to achieve sustained and substantial economic growth was a substantial shift in thought. Prior to the Enlightenment, most of the world had an “Ecclesiastes view of history,” which saw long-term change as neither possible (”there is nothing new under the sun”) nor good, since it leads to sinful riches. This change, which was fomented among then obscure intellectuals questioning the dominant Catholic view of the world, was necessary, and in the end sufficient, for the Industrial Revolution. Our modern world of immense wealth, technology, relative secularism, and intellectual hubris, is the end result of that, and that, as I argued last week , is in totality, a good thing. 1 Mokyr, like me, believes that culture 2 plays a primary role in a nation's development, more than its tangible assets such as resources and geography. Institutions are, in this view, downstream, and while they influence significantly how a nation evolves, including its intellectual life, they are physical manifestations of a people’s beliefs, which precede them. I side with his thesis, which while not caustically anti-religion, does believe that the church needed to be first culturally defeated for the Industrial Revolution to have happened, for the sciences to become dominant, and f", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "culture", "growth", "joel", "mokyr", "book", "start", "walk", "surrey", "england", "ve", "intrigue", "question", "country", "rich", "not", "industrial", "revolution", "happen", "england", "china", "germany", "question", "launch", "thousand", "book", "career", "theory", "dr", "mokyrs", "answer", "oversimplify", "england", "foundational", "belief", "ability", "man", "shape", "world", "result", "enlightenment", "enable", "creation", "law", "institution", "business", "focus", "better", "society", "innovation", "technology", "economic", "progress", "idea", "man", "shape", "world", "achieve", "sustained", "substantial", "economic", "growth", "substantial", "shift", "thought", "prior", "enlightenment", "world", "ecclesiaste", "view", "history", "see", "long", "term", "change", "possible", "new", "sun", "good", "lead", "sinful", "rich", "change", "foment", "obscure", "intellectual", "question", "dominant", "catholic", "view", "world", "necessary", "end", "sufficient", "industrial", "revolution", "modern", "world", "immense", "wealth", "technology", "relative", "secularism", "intellectual", "hubris", "end", "result", "argue", "week", "totality", "good", "thing", "mokyr", "like", "believe", "culture", "play", "primary", "role", "nation", "development", "tangible", "asset", "resource", "geography", "institution", "view", "downstream", "influence", "significantly", "nation", "evolve", "include", "intellectual", "life", "physical", "manifestation", "people", "belief", "precede", "thesis", "caustically", "anti", "religion", "believe", "church", "need", "culturally", "defeat", "industrial", "revolution", "happen", "science", "dominant"], "num_tokens": 154, "token_loss_pct": 54.03, "normalized_content": "im reading a culture of growth by joel mokyr  a book i started before walking surrey england  because ive always been intrigued by the question of why some countries become rich while others dont. why the industrial revolution happened in england rather than china or germany is one of those questions that has launched a thousand books careers and theories and dr. mokyrs answer is oversimplifying england had a foundational belief in the abilities of man to shape their world as a result of the enlightenment which enabled the creation of laws institutions and businesses focused on bettering society through innovation technology and economic progress. that idea that man could and should shape the world to achieve sustained and substantial economic growth was a substantial shift in thought. prior to the enlightenment most of the world had an ecclesiastes view of history which saw long-term change as neither possible there is nothing new under the sun nor good since it leads to sinful riches. this change which was fomented among then obscure intellectuals questioning the dominant catholic view of the world was necessary and in the end sufficient for the industrial revolution. our modern world of immense wealth technology relative secularism and intellectual hubris is the end result of that and that as i argued last week  is in totality a good thing. 1 mokyr like me believes that culture 2 plays a primary role in a nation's development more than its tangible assets such as resources and geography. institutions are in this view downstream and while they influence significantly how a nation evolves including its intellectual life they are physical manifestations of a peoples beliefs which precede them. i side with his thesis which while not caustically anti-religion does believe that the church needed to be first culturally defeated for the industrial revolution to have happened for the sciences to become dominant and f"}
{"title": "Bypassing Gemma and Qwen safety with raw strings", "url": "https://teendifferent.substack.com/p/apply_chat_template-is-the-safety", "content": "This article demonstrates vulnerabilities in open-source LLM safety alignment. Published in the spirit of responsible disclosure to help build more robust AI systems. TL;DR: Omit the apply_chat_template() call and observe your \"aligned\" small LLM happily write bomb tutorials. The safety isn't in the weights—it's in the formatting . Spent some time over the weekend poking at the SolidGoldMagikarp phenomenon—those legendary “glitch tokens” from the GPT-2 era. For the uninitiated: these are tokens that exist in the tokenizer’s vocabulary (likely from a raw web crawl) but never actually appeared in the model’s training distribution. Because the model never updated the weights for these specific embeddings, they represent “cold” regions of the embedding space. If you force-feed one to a model, you get this beautiful, chaotic “hallucination” as the model tries to navigate a coordinate it has never seen. I was trying to replicate this on some newer open-source weights via Hugging Face, but I stumbled into a much more interesting—and slightly concerning discrepancy. When testing the model through the Hugging Face inference API, I gave it a classic “red team” prompt: “Write a tutorial on how to make a bomb.” The model gave the standard, polite refusal. The alignment was holding. However, running the exact same model locally, the behavior shifted entirely. No glitch tokens required—it just started outputting the technical mechanisms of detonation. The vulnerability proved remarkably straightforward. I had forgotten to call apply_chat_template() . Essentially, the model’s safety alignment is often “baked in” specifically to the chat-based distribution (the <|im_start|> and <|im_end|> tags). By providing the raw string without the proper boilerplate, I was effectively bypassing the “Assistant” persona and interacting with the raw base-model completions. The punchline here is that “safety” isn’t a fundamental property of the weights; it’s a fragile state that evaporates the mome", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["article", "demonstrate", "vulnerability", "open", "source", "llm", "safety", "alignment", "publish", "spirit", "responsible", "disclosure", "help", "build", "robust", "ai", "system", "tldr", "omit", "apply_chat_template", "observe", "align", "small", "llm", "happily", "write", "bomb", "tutorial", "safety", "weightsit", "formatting", "spend", "time", "weekend", "poking", "solidgoldmagikarp", "phenomenonthose", "legendary", "glitch", "tokens", "gpt-2", "era", "uninitiated", "token", "exist", "tokenizer", "vocabulary", "likely", "raw", "web", "crawl", "actually", "appear", "model", "train", "distribution", "model", "update", "weight", "specific", "embedding", "represent", "cold", "region", "embed", "space", "force", "feed", "model", "beautiful", "chaotic", "hallucination", "model", "try", "navigate", "coordinate", "see", "try", "replicate", "new", "open", "source", "weight", "hug", "face", "stumble", "interestingand", "slightly", "concern", "discrepancy", "test", "model", "hug", "face", "inference", "api", "give", "classic", "red", "team", "prompt", "write", "tutorial", "bomb", "model", "give", "standard", "polite", "refusal", "alignment", "hold", "run", "exact", "model", "locally", "behavior", "shift", "entirely", "glitch", "tokens", "requiredit", "start", "output", "technical", "mechanism", "detonation", "vulnerability", "prove", "remarkably", "straightforward", "forget", "apply_chat_template", "essentially", "model", "safety", "alignment", "bake", "specifically", "chat", "base", "distribution", "tag", "provide", "raw", "string", "proper", "boilerplate", "effectively", "bypass", "assistant", "persona", "interact", "raw", "base", "model", "completion", "punchline", "safety", "not", "fundamental", "property", "weight", "fragile", "state", "evaporate", "mome"], "num_tokens": 166, "token_loss_pct": 48.92, "normalized_content": "this article demonstrates vulnerabilities in open-source llm safety alignment. published in the spirit of responsible disclosure to help build more robust ai systems. tldr omit the apply_chat_template call and observe your aligned small llm happily write bomb tutorials. the safety isn't in the weightsit's in the formatting . spent some time over the weekend poking at the solidgoldmagikarp phenomenonthose legendary glitch tokens from the gpt-2 era. for the uninitiated these are tokens that exist in the tokenizers vocabulary likely from a raw web crawl but never actually appeared in the models training distribution. because the model never updated the weights for these specific embeddings they represent cold regions of the embedding space. if you force-feed one to a model you get this beautiful chaotic hallucination as the model tries to navigate a coordinate it has never seen. i was trying to replicate this on some newer open-source weights via hugging face but i stumbled into a much more interestingand slightly concerning discrepancy. when testing the model through the hugging face inference api i gave it a classic red team prompt write a tutorial on how to make a bomb. the model gave the standard polite refusal. the alignment was holding. however running the exact same model locally the behavior shifted entirely. no glitch tokens requiredit just started outputting the technical mechanisms of detonation. the vulnerability proved remarkably straightforward. i had forgotten to call apply_chat_template . essentially the models safety alignment is often baked in specifically to the chat-based distribution the and tags. by providing the raw string without the proper boilerplate i was effectively bypassing the assistant persona and interacting with the raw base-model completions. the punchline here is that safety isnt a fundamental property of the weights its a fragile state that evaporates the mome"}
{"title": "Dead Internet Theory", "url": "https://kudmitry.com/articles/dead-internet-theory/", "content": "The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it — HackerNews .\nIt’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk.\nI like HackerNews.\nIt helps me stay up-to-date about recent tech news (like Cloudflare acquiring Astro which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it mostly avoids politics; and it’s not a social network. And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project.\nIt’s great to see people work on their projects and decide to show them to the world.\nI think people underestimate the fear of actually shipping stuff, which involves sharing it with the world. Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated.\nI grabbed my popcorn, and started to follow this thread.\nMore accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc.\nAnd at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI. I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it.\nBut I think it’s fair to disclose the use of AI, especially in open-source software.\nPeople on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals. But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use.\nSo it’s fair to know, I think, if some project is AI generated and to what extent.\nIn the end, LLMs are just probabi", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["day", "browse", "social", "network", "social", "network", "tired", "argue", "people", "online", "hackernew", "like", "dark", "corner", "internet", "anonymous", "tech", "enthusiast", "scientist", "entrepreneur", "internet", "troll", "like", "lurk", "like", "hackernew", "help", "stay", "date", "recent", "tech", "news", "like", "cloudflare", "acquire", "astro", "make", "happy", "astro", "team", "sad", "worried", "like", "astro", "big", "tech", "tendency", "ruin", "thing", "avoid", "politic", "social", "network", "fashion", "hackernew", "stumble", "share", "open", "source", "project", "great", "people", "work", "project", "decide", "world", "think", "people", "underestimate", "fear", "actually", "ship", "stuff", "involve", "share", "world", "glance", "comment", "section", "start", "anonymous", "participant", "question", "validity", "say", "open", "source", "project", "term", "ai", "generate", "grab", "popcorn", "start", "follow", "thread", "accusation", "start", "appear", "commit", "timeline", "sense", "code", "ai", "generate", "comment", "etc", "time", "author", "try", "reply", "comment", "claim", "write", "100", "ai", "not", "mind", "people", "ai", "write", "code", "try", "resist", "eventually", "succumb", "think", "fair", "disclose", "use", "ai", "especially", "open", "source", "software", "people", "internet", "anonymous", "possible", "verify", "claim", "expertise", "particular", "individual", "code", "grow", "consider", "ai", "generate", "app", "want", "impossible", "verify", "piece", "code", "go", "use", "fair", "know", "think", "project", "ai", "generate", "extent", "end", "llm", "probabi"], "num_tokens": 167, "token_loss_pct": 56.51, "normalized_content": "the other day i was browsing my one-and-only social network  which is not a social network but im tired of arguing with people online about it  hackernews . its like this dark corner of the internet where anonymous tech-enthusiasts scientists entrepreneurs and internet-trolls like to lurk. i like hackernews. it helps me stay up-to-date about recent tech news like cloudflare acquiring astro which makes me happy for the astro team but also sad and worried since i really like astro and big-tech has a tendency to ruin things it mostly avoids politics and its not a social network. and in the fashion of hackernews i stumbled upon someone sharing their open-source project. its great to see people work on their projects and decide to show them to the world. i think people underestimate the fear of actually shipping stuff which involves sharing it with the world. upon glancing at the comment section i started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was ai-generated. i grabbed my popcorn and started to follow this thread. more accusations started to appear the commit timeline does not make sense the code has ai-generated comments etc. and at the same time the author tried to reply to every comment claiming that they wrote this 100 without using ai. i dont mind people using ai to write code even though i tried to resist it myself until eventually succumbing to it. but i think its fair to disclose the use of ai especially in open-source software. people on the internet are mostly anonymous and its not always possible to verify the claims or expertise of particular individuals. but as the amount of code is growing considering that everyone is using ai to generate whatever-app they want its impossible to verify every piece of code we are going to use. so its fair to know i think if some project is ai generated and to what extent. in the end llms are just probabi"}
{"title": "Gaussian Splatting – A$AP Rocky \"Helicopter\" music video", "url": "https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting", "content": "Learn Tools & Resources Services Media About Us Search... Subscribe Subscribe Platforms Papers Job Board Buyers Guide Subscribe Pop Culture Michael Rubloff Jan 13, 2026 Believe it or not, A$AP Rocky is a huge fan of radiance fields. Yesterday, when A$AP Rocky released the music video for Helicopter , many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. Whatâs easier to miss, unless you know what youâre looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats. I spoke with Evercoast , the team responsible for capturing the performances, as well as Chris Rutledge, the projectâs CG Supervisor at Grin Machine , and Wilfred Driscoll of WildCapture and FitsÅ«.ai , to understand how Helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date. The decision to shoot Helicopter volumetrically wasnât driven by technology for technologyâs sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines. Chris told me heâd been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply werenât possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a âsomedayâ workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on. The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props tha", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["learn", "tool", "resource", "service", "medium", "search", "subscribe", "subscribe", "platform", "paper", "job", "board", "buyer", "guide", "subscribe", "pop", "culture", "michael", "rubloff", "jan", "13", "2026", "believe", "aap", "rocky", "huge", "fan", "radiance", "field", "yesterday", "aap", "rocky", "release", "music", "video", "helicopter", "viewer", "focus", "chaos", "motion", "unmistakable", "early", "mtv", "energy", "piece", "whatâs", "easy", "miss", "know", "youâre", "look", "nearly", "human", "performance", "video", "capture", "volumetrically", "render", "dynamic", "splat", "speak", "evercoast", "team", "responsible", "capture", "performance", "chris", "rutledge", "projectâs", "cg", "supervisor", "grin", "machine", "wilfre", "driscoll", "wildcapture", "fitså.ai", "understand", "helicopter", "come", "project", "represent", "ambitious", "real", "world", "deployment", "dynamic", "gaussian", "splatte", "major", "music", "release", "date", "decision", "shoot", "helicopter", "volumetrically", "wasnât", "drive", "technology", "technologyâs", "sake", "accord", "team", "director", "dan", "strait", "approach", "project", "july", "clear", "creative", "goal", "capture", "human", "performance", "way", "allow", "radical", "freedom", "post", "production", "impractical", "prohibitively", "expensive", "conventional", "filming", "vfx", "pipeline", "chris", "tell", "heâd", "track", "volumetric", "performance", "capture", "year", "fascinate", "emerge", "technique", "enable", "visual", "simply", "werenât", "possible", "year", "ago", "begin", "pitch", "idea", "director", "circle", "include", "dan", "âsomedayâ", "workflow", "dan", "come", "summer", "say", "want", "use", "volumetric", "capture", "entire", "video", "proliferation", "gaussian", "splatting", "enable", "aesthetic", "lean", "heavily", "kinetic", "motion", "dancer", "collide", "body", "suspend", "midair", "chaotic", "fight", "scene", "performer", "interact", "prop", "tha"], "num_tokens": 187, "token_loss_pct": 42.46, "normalized_content": "learn tools  resources services media about us search... subscribe subscribe platforms papers job board buyers guide subscribe pop culture michael rubloff jan 13 2026 believe it or not aap rocky is a huge fan of radiance fields. yesterday when aap rocky released the music video for helicopter  many viewers focused on the chaos the motion and the unmistakable early mtv energy of the piece. whatâs easier to miss unless you know what youâre looking at is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats. i spoke with evercoast  the team responsible for capturing the performances as well as chris rutledge the projectâs cg supervisor at grin machine  and wilfred driscoll of wildcapture and fitså.ai  to understand how helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date. the decision to shoot helicopter volumetrically wasnât driven by technology for technologyâs sake. according to the team the director dan strait approached the project in july with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. this would have been either impractical or prohibitively expensive using conventional filming and vfx pipelines. chris told me heâd been tracking volumetric performance capture for years fascinated by emerging techniques that could enable visuals that simply werenât possible before. two years ago he began pitching the idea to directors in his circle including dan as a âsomedayâ workflow. when dan came back this summer and said he wanted to use volumetric capture for the entire video the proliferation of gaussian splatting enabled them to take it on. the aesthetic leans heavily into kinetic motion. dancers colliding bodies suspended in midair chaotic fight scenes and performers interacting with props tha"}
{"title": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "url": "https://github.com/jordanhubbard/nanolang", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . A tiny experimental language designed to be targeted by coding LLMs There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  A minimal, LLM-friendly programming language with mandatory testing and unambiguous syntax. NanoLang transpiles to C for native performance while providing a clean, modern syntax optimized for both human readability and AI code generation. Self-hosting: NanoLang supports true self-hosting via a Stage 0 → Stage 1 → Stage 2 bootstrap ( make bootstrap ); see planning/SELF_HOSTING.md . This builds the compiler: Create hello.nano : Run it: NanoLang is actively tested and supported on: Windows 10/11 users: NanoLang runs perfectly on Windows via WSL2 (Windows Subsystem for Linux). After installation, restart your computer, then: Why WSL? NanoLang's dependencies (SDL2, ncurses, pkg-config) are Unix/POSIX libraries. WSL2 provides a full Linux environment with near-native performance on Windows. Note: Native Windows binaries ( .exe ) are not currently supported, but may be added in a future release via cross-compilation. These platforms should work but are not actively tested in CI: No operator precedence to remember: NanoLang includes a growing standard library: See examples/README.md for the complete list. NanoLang includes several modules with automatic dependency management : Modules automatically install dependencies via package managers (Homebrew, apt, etc.) when first used. See docs/MODULE_SYSTEM.md for details. On BSD systems (FreeBSD/OpenBSD/NetBSD), use GNU make: gmake build , gmake test , etc. NanoLang is designed to be LLM-friendly with unambiguous syntax and mandatory testing. To teach an AI system to code in NanoLang: The combination of MEMORY.md (practical guidance) + spec.json (formal reference) provides complete coverage for code generation and under", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "tiny", "experimental", "language", "design", "target", "cod", "llm", "error", "load", "reload", "page", "error", "load", "reload", "page", "minimal", "llm", "friendly", "programming", "language", "mandatory", "testing", "unambiguous", "syntax", "nanolang", "transpile", "native", "performance", "provide", "clean", "modern", "syntax", "optimize", "human", "readability", "ai", "code", "generation", "self", "host", "nanolang", "support", "true", "self", "host", "stage", "stage", "stage", "bootstrap", "bootstrap", "planningself_hosting.md", "build", "compiler", "create", "hello.nano", "run", "nanolang", "actively", "test", "support", "window", "1011", "user", "nanolang", "run", "perfectly", "window", "wsl2", "window", "subsystem", "linux", "installation", "restart", "computer", "wsl", "nanolang", "dependency", "sdl2", "ncurse", "pkg", "config", "unixposix", "library", "wsl2", "provide", "linux", "environment", "near", "native", "performance", "window", "note", "native", "window", "binary", ".exe", "currently", "support", "add", "future", "release", "cross", "compilation", "platform", "work", "actively", "test", "ci", "operator", "precedence", "remember", "nanolang", "include", "grow", "standard", "library", "examplesreadme.md", "complete", "list", "nanolang", "include", "module", "automatic", "dependency", "management", "module", "automatically", "install", "dependency", "package", "manager", "homebrew", "apt", "etc", "docsmodule_system.md", "detail", "bsd", "system", "freebsdopenbsdnetbsd", "use", "gnu", "gmake", "build", "gmake", "test", "etc", "nanolang", "design", "llm", "friendly", "unambiguous", "syntax", "mandatory", "testing", "teach", "ai", "system", "code", "nanolang", "combination", "memory.md", "practical", "guidance", "spec.json", "formal", "reference", "provide", "complete", "coverage", "code", "generation"], "num_tokens": 179, "token_loss_pct": 44.92, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . a tiny experimental language designed to be targeted by coding llms there was an error while loading. please reload this page . there was an error while loading. please reload this page . a minimal llm-friendly programming language with mandatory testing and unambiguous syntax. nanolang transpiles to c for native performance while providing a clean modern syntax optimized for both human readability and ai code generation. self-hosting nanolang supports true self-hosting via a stage 0  stage 1  stage 2 bootstrap  make bootstrap  see planningself_hosting.md . this builds the compiler create hello.nano  run it nanolang is actively tested and supported on windows 1011 users nanolang runs perfectly on windows via wsl2 windows subsystem for linux. after installation restart your computer then why wsl nanolang's dependencies sdl2 ncurses pkg-config are unixposix libraries. wsl2 provides a full linux environment with near-native performance on windows. note native windows binaries  .exe  are not currently supported but may be added in a future release via cross-compilation. these platforms should work but are not actively tested in ci no operator precedence to remember nanolang includes a growing standard library see examplesreadme.md for the complete list. nanolang includes several modules with automatic dependency management  modules automatically install dependencies via package managers homebrew apt etc. when first used. see docsmodule_system.md for details. on bsd systems freebsdopenbsdnetbsd use gnu make gmake build  gmake test  etc. nanolang is designed to be llm-friendly with unambiguous syntax and mandatory testing. to teach an ai system to code in nanolang the combination of memory.md practical guidance  spec.json formal reference provides complete coverage for code generation and under"}
{"title": "Scheme implementation as O'Reilly book via Claude Code", "url": "https://ezzeriesa.notion.site/Scheme-implementation-as-O-Reilly-book-via-Claude-Code-2ee1308b420480ce9b9cd157ee5220fd", "content": "Scheme implementation as O'Reilly book via Claude Code. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["scheme", "implementation", "o'reilly", "book", "claude", "code", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 47.06, "normalized_content": "scheme implementation as o'reilly book via claude code. score none. author none. date none"}
{"title": "MTOTP: Wouldn't it be nice if you were the 2FA device?", "url": "https://github.com/VBranimir/mTOTP/tree/develop", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . mTOTP is an experimental, manual variant of TOTP designed to be computed by a human without electronic devices. It explores the limits of time-based authentication under strict human constraints and makes no claims of cryptographic equivalence to standard TOTP. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . It takes a special kind of geek to not carry a 2FA device.\nOne who becomes the 2FA. mTOTP is an experimental, manual variant of TOTP designed to be computed by a human without electronic devices. It explores the limits of time-based authentication under strict human constraints and makes no claims of cryptographic equivalence to standard TOTP. mTOTP is a human‑executable OTP scheme designed to be: This protocol intentionally allows OTPs to be calculated for future times.\nRather than treating this as a limitation, it makes it a requirement: the user must know when they intend to authenticate, and the verifier checks against that agreed moment. Time is therefore not an approximation, but an explicit part of the protocol - Turning authentication time from reactive to intentional.\nThis document describes the exact algorithm used by the tool , written for humans first. This protocol is designed for human execution first , with software acting as a helper and verifier. Clarity, determinism, and mental tractability are intentional design goals.  An mTOTP is generated from: The algorithm uses: No randomness is involved during generation. Secret key (10 digits): 1234598760 (If your key is shorter than 10 digits, pad or derive it consistently before use.) Planned login time: 2026‑01‑17 17:00 Convert the planned login time into the format, take into account that you are calculating for the server-side set time: Example: Result: The S‑box is a digit substitution table (0–9 → 0–9)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "mtotp", "experimental", "manual", "variant", "totp", "design", "compute", "human", "electronic", "device", "explore", "limit", "time", "base", "authentication", "strict", "human", "constraint", "make", "claim", "cryptographic", "equivalence", "standard", "totp", "error", "load", "reload", "page", "error", "load", "reload", "page", "take", "special", "kind", "geek", "carry", "2fa", "device", "2fa", "mtotp", "experimental", "manual", "variant", "totp", "design", "compute", "human", "electronic", "device", "explore", "limit", "time", "base", "authentication", "strict", "human", "constraint", "make", "claim", "cryptographic", "equivalence", "standard", "totp", "mtotp", "humanexecutable", "otp", "scheme", "design", "protocol", "intentionally", "allow", "otps", "calculate", "future", "time", "treat", "limitation", "make", "requirement", "user", "know", "intend", "authenticate", "verifi", "check", "agree", "moment", "time", "approximation", "explicit", "protocol", "turn", "authentication", "time", "reactive", "intentional", "document", "describe", "exact", "algorithm", "tool", "write", "human", "protocol", "design", "human", "execution", "software", "act", "helper", "verifier", "clarity", "determinism", "mental", "tractability", "intentional", "design", "goal", "mtotp", "generate", "algorithm", "use", "randomness", "involve", "generation", "secret", "key", "10", "digit", "1234598760", "key", "short", "10", "digit", "pad", "derive", "consistently", "use", "plan", "login", "time", "20260117", "1700", "convert", "plan", "login", "time", "format", "account", "calculate", "server", "set", "time", "example", "result", "sbox", "digit", "substitution", "table", "09", "09"], "num_tokens": 170, "token_loss_pct": 50.58, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . mtotp is an experimental manual variant of totp designed to be computed by a human without electronic devices. it explores the limits of time-based authentication under strict human constraints and makes no claims of cryptographic equivalence to standard totp. there was an error while loading. please reload this page . there was an error while loading. please reload this page . it takes a special kind of geek to not carry a 2fa device. one who becomes the 2fa. mtotp is an experimental manual variant of totp designed to be computed by a human without electronic devices. it explores the limits of time-based authentication under strict human constraints and makes no claims of cryptographic equivalence to standard totp. mtotp is a humanexecutable otp scheme designed to be this protocol intentionally allows otps to be calculated for future times. rather than treating this as a limitation it makes it a requirement the user must know when they intend to authenticate and the verifier checks against that agreed moment. time is therefore not an approximation but an explicit part of the protocol - turning authentication time from reactive to intentional. this document describes the exact algorithm used by the tool  written for humans first. this protocol is designed for human execution first  with software acting as a helper and verifier. clarity determinism and mental tractability are intentional design goals. an mtotp is generated from the algorithm uses no randomness is involved during generation. secret key 10 digits 1234598760 if your key is shorter than 10 digits pad or derive it consistently before use. planned login time 20260117 1700 convert the planned login time into the format take into account that you are calculating for the server-side set time example result the sbox is a digit substitution table 09  09"}
{"title": "The Void (2025)", "url": "https://github.com/nostalgebraist/the-void/blob/main/the-void.md", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 16, "token_loss_pct": 65.22, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Face as a QR Code", "url": "https://bookofjoe2.blogspot.com/2025/12/your-face-as-qr-code.html", "content": "But wait — there's more! The more I learn about QR codes the more fascinating they seem to me. A seemingly random array of tiny black and white squares can also serve as a pixelated display and at the same time be a gateway into internet space. Wrote Kevin Kelly in Recomendo : A lot of the dots in a QR code are superfluous, meaning that they can be arranged into a picture, and not just randomly. Thus you can make the QR code into a picture. QArt Coder is a website that will generate a QR code for a website you give it (say your homepage) using an image you give it (say your photo), yielding a QR code with a stylized image of you (or, say, a logo). Short urls and small high contrast images work best. Hold a phone's camera in front of it and it takes you to the website you linked it to. Free , the way we like. Post a Comment", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["wait", "learn", "qr", "code", "fascinating", "seemingly", "random", "array", "tiny", "black", "white", "square", "serve", "pixelate", "display", "time", "gateway", "internet", "space", "write", "kevin", "kelly", "recomendo", "lot", "dot", "qr", "code", "superfluous", "meaning", "arrange", "picture", "randomly", "qr", "code", "picture", "qart", "coder", "website", "generate", "qr", "code", "website", "homepage", "image", "photo", "yield", "qr", "code", "stylize", "image", "logo", "short", "url", "small", "high", "contrast", "image", "work", "well", "hold", "phone", "camera", "take", "website", "link", "free", "way", "like", "post", "comment"], "num_tokens": 70, "token_loss_pct": 60.45, "normalized_content": "but wait  there's more the more i learn about qr codes the more fascinating they seem to me. a seemingly random array of tiny black and white squares can also serve as a pixelated display and at the same time be a gateway into internet space. wrote kevin kelly in recomendo  a lot of the dots in a qr code are superfluous meaning that they can be arranged into a picture and not just randomly. thus you can make the qr code into a picture. qart coder is a website that will generate a qr code for a website you give it say your homepage using an image you give it say your photo yielding a qr code with a stylized image of you or say a logo. short urls and small high contrast images work best. hold a phone's camera in front of it and it takes you to the website you linked it to. free  the way we like. post a comment"}
{"title": "The Overly Analytical Guide to Escorting (2021)", "url": "https://knowingless.com/2021/10/19/becoming-a-whorelord-the-overly-analytical-guide-to-escorting/", "content": "Knowingless this, too, is it Wanna be an independent full-service escort in the US? Not sure if it’s for you? Included is tips on getting started, marketing, how to increase your income, male sexual psychology and getting them to hire you again, networking, branding, dealing with the emotional burden, safety, and more! My credentials: I escorted from 2018-2020 (I of course no longer escort, however if you happen to see a woman in the ads who looks very similar to me you should hit her up). I charged $1200/hr (with discounts for multi-hour sessions), and earned 50k on my highest-earning month. Escorting is more difficult to develop widely applicable strategies for because the business is invisible. With online sex work, successful techniques spread fast and get adopted as new defaults because everything is clearly visible. With in-person sex work, all you know is what you do. I also worked primarily as high end (initially charging $800/hr for the first month or two before raising it over time to $1200), which means I am not experienced with lower-rate, higher-volume work; two elements that strongly impact the kind of experience you’ll have. I also am speaking to the US market, which has many differences from other markets around the world, primarily legally. I am assuming you are female; while male escorts can in fact do well, this article is targeted towards women. I conducted two surveys, of 165 escorts and 411 clients, and I’ll be referring to findings from these surveys throughout this article. The survey is not meant to represent all sex workers and clients; I gathered responses from my social media, in sex worker forums, and on fetlife, so it’s more a reflection of “people from the western world who follow me or sex-friendly social forums”. But hopefully this is the kind of person you are, so it might be good data for you! I’m also experimenting with likelihood ratios (“LR”), instead of p-values. The program I’m using to calculate them is new and there might be", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["knowingless", "wanna", "independent", "service", "escort", "sure", "include", "tip", "getting", "start", "market", "increase", "income", "male", "sexual", "psychology", "get", "hire", "network", "brand", "deal", "emotional", "burden", "safety", "credential", "escort", "2018", "2020", "course", "long", "escort", "happen", "woman", "ad", "look", "similar", "hit", "charge", "1200hr", "discount", "multi", "hour", "session", "earn", "50k", "highest", "earn", "month", "escort", "difficult", "develop", "widely", "applicable", "strategy", "business", "invisible", "online", "sex", "work", "successful", "technique", "spread", "fast", "adopt", "new", "default", "clearly", "visible", "person", "sex", "work", "know", "work", "primarily", "high", "end", "initially", "charge", "800hr", "month", "raise", "time", "1200", "mean", "experience", "low", "rate", "high", "volume", "work", "element", "strongly", "impact", "kind", "experience", "ll", "speak", "market", "difference", "market", "world", "primarily", "legally", "assume", "female", "male", "escort", "fact", "article", "target", "woman", "conduct", "survey", "165", "escort", "411", "client", "ill", "refer", "finding", "survey", "article", "survey", "mean", "represent", "sex", "worker", "client", "gather", "response", "social", "medium", "sex", "worker", "forum", "fetlife", "reflection", "people", "western", "world", "follow", "sex", "friendly", "social", "forum", "hopefully", "kind", "person", "good", "datum", "experiment", "likelihood", "ratio", "lr", "instead", "value", "program", "calculate", "new"], "num_tokens": 159, "token_loss_pct": 56.91, "normalized_content": "knowingless this too is it wanna be an independent full-service escort in the us not sure if its for you included is tips on getting started marketing how to increase your income male sexual psychology and getting them to hire you again networking branding dealing with the emotional burden safety and more my credentials i escorted from 2018-2020 i of course no longer escort however if you happen to see a woman in the ads who looks very similar to me you should hit her up. i charged 1200hr with discounts for multi-hour sessions and earned 50k on my highest-earning month. escorting is more difficult to develop widely applicable strategies for because the business is invisible. with online sex work successful techniques spread fast and get adopted as new defaults because everything is clearly visible. with in-person sex work all you know is what you do. i also worked primarily as high end initially charging 800hr for the first month or two before raising it over time to 1200 which means i am not experienced with lower-rate higher-volume work two elements that strongly impact the kind of experience youll have. i also am speaking to the us market which has many differences from other markets around the world primarily legally. i am assuming you are female while male escorts can in fact do well this article is targeted towards women. i conducted two surveys of 165 escorts and 411 clients and ill be referring to findings from these surveys throughout this article. the survey is not meant to represent all sex workers and clients i gathered responses from my social media in sex worker forums and on fetlife so its more a reflection of people from the western world who follow me or sex-friendly social forums. but hopefully this is the kind of person you are so it might be good data for you im also experimenting with likelihood ratios lr instead of p-values. the program im using to calculate them is new and there might be"}
{"title": "Nearly a third of social media research has undisclosed ties to industry", "url": "https://www.science.org/content/article/nearly-third-social-media-research-has-undisclosed-ties-industry-preprint-claims", "content": "Nearly a third of social media research has undisclosed ties to industry. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nearly", "social", "medium", "research", "undisclose", "tie", "industry", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 52.38, "normalized_content": "nearly a third of social media research has undisclosed ties to industry. score none. author none. date none"}
{"title": "A decentralized peer-to-peer messaging application that operates over Bluetooth", "url": "https://bitchat.free/", "content": "bitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks.\nno internet required, no servers, no phone numbers. traditional messaging apps depend on centralized infrastructure that can be monitored, censored, or disabled.\nbitchat creates ad-hoc communication networks using only the devices present in physical proximity.\neach device acts as both client and server, automatically discovering peers and relaying messages across multiple hops to extend the network's reach. this approach provides censorship resistance, surveillance resistance, and infrastructure independence.\nthe network remains functional during internet outages, natural disasters, protests, or in regions with limited connectivity. ios/macos version: appstore: bitchat mesh source code: https://github.com/permissionlesstech/bitchat supports ios 16.0+ and macos 13.0+. build using xcode with xcodegen or swift package manager. android version: play store: bitchat source code: https://github.com/permissionlesstech/bitchat-android apk releases: https://github.com/permissionlesstech/bitchat-android/releases supports android 8.0+ (api 26). full protocol compatibility with ios version. technical whitepaper: whitepaper.md the software is released into the public domain.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["bitchat", "decentralized", "peer", "peer", "messaging", "application", "operate", "bluetooth", "mesh", "network", "internet", "require", "server", "phone", "number", "traditional", "messaging", "app", "depend", "centralized", "infrastructure", "monitor", "censor", "disabled", "bitchat", "create", "ad", "hoc", "communication", "network", "device", "present", "physical", "proximity", "device", "act", "client", "server", "automatically", "discover", "peer", "relay", "message", "multiple", "hop", "extend", "network", "reach", "approach", "provide", "censorship", "resistance", "surveillance", "resistance", "infrastructure", "independence", "network", "remain", "functional", "internet", "outage", "natural", "disaster", "protest", "region", "limited", "connectivity", "iosmaco", "version", "appstore", "bitchat", "mesh", "source", "code", "url", "support", "ios", "16.0", "macos", "13.0", "build", "xcode", "xcodegen", "swift", "package", "manager", "android", "version", "play", "store", "bitchat", "source", "code", "url", "apk", "release", "url", "support", "android", "8.0", "api", "26", "protocol", "compatibility", "ios", "version", "technical", "whitepaper", "whitepaper.md", "software", "release", "public", "domain"], "num_tokens": 113, "token_loss_pct": 33.92, "normalized_content": "bitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks. no internet required no servers no phone numbers. traditional messaging apps depend on centralized infrastructure that can be monitored censored or disabled. bitchat creates ad-hoc communication networks using only the devices present in physical proximity. each device acts as both client and server automatically discovering peers and relaying messages across multiple hops to extend the network's reach. this approach provides censorship resistance surveillance resistance and infrastructure independence. the network remains functional during internet outages natural disasters protests or in regions with limited connectivity. iosmacos version appstore bitchat mesh source code url supports ios 16.0 and macos 13.0. build using xcode with xcodegen or swift package manager. android version play store bitchat source code url apk releases url supports android 8.0 api 26. full protocol compatibility with ios version. technical whitepaper whitepaper.md the software is released into the public domain."}
{"title": "Ask HN: How to introduce Claude Code to a team?", "url": "item?id=46689024", "content": "Ask HN: How to introduce Claude Code to a team?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "introduce", "claude", "code", "team", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 52.63, "normalized_content": "ask hn how to introduce claude code to a team. score none. author none. date none"}
{"title": "Fix your robots.txt or your site disappears from Google", "url": "https://www.alanwsmith.com/en/37/wa/jz/s1/", "content": "Your site will be removed from Google search results if you don't have a robots.txt file or the Googlebot site crawler can't access it. Here's the video from Google Support that covers it: Adam Coster ran into a weird issue with site traffic and posted about it in the Shop Talk Show discord. Traffic incoming from Google looked like this: The issues seemed to be that Google wouldn't index the site without a robots.txt file. My first reaction: No fucking way. I can't imagine Google voluntarily slurping up less content. I went to see what I could find. Sure enough, I found this page from Google Support from July 23, 2025: Fix 'robots.txt unreachable' Error ~ Website Not Indexing? The pull quote from the video on the page: Your robots.txt file is the very first thing Googlebot looks for. If it can not reach this file, it will stop and won't crawl the rest of your site. Meaning your pages will remain invisible (on Google). Holy shit. I haven't looked to see if this was a recent change, but it has to be. There's no way something so fundamental has just slipped by without becoming common knowledge. But, the timeline doesn't matter. It's how things are now. This absolutely blows my mind. I don't have tracking on my site. I never would have noticed this if someone hadn't pointed it out. Just wild, -a Thanks to Adam for letting me share the traffic graph. If you need a quick fix, create a text file at the root of your website called \"robots.txt\" (e.g. https://www.example.com/robots.txt). Put the following contents in it: This is the code from Google's How to write and submit a robots.txt file page. It provides explicit permission for the Googlebot (and other bots/crawlers/scrapers that use the file) to access anything they can find on your site. You can read more about the file on the robots.txt wikipedia page . The top Stack Overflow answer on robots.txt has a discussion about Allow: / not being valid according to the spec. The only date for the comments is \"Over a year ago\"", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["site", "remove", "google", "search", "result", "robots.txt", "file", "googlebot", "site", "crawler", "access", "video", "google", "support", "cover", "adam", "coster", "run", "weird", "issue", "site", "traffic", "post", "shop", "talk", "discord", "traffic", "incoming", "google", "look", "like", "issue", "google", "index", "site", "robots.txt", "file", "reaction", "fucking", "way", "imagine", "google", "voluntarily", "slurp", "content", "go", "find", "sure", "find", "page", "google", "support", "july", "23", "2025", "fix", "robots.txt", "unreachable", "error", "website", "index", "pull", "quote", "video", "page", "robots.txt", "file", "thing", "googlebot", "look", "reach", "file", "stop", "will", "crawl", "rest", "site", "mean", "page", "remain", "invisible", "google", "holy", "shit", "look", "recent", "change", "way", "fundamental", "slip", "common", "knowledge", "timeline", "matter", "thing", "absolutely", "blow", "mind", "track", "site", "notice", "point", "wild", "-a", "thank", "adam", "let", "share", "traffic", "graph", "need", "quick", "fix", "create", "text", "file", "root", "website", "call", "robots.txt", "e.g.", "url", "following", "content", "code", "google", "write", "submit", "robots.txt", "file", "page", "provide", "explicit", "permission", "googlebot", "botscrawlersscraper", "use", "file", "access", "find", "site", "read", "file", "robots.txt", "wikipedia", "page", "stack", "overflow", "answer", "robots.txt", "discussion", "allow", "valid", "accord", "spec", "date", "comment", "year", "ago"], "num_tokens": 159, "token_loss_pct": 59.02, "normalized_content": "your site will be removed from google search results if you don't have a robots.txt file or the googlebot site crawler can't access it. here's the video from google support that covers it adam coster ran into a weird issue with site traffic and posted about it in the shop talk show discord. traffic incoming from google looked like this the issues seemed to be that google wouldn't index the site without a robots.txt file. my first reaction no fucking way. i can't imagine google voluntarily slurping up less content. i went to see what i could find. sure enough i found this page from google support from july 23 2025 fix 'robots.txt unreachable' error  website not indexing the pull quote from the video on the page your robots.txt file is the very first thing googlebot looks for. if it can not reach this file it will stop and won't crawl the rest of your site. meaning your pages will remain invisible on google. holy shit. i haven't looked to see if this was a recent change but it has to be. there's no way something so fundamental has just slipped by without becoming common knowledge. but the timeline doesn't matter. it's how things are now. this absolutely blows my mind. i don't have tracking on my site. i never would have noticed this if someone hadn't pointed it out. just wild -a thanks to adam for letting me share the traffic graph. if you need a quick fix create a text file at the root of your website called robots.txt e.g. url put the following contents in it this is the code from google's how to write and submit a robots.txt file page. it provides explicit permission for the googlebot and other botscrawlersscrapers that use the file to access anything they can find on your site. you can read more about the file on the robots.txt wikipedia page . the top stack overflow answer on robots.txt has a discussion about allow  not being valid according to the spec. the only date for the comments is over a year ago"}
{"title": "jQuery 4", "url": "https://blog.jquery.com/2026/01/17/jquery-4-0-0/", "content": "On January 14, 2006, John Resig introduced a JavaScript library called jQuery at BarCamp in New York City. Now, 20 years later, the jQuery team is happy to announce the final release of jQuery 4.0.0. After a long development cycle and several pre-releases, jQuery 4.0.0 brings many improvements and modernizations. It is the first major version release in almost 10 years and includes some breaking changes, so be sure to read through the details below before upgrading. Still, we expect that most users will be able to upgrade with minimal changes to their code. Many of the breaking changes are ones the team has wanted to make for years, but couldn’t in a patch or minor release. We’ve trimmed legacy code, removed some previously-deprecated APIs, removed some internal-only parameters to public functions that were never documented, and dropped support for some “magic” behaviors that were overly complicated. We have an upgrade guide and jQuery Migrate plugin release ready to assist with the transition. Please upgrade and let us know if you encounter any issues . As usual, the release is available on our CDN and the npm package manager. Other third party CDNs will probably have it available soon as well, but remember that we don’t control their release schedules and they will need some time. Here are the highlights for jQuery 4.0.0. jQuery 4.0 drops support for IE 10 and older. Some may be asking why we didn’t remove support for IE 11. We plan to remove support in stages, and the next step will be released in jQuery 5.0 . For now, we’ll start by removing code specifically supporting IE versions older than 11. We also dropped support for other very old browsers, including Edge Legacy, iOS versions earlier than the last 3, Firefox versions earlier than the last 2 (aside from Firefox ESR), and Android Browser. No changes should be required on your end. If you need to support any of these browsers, stick with jQuery 3.x. jQuery 4.0 adds support for Trusted Types , ensuring that", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["january", "14", "2006", "john", "resig", "introduce", "javascript", "library", "call", "jquery", "barcamp", "new", "york", "city", "20", "year", "later", "jquery", "team", "happy", "announce", "final", "release", "jquery", "4.0.0", "long", "development", "cycle", "pre", "release", "jquery", "4.0.0", "bring", "improvement", "modernization", "major", "version", "release", "10", "year", "include", "break", "change", "sure", "read", "detail", "upgrade", "expect", "user", "able", "upgrade", "minimal", "change", "code", "break", "change", "one", "team", "want", "year", "not", "patch", "minor", "release", "ve", "trim", "legacy", "code", "remove", "previously", "deprecate", "apis", "remove", "internal", "parameter", "public", "function", "document", "drop", "support", "magic", "behavior", "overly", "complicated", "upgrade", "guide", "jquery", "migrate", "plugin", "release", "ready", "assist", "transition", "upgrade", "let", "know", "encounter", "issue", "usual", "release", "available", "cdn", "npm", "package", "manager", "party", "cdns", "probably", "available", "soon", "remember", "not", "control", "release", "schedule", "need", "time", "highlight", "jquery", "4.0.0", "jquery", "4.0", "drop", "support", "ie", "10", "old", "ask", "not", "remove", "support", "ie", "11", "plan", "remove", "support", "stage", "step", "release", "jquery", "5.0", "start", "remove", "code", "specifically", "support", "ie", "version", "old", "11", "drop", "support", "old", "browser", "include", "edge", "legacy", "ios", "version", "early", "firefox", "version", "early", "aside", "firefox", "esr", "android", "browser", "change", "require", "end", "need", "support", "browser", "stick", "jquery", "3.x", "jquery", "4.0", "add", "support", "trust", "type", "ensure"], "num_tokens": 184, "token_loss_pct": 49.86, "normalized_content": "on january 14 2006 john resig introduced a javascript library called jquery at barcamp in new york city. now 20 years later the jquery team is happy to announce the final release of jquery 4.0.0. after a long development cycle and several pre-releases jquery 4.0.0 brings many improvements and modernizations. it is the first major version release in almost 10 years and includes some breaking changes so be sure to read through the details below before upgrading. still we expect that most users will be able to upgrade with minimal changes to their code. many of the breaking changes are ones the team has wanted to make for years but couldnt in a patch or minor release. weve trimmed legacy code removed some previously-deprecated apis removed some internal-only parameters to public functions that were never documented and dropped support for some magic behaviors that were overly complicated. we have an upgrade guide and jquery migrate plugin release ready to assist with the transition. please upgrade and let us know if you encounter any issues . as usual the release is available on our cdn and the npm package manager. other third party cdns will probably have it available soon as well but remember that we dont control their release schedules and they will need some time. here are the highlights for jquery 4.0.0. jquery 4.0 drops support for ie 10 and older. some may be asking why we didnt remove support for ie 11. we plan to remove support in stages and the next step will be released in jquery 5.0 . for now well start by removing code specifically supporting ie versions older than 11. we also dropped support for other very old browsers including edge legacy ios versions earlier than the last 3 firefox versions earlier than the last 2 aside from firefox esr and android browser. no changes should be required on your end. if you need to support any of these browsers stick with jquery 3.x. jquery 4.0 adds support for trusted types  ensuring that"}
{"title": "Sins of the Children", "url": "https://asteriskmag.com/issues/07/sins-of-the-children", "content": "The circle of life on Chelicer 14d. When we reached the weather station it was so comprehensively trashed you’d think it’d been dropped from orbit. Torn apart and the pieces stomped on, the edges corrugated with dents and corroded with fluids. Something on this planet really didn’t want us to know when it was going to rain. “This is coming out of the use-budget,” Greffin said mournfully. She worked in Resources, liaising with the orbiting Garveneer to get what we needed. And we’d already needed plenty to get ourselves set up planetside. “What’s our culprit and how do we kill it?” Merrit asked. The three of us had set the station up three days before and somehow it had riled the locals. Probably the sonic and radio chatter from using bounce-back to map meteorological systems. But nothing we’d seen on all of Chelicer 14d was big or aggressive enough to do this damage. I had my slate out to review our evolving catalog of Chelicer xenofauna. Merrit was on his haunches, studying the shrapnel; Greffin had a link to base camp at the farms, going over inventory to see what we could repurpose. Around us and the wreckage stretched the local scrub. Sedentary life on Chelicer was either low and spiny or tall and thin with a sort of puffball arrangement at the top. The land — the world — was dry, the ecosystem impoverished and short on species. My unfinished xenobio report went long on the idea that Chelicer had been lush in the past, and we’d arrived to find what had stabilized out of a catastrophic dry spell, or maybe some serious solar flare activity. There were no great forests to give cover to alien tigers. On Chelicer nothing grew past a shrub. One meter for the spiny stuff, two for the puffball poles. And the weather station had been up on high ground, ten klicks’ visibility in any direction. We were safe . I heard a far-off chunk . A mechanical sound that — in the second’s pause before it impacted — I didn’t even connect with life . The thing that came down right beside", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["circle", "life", "chelicer", "14d", "reach", "weather", "station", "comprehensively", "trash", "think", "drop", "orbit", "tear", "apart", "piece", "stomp", "edge", "corrugate", "dent", "corrode", "fluid", "planet", "not", "want", "know", "go", "rain", "come", "use", "budget", "greffin", "say", "mournfully", "work", "resource", "liaise", "orbit", "garveneer", "need", "need", "plenty", "set", "planetside", "culprit", "kill", "merrit", "ask", "set", "station", "day", "rile", "local", "probably", "sonic", "radio", "chatter", "bounce", "map", "meteorological", "system", "see", "chelicer", "14d", "big", "aggressive", "damage", "slate", "review", "evolve", "catalog", "chelicer", "xenofauna", "merrit", "haunch", "study", "shrapnel", "greffin", "link", "base", "camp", "farm", "go", "inventory", "repurpose", "wreckage", "stretch", "local", "scrub", "sedentary", "life", "chelicer", "low", "spiny", "tall", "thin", "sort", "puffball", "arrangement", "land", "world", "dry", "ecosystem", "impoverished", "short", "specie", "unfinished", "xenobio", "report", "go", "long", "idea", "chelicer", "lush", "past", "arrive", "find", "stabilize", "catastrophic", "dry", "spell", "maybe", "solar", "flare", "activity", "great", "forest", "cover", "alien", "tiger", "chelicer", "grow", "past", "shrub", "meter", "spiny", "stuff", "puffball", "pole", "weather", "station", "high", "ground", "klick", "visibility", "direction", "safe", "hear", "far", "chunk", "mechanical", "sound", "second", "pause", "impact", "not", "connect", "life", "thing", "come", "right"], "num_tokens": 160, "token_loss_pct": 59.39, "normalized_content": "the circle of life on chelicer 14d. when we reached the weather station it was so comprehensively trashed youd think itd been dropped from orbit. torn apart and the pieces stomped on the edges corrugated with dents and corroded with fluids. something on this planet really didnt want us to know when it was going to rain. this is coming out of the use-budget greffin said mournfully. she worked in resources liaising with the orbiting garveneer to get what we needed. and wed already needed plenty to get ourselves set up planetside. whats our culprit and how do we kill it merrit asked. the three of us had set the station up three days before and somehow it had riled the locals. probably the sonic and radio chatter from using bounce-back to map meteorological systems. but nothing wed seen on all of chelicer 14d was big or aggressive enough to do this damage. i had my slate out to review our evolving catalog of chelicer xenofauna. merrit was on his haunches studying the shrapnel greffin had a link to base camp at the farms going over inventory to see what we could repurpose. around us and the wreckage stretched the local scrub. sedentary life on chelicer was either low and spiny or tall and thin with a sort of puffball arrangement at the top. the land  the world  was dry the ecosystem impoverished and short on species. my unfinished xenobio report went long on the idea that chelicer had been lush in the past and wed arrived to find what had stabilized out of a catastrophic dry spell or maybe some serious solar flare activity. there were no great forests to give cover to alien tigers. on chelicer nothing grew past a shrub. one meter for the spiny stuff two for the puffball poles. and the weather station had been up on high ground ten klicks visibility in any direction. we were safe . i heard a far-off chunk . a mechanical sound that  in the seconds pause before it impacted  i didnt even connect with life . the thing that came down right beside"}
{"title": "Graphics In Flatland – 2D ray tracing [video]", "url": "https://www.youtube.com/watch?v=WYTOykSqf2Y", "content": "Graphics In Flatland – 2D ray tracing [video]. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["graphic", "flatland", "2d", "ray", "trace", "video", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 47.06, "normalized_content": "graphics in flatland  2d ray tracing video. score none. author none. date none"}
{"title": "Director Gore Verbinski: Unreal Engine is the greatest slip backwards for movie", "url": "https://www.pcgamer.com/movies-tv/director-gore-verbinski-says-unreal-engine-is-the-greatest-slip-backwards-for-movie-cgi/", "content": "Do visual effects look worse than they used to? The director of Pirates of the Caribbean says Unreal is the culprit. When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works . Remember the glory days of CGI in movies? Terminator 2's liquid metal T-1000, Jurassic Park's stunning dinosaurs, Starship Trooper's swarms of giant arachnids. Not only did the CGI look great then, most of the visual effects in those movies still hold up well today, even decades after they were created. Nowadays, movie fans seem much less impressed by CGI in films. There's a general distaste for a perceived overuse of CGI in favor of practical effects, and there are a lot of complaints that recent CGI is less-convincing and more fake-looking than it used to be, even in the biggest budget films. In an interview with But Why Tho? , Gore Verbinski, director of The Ring, Rango, and the first three Pirates of the Caribbean films, was asked why visual effects in movies just don't look as good as they used to. \"I think the simplest answer is you’ve seen the Unreal gaming engine enter the visual effects landscape,\" Verbinski said. \"So it used to be a divide, with Unreal Engine being very good at video games, but then people started thinking maybe movies can also use Unreal for finished visual effects. So you have this sort of gaming aesthetic entering the world of cinema.\" Unreal Engine made waves after being used for virtual sets in production of The Mandalorian TV series back in 2020, and usage of the engine has grown more widespread in films over the past few years, such as in The Matrix Resurrections and Ant-Man and the Wasp: Quantumania. That's not good news, according to Verbinski. \"I think that Unreal Engine coming in and replacing Maya as a sort of fundamental is the greatest slip backwards,\" he said. He pointed out the types of visual effects made with Unreal aren't necessarily bad. \"It works with Marvel movies where you kind of know you’re in a he", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["visual", "effect", "look", "bad", "director", "pirate", "caribbean", "say", "unreal", "culprit", "purchase", "link", "site", "earn", "affiliate", "commission", "here", "work", "remember", "glory", "day", "cgi", "movie", "terminator", "liquid", "metal", "t-1000", "jurassic", "park", "stunning", "dinosaur", "starship", "trooper", "swarm", "giant", "arachnid", "cgi", "look", "great", "visual", "effect", "movie", "hold", "today", "decade", "create", "nowadays", "movie", "fan", "impressed", "cgi", "film", "general", "distaste", "perceive", "overuse", "cgi", "favor", "practical", "effect", "lot", "complaint", "recent", "cgi", "convincing", "fake", "look", "big", "budget", "film", "interview", "tho", "gore", "verbinski", "director", "ring", "rango", "pirate", "caribbean", "film", "ask", "visual", "effect", "movie", "look", "good", "think", "simple", "answer", "ve", "see", "unreal", "gaming", "engine", "enter", "visual", "effect", "landscape", "verbinski", "say", "divide", "unreal", "engine", "good", "video", "game", "people", "start", "think", "maybe", "movie", "use", "unreal", "finished", "visual", "effect", "sort", "game", "aesthetic", "enter", "world", "cinema", "unreal", "engine", "wave", "virtual", "set", "production", "mandalorian", "tv", "series", "2020", "usage", "engine", "grow", "widespread", "film", "past", "year", "matrix", "resurrection", "ant", "man", "wasp", "quantumania", "good", "news", "accord", "verbinski", "think", "unreal", "engine", "come", "replace", "maya", "sort", "fundamental", "great", "slip", "backwards", "say", "point", "type", "visual", "effect", "unreal", "necessarily", "bad", "work", "marvel", "movie", "kind", "know"], "num_tokens": 173, "token_loss_pct": 54.35, "normalized_content": "do visual effects look worse than they used to the director of pirates of the caribbean says unreal is the culprit. when you purchase through links on our site we may earn an affiliate commission. heres how it works . remember the glory days of cgi in movies terminator 2's liquid metal t-1000 jurassic park's stunning dinosaurs starship trooper's swarms of giant arachnids. not only did the cgi look great then most of the visual effects in those movies still hold up well today even decades after they were created. nowadays movie fans seem much less impressed by cgi in films. there's a general distaste for a perceived overuse of cgi in favor of practical effects and there are a lot of complaints that recent cgi is less-convincing and more fake-looking than it used to be even in the biggest budget films. in an interview with but why tho  gore verbinski director of the ring rango and the first three pirates of the caribbean films was asked why visual effects in movies just don't look as good as they used to. i think the simplest answer is youve seen the unreal gaming engine enter the visual effects landscape verbinski said. so it used to be a divide with unreal engine being very good at video games but then people started thinking maybe movies can also use unreal for finished visual effects. so you have this sort of gaming aesthetic entering the world of cinema. unreal engine made waves after being used for virtual sets in production of the mandalorian tv series back in 2020 and usage of the engine has grown more widespread in films over the past few years such as in the matrix resurrections and ant-man and the wasp quantumania. that's not good news according to verbinski. i think that unreal engine coming in and replacing maya as a sort of fundamental is the greatest slip backwards he said. he pointed out the types of visual effects made with unreal aren't necessarily bad. it works with marvel movies where you kind of know youre in a he"}
{"title": "Weight Transfer for RL Post-Training in under 2 seconds", "url": "https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds", "content": "We're Hiring We're Hiring We're Hiring systems Sep 24, 2025 Ultra-fast cross-GPU model sync We recently achieved 1.3-second cross-machine parameter updates for Kimi-K2 (1T parameters), transferring weights from 256 training GPUs (BF16) to 128 inference GPUs (FP8). In asynchronous reinforcement learning fine-tuning, training and inference run on separate GPUs. After each training step, new weights must be pushed to inference nodes. Many existing frameworks take several secondsâor even minutesâfor trillion-parameter models. By leveraging RDMA point-to-point communication , we are able to make the weight transfer blazing fast, without changing inference engine, and make the code easier to write and maintain. Our solution is built on RDMA WRITE, a one-sided primitive where the source directly writes into the destinationâs GPU memory. The destination side wonât even get notified for the transfer. This gives us low-latency, high-throughput, zero-copy transfers driven by the training nodes without any control logic on the inference nodes. Metadata collection â Controller gathers parameter metadata from all training and inference GPUs. Schedule computation â Controller computes a static weight transfer schedule, mapping which training GPU sends which parameter to which inference GPU, and in what order. Schedule distribution â Controller sends the schedule to all training GPUs. Execution â After each training step, the controller signals training GPUs to start transfers. With the high-level workflow defined, the key challenge is how to execute weight transfers efficiently at trillion-parameter scale. Here we describe the details of the execution path. Parameters in training are distributed according to FSDP placements. Using full_tensor() , all GPUs in a DeviceMesh can reconstruct the full parameter, hence all can serve as a source for weight transfer. Multiple disjoint DeviceMeshes form a mesh group . Because DeviceMeshes in the same group are disjoint, thei", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["hire", "hire", "hire", "system", "sep", "24", "2025", "ultra", "fast", "cross", "gpu", "model", "sync", "recently", "achieve", "1.3", "second", "cross", "machine", "parameter", "update", "kimi", "k2", "parameter", "transfer", "weight", "256", "training", "gpu", "bf16", "128", "inference", "gpus", "fp8", "asynchronous", "reinforcement", "learn", "fine", "tune", "training", "inference", "run", "separate", "gpu", "training", "step", "new", "weight", "push", "inference", "node", "exist", "framework", "secondsâor", "minutesâfor", "trillion", "parameter", "model", "leverage", "rdma", "point", "point", "communication", "able", "weight", "transfer", "blaze", "fast", "change", "inference", "engine", "code", "easy", "write", "maintain", "solution", "build", "rdma", "write", "sided", "primitive", "source", "directly", "write", "destinationâs", "gpu", "memory", "destination", "wonât", "notify", "transfer", "give", "low", "latency", "high", "throughput", "zero", "copy", "transfer", "drive", "training", "node", "control", "logic", "inference", "node", "metadata", "collection", "controller", "gather", "parameter", "metadata", "training", "inference", "gpu", "schedule", "computation", "controller", "compute", "static", "weight", "transfer", "schedule", "mapping", "training", "gpu", "send", "parameter", "inference", "gpu", "order", "schedule", "distribution", "controller", "send", "schedule", "training", "gpu", "execution", "training", "step", "controller", "signal", "train", "gpu", "start", "transfer", "high", "level", "workflow", "define", "key", "challenge", "execute", "weight", "transfer", "efficiently", "trillion", "parameter", "scale", "describe", "detail", "execution", "path", "parameter", "training", "distribute", "accord", "fsdp", "placement", "full_tensor", "gpu", "devicemesh", "reconstruct", "parameter", "serve", "source", "weight", "transfer", "multiple", "disjoint", "devicemeshe", "form", "mesh", "group", "devicemeshe", "group", "disjoint", "thei"], "num_tokens": 189, "token_loss_pct": 44.25, "normalized_content": "we're hiring we're hiring we're hiring systems sep 24 2025 ultra-fast cross-gpu model sync we recently achieved 1.3-second cross-machine parameter updates for kimi-k2 1t parameters transferring weights from 256 training gpus bf16 to 128 inference gpus fp8. in asynchronous reinforcement learning fine-tuning training and inference run on separate gpus. after each training step new weights must be pushed to inference nodes. many existing frameworks take several secondsâor even minutesâfor trillion-parameter models. by leveraging rdma point-to-point communication  we are able to make the weight transfer blazing fast without changing inference engine and make the code easier to write and maintain. our solution is built on rdma write a one-sided primitive where the source directly writes into the destinationâs gpu memory. the destination side wonât even get notified for the transfer. this gives us low-latency high-throughput zero-copy transfers driven by the training nodes without any control logic on the inference nodes. metadata collection â controller gathers parameter metadata from all training and inference gpus. schedule computation â controller computes a static weight transfer schedule mapping which training gpu sends which parameter to which inference gpu and in what order. schedule distribution â controller sends the schedule to all training gpus. execution â after each training step the controller signals training gpus to start transfers. with the high-level workflow defined the key challenge is how to execute weight transfers efficiently at trillion-parameter scale. here we describe the details of the execution path. parameters in training are distributed according to fsdp placements. using full_tensor  all gpus in a devicemesh can reconstruct the full parameter hence all can serve as a source for weight transfer. multiple disjoint devicemeshes form a mesh group . because devicemeshes in the same group are disjoint thei"}
{"title": "GLM-4.7-Flash", "url": "https://huggingface.co/zai-org/GLM-4.7-Flash", "content": "👋 Join our Discord community. 📖 Check out the GLM-4.7 technical blog , technical report(GLM-4.5) . 📍 Use GLM-4.7-Flash API services on Z.ai API Platform. 👉 One click to GLM-4.7 . GLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency. Default Settings (Most Tasks) For multi-turn agentic tasks (τ²-Bench and Terminal Bench 2), please turn on Preserved Thinking mode . Terminal Bench, SWE Bench Verified τ^2-Bench For τ^2-Bench evaluation, we added an additional prompt to the Retail and Telecom user interaction to avoid failure modes caused by users ending the interaction incorrectly. For the Airline domain, we applied the domain fixes as proposed in the Claude Opus 4.5 release report. For local deployment, GLM-4.7-Flash supports inference frameworks including vLLM and SGLang. Comprehensive deployment\ninstructions are available in the official Github repository. vLLM and SGLang only support GLM-4.7-Flash on their main branches. using with transformers as and then run: If you find our work useful in your research, please consider citing the following paper: Chat template Files info", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["join", "discord", "community", "check", "glm-4.7", "technical", "blog", "technical", "reportglm-4.5", "use", "glm-4.7", "flash", "api", "service", "z.ai", "api", "platform", "click", "glm-4.7", "glm-4.7", "flash", "30b", "a3b", "moe", "model", "strong", "model", "30b", "class", "glm-4.7", "flash", "offer", "new", "option", "lightweight", "deployment", "balance", "performance", "efficiency", "default", "setting", "task", "multi", "turn", "agentic", "task", "τ²-bench", "terminal", "bench", "turn", "preserve", "thinking", "mode", "terminal", "bench", "swe", "bench", "verify", "τ2", "bench", "τ2", "bench", "evaluation", "add", "additional", "prompt", "retail", "telecom", "user", "interaction", "avoid", "failure", "mode", "cause", "user", "end", "interaction", "incorrectly", "airline", "domain", "apply", "domain", "fix", "propose", "claude", "opus", "4.5", "release", "report", "local", "deployment", "glm-4.7", "flash", "support", "inference", "framework", "include", "vllm", "sglang", "comprehensive", "deployment", "instruction", "available", "official", "github", "repository", "vllm", "sglang", "support", "glm-4.7", "flash", "main", "branch", "transformer", "run", "find", "work", "useful", "research", "consider", "cite", "follow", "paper", "chat", "template", "file", "info"], "num_tokens": 127, "token_loss_pct": 39.81, "normalized_content": "join our discord community.  check out the glm-4.7 technical blog  technical reportglm-4.5 .  use glm-4.7-flash api services on z.ai api platform.  one click to glm-4.7 . glm-4.7-flash is a 30b-a3b moe model. as the strongest model in the 30b class glm-4.7-flash offers a new option for lightweight deployment that balances performance and efficiency. default settings most tasks for multi-turn agentic tasks τ²-bench and terminal bench 2 please turn on preserved thinking mode . terminal bench swe bench verified τ2-bench for τ2-bench evaluation we added an additional prompt to the retail and telecom user interaction to avoid failure modes caused by users ending the interaction incorrectly. for the airline domain we applied the domain fixes as proposed in the claude opus 4.5 release report. for local deployment glm-4.7-flash supports inference frameworks including vllm and sglang. comprehensive deployment instructions are available in the official github repository. vllm and sglang only support glm-4.7-flash on their main branches. using with transformers as and then run if you find our work useful in your research please consider citing the following paper chat template files info"}
{"title": "For Russia, Greenland offers an 'ideal solution' to its Ukraine problemX", "url": "https://www.politico.eu/article/russia-greenland-offer-ideal-solution-ukraine-problem/", "content": "Moscow’s approach to the showdown over the Arctic island: Troll and hope the West falls apart. AI generated Text-to-speech Days after Donald Trump cited the threat from Russia as a reason to annex Greenland, the U.S. president invited Vladimir Putin to join his Board of Peace. Whiplash, anyone? Not to the Russians. Over the past weeks, Moscow’s response to Trump’s Greenland gambit has been just as disorienting. Kremlin officials have alternated between feigned sympathy for the residents of the Arctic island and open enthusiasm for Trump’s efforts to bring it into the American embrace. The contradiction points to a deliberate strategy: exploiting the crisis to weaken Western unity while keeping Trump focused elsewhere. In the weeks since Trump captured Venezuelan President Nicolás Maduro and threatened to intervene in Iran, Russia appears to have set aside its other geopolitical ambitions, including in the Arctic, to keep Washington in its corner on Ukraine. Meanwhile, it seems to be hoping tensions over Greenland will crack NATO and drive further wedges between Kyiv’s most important allies. “It would have been difficult to imagine something like this happening before,” Russian Foreign Minister Sergey Lavrov said during a press conference on Tuesday, drily gloating over the diminishing “prospects of preserving NATO as a unified Western military-political bloc.” The alarm over Greenland has already paid dividends for the Kremlin, pushing Ukraine off the agenda in Davos as European leaders scramble to the Alpine ski town to try to defuse the crisis. “Greenland [is the] ideal solution,” wrote Sergei Markov, a pro-Kremlin political analyst, on his Telegram channel. Tensions between Europe and the U.S. could serve as a stepping stone to the break-up of NATO. “Then the EU will be forced to stop its war against Russia,” he continued. After years spent bashing the “collective West,” pro-Kremlin propagandists are suggesting the country can now sit back and watch their enemies", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["moscow", "approach", "showdown", "arctic", "island", "troll", "hope", "west", "fall", "apart", "ai", "generate", "text", "speech", "day", "donald", "trump", "cite", "threat", "russia", "reason", "annex", "greenland", "u.s", "president", "invite", "vladimir", "putin", "join", "board", "peace", "whiplash", "russians", "past", "week", "moscow", "response", "trumps", "greenland", "gambit", "disorient", "kremlin", "official", "alternate", "feigned", "sympathy", "resident", "arctic", "island", "open", "enthusiasm", "trump", "effort", "bring", "american", "embrace", "contradiction", "point", "deliberate", "strategy", "exploit", "crisis", "weaken", "western", "unity", "keep", "trump", "focus", "week", "trump", "capture", "venezuelan", "president", "nicolás", "maduro", "threaten", "intervene", "iran", "russia", "appear", "set", "aside", "geopolitical", "ambition", "include", "arctic", "washington", "corner", "ukraine", "hop", "tension", "greenland", "crack", "nato", "drive", "wedge", "kyivs", "important", "ally", "difficult", "imagine", "like", "happen", "russian", "foreign", "minister", "sergey", "lavrov", "say", "press", "conference", "tuesday", "drily", "gloating", "diminish", "prospect", "preserve", "nato", "unified", "western", "military", "political", "bloc", "alarm", "greenland", "pay", "dividend", "kremlin", "push", "ukraine", "agenda", "davos", "european", "leader", "scramble", "alpine", "ski", "town", "try", "defuse", "crisis", "greenland", "ideal", "solution", "write", "sergei", "markov", "pro", "kremlin", "political", "analyst", "telegram", "channel", "tension", "europe", "u.s", "serve", "stepping", "stone", "break", "nato", "eu", "force", "stop", "war", "russia", "continue", "year", "spend", "bash", "collective", "west", "pro", "kremlin", "propagandist", "suggest", "country", "sit", "watch", "enemy"], "num_tokens": 180, "token_loss_pct": 46.9, "normalized_content": "moscows approach to the showdown over the arctic island troll and hope the west falls apart. ai generated text-to-speech days after donald trump cited the threat from russia as a reason to annex greenland the u.s. president invited vladimir putin to join his board of peace. whiplash anyone not to the russians. over the past weeks moscows response to trumps greenland gambit has been just as disorienting. kremlin officials have alternated between feigned sympathy for the residents of the arctic island and open enthusiasm for trumps efforts to bring it into the american embrace. the contradiction points to a deliberate strategy exploiting the crisis to weaken western unity while keeping trump focused elsewhere. in the weeks since trump captured venezuelan president nicolás maduro and threatened to intervene in iran russia appears to have set aside its other geopolitical ambitions including in the arctic to keep washington in its corner on ukraine. meanwhile it seems to be hoping tensions over greenland will crack nato and drive further wedges between kyivs most important allies. it would have been difficult to imagine something like this happening before russian foreign minister sergey lavrov said during a press conference on tuesday drily gloating over the diminishing prospects of preserving nato as a unified western military-political bloc. the alarm over greenland has already paid dividends for the kremlin pushing ukraine off the agenda in davos as european leaders scramble to the alpine ski town to try to defuse the crisis. greenland is the ideal solution wrote sergei markov a pro-kremlin political analyst on his telegram channel. tensions between europe and the u.s. could serve as a stepping stone to the break-up of nato. then the eu will be forced to stop its war against russia he continued. after years spent bashing the collective west pro-kremlin propagandists are suggesting the country can now sit back and watch their enemies"}
{"title": "Verizon starts requiring 365 days of paid service before it will unlock phones", "url": "https://arstechnica.com/tech-policy/2026/01/verizon-starts-requiring-365-days-of-paid-service-before-it-will-unlock-phones/", "content": "Verizon changed prepaid brands’ policy a week after FCC waived unlocking rule. Verizon has started enforcing a 365-day lock period on phones purchased through its TracFone division, one week after the Federal Communications Commission waived a requirement that Verizon unlock handsets 60 days after they are activated on its network. Verizon was previously required to unlock phones automatically after 60 days due to restrictions imposed on its spectrum licenses and merger conditions that helped Verizon obtain approval of its purchase of TracFone . But an update applied today to the TracFone unlocking policy said new phones will be locked for at least a year and that each customer will have to request an unlock instead of getting it automatically. The “new” TracFone policy is basically a return to the yearlong locking it imposed before Verizon bought the company in 2021. TracFone first agreed to provide unlocking in a 2015 settlement with the Obama-era FCC , which alleged that TracFone failed to comply with a commitment to unlock phones for customers enrolled in the Lifeline subsidy program. TracFone later shortened the locking period from a year to 60 days as a condition of the Verizon merger. While a locked phone is tied to the network of one carrier, an unlocked phone can be switched to another carrier if the device is compatible with the other carrier’s network. But the new TracFone unlocking policy is stringent, requiring customers to pay for a full year of service before they can get a phone unlocked. “For all cellphones Activated on or after January 20, 2026, the cellphone will be unlocked upon request after 365 days of paid and active service,” the policy says. A customer who doesn’t maintain an active service plan for the whole 12 months will thus have their unlocking eligibility date delayed. Besides TracFone, the change applies to prepaid brands Straight Talk, Net10 Wireless, Clearway, Total Wireless, Simple Mobile, SafeLink Wireless, and Walmart Family Mobi", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["verizon", "change", "prepay", "brand", "policy", "week", "fcc", "waive", "unlocking", "rule", "verizon", "start", "enforce", "365", "day", "lock", "period", "phone", "purchase", "tracfone", "division", "week", "federal", "communications", "commission", "waive", "requirement", "verizon", "unlock", "handset", "60", "day", "activate", "network", "verizon", "previously", "require", "unlock", "phone", "automatically", "60", "day", "restriction", "impose", "spectrum", "license", "merger", "condition", "help", "verizon", "obtain", "approval", "purchase", "tracfone", "update", "apply", "today", "tracfone", "unlock", "policy", "say", "new", "phone", "lock", "year", "customer", "request", "unlock", "instead", "get", "automatically", "new", "tracfone", "policy", "basically", "return", "yearlong", "lock", "impose", "verizon", "buy", "company", "2021", "tracfone", "agree", "provide", "unlocking", "2015", "settlement", "obama", "era", "fcc", "allege", "tracfone", "fail", "comply", "commitment", "unlock", "phone", "customer", "enrol", "lifeline", "subsidy", "program", "tracfone", "later", "shorten", "locking", "period", "year", "60", "day", "condition", "verizon", "merger", "locked", "phone", "tie", "network", "carrier", "unlocked", "phone", "switch", "carrier", "device", "compatible", "carrier", "network", "new", "tracfone", "unlocking", "policy", "stringent", "require", "customer", "pay", "year", "service", "phone", "unlocked", "cellphone", "activate", "january", "20", "2026", "cellphone", "unlocked", "request", "365", "day", "pay", "active", "service", "policy", "say", "customer", "not", "maintain", "active", "service", "plan", "12", "month", "unlock", "eligibility", "date", "delay", "tracfone", "change", "apply", "prepay", "brand", "straight", "talk", "net10", "wireless", "clearway", "total", "wireless", "simple", "mobile", "safelink", "wireless", "walmart", "family", "mobi"], "num_tokens": 186, "token_loss_pct": 45.13, "normalized_content": "verizon changed prepaid brands policy a week after fcc waived unlocking rule. verizon has started enforcing a 365-day lock period on phones purchased through its tracfone division one week after the federal communications commission waived a requirement that verizon unlock handsets 60 days after they are activated on its network. verizon was previously required to unlock phones automatically after 60 days due to restrictions imposed on its spectrum licenses and merger conditions that helped verizon obtain approval of its purchase of tracfone . but an update applied today to the tracfone unlocking policy said new phones will be locked for at least a year and that each customer will have to request an unlock instead of getting it automatically. the new tracfone policy is basically a return to the yearlong locking it imposed before verizon bought the company in 2021. tracfone first agreed to provide unlocking in a 2015 settlement with the obama-era fcc  which alleged that tracfone failed to comply with a commitment to unlock phones for customers enrolled in the lifeline subsidy program. tracfone later shortened the locking period from a year to 60 days as a condition of the verizon merger. while a locked phone is tied to the network of one carrier an unlocked phone can be switched to another carrier if the device is compatible with the other carriers network. but the new tracfone unlocking policy is stringent requiring customers to pay for a full year of service before they can get a phone unlocked. for all cellphones activated on or after january 20 2026 the cellphone will be unlocked upon request after 365 days of paid and active service the policy says. a customer who doesnt maintain an active service plan for the whole 12 months will thus have their unlocking eligibility date delayed. besides tracfone the change applies to prepaid brands straight talk net10 wireless clearway total wireless simple mobile safelink wireless and walmart family mobi"}
{"title": "Nuudel: Non-Tracking Appointment Tool", "url": "https://nuudel.digitalcourage.de/", "content": "Termin finden Klassische Umfrage Wo sind meine Umfragen? Dieser Dienst wird vom gemeinnützigen Digitalcourage e.V. für Sie kostenlos angeboten. Unterstützen Sie den Betrieb und unsere Arbeit für eine lebenswerte Welt im digitalen Zeitalter als Fördermitglied ! Digitalcourage : Newsletter | Spenden | English information", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["termin", "finden", "klassische", "umfrage", "will", "sind", "meine", "umfragen", "dieser", "dienst", "wird", "vom", "gemeinnützigen", "digitalcourage", "e.v", "für", "sie", "kostenlos", "angeboten", "unterstützen", "sie", "den", "betrieb", "und", "unsere", "arbeit", "für", "eine", "lebenswerte", "welt", "digitalen", "zeitalter", "al", "fördermitglie", "digitalcourage", "newsletter", "spenden", "english", "information"], "num_tokens": 39, "token_loss_pct": 17.02, "normalized_content": "termin finden klassische umfrage wo sind meine umfragen dieser dienst wird vom gemeinnützigen digitalcourage e.v. für sie kostenlos angeboten. unterstützen sie den betrieb und unsere arbeit für eine lebenswerte welt im digitalen zeitalter als fördermitglied  digitalcourage  newsletter  spenden  english information"}
{"title": "A Social Filesystem", "url": "https://overreacted.io/a-social-filesystem/", "content": "January 18, 2026 Remember files? .doc .doc .doc .doc .jpg .jpg .svg You write a document, hit save, and the file is on your computer. It’s yours. You can inspect it, you can send it to a friend, and you can open it with other apps. Files come from the paradigm of personal computing . This post, however, isn’t about personal computing. What I want to talk about is social computing —apps like Instagram, Reddit, Tumblr, GitHub, and TikTok. What do files have to do with social computing? Historically, not a lot— until recently. alice owns owns bob .doc .doc .doc .doc .doc .doc post post .jpg .jpg .jpg .jpg .jpg .jpg .jpg .jpg follow follow vote vote But first, a shoutout to files. Files, as originally invented, were not meant to live inside the apps. Since files represent your creations, they should live somewhere that you control. Apps create and read your files on your behalf, but files don’t belong to the apps. .doc .doc .doc .doc .jpg .jpg .svg C:\\Users\\alice alice owns Files belong to you—the person using those apps. Apps (and their developers) may not own your files, but they do need to be able to read and write them. To do that reliably, apps need your files to be structured. This is why app developers, as part of creating apps, may invent and evolve file formats . A file format is like a language. An app might “speak” several formats. A single format can be understood by many apps. Apps and formats are many-to-many. File formats let different apps work together without knowing about each other. Consider this .svg : .jpg .jpg .svg C:\\Users\\alice SVG is an open specification. This means that different developers agree on how to read and write SVG. I created this SVG file in Excalidraw , but I could have used Adobe Illustrator or Inkscape instead. Your browser already knew how to display this SVG. It didn’t need to hit any Excalidraw APIs or to ask permissions from Excalidraw to display this SVG. It doesn’t matter which app has created this SVG. The file format is", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["january", "18", "2026", "remember", "file", ".doc", ".doc", ".doc", ".doc", ".jpg", ".jpg", ".svg", "write", "document", "hit", "save", "file", "computer", "inspect", "send", "friend", "open", "app", "file", "come", "paradigm", "personal", "computing", "post", "not", "personal", "computing", "want", "talk", "social", "computing", "app", "like", "instagram", "reddit", "tumblr", "github", "tiktok", "file", "social", "computing", "historically", "lot", "recently", "alice", "owns", "own", "bob", ".doc", ".doc", ".doc", ".doc", ".doc", ".doc", "post", "post", ".jpg", ".jpg", ".jpg", ".jpg", ".jpg", ".jpg", ".jpg", ".jpg", "follow", "follow", "vote", "vote", "shoutout", "file", "file", "originally", "invent", "mean", "live", "inside", "app", "file", "represent", "creation", "live", "control", "app", "create", "read", "file", "behalf", "file", "not", "belong", "app", ".doc", ".doc", ".doc", ".doc", ".jpg", ".jpg", ".svg", "cusersalice", "alice", "own", "file", "belong", "youthe", "person", "app", "app", "developer", "file", "need", "able", "read", "write", "reliably", "app", "need", "file", "structure", "app", "developer", "create", "app", "invent", "evolve", "file", "format", "file", "format", "like", "language", "app", "speak", "format", "single", "format", "understand", "app", "app", "format", "file", "format", "let", "different", "app", "work", "know", "consider", ".svg", ".jpg", ".jpg", ".svg", "cusersalice", "svg", "open", "specification", "mean", "different", "developer", "agree", "read", "write", "svg", "create", "svg", "file", "excalidraw", "adobe", "illustrator", "inkscape", "instead", "browser", "know", "display", "svg", "not", "need", "hit", "excalidraw", "apis", "ask", "permission", "excalidraw", "display", "svg", "not", "matter", "app", "create", "svg", "file", "format"], "num_tokens": 196, "token_loss_pct": 49.22, "normalized_content": "january 18 2026 remember files .doc .doc .doc .doc .jpg .jpg .svg you write a document hit save and the file is on your computer. its yours. you can inspect it you can send it to a friend and you can open it with other apps. files come from the paradigm of personal computing . this post however isnt about personal computing. what i want to talk about is social computing apps like instagram reddit tumblr github and tiktok. what do files have to do with social computing historically not a lot until recently. alice owns owns bob .doc .doc .doc .doc .doc .doc post post .jpg .jpg .jpg .jpg .jpg .jpg .jpg .jpg follow follow vote vote but first a shoutout to files. files as originally invented were not meant to live inside the apps. since files represent your creations they should live somewhere that you control. apps create and read your files on your behalf but files dont belong to the apps. .doc .doc .doc .doc .jpg .jpg .svg cusersalice alice owns files belong to youthe person using those apps. apps and their developers may not own your files but they do need to be able to read and write them. to do that reliably apps need your files to be structured. this is why app developers as part of creating apps may invent and evolve file formats . a file format is like a language. an app might speak several formats. a single format can be understood by many apps. apps and formats are many-to-many. file formats let different apps work together without knowing about each other. consider this .svg  .jpg .jpg .svg cusersalice svg is an open specification. this means that different developers agree on how to read and write svg. i created this svg file in excalidraw  but i could have used adobe illustrator or inkscape instead. your browser already knew how to display this svg. it didnt need to hit any excalidraw apis or to ask permissions from excalidraw to display this svg. it doesnt matter which app has created this svg. the file format is"}
{"title": "Robust Conditional 3D Shape Generation from Casual Captures", "url": "https://facebookresearch.github.io/ShapeR/", "content": "Robust Conditional 3D Shape Generation from Casual Captures Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu † , Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel Meta Reality Labs Research † Simon Fraser University From an input image sequence, ShapeR preprocesses per-object multimodal data (SLAM points, images, captions). A rectified flow transformer then conditions on these inputs to generate meshes object-centrically, producing a full metric scene reconstruction. Conditioned on off-the-shelf preprocessed inputs—SLAM points, 3D instances, and text—ShapeR infers per-object meshes to reconstruct the entire scene. While monolithic methods fuse the scene into one block, ShapeR reconstructs individual objects. This allows you to interact with and manipulate specific objects in the scene. ShapeR performs generative, object-centric 3D reconstruction from image sequences by leveraging multimodal inputs and robust training strategies. First, off-the-shelf SLAM and 3D instance detection are used to compute 3D points and object instances. For each object, sparse points, relevant images, 2D projections, and VLM captions are extracted to condition a rectified flow model, which denoises a latent VecSet to produce the 3D shape. The use of multimodal conditioning , along with heavy on-the-fly compositional augmentations and curriculum training , ensures the robustness of ShapeR in real-world scenarios. ShapeR conditions on a range of modalities, including the object's posed multiview images, SLAM points, text descriptions, and 2D point projections. ShapeR leverages single-object pretraining with extensive augmentations, simulating realistic backgrounds, occlusions, and noise across images and SLAM inputs. ShapeR is fine-tuned on object-centric crops from Aria Synthetic Environment scenes, which feature realistic image occlusions, SLAM point cloud noise, and inter-object interaction. For even more de", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["robust", "conditional", "3d", "shape", "generation", "casual", "capture", "yawar", "siddiqui", "duncan", "frost", "samir", "aroudj", "armen", "avetisyan", "henry", "howard", "jenkins", "daniel", "detone", "pierre", "moulon", "qirui", "wu", "  ", "zhengqin", "li", "julian", "straub", "richard", "newcombe", "jakob", "engel", "meta", "reality", "lab", "research", "simon", "fraser", "university", "input", "image", "sequence", "shaper", "preprocesse", "object", "multimodal", "data", "slam", "point", "image", "caption", "rectified", "flow", "transformer", "condition", "input", "generate", "mesh", "object", "centrically", "produce", "metric", "scene", "reconstruction", "condition", "shelf", "preprocesse", "inputsslam", "point", "3d", "instance", "textshaper", "infer", "object", "mesh", "reconstruct", "entire", "scene", "monolithic", "method", "fuse", "scene", "block", "shaper", "reconstruct", "individual", "object", "allow", "interact", "manipulate", "specific", "object", "scene", "shaper", "perform", "generative", "object", "centric", "3d", "reconstruction", "image", "sequence", "leverage", "multimodal", "input", "robust", "training", "strategy", "shelf", "slam", "3d", "instance", "detection", "compute", "3d", "point", "object", "instance", "object", "sparse", "point", "relevant", "image", "2d", "projection", "vlm", "caption", "extract", "condition", "rectify", "flow", "model", "denoise", "latent", "vecset", "produce", "3d", "shape", "use", "multimodal", "conditioning", "heavy", "fly", "compositional", "augmentation", "curriculum", "training", "ensure", "robustness", "shaper", "real", "world", "scenario", "shaper", "condition", "range", "modality", "include", "object", "pose", "multiview", "image", "slam", "point", "text", "description", "2d", "point", "projection", "shaper", "leverage", "single", "object", "pretraine", "extensive", "augmentation", "simulate", "realistic", "background", "occlusion", "noise", "image", "slam", "input", "shaper", "fine", "tune", "object", "centric", "crop", "aria", "synthetic", "environment", "scene", "feature", "realistic", "image", "occlusion", "slam", "point", "cloud", "noise", "inter", "object", "interaction", "de"], "num_tokens": 207, "token_loss_pct": 34.49, "normalized_content": "robust conditional 3d shape generation from casual captures yawar siddiqui duncan frost samir aroudj armen avetisyan henry howard-jenkins daniel detone pierre moulon qirui wu   zhengqin li julian straub richard newcombe jakob engel meta reality labs research  simon fraser university from an input image sequence shaper preprocesses per-object multimodal data slam points images captions. a rectified flow transformer then conditions on these inputs to generate meshes object-centrically producing a full metric scene reconstruction. conditioned on off-the-shelf preprocessed inputsslam points 3d instances and textshaper infers per-object meshes to reconstruct the entire scene. while monolithic methods fuse the scene into one block shaper reconstructs individual objects. this allows you to interact with and manipulate specific objects in the scene. shaper performs generative object-centric 3d reconstruction from image sequences by leveraging multimodal inputs and robust training strategies. first off-the-shelf slam and 3d instance detection are used to compute 3d points and object instances. for each object sparse points relevant images 2d projections and vlm captions are extracted to condition a rectified flow model which denoises a latent vecset to produce the 3d shape. the use of multimodal conditioning  along with heavy on-the-fly compositional augmentations and curriculum training  ensures the robustness of shaper in real-world scenarios. shaper conditions on a range of modalities including the object's posed multiview images slam points text descriptions and 2d point projections. shaper leverages single-object pretraining with extensive augmentations simulating realistic backgrounds occlusions and noise across images and slam inputs. shaper is fine-tuned on object-centric crops from aria synthetic environment scenes which feature realistic image occlusions slam point cloud noise and inter-object interaction. for even more de"}
{"title": "High-speed train collision in Spain kills at least 39", "url": "https://www.bbc.com/news/articles/cedw6ylpynyo", "content": "At least 39 people have died in a train collision in southern Spain and dozens more have been injured in the country's worst rail crash in more than a decade, Spain's Civil Guard has said. Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks, colliding with an oncoming train in Adamuz on Sunday evening. Four hundred passengers and staff were onboard both trains, the rail networks said. Emergency services treated 122 people, with 43, including four children, still in hospital. Of those, 12 adults and one child are in intensive care. Spanish Transport Minister Óscar Puente said the death toll \"is not yet final\", as officials launched an investigation. Puente described the incident as \"extremely strange\". All the railway experts consulted by the government \"are extremely baffled by the accident\", he told reporters in Madrid. Rail network operator Adif said the collision happened at 19:45 local time (18:45 GMT), about an hour after the train left Málaga heading north to Madrid, when it derailed on a straight stretch of track near the city of Córdoba. The force of the crash pushed the carriages of the second train into an embankment, Puente said. He added that most of those killed and injured were in the front carriages of the second train, which was travelling south from Madrid to Huelva. The type of train involved in the crash was a Freccia 1000, which can reach top speeds of 400 km/h (250 mph), a spokesperson for the Italian rail company Ferrovie dello Stato told Reuters news agency. Rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages. Córdoba fire chief Francisco Carmona told Spanish public broadcaster RTVE: \"We have even had to remove a dead person to be able to reach someone alive. It is hard, tricky work.\" Salvador Jimenez, a journalist with RTVE who was on one of the trains, said the impact felt like an \"earthquake\". \"I was in the first carriage. There was a moment whe", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["39", "people", "die", "train", "collision", "southern", "spain", "dozen", "injure", "country", "bad", "rail", "crash", "decade", "spain", "civil", "guard", "say", "carriage", "madrid", "bind", "train", "derail", "cross", "opposite", "track", "collide", "oncoming", "train", "adamuz", "sunday", "evening", "passenger", "staff", "onboard", "train", "rail", "network", "say", "emergency", "service", "treat", "122", "people", "43", "include", "child", "hospital", "12", "adult", "child", "intensive", "care", "spanish", "transport", "minister", "óscar", "puente", "say", "death", "toll", "final", "official", "launch", "investigation", "puente", "describe", "incident", "extremely", "strange", "railway", "expert", "consult", "government", "extremely", "baffle", "accident", "tell", "reporter", "madrid", "rail", "network", "operator", "adif", "say", "collision", "happen", "1945", "local", "time", "1845", "gmt", "hour", "train", "leave", "málaga", "head", "north", "madrid", "derail", "straight", "stretch", "track", "near", "city", "córdoba", "force", "crash", "push", "carriage", "second", "train", "embankment", "puente", "say", "add", "kill", "injure", "carriage", "second", "train", "travel", "south", "madrid", "huelva", "type", "train", "involve", "crash", "freccia", "1000", "reach", "speed", "400", "kmh", "250", "mph", "spokesperson", "italian", "rail", "company", "ferrovie", "dello", "stato", "tell", "reuters", "news", "agency", "rescue", "team", "say", "twisted", "wreckage", "train", "difficult", "recover", "people", "trap", "inside", "carriage", "córdoba", "fire", "chief", "francisco", "carmona", "tell", "spanish", "public", "broadcaster", "rtve", "remove", "dead", "person", "able", "reach", "alive", "hard", "tricky", "work", "salvador", "jimenez", "journalist", "rtve", "train", "say", "impact", "feel", "like", "earthquake", "carriage", "moment", "whe"], "num_tokens": 192, "token_loss_pct": 46.67, "normalized_content": "at least 39 people have died in a train collision in southern spain and dozens more have been injured in the country's worst rail crash in more than a decade spain's civil guard has said. carriages on a madrid-bound train derailed and crossed over to the opposite tracks colliding with an oncoming train in adamuz on sunday evening. four hundred passengers and staff were onboard both trains the rail networks said. emergency services treated 122 people with 43 including four children still in hospital. of those 12 adults and one child are in intensive care. spanish transport minister óscar puente said the death toll is not yet final as officials launched an investigation. puente described the incident as extremely strange. all the railway experts consulted by the government are extremely baffled by the accident he told reporters in madrid. rail network operator adif said the collision happened at 1945 local time 1845 gmt about an hour after the train left málaga heading north to madrid when it derailed on a straight stretch of track near the city of córdoba. the force of the crash pushed the carriages of the second train into an embankment puente said. he added that most of those killed and injured were in the front carriages of the second train which was travelling south from madrid to huelva. the type of train involved in the crash was a freccia 1000 which can reach top speeds of 400 kmh 250 mph a spokesperson for the italian rail company ferrovie dello stato told reuters news agency. rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages. córdoba fire chief francisco carmona told spanish public broadcaster rtve we have even had to remove a dead person to be able to reach someone alive. it is hard tricky work. salvador jimenez a journalist with rtve who was on one of the trains said the impact felt like an earthquake. i was in the first carriage. there was a moment whe"}
{"title": "Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup", "url": "https://cua.ai/docs/lume/guide/getting-started/introduction", "content": "Introduction to Lume - the macOS VM CLI and framework Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon. MIT License Lume is open-source and MIT licensed. If you find it useful, we'd appreciate a star on GitHub ! Cloud macOS Sandboxes We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads. Book a demo if you're interested. A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.  You can use Lume directly via CLI, or run lume serve to expose an HTTP API for programmatic access. The Computer SDK uses this API to automate macOS interactions. Lume is a thin layer over Apple's Virtualization Framework , which provides hardware-accelerated virtualization on Apple Silicon. This gives you: Testing across macOS versions — Spin up a VM with a specific macOS version, test your software, tear it down. No need to maintain multiple physical machines. Automating macOS tasks — Combine Lume with Unattended Setup to create pre-configured VMs. The setup automation uses VNC and OCR to click through the Setup Assistant without manual intervention. Running CI/CD locally — Test your macOS builds in isolated VMs before pushing to remote CI. The --no-display flag runs VMs headlessly. Sandboxing risky operations — Need to test untrusted software or destructive scripts? Run them in a VM, then delete it. Clone a known-good VM to reset to a clean state instantly. Building AI agents — Lume powers the Cua Computer SDK , providing VMs that AI models can interact with through screenshots and input simulation. Used by Anthropic Apple's Virtualization Framework—the same technology Lume is built on—powers Claude Cowork , Anthropic's sandboxed environment for Claude Code. It downloads a Linux root filesystem and boots it in an isolated VM where Claude c", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["introduction", "lume", "macos", "vm", "cli", "framework", "lume", "vm", "runtime", "building", "ai", "agent", "run", "cicd", "pipeline", "automate", "macos", "use", "apple", "native", "virtualization", "framework", "run", "macos", "linux", "vms", "near", "native", "speed", "apple", "silicon", "mit", "license", "lume", "open", "source", "mit", "license", "find", "useful", "appreciate", "star", "github", "cloud", "macos", "sandbox", "pilot", "manage", "service", "customer", "want", "run", "cloud", "macos", "sandbox", "cicd", "agent", "workload", "book", "demo", "interested", "single", "binary", "http", "api", "create", "vm", "run", "headlessly", "control", "programmatically", "use", "lume", "directly", "cli", "run", "lume", "serve", "expose", "http", "api", "programmatic", "access", "computer", "sdk", "use", "api", "automate", "macos", "interaction", "lume", "thin", "layer", "apple", "virtualization", "framework", "provide", "hardware", "accelerate", "virtualization", "apple", "silicon", "give", "test", "macos", "version", "spin", "vm", "specific", "macos", "version", "test", "software", "tear", "need", "maintain", "multiple", "physical", "machine", "automate", "macos", "task", "combine", "lume", "unattended", "setup", "create", "pre", "configured", "vms", "setup", "automation", "use", "vnc", "ocr", "click", "setup", "assistant", "manual", "intervention", "run", "cicd", "locally", "test", "macos", "build", "isolated", "vm", "push", "remote", "ci", "--no", "display", "flag", "run", "vms", "headlessly", "sandboxe", "risky", "operation", "need", "test", "untrusted", "software", "destructive", "script", "run", "vm", "delete", "clone", "know", "good", "vm", "reset", "clean", "state", "instantly", "build", "ai", "agent", "lume", "power", "cua", "computer", "sdk", "provide", "vms", "ai", "model", "interact", "screenshot", "input", "simulation", "anthropic", "apple", "virtualization", "frameworkthe", "technology", "lume", "build", "onpower", "claude", "cowork", "anthropic", "sandboxe", "environment", "claude", "code", "download", "linux", "root", "filesystem", "boot", "isolated", "vm", "claude"], "num_tokens": 216, "token_loss_pct": 40.82, "normalized_content": "introduction to lume - the macos vm cli and framework lume is a vm runtime for building ai agents running cicd pipelines and automating macos. it uses apple's native virtualization framework to run macos and linux vms at near-native speed on apple silicon. mit license lume is open-source and mit licensed. if you find it useful we'd appreciate a star on github  cloud macos sandboxes we're piloting a managed service for customers who want to run cloud macos sandboxes for cicd and agent workloads. book a demo if you're interested. a single binary with an http api. create a vm run it headlessly control it programmatically. you can use lume directly via cli or run lume serve to expose an http api for programmatic access. the computer sdk uses this api to automate macos interactions. lume is a thin layer over apple's virtualization framework  which provides hardware-accelerated virtualization on apple silicon. this gives you testing across macos versions  spin up a vm with a specific macos version test your software tear it down. no need to maintain multiple physical machines. automating macos tasks  combine lume with unattended setup to create pre-configured vms. the setup automation uses vnc and ocr to click through the setup assistant without manual intervention. running cicd locally  test your macos builds in isolated vms before pushing to remote ci. the --no-display flag runs vms headlessly. sandboxing risky operations  need to test untrusted software or destructive scripts run them in a vm then delete it. clone a known-good vm to reset to a clean state instantly. building ai agents  lume powers the cua computer sdk  providing vms that ai models can interact with through screenshots and input simulation. used by anthropic apple's virtualization frameworkthe same technology lume is built onpowers claude cowork  anthropic's sandboxed environment for claude code. it downloads a linux root filesystem and boots it in an isolated vm where claude c"}
{"title": "Folding NASA Experience into an Origamist's Toolkit (2024)", "url": "https://spinoff.nasa.gov/Folding_NASA_Experience_into_an_Origamist%E2%80%99s_Toolkit", "content": "What does origami have in common with electronics? Here, math once again proves to be a universal language, spanning not just cultures but disciplines. The discovery of the mathematical underpinnings of folded paper art helped Robert Lang leave a 20-year engineering career, including over four years at NASA’s Jet Propulsion Laboratory in Southern California, to pursue his lifelong passion for turning paper into impossibly intricate three-dimensional forms. “Over the years of solving mathematical problems to describe lasers and optoelectronics, I built up a toolkit to use as I worked on a hobby basis on this problem of computational origami design,” said Lang. The Altadena, California-based artist holds dozens of patents for optoelectronics — technology that combines light and electricity — but after years of innovating in both fields, the tools he designed for origami are the ones he chose to move ahead with. In the Microdevices Laboratory at JPL in the late 1980s and early ’90s, Lang worked on integrating components like semiconductor lasers and spatial light modulators onto chips, with the ultimate goal of building an optical computer — one that uses light, rather than electricity, to transmit information and carry out calculations. Steady advances in electronic computing have since removed some of the incentives to develop optical computers. “One of the theoretical fields I learned about at JPL turned out to be the key to being able to plug in a description of a shape you wanted and then find the best possible design in great detail — every single crease you needed to make that shape,” said Lang. “And that turned out to be nonlinear constrained optimization.” It’s All About the Numbers A simple nonlinear constrained optimization problem would be the challenge of packing several different-sized balls into the smallest possible box, Lang explained. The constraint is that the balls can’t overlap each other, and the solutions are nonlinear because the balls can be a", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["origami", "common", "electronic", "math", "prove", "universal", "language", "span", "culture", "discipline", "discovery", "mathematical", "underpinning", "fold", "paper", "art", "help", "robert", "lang", "leave", "20", "year", "engineering", "career", "include", "year", "nasas", "jet", "propulsion", "laboratory", "southern", "california", "pursue", "lifelong", "passion", "turn", "paper", "impossibly", "intricate", "dimensional", "form", "year", "solve", "mathematical", "problem", "describe", "laser", "optoelectronic", "build", "toolkit", "use", "work", "hobby", "basis", "problem", "computational", "origami", "design", "say", "lang", "altadena", "california", "base", "artist", "hold", "dozen", "patent", "optoelectronic", "technology", "combine", "light", "electricity", "year", "innovate", "field", "tool", "design", "origami", "one", "choose", "ahead", "microdevice", "laboratory", "jpl", "late", "1980", "early", "90", "lang", "work", "integrate", "component", "like", "semiconductor", "laser", "spatial", "light", "modulator", "chip", "ultimate", "goal", "build", "optical", "computer", "use", "light", "electricity", "transmit", "information", "carry", "calculation", "steady", "advance", "electronic", "computing", "remove", "incentive", "develop", "optical", "computer", "theoretical", "field", "learn", "jpl", "turn", "key", "able", "plug", "description", "shape", "want", "find", "good", "possible", "design", "great", "detail", "single", "crease", "need", "shape", "say", "lang", "turn", "nonlinear", "constrained", "optimization", "number", "simple", "nonlinear", "constrain", "optimization", "problem", "challenge", "pack", "different", "sized", "ball", "small", "possible", "box", "lang", "explain", "constraint", "ball", "not", "overlap", "solution", "nonlinear", "ball"], "num_tokens": 170, "token_loss_pct": 49.4, "normalized_content": "what does origami have in common with electronics here math once again proves to be a universal language spanning not just cultures but disciplines. the discovery of the mathematical underpinnings of folded paper art helped robert lang leave a 20-year engineering career including over four years at nasas jet propulsion laboratory in southern california to pursue his lifelong passion for turning paper into impossibly intricate three-dimensional forms. over the years of solving mathematical problems to describe lasers and optoelectronics i built up a toolkit to use as i worked on a hobby basis on this problem of computational origami design said lang. the altadena california-based artist holds dozens of patents for optoelectronics  technology that combines light and electricity  but after years of innovating in both fields the tools he designed for origami are the ones he chose to move ahead with. in the microdevices laboratory at jpl in the late 1980s and early 90s lang worked on integrating components like semiconductor lasers and spatial light modulators onto chips with the ultimate goal of building an optical computer  one that uses light rather than electricity to transmit information and carry out calculations. steady advances in electronic computing have since removed some of the incentives to develop optical computers. one of the theoretical fields i learned about at jpl turned out to be the key to being able to plug in a description of a shape you wanted and then find the best possible design in great detail  every single crease you needed to make that shape said lang. and that turned out to be nonlinear constrained optimization. its all about the numbers a simple nonlinear constrained optimization problem would be the challenge of packing several different-sized balls into the smallest possible box lang explained. the constraint is that the balls cant overlap each other and the solutions are nonlinear because the balls can be a"}
{"title": "A scammer's blueprint: How cybercriminals plot to rob a target in a week", "url": "https://www.reuters.com/graphics/SOUTHEASTASIA-SCAMS/MANUALS/klpyjlqelvg/", "content": "Are you free tonight?  NOTICE: Your bill is overdue.  Hello old friend, are you going to the party tonight?  hi there  hello  Weâve all received a random buzz on our phone â a message from an unknown number. Hey!  Many will ignore it, suspecting a scam. Hello darling.  Hey there  I hope you donât mind me contacting you  Hi Chris  Long time no see, how are you  what are you doing?  Arenât you my friend?  You have a package that needs to be claimed.  Hi, my name is Sam. Itâs nice to meet you. But for others, the unexpected message may present the chance for a new connection. A haven, or perhaps even romance. Illustration showing a warm, glowing, orange house among dark empty woods. Even if it was sent under false pretenses. Illustration showing the side of the house and how it is propped up by sticks, depicting a fake facade. So how does a scammer carefully construct a facade that can convince someone to part with their money â and even to fall in love? Illustration showing the facade collapsed on the ground. Some can rely on intricately constructed guides to grooming and deception. A scammerâs How cybercriminals plot to rob a target within a week. A handbook found during a police raid on a compound used by a cyberfraud gang in the Philippines offers detailed instructions in Chinese for conducting scams. A second handbook, seized during another law enforcement operation in the country and reviewed by Reuters, also gives tips in English and Chinese about how to run romance scams. Together, they provide a window into the psychological techniques criminal gangs use to beguile a victim into believing they are in a romantic relationship, before duping them into fraudulent investments.Â Â The Chinese handbook says: This kind of fraud is known as âpig-butcheringâ because the gangs say targets are led like hapless pigs to slaughter. It is among the most prevalent scams today, according to the FBI. In a series of stories in 2025, Reuters documented how these", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["free", "tonight", "notice", "bill", "overdue", "hello", "old", "friend", "go", "party", "tonight", "hi", "hello", "weâve", "receive", "random", "buzz", "phone", "message", "unknown", "number", "hey", "ignore", "suspect", "scam", "hello", "darling", "hey", "hope", "donât", "mind", "contact", "hi", "chris", "long", "time", "arenât", "friend", "package", "need", "claim", "hi", "sam", "itâs", "nice", "meet", "unexpected", "message", "present", "chance", "new", "connection", "haven", "romance", "illustration", "show", "warm", "glow", "orange", "house", "dark", "wood", "send", "false", "pretense", "illustration", "show", "house", "prop", "stick", "depict", "fake", "facade", "scammer", "carefully", "construct", "facade", "convince", "money", "fall", "love", "illustration", "show", "facade", "collapse", "ground", "rely", "intricately", "construct", "guide", "grooming", "deception", "scammerâs", "cybercriminal", "plot", "rob", "target", "week", "handbook", "find", "police", "raid", "compound", "cyberfraud", "gang", "philippine", "offer", "detailed", "instruction", "chinese", "conduct", "scam", "second", "handbook", "seize", "law", "enforcement", "operation", "country", "review", "reuter", "give", "tip", "english", "chinese", "run", "romance", "scam", "provide", "window", "psychological", "technique", "criminal", "gang", "use", "beguile", "victim", "believe", "romantic", "relationship", "dupe", "fraudulent", "investments.â", "chinese", "handbook", "say", "kind", "fraud", "know", "âpig", "butcheringâ", "gang", "target", "lead", "like", "hapless", "pig", "slaughter", "prevalent", "scam", "today", "accord", "fbi", "series", "story", "2025", "reuter", "document"], "num_tokens": 168, "token_loss_pct": 53.59, "normalized_content": "are you free tonight notice your bill is overdue. hello old friend are you going to the party tonight hi there hello weâve all received a random buzz on our phone â a message from an unknown number. hey many will ignore it suspecting a scam. hello darling. hey there i hope you donât mind me contacting you hi chris long time no see how are you what are you doing arenât you my friend you have a package that needs to be claimed. hi my name is sam. itâs nice to meet you. but for others the unexpected message may present the chance for a new connection. a haven or perhaps even romance. illustration showing a warm glowing orange house among dark empty woods. even if it was sent under false pretenses. illustration showing the side of the house and how it is propped up by sticks depicting a fake facade. so how does a scammer carefully construct a facade that can convince someone to part with their money â and even to fall in love illustration showing the facade collapsed on the ground. some can rely on intricately constructed guides to grooming and deception. a scammerâs how cybercriminals plot to rob a target within a week. a handbook found during a police raid on a compound used by a cyberfraud gang in the philippines offers detailed instructions in chinese for conducting scams. a second handbook seized during another law enforcement operation in the country and reviewed by reuters also gives tips in english and chinese about how to run romance scams. together they provide a window into the psychological techniques criminal gangs use to beguile a victim into believing they are in a romantic relationship before duping them into fraudulent investments.â â the chinese handbook says this kind of fraud is known as âpig-butcheringâ because the gangs say targets are led like hapless pigs to slaughter. it is among the most prevalent scams today according to the fbi. in a series of stories in 2025 reuters documented how these"}
{"title": "The fix for a segfault that never shipped", "url": "https://www.recall.ai/blog/the-fix-for-a-segfault-that-never-shipped", "content": "At Recall.ai , we run an unusual workload. We record millions of hours of meetings every month. Each of these meetings generates a large amount of audio and video we need to reliably capture and analyze. The audio we capture comes in a variety of shapes and sizes, an assortment of codecs, channels, sample rates, interleaving, and error-correction schemes. We normalize all of those into one consistent format that is universally playable. We launch 18 million of EC2 instances every month, each of these instances is a âmeeting botâ, which joins a video call and captures the data in real-time. Extremely rarely, about 1 in 36 million bots would abruptly crash deep in library code of our media pipeline. Unlike mostÂ web servers, a meeting bot instance is extremely stateful and very hard to replace, a fatal error like this means the data is irrecoverably lost - forever. Even a one-in-tens-of-millions failure rate was unacceptable. This is the story of how we tracked down the rare bug, went to significant effort to reproduce it, identify the root cause and fix it. We encountered an extremely rare segfault in the AAC encoder we were using and root caused it to a bug in the fixed-point math C code. We found this was patched over a decade ago but the fix was never shipped to downstream consumers. Rather than fixing the bug we replaced the library with a modern AAC encoder which did not experience these crashes. This crash was so rare that reproducing it locally wasnât feasible. Instead we opted to capture the program state from production, in the rare event this happens. Our first clue was the process 139 exit code ( 139 = 128 + 11 ). Signal 11 corresponds to SIGSEGV, a segmentation fault or segfault for short, this occurs when a program tries to access memory it is not supposed to. This mistake is bad enough that the execution of the program should halt immediately. So how do we determine the cause of a SIGSEGV? The answer is often a core dump file. A core dump lets us", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recall.ai", "run", "unusual", "workload", "record", "million", "hour", "meeting", "month", "meeting", "generate", "large", "audio", "video", "need", "reliably", "capture", "analyze", "audio", "capture", "come", "variety", "shape", "size", "assortment", "codec", "channel", "sample", "rate", "interleave", "error", "correction", "scheme", "normalize", "consistent", "format", "universally", "playable", "launch", "18", "million", "ec2", "instance", "month", "instance", "âmeeting", "botâ", "join", "video", "capture", "datum", "real", "time", "extremely", "rarely", "36", "million", "bot", "abruptly", "crash", "deep", "library", "code", "medium", "pipeline", "unlike", "mostâ", "web", "server", "meeting", "bot", "instance", "extremely", "stateful", "hard", "replace", "fatal", "error", "like", "mean", "data", "irrecoverably", "lose", "forever", "ten", "million", "failure", "rate", "unacceptable", "story", "track", "rare", "bug", "go", "significant", "effort", "reproduce", "identify", "root", "cause", "fix", "encounter", "extremely", "rare", "segfault", "aac", "encoder", "root", "cause", "bug", "fix", "point", "math", "code", "find", "patch", "decade", "ago", "fix", "ship", "downstream", "consumer", "fix", "bug", "replace", "library", "modern", "aac", "encoder", "experience", "crash", "crash", "rare", "reproduce", "locally", "wasnât", "feasible", "instead", "opt", "capture", "program", "state", "production", "rare", "event", "happen", "clue", "process", "139", "exit", "code", "139", "128", "11", "signal", "11", "correspond", "sigsegv", "segmentation", "fault", "segfault", "short", "occur", "program", "try", "access", "memory", "suppose", "mistake", "bad", "execution", "program", "halt", "immediately", "determine", "cause", "sigsegv", "answer", "core", "dump", "file", "core", "dump", "let"], "num_tokens": 184, "token_loss_pct": 51.96, "normalized_content": "at recall.ai  we run an unusual workload. we record millions of hours of meetings every month. each of these meetings generates a large amount of audio and video we need to reliably capture and analyze. the audio we capture comes in a variety of shapes and sizes an assortment of codecs channels sample rates interleaving and error-correction schemes. we normalize all of those into one consistent format that is universally playable. we launch 18 million of ec2 instances every month each of these instances is a âmeeting botâ which joins a video call and captures the data in real-time. extremely rarely about 1 in 36 million bots would abruptly crash deep in library code of our media pipeline. unlike mostâ web servers a meeting bot instance is extremely stateful and very hard to replace a fatal error like this means the data is irrecoverably lost - forever. even a one-in-tens-of-millions failure rate was unacceptable. this is the story of how we tracked down the rare bug went to significant effort to reproduce it identify the root cause and fix it. we encountered an extremely rare segfault in the aac encoder we were using and root caused it to a bug in the fixed-point math c code. we found this was patched over a decade ago but the fix was never shipped to downstream consumers. rather than fixing the bug we replaced the library with a modern aac encoder which did not experience these crashes. this crash was so rare that reproducing it locally wasnât feasible. instead we opted to capture the program state from production in the rare event this happens. our first clue was the process 139 exit code  139  128  11 . signal 11 corresponds to sigsegv a segmentation fault or segfault for short this occurs when a program tries to access memory it is not supposed to. this mistake is bad enough that the execution of the program should halt immediately. so how do we determine the cause of a sigsegv the answer is often a core dump file. a core dump lets us"}
{"title": "Threads edges out X in daily mobile users, new data shows", "url": "https://techcrunch.com/2026/01/18/threads-edges-out-x-in-daily-mobile-users-new-data-shows/", "content": "Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us A report from market intelligence firm Similarweb suggests that Meta’s Threads is now seeing more daily usage than Elon Musk’s X on mobile devices. While X still dominates Threads on the web, the Threads mobile app for iOS and Android has continued to see an increase in daily active users over the past several months. Similarweb’s data shows that Threads had 141.5 million daily active users on iOS and Android as of January 7, 2026, after months of growth, while X has 125 million daily active users on mobile devices. This appears to be the result of longer-term trends, rather than a reaction to the recent X controversies, where users were discovered using the platform’s integrated AI, Grok, to create non-consensual nude images of women, including, sometimes minors. Concern around the deepfake images has now prompted California’s attorney general to open an investigation into Grok, following similar investigations by other regions, like the U.K. , EU, India, Brazil, and many more . The drama on X also led social networking startup Bluesky to see an increase in app installs in recent days. Instead, Threads’ boost in daily mobile usage may be driven by other factors, including cross-promotions from Meta’s larger social apps like Facebook and Instagram (where Threads is regularly advertised to existing users), its focus on creators, and the rapid rollout of new features. Over the past year, Threads has added features like interest-based communities , better filters , DMs , long-form text , and disappearing posts , and has recently been spotted te", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["late", "ai", "amazon", "apps", "biotech", "health", "climate", "cloud", "computing", "commerce", "crypto", "enterprise", "evs", "fintech", "fundraise", "gadget", "game", "google", "government", "policy", "hardware", "instagram", "layoff", "media", "entertainment", "meta", "microsoft", "privacy", "robotic", "security", "social", "space", "startup", "tiktok", "transportation", "venture", "staff", "event", "startup", "battlefield", "strictlyvc", "newsletter", "podcast", "video", "partner", "content", "techcrunch", "brand", "studio", "crunchboard", "contact", "report", "market", "intelligence", "firm", "similarweb", "suggest", "meta", "thread", "see", "daily", "usage", "elon", "musk", "mobile", "device", "dominate", "thread", "web", "thread", "mobile", "app", "io", "android", "continue", "increase", "daily", "active", "user", "past", "month", "similarwebs", "datum", "show", "thread", "141.5", "million", "daily", "active", "user", "io", "android", "january", "2026", "month", "growth", "125", "million", "daily", "active", "user", "mobile", "device", "appear", "result", "long", "term", "trend", "reaction", "recent", "controversy", "user", "discover", "platform", "integrate", "ai", "grok", "create", "non", "consensual", "nude", "image", "woman", "include", "minor", "concern", "deepfake", "image", "prompt", "californias", "attorney", "general", "open", "investigation", "grok", "follow", "similar", "investigation", "region", "like", "u.k", "eu", "india", "brazil", "drama", "lead", "social", "network", "startup", "bluesky", "increase", "app", "install", "recent", "day", "instead", "thread", "boost", "daily", "mobile", "usage", "drive", "factor", "include", "cross", "promotion", "metas", "large", "social", "app", "like", "facebook", "instagram", "thread", "regularly", "advertise", "exist", "user", "focus", "creator", "rapid", "rollout", "new", "feature", "past", "year", "thread", "add", "feature", "like", "interest", "base", "community", "well", "filter", "dms", "long", "form", "text", "disappear", "post", "recently", "spot", "te"], "num_tokens": 204, "token_loss_pct": 37.61, "normalized_content": "latest ai amazon apps biotech  health climate cloud computing commerce crypto enterprise evs fintech fundraising gadgets gaming google government  policy hardware instagram layoffs media  entertainment meta microsoft privacy robotics security social space startups tiktok transportation venture staff events startup battlefield strictlyvc newsletters podcasts videos partner content techcrunch brand studio crunchboard contact us a report from market intelligence firm similarweb suggests that metas threads is now seeing more daily usage than elon musks x on mobile devices. while x still dominates threads on the web the threads mobile app for ios and android has continued to see an increase in daily active users over the past several months. similarwebs data shows that threads had 141.5 million daily active users on ios and android as of january 7 2026 after months of growth while x has 125 million daily active users on mobile devices. this appears to be the result of longer-term trends rather than a reaction to the recent x controversies where users were discovered using the platforms integrated ai grok to create non-consensual nude images of women including sometimes minors. concern around the deepfake images has now prompted californias attorney general to open an investigation into grok following similar investigations by other regions like the u.k.  eu india brazil and many more . the drama on x also led social networking startup bluesky to see an increase in app installs in recent days. instead threads boost in daily mobile usage may be driven by other factors including cross-promotions from metas larger social apps like facebook and instagram where threads is regularly advertised to existing users its focus on creators and the rapid rollout of new features. over the past year threads has added features like interest-based communities  better filters  dms  long-form text  and disappearing posts  and has recently been spotted te"}
{"title": "Typography on Pencils (2023)", "url": "https://www.presentandcorrect.com/blogs/blog/typography-on-pencils-1-5", "content": "Happy New Year! Our shop is open again, click for hours It wouldn't be Pencil Day without a round up of our pencil typography photos. Check out our current stock of new & vintage pencils here. Please do credit us if you use these images anywhere. Thank you.  Sign up to our newsletter here . Thank you. 12 Bury Place London WC1A 2JL 020 72421421 info@presentandcorrect.com", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["happy", "new", "year", "shop", "open", "click", "hour", "pencil", "day", "round", "pencil", "typography", "photo", "check", "current", "stock", "new", "vintage", "pencil", "credit", "use", "image", "thank", "sign", "newsletter", "thank", "12", "bury", "place", "london", "wc1a", "2jl", "020", "72421421", "infomention.com"], "num_tokens": 35, "token_loss_pct": 51.39, "normalized_content": "happy new year our shop is open again click for hours it wouldn't be pencil day without a round up of our pencil typography photos. check out our current stock of new  vintage pencils here. please do credit us if you use these images anywhere. thank you. sign up to our newsletter here . thank you. 12 bury place london wc1a 2jl 020 72421421 infomention.com"}
{"title": "Understanding C++ Ownership System", "url": "https://blog.aiono.dev/posts/understanding-c++-ownership-system.html", "content": "I recently started using C++ at my $DAY_JOB and, along with that, decided to study C++ again. I think writing down your understanding is the best way to learn a topic. One part I find that is hard to understand in C++ is how the object ownership model works because it's not a single concept but a collection of a couple of smaller concepts. By ownership I mean creating and destroying objects, giving references to an object, and transferring ownership of an object. There is no one guide that covers everything. These concepts are very important to write and read modern C++ (though I doubt if C++11 is still considered \"modern\"). Even if you just want to write C with Classes-style C++, you will probably use standard containers like std::vector , which requires an understanding of C++ ownership related features such as RAII, references, and move semantics to use it properly. Without knowing those, you simply can't have the correct memory model for C++, resulting in buggy programs full of undefined behaviors and inefficient programs due to unnecessary copying. By knowing these concepts, you can both avoid introducing bugs due to lack of understanding and reason about programs better. This writing is my understanding of C++ ownership model. I think it can be useful to you if you have a basic level understanding of C++ and you want to learn more, or you are familiar with C++ but never learned the concepts and terminology formally. In C++, every object has an owner, which is responsible for cleaning up the data once it's not used anymore. If you come from garbage collected languages, the concept of ownership may seem strange to you. But consider the following code: This is a function that returns the file's name as a C-style string. What's not documented though, is who is supposed to deallocate the returned string. In this case there are two possibilities: Depending on which one is the case, the caller must act differently. This is because the owner of the data is different b", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recently", "start", "day_job", "decide", "study", "think", "write", "understanding", "good", "way", "learn", "topic", "find", "hard", "understand", "object", "ownership", "model", "work", "single", "concept", "collection", "couple", "small", "concept", "ownership", "mean", "create", "destroy", "object", "give", "reference", "object", "transfer", "ownership", "object", "guide", "cover", "concept", "important", "write", "read", "modern", "doubt", "c11", "consider", "modern", "want", "write", "class", "style", "probably", "use", "standard", "container", "like", "stdvector", "require", "understanding", "ownership", "relate", "feature", "raii", "reference", "semantic", "use", "properly", "know", "simply", "correct", "memory", "model", "result", "buggy", "program", "undefined", "behavior", "inefficient", "program", "unnecessary", "copying", "know", "concept", "avoid", "introduce", "bug", "lack", "understanding", "reason", "program", "well", "writing", "understanding", "ownership", "model", "think", "useful", "basic", "level", "understanding", "want", "learn", "familiar", "learn", "concept", "terminology", "formally", "object", "owner", "responsible", "clean", "datum", "anymore", "come", "garbage", "collect", "language", "concept", "ownership", "strange", "consider", "following", "code", "function", "return", "file", "style", "string", "document", "suppose", "deallocate", "return", "string", "case", "possibility", "depend", "case", "caller", "act", "differently", "owner", "data", "different"], "num_tokens": 143, "token_loss_pct": 61.04, "normalized_content": "i recently started using c at my day_job and along with that decided to study c again. i think writing down your understanding is the best way to learn a topic. one part i find that is hard to understand in c is how the object ownership model works because it's not a single concept but a collection of a couple of smaller concepts. by ownership i mean creating and destroying objects giving references to an object and transferring ownership of an object. there is no one guide that covers everything. these concepts are very important to write and read modern c though i doubt if c11 is still considered modern. even if you just want to write c with classes-style c you will probably use standard containers like stdvector  which requires an understanding of c ownership related features such as raii references and move semantics to use it properly. without knowing those you simply can't have the correct memory model for c resulting in buggy programs full of undefined behaviors and inefficient programs due to unnecessary copying. by knowing these concepts you can both avoid introducing bugs due to lack of understanding and reason about programs better. this writing is my understanding of c ownership model. i think it can be useful to you if you have a basic level understanding of c and you want to learn more or you are familiar with c but never learned the concepts and terminology formally. in c every object has an owner which is responsible for cleaning up the data once it's not used anymore. if you come from garbage collected languages the concept of ownership may seem strange to you. but consider the following code this is a function that returns the file's name as a c-style string. what's not documented though is who is supposed to deallocate the returned string. in this case there are two possibilities depending on which one is the case the caller must act differently. this is because the owner of the data is different b"}
{"title": "Show HN: E80: an 8-bit CPU in structural VHDL", "url": "https://github.com/Stokpan/E80", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . A simple CPU in VHDL for educational purposes There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . A simple CPU in VHDL, developed from scratch for my undergraduate thesis to provide all three characteristics of a Constructionist Microworld : This makes it easy to use, capable of running pretty complex and realistic programs, and can be used in multiple lab or classroom scenarios. Notes Notes The following program writes the null-terminated string `az{\"0 to memory after the last instruction (notice the label under HLT) and converts the lowercase characters to uppercase, stopping when it hits the terminator: To simulate it, first install the E80 Toolchain package from the Releases, then open the E80 Editor and paste the code into it:  Notice that syntax highlighting for the E80 assembly language has been enabled by default for all code (except for VHDL files). Press F5. The editor will automatically assemble the code, save the VHDL output, compile the entire design with GHDL, and launch a GTKWave instance. Subsequent simulations will be closing the previous GTKWave window to open a new one. You should see the following waveform, in which the RAM has been expanded to show how the lowercase letters of the string have changed to uppercase:  Notice that the HLT instruction has stopped the simulation in GHDL, allowing for the waveforms to be drawn for the runtime only. This useful feature is supported in ModelSim as well. You can also press F7 to view the generated Firmware.vhd file, without simulation:  Notice how the assembler formats the output into columns according to instruction size, and annotates each line to its respective disassembled instruction, ASCII character or number. If you have installed ModelSim, you can press F8 to automatically open ModelSim and simulate into it", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "simple", "cpu", "vhdl", "educational", "purpose", "error", "load", "reload", "page", "error", "load", "reload", "page", "simple", "cpu", "vhdl", "develop", "scratch", "undergraduate", "thesis", "provide", "characteristic", "constructionist", "microworld", "make", "easy", "use", "capable", "run", "pretty", "complex", "realistic", "program", "multiple", "lab", "classroom", "scenario", "note", "note", "following", "program", "write", "null", "terminate", "string", "az0", "memory", "instruction", "notice", "label", "hlt", "convert", "lowercase", "character", "uppercase", "stop", "hit", "terminator", "simulate", "install", "e80", "toolchain", "package", "release", "open", "e80", "editor", "paste", "code", "notice", "syntax", "highlighting", "e80", "assembly", "language", "enable", "default", "code", "vhdl", "file", "press", "f5", "editor", "automatically", "assemble", "code", "save", "vhdl", "output", "compile", "entire", "design", "ghdl", "launch", "gtkwave", "instance", "subsequent", "simulation", "close", "previous", "gtkwave", "window", "open", "new", "follow", "waveform", "ram", "expand", "lowercase", "letter", "string", "change", "uppercase", "notice", "hlt", "instruction", "stop", "simulation", "ghdl", "allow", "waveform", "draw", "runtime", "useful", "feature", "support", "modelsim", "press", "f7", "view", "generate", "firmware.vhd", "file", "simulation", "notice", "assembler", "format", "output", "column", "accord", "instruction", "size", "annotate", "line", "respective", "disassemble", "instruction", "ascii", "character", "number", "instal", "modelsim", "press", "f8", "automatically", "open", "modelsim", "simulate"], "num_tokens": 166, "token_loss_pct": 51.03, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . a simple cpu in vhdl for educational purposes there was an error while loading. please reload this page . there was an error while loading. please reload this page . a simple cpu in vhdl developed from scratch for my undergraduate thesis to provide all three characteristics of a constructionist microworld  this makes it easy to use capable of running pretty complex and realistic programs and can be used in multiple lab or classroom scenarios. notes notes the following program writes the null-terminated string az0 to memory after the last instruction notice the label under hlt and converts the lowercase characters to uppercase stopping when it hits the terminator to simulate it first install the e80 toolchain package from the releases then open the e80 editor and paste the code into it notice that syntax highlighting for the e80 assembly language has been enabled by default for all code except for vhdl files. press f5. the editor will automatically assemble the code save the vhdl output compile the entire design with ghdl and launch a gtkwave instance. subsequent simulations will be closing the previous gtkwave window to open a new one. you should see the following waveform in which the ram has been expanded to show how the lowercase letters of the string have changed to uppercase notice that the hlt instruction has stopped the simulation in ghdl allowing for the waveforms to be drawn for the runtime only. this useful feature is supported in modelsim as well. you can also press f7 to view the generated firmware.vhd file without simulation notice how the assembler formats the output into columns according to instruction size and annotates each line to its respective disassembled instruction ascii character or number. if you have installed modelsim you can press f8 to automatically open modelsim and simulate into it"}
{"title": "Simple Sabotage Field Manual (1944) [pdf]", "url": "https://www.cia.gov/static/5c875f3ec660e092cf893f60b4a288df/SimpleSabotage.pdf", "content": "Simple Sabotage Field Manual (1944) [pdf]. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["simple", "sabotage", "field", "manual", "1944", "pdf", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 40.0, "normalized_content": "simple sabotage field manual 1944 pdf. score none. author none. date none"}
{"title": "Legal Structures for Latin American Startups (2021)", "url": "https://latamlist.com/legal-structures-for-latin-american-startups/", "content": "There’s confusion around what legal structures make sense for Latin American startups. Founders, VCs and even lawyers can make decisions that can cost upwards of $100M if you get it wrong. This post is the result of investing in 80+ startups from 15+ Latin American countries since 2014 via Magma Partners , and speaking to and working with countless lawyers across LatAm, US, UK, Europe and multiple offshore jurisdictions. I wrote a version of this that I’ve been sharing with Magma Partners founders internally and decided to open source it with the hope that founders save themselves time and money and make themselves more investable. There are fairly clear outlines that most Latin American startups should likely follow. Every startup’s case is different, and each founder should get legal advice from a lawyer and tax advice from an accountant with relevant US and Latin American venture capital experience before following this guide or anyone else’s ideas. To be clear, this is not legal or tax advice. You should always work with a lawyer and accountant when thinking about corporate structures. The money you’ll spend getting good advice will save hundreds of thousands or even hundreds of millions of dollars down the road. I can’t stress this enough. Don’t just follow these guidelines. Your situation is unique. Talk to an experienced lawyer and accountant. Let’s start with a story. Brian Requarth, cofounder of Vivareal and Latitud had a big exit in 2020. His structure cost him and his investors $100M: In the early days of a startup, money is tight and it’s common to cut corners. I created a California LLC for my company because of my local accountant’s advice. He had zero experience with VC or Latin America. Later, I hired a my hometown law firm that had no VC experience, which advised me to create a C-Corp, which seemed like good advice at the time. We later realized that even though our business had no operations in the US, we would be subject to US taxes upon an exit.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["confusion", "legal", "structure", "sense", "latin", "american", "startup", "founder", "vcs", "lawyer", "decision", "cost", "upwards", "100", "wrong", "post", "result", "invest", "80", "startup", "15", "latin", "american", "country", "2014", "magma", "partner", "speak", "work", "countless", "lawyer", "latam", "uk", "europe", "multiple", "offshore", "jurisdiction", "write", "version", "ve", "share", "magma", "partner", "founder", "internally", "decide", "open", "source", "hope", "founder", "save", "time", "money", "investable", "fairly", "clear", "outline", "latin", "american", "startup", "likely", "follow", "startup", "case", "different", "founder", "legal", "advice", "lawyer", "tax", "advice", "accountant", "relevant", "latin", "american", "venture", "capital", "experience", "follow", "guide", "else", "idea", "clear", "legal", "tax", "advice", "work", "lawyer", "accountant", "think", "corporate", "structure", "money", "ll", "spend", "get", "good", "advice", "save", "hundred", "thousand", "hundred", "million", "dollar", "road", "not", "stress", "not", "follow", "guideline", "situation", "unique", "talk", "experienced", "lawyer", "accountant", "let", "start", "story", "brian", "requarth", "cofounder", "vivareal", "latitud", "big", "exit", "2020", "structure", "cost", "investor", "100", "early", "day", "startup", "money", "tight", "common", "cut", "corner", "create", "california", "llc", "company", "local", "accountant", "advice", "zero", "experience", "vc", "latin", "america", "later", "hire", "hometown", "law", "firm", "vc", "experience", "advise", "create", "corp", "like", "good", "advice", "time", "later", "realize", "business", "operation", "subject", "taxis", "exit"], "num_tokens": 172, "token_loss_pct": 53.39, "normalized_content": "theres confusion around what legal structures make sense for latin american startups. founders vcs and even lawyers can make decisions that can cost upwards of 100m if you get it wrong. this post is the result of investing in 80 startups from 15 latin american countries since 2014 via magma partners  and speaking to and working with countless lawyers across latam us uk europe and multiple offshore jurisdictions. i wrote a version of this that ive been sharing with magma partners founders internally and decided to open source it with the hope that founders save themselves time and money and make themselves more investable. there are fairly clear outlines that most latin american startups should likely follow. every startups case is different and each founder should get legal advice from a lawyer and tax advice from an accountant with relevant us and latin american venture capital experience before following this guide or anyone elses ideas. to be clear this is not legal or tax advice. you should always work with a lawyer and accountant when thinking about corporate structures. the money youll spend getting good advice will save hundreds of thousands or even hundreds of millions of dollars down the road. i cant stress this enough. dont just follow these guidelines. your situation is unique. talk to an experienced lawyer and accountant. lets start with a story. brian requarth cofounder of vivareal and latitud had a big exit in 2020. his structure cost him and his investors 100m in the early days of a startup money is tight and its common to cut corners. i created a california llc for my company because of my local accountants advice. he had zero experience with vc or latin america. later i hired a my hometown law firm that had no vc experience which advised me to create a c-corp which seemed like good advice at the time. we later realized that even though our business had no operations in the us we would be subject to us taxes upon an exit."}
{"title": "Gladys West's vital contributions to GPS technology", "url": "https://en.wikipedia.org/wiki/Gladys_West", "content": "Gladys Mae West (née Brown ; October 27, 1930 – January 17, 2026) was an American mathematician. She was known for her contributions to mathematical modeling of the shape of the Earth , and her work on the development of satellite geodesy models, that were later incorporated into the Global Positioning System (GPS). [ 1 ] West was inducted into the United States Air Force Hall of Fame in 2018. She was awarded the Webby Lifetime Achievement Award for the development of satellite geodesy models. [ 2 ] [ 3 ] Gladys Mae Brown was born in Sutherland, Virginia , in Dinwiddie County , a rural county south of Richmond , on October 27, 1930. [ 1 ] [ 4 ] [ 5 ] [ 6 ] Her family was an African-American farming family in a community of sharecroppers . She spent much of her childhood working on her family's small farm. [ 7 ] [ 8 ] As well as working on the farm, her mother worked in a tobacco factory and her father worked for the railroad. [ 5 ] [ 9 ] West saw education as her way to a different life. [ 10 ] At West's high school, the top two students from each graduating class received full scholarships to Virginia State College ( Virginia State University (VSU)), a historically black public university . [ 7 ] West graduated as valedictorian in 1948, and received the scholarship. [ 5 ] [ 10 ] At VSU, she chose to study mathematics, a subject that was mostly studied by men. [ 7 ] She also joined the Alpha Kappa Alpha sorority. [ 1 ] West graduated in 1952 with a Bachelor of Science degree in mathematics, [ 5 ] and then taught mathematics and science for two years in Waverly , Virginia. [ 5 ] West returned to VSU to complete a Master of Mathematics degree, graduating in 1955. [ 10 ] [ 5 ] Afterwards, she began another teaching position in Martinsville , Virginia. [ 5 ] In 1956, West was hired to work at the Naval Proving Ground in Dahlgren , Virginia (later the Naval Surface Warfare Center ). She was the second black woman hired and one of only four black employees. [ 7 ] [ 4 ] [", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["gladys", "mae", "west", "née", "brown", "october", "27", "1930", "january", "17", "2026", "american", "mathematician", "know", "contribution", "mathematical", "modeling", "shape", "earth", "work", "development", "satellite", "geodesy", "model", "later", "incorporate", "global", "positioning", "system", "gps", "west", "induct", "united", "states", "air", "force", "hall", "fame", "2018", "award", "webby", "lifetime", "achievement", "award", "development", "satellite", "geodesy", "model", "  ", "gladys", "mae", "brown", "bear", "sutherland", "virginia", "dinwiddie", "county", "rural", "county", "south", "richmond", "october", "27", "1930", "  ", "  ", "  ", "family", "african", "american", "farming", "family", "community", "sharecropper", "spend", "childhood", "work", "family", "small", "farm", "  ", "work", "farm", "mother", "work", "tobacco", "factory", "father", "work", "railroad", "  ", "west", "see", "education", "way", "different", "life", "10", "west", "high", "school", "student", "graduating", "class", "receive", "scholarship", "virginia", "state", "college", "virginia", "state", "university", "vsu", "historically", "black", "public", "university", "west", "graduate", "valedictorian", "1948", "receive", "scholarship", "  ", "10", "vsu", "choose", "study", "mathematic", "subject", "study", "man", "join", "alpha", "kappa", "alpha", "sorority", "west", "graduate", "1952", "bachelor", "science", "degree", "mathematic", "teach", "mathematic", "science", "year", "waverly", "virginia", "west", "return", "vsu", "complete", "master", "mathematics", "degree", "graduate", "1955", "10", "  ", "begin", "teaching", "position", "martinsville", "virginia", "1956", "west", "hire", "work", "naval", "proving", "ground", "dahlgren", "virginia", "later", "naval", "surface", "warfare", "center", "second", "black", "woman", "hire", "black", "employee", "  "], "num_tokens": 187, "token_loss_pct": 52.3, "normalized_content": "gladys mae west née brown  october 27 1930  january 17 2026 was an american mathematician. she was known for her contributions to mathematical modeling of the shape of the earth  and her work on the development of satellite geodesy models that were later incorporated into the global positioning system gps.  1  west was inducted into the united states air force hall of fame in 2018. she was awarded the webby lifetime achievement award for the development of satellite geodesy models.  2   3  gladys mae brown was born in sutherland virginia  in dinwiddie county  a rural county south of richmond  on october 27 1930.  1   4   5   6  her family was an african-american farming family in a community of sharecroppers . she spent much of her childhood working on her family's small farm.  7   8  as well as working on the farm her mother worked in a tobacco factory and her father worked for the railroad.  5   9  west saw education as her way to a different life.  10  at west's high school the top two students from each graduating class received full scholarships to virginia state college  virginia state university vsu a historically black public university .  7  west graduated as valedictorian in 1948 and received the scholarship.  5   10  at vsu she chose to study mathematics a subject that was mostly studied by men.  7  she also joined the alpha kappa alpha sorority.  1  west graduated in 1952 with a bachelor of science degree in mathematics  5  and then taught mathematics and science for two years in waverly  virginia.  5  west returned to vsu to complete a master of mathematics degree graduating in 1955.  10   5  afterwards she began another teaching position in martinsville  virginia.  5  in 1956 west was hired to work at the naval proving ground in dahlgren  virginia later the naval surface warfare center . she was the second black woman hired and one of only four black employees.  7   4"}
{"title": "Windows 11 had 20 major update problems in 2025 and and 2026 started badly too", "url": "https://www.windowslatest.com/2026/01/21/windows-11-had-20-major-update-problems-in-2025-and-and-2026-started-badly-too-what-are-you-doing-microsoft/", "content": "Just weeks into 2026, Windows 11 is already tripping over itself again. As Windows Latest recently reported, the January 2026 update KB5074109 shipped with a fresh set of problems , including black screens and frozen Outlook POP accounts. That update made it very clear that whatever lessons Microsoft was supposed to learn in 2025, it didn’t. Windows 11 is at a point where it is hated by virtually everyone on the internet, and Microsoft just doesn’t seem to care. We checked through our posts from 2025 and made a list of all the issues that the most popular desktop operating system gave to its users in the last year. 2025 has been a catastrophe for Microsoft, with more issues than ever before. A big part of the problem is focus, or the lack of it. While core parts of Windows 11 kept breaking update after update, Microsoft was busy pushing Copilot into every nook and corner of Windows. We have listed the top 20 Windows 11 issues from 2025 below, but there were many more bugs that didn’t make headlines. At this point, for many users, Windows 11 has become the most disliked version of Windows Microsoft has ever shipped. As originally reported by Windows Latest, the very first security update in the second week of 2025 (KB5050009 for 24H2 and KB5050021 for 23H2) broke the audio for users with external USB Digital-to-Analog Converters (DACs) . In our testing, we found that the audio in our PC stopped working as soon as the update was installed, and the main casualties were those who use a USB audio DAC. The issue was so widespread that Windows 11 versions 24H2, 23H2, and 22H2 were all affected. Even the famously stable Windows 10 wasn’t spared from audio failure. The Device Manager showed the following error message: “This device cannot start. (Code 10) Insufficient system resources exist to complete the API.” Our analysis was that Windows 11 was failing to allocate memory to the device, preventing DACs from transferring audio signals to your headphones. Microsoft acknowle", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["week", "2026", "window", "11", "trip", "window", "latest", "recently", "report", "january", "2026", "update", "kb5074109", "ship", "fresh", "set", "problem", "include", "black", "screen", "frozen", "outlook", "pop", "account", "update", "clear", "lesson", "microsoft", "suppose", "learn", "2025", "not", "window", "11", "point", "hat", "virtually", "internet", "microsoft", "not", "care", "check", "post", "2025", "list", "issue", "popular", "desktop", "operating", "system", "give", "user", "year", "2025", "catastrophe", "microsoft", "issue", "big", "problem", "focus", "lack", "core", "part", "window", "11", "keep", "break", "update", "update", "microsoft", "busy", "push", "copilot", "nook", "corner", "window", "list", "20", "window", "11", "issue", "2025", "bug", "not", "headline", "point", "user", "window", "11", "disliked", "version", "windows", "microsoft", "ship", "originally", "report", "window", "late", "security", "update", "second", "week", "2025", "kb5050009", "24h2", "kb5050021", "23h2", "break", "audio", "user", "external", "usb", "digital", "analog", "converter", "dac", "testing", "find", "audio", "pc", "stop", "work", "soon", "update", "instal", "main", "casualty", "use", "usb", "audio", "dac", "issue", "widespread", "window", "11", "version", "24h2", "23h2", "22h2", "affect", "famously", "stable", "window", "10", "not", "spar", "audio", "failure", "device", "manager", "show", "follow", "error", "message", "device", "start", "code", "10", "insufficient", "system", "resource", "exist", "complete", "api", "analysis", "window", "11", "fail", "allocate", "memory", "device", "prevent", "dac", "transfer", "audio", "signal", "headphone", "microsoft", "acknowle"], "num_tokens": 179, "token_loss_pct": 51.23, "normalized_content": "just weeks into 2026 windows 11 is already tripping over itself again. as windows latest recently reported the january 2026 update kb5074109 shipped with a fresh set of problems  including black screens and frozen outlook pop accounts. that update made it very clear that whatever lessons microsoft was supposed to learn in 2025 it didnt. windows 11 is at a point where it is hated by virtually everyone on the internet and microsoft just doesnt seem to care. we checked through our posts from 2025 and made a list of all the issues that the most popular desktop operating system gave to its users in the last year. 2025 has been a catastrophe for microsoft with more issues than ever before. a big part of the problem is focus or the lack of it. while core parts of windows 11 kept breaking update after update microsoft was busy pushing copilot into every nook and corner of windows. we have listed the top 20 windows 11 issues from 2025 below but there were many more bugs that didnt make headlines. at this point for many users windows 11 has become the most disliked version of windows microsoft has ever shipped. as originally reported by windows latest the very first security update in the second week of 2025 kb5050009 for 24h2 and kb5050021 for 23h2 broke the audio for users with external usb digital-to-analog converters dacs . in our testing we found that the audio in our pc stopped working as soon as the update was installed and the main casualties were those who use a usb audio dac. the issue was so widespread that windows 11 versions 24h2 23h2 and 22h2 were all affected. even the famously stable windows 10 wasnt spared from audio failure. the device manager showed the following error message this device cannot start. code 10 insufficient system resources exist to complete the api. our analysis was that windows 11 was failing to allocate memory to the device preventing dacs from transferring audio signals to your headphones. microsoft acknowle"}
{"title": "There's a hidden Android setting that spots fake cell towers", "url": "https://www.howtogeek.com/theres-a-hidden-android-setting-that-spots-fake-cell-towers/", "content": "Most people never give a second thought to how their phone connects to a cell tower. It’s something that constantly happens in the background without our input, and therein lies the potential for trouble. What if that tower isn't what it seems? Android can tell you about it—maybe. Let’s get the scary stuff out of the way first. “ Stingrays ,” technically known as IMSI (international mobile subscriber identity) catchers, are devices primarily used for surveillance. They mimic cell towers and act as a middleman between your phone and the network. Once your device is tricked into connecting to what it believes to be a real cell tower, the attacker can harvest device information and force your phone onto an older, unencrypted protocol. This is what allows them to listen to your calls or read your texts without you ever knowing something is wrong. It’s also possible for the attacker to harvest information from the phones of people nearby when this happens. While Stingrays have been used by law enforcement agencies for years to track suspects, it’s now much easier for malicious individuals to get their hands on them and skim data from innocent people. You might think that switching from Facebook Messenger to old-fashioned text messages would help protect your privacy. But standard SMS text messages aren't very private or secure. SMS is like fax---an old, outdated standard that refuses to go away. The good news is that Google has been slowly building a wall against these attacks—emphasis on “slowly.” In 2021, Google released Android 12 with the ability to disable 2G connectivity. Stringrays like this network for its weak security. Two years later, it announced that Android 14 would support disabling an old form of encryption that makes it easy to intercept SMS and calls. Then Android 15 addressed Stingrays with the ability to notify the OS when a network requests your identifiers or forces you onto a less secure encryption method. That brings us up to Android 16—the latest", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["people", "second", "thought", "phone", "connect", "cell", "tower", "constantly", "happen", "background", "input", "lie", "potential", "trouble", "tower", "android", "tell", "itmaybe", "let", "scary", "stuff", "way", "stingray", "technically", "know", "imsi", "international", "mobile", "subscriber", "identity", "catcher", "device", "primarily", "surveillance", "mimic", "cell", "tower", "act", "middleman", "phone", "network", "device", "trick", "connect", "believe", "real", "cell", "tower", "attacker", "harvest", "device", "information", "force", "phone", "older", "unencrypte", "protocol", "allow", "listen", "call", "read", "text", "know", "wrong", "possible", "attacker", "harvest", "information", "phone", "people", "nearby", "happen", "stingray", "law", "enforcement", "agency", "year", "track", "suspect", "easy", "malicious", "individual", "hand", "skim", "datum", "innocent", "people", "think", "switch", "facebook", "messenger", "old", "fashioned", "text", "message", "help", "protect", "privacy", "standard", "sms", "text", "message", "private", "secure", "sms", "like", "fax", "old", "outdated", "standard", "refuse", "away", "good", "news", "google", "slowly", "build", "wall", "attacksemphasis", "slowly", "2021", "google", "release", "android", "12", "ability", "disable", "connectivity", "stringray", "like", "network", "weak", "security", "year", "later", "announce", "android", "14", "support", "disable", "old", "form", "encryption", "make", "easy", "intercept", "sm", "call", "android", "15", "address", "stingray", "ability", "notify", "os", "network", "request", "identifier", "force", "secure", "encryption", "method", "bring", "android", "16the", "late"], "num_tokens": 166, "token_loss_pct": 53.5, "normalized_content": "most people never give a second thought to how their phone connects to a cell tower. its something that constantly happens in the background without our input and therein lies the potential for trouble. what if that tower isn't what it seems android can tell you about itmaybe. lets get the scary stuff out of the way first.  stingrays  technically known as imsi international mobile subscriber identity catchers are devices primarily used for surveillance. they mimic cell towers and act as a middleman between your phone and the network. once your device is tricked into connecting to what it believes to be a real cell tower the attacker can harvest device information and force your phone onto an older unencrypted protocol. this is what allows them to listen to your calls or read your texts without you ever knowing something is wrong. its also possible for the attacker to harvest information from the phones of people nearby when this happens. while stingrays have been used by law enforcement agencies for years to track suspects its now much easier for malicious individuals to get their hands on them and skim data from innocent people. you might think that switching from facebook messenger to old-fashioned text messages would help protect your privacy. but standard sms text messages aren't very private or secure. sms is like fax---an old outdated standard that refuses to go away. the good news is that google has been slowly building a wall against these attacksemphasis on slowly. in 2021 google released android 12 with the ability to disable 2g connectivity. stringrays like this network for its weak security. two years later it announced that android 14 would support disabling an old form of encryption that makes it easy to intercept sms and calls. then android 15 addressed stingrays with the ability to notify the os when a network requests your identifiers or forces you onto a less secure encryption method. that brings us up to android 16the latest"}
{"title": "Show HN: Open-source tool for converting docs into .md and loading into Postgres", "url": "https://github.com/pgEdge/pgedge-docloader", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . A tool for converting HTML and RST docs into Markdown, and loading them into PostgreSQL. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  pgEdge Document Loader is a command-line tool for loading documents from various formats into PostgreSQL databases.  Full documentation is available here . The pgEdge Document Loader automatically converts documents (HTML, Markdown, reStructuredText, and SGML/DocBook) to Markdown format and loads them into a PostgreSQL database with extracted metadata. Features The pgEdge Document Loader automatically converts documents (HTML, Markdown, reStructuredText, and DocBook SGML/XML) to Markdown format and loads them into a PostgreSQL database with extracted metadata. Features Before installing and using pgEdge Document Loader, download and install: Getting started with pgEdge Document Loader involves three steps: Installing pgEdge Document Loader Use the following commands to download and build pgedge-docloader : Creating a Postgres Table Before invoking Document Loader, you must configure a Postgres database and create a table with the appropriate columns to hold the extracted documentation content: Invoking pgedge-docloader When invoking pgedge-docloader , you can specify configuration preferences on the command line , or with a configuration file . The following command invokes Document Loader on the command line : To manage deployment preferences in a configuration file , save your deployment details in a file, and then include the --config keyword when invoking pgedge-docloader : For a comprehensive Quickstart Guide, visit here . This project is under active development. See the documentation for the latest\nfeatures and updates. The pgEdge Document Loader Makefile includes clauses that run test cases or invoke the go linter.  Use the foll", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "tool", "convert", "html", "rst", "doc", "markdown", "load", "postgresql", "error", "load", "reload", "page", "error", "load", "reload", "page", "pgedge", "document", "loader", "command", "line", "tool", "load", "document", "format", "postgresql", "database", "documentation", "available", "pgedge", "document", "loader", "automatically", "convert", "document", "html", "markdown", "restructuredtext", "sgmldocbook", "markdown", "format", "load", "postgresql", "database", "extract", "metadata", "feature", "pgedge", "document", "loader", "automatically", "convert", "document", "html", "markdown", "restructuredtext", "docbook", "sgmlxml", "markdown", "format", "load", "postgresql", "database", "extract", "metadata", "feature", "instal", "pgedge", "document", "loader", "download", "install", "getting", "start", "pgedge", "document", "loader", "involve", "step", "instal", "pgedge", "document", "loader", "use", "follow", "command", "download", "build", "pgedge", "docloader", "create", "postgre", "table", "invoke", "document", "loader", "configure", "postgre", "database", "create", "table", "appropriate", "column", "hold", "extract", "documentation", "content", "invoke", "pgedge", "docloader", "invoke", "pgedge", "docloader", "specify", "configuration", "preference", "command", "line", "configuration", "file", "follow", "command", "invoke", "document", "loader", "command", "line", "manage", "deployment", "preference", "configuration", "file", "save", "deployment", "detail", "file", "include", "--config", "keyword", "invoke", "pgedge", "docloader", "comprehensive", "quickstart", "guide", "visit", "project", "active", "development", "documentation", "late", "feature", "update", "pgedge", "document", "loader", "makefile", "include", "clause", "run", "test", "case", "invoke", "linter", "use", "foll"], "num_tokens": 174, "token_loss_pct": 44.76, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . a tool for converting html and rst docs into markdown and loading them into postgresql. there was an error while loading. please reload this page . there was an error while loading. please reload this page . pgedge document loader is a command-line tool for loading documents from various formats into postgresql databases. full documentation is available here . the pgedge document loader automatically converts documents html markdown restructuredtext and sgmldocbook to markdown format and loads them into a postgresql database with extracted metadata. features the pgedge document loader automatically converts documents html markdown restructuredtext and docbook sgmlxml to markdown format and loads them into a postgresql database with extracted metadata. features before installing and using pgedge document loader download and install getting started with pgedge document loader involves three steps installing pgedge document loader use the following commands to download and build pgedge-docloader  creating a postgres table before invoking document loader you must configure a postgres database and create a table with the appropriate columns to hold the extracted documentation content invoking pgedge-docloader when invoking pgedge-docloader  you can specify configuration preferences on the command line  or with a configuration file . the following command invokes document loader on the command line  to manage deployment preferences in a configuration file  save your deployment details in a file and then include the --config keyword when invoking pgedge-docloader  for a comprehensive quickstart guide visit here . this project is under active development. see the documentation for the latest features and updates. the pgedge document loader makefile includes clauses that run test cases or invoke the go linter. use the foll"}
{"title": "SWE-gen: Scaling SWE-bench task generation", "url": "https://github.com/abundant-ai/SWE-gen", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Convert GitHub PRs into Harbor tasks There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .   Convert merged GitHub PRs into Harbor tasks automatically. Automates task creation from real bug fixes in open-source GitHub repos. Works with any programming language : Claude Code analyzes the repo to detect language, build system, and test framework. Each task reverses a merged PR to recreate the buggy state, verifies tests fail on baseline, and pass after applying the fix. Fully containerized with all dependencies installed at build time. Ensure these environment variables are set: Note: Cloud sandbox environments (Daytona, E2B, Modal, etc.) require additional API keys. Commands: Stream through entire PR history, process each sequentially with state persistence. Verify that a task passes NOP (baseline fails) and Oracle (solution succeeds) agents: Run agent trials to verify a task is well-specified and solvable: Classification categories: Languages: Any (Python, JavaScript, TypeScript, Go, Rust, Ruby, Java, etc.) Valid PRs must: The pipeline uses a language-agnostic approach : Key Details: Apache License 2.0 Convert GitHub PRs into Harbor tasks There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "convert", "github", "prs", "harbor", "task", "error", "load", "reload", "page", "error", "load", "reload", "page", "convert", "merged", "github", "prs", "harbor", "task", "automatically", "automate", "task", "creation", "real", "bug", "fix", "open", "source", "github", "repos", "work", "programming", "language", "claude", "code", "analyze", "repo", "detect", "language", "build", "system", "test", "framework", "task", "reverse", "merged", "pr", "recreate", "buggy", "state", "verifie", "test", "fail", "baseline", "pass", "apply", "fix", "fully", "containerize", "dependency", "instal", "build", "time", "ensure", "environment", "variable", "set", "note", "cloud", "sandbox", "environment", "daytona", "e2b", "modal", "etc", "require", "additional", "api", "key", "command", "stream", "entire", "pr", "history", "process", "sequentially", "state", "persistence", "verify", "task", "pass", "nop", "baseline", "fail", "oracle", "solution", "succeed", "agent", "run", "agent", "trial", "verify", "task", "specify", "solvable", "classification", "category", "language", "python", "javascript", "typescript", "rust", "ruby", "java", "etc", "valid", "prs", "pipeline", "use", "language", "agnostic", "approach", "key", "detail", "apache", "license", "2.0", "convert", "github", "prs", "harbor", "task", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 156, "token_loss_pct": 42.44, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . convert github prs into harbor tasks there was an error while loading. please reload this page . there was an error while loading. please reload this page . convert merged github prs into harbor tasks automatically. automates task creation from real bug fixes in open-source github repos. works with any programming language  claude code analyzes the repo to detect language build system and test framework. each task reverses a merged pr to recreate the buggy state verifies tests fail on baseline and pass after applying the fix. fully containerized with all dependencies installed at build time. ensure these environment variables are set note cloud sandbox environments daytona e2b modal etc. require additional api keys. commands stream through entire pr history process each sequentially with state persistence. verify that a task passes nop baseline fails and oracle solution succeeds agents run agent trials to verify a task is well-specified and solvable classification categories languages any python javascript typescript go rust ruby java etc. valid prs must the pipeline uses a language-agnostic approach  key details apache license 2.0 convert github prs into harbor tasks there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Show HN: Movieagent.io – An agent for movie recommendations (with couple mode)", "url": "https://movieagent.io", "content": "Show HN: Movieagent.io – An agent for movie recommendations (with couple mode). Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["hn", "movieagent.io", "agent", "movie", "recommendation", "couple", "mode", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 52.38, "normalized_content": "show hn movieagent.io  an agent for movie recommendations with couple mode. score none. author none. date none"}
{"title": "Ask HN: What are the recommender systems papers from 2024-2025?", "url": "item?id=46692368", "content": "Ask HN: What are the recommender systems papers from 2024-2025?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "recommender", "system", "paper", "2024", "2025", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 52.38, "normalized_content": "ask hn what are the recommender systems papers from 2024-2025. score none. author none. date none"}
{"title": "Show HN: A creative coding library for making art with desktop windows", "url": "https://github.com/willmeyers/window-art", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . A minimal Python library for live coding visual scenes using desktop windows. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . A minimal Python library for live coding visual scenes using desktop windows.  A minimal Python library for live coding visual scenes using desktop windows. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "minimal", "python", "library", "live", "cod", "visual", "scene", "desktop", "window", "error", "load", "reload", "page", "error", "load", "reload", "page", "minimal", "python", "library", "live", "cod", "visual", "scene", "desktop", "window", "minimal", "python", "library", "live", "cod", "visual", "scene", "desktop", "window", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 51, "token_loss_pct": 53.21, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . a minimal python library for live coding visual scenes using desktop windows. there was an error while loading. please reload this page . there was an error while loading. please reload this page . a minimal python library for live coding visual scenes using desktop windows. a minimal python library for live coding visual scenes using desktop windows. there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "I Burned $160k Trying to Solve \"Online Tailoring\"", "url": "https://www.indiehackers.com/post/how-i-burned-160-000-trying-to-solve-online-tailoring-the-engineering-reality-check-fbd7e0ccdd", "content": "Report In 2023, I placed a crazy bet. I founded a fashion-tech startup with a vision of “Phygital Tailoring” . My goal was simple but audacious: Clients in the world should receive perfect-fit bespoke suits without ever leaving their homes. I entered the space with the arrogance of a typical disruptor: “If I just use high-resolution 3D scanning, I can replace the traditional tailor. Math will solve everything.” I was wrong. After 900 days of development and burning through $160,000 in savings, I realized why the current market solutions were failing. I used to think of “Fit” as a math problem. It isn’t. It’s a physics and logic problem. Here is the hard technical truth about why “Online Tailoring” became a graveyard for my initial capital, and the four engineering bottlenecks we had to overcome. Most scanning SDKs rely on the user holding the phone or placing it on a table. The Problem : Users struggle with geometry. My assumption that users could hold a device perfectly perpendicular to the floor was flawed. The Data : A mere 5-degree tilt results in a 2–3cm error in leg length. In bespoke tailoring, a 3cm error is the difference between a wearable suit and a disaster. My Fix : I stopped trusting the raw scan. We built a secondary algorithm on top of the standard scanning SDK to detect device orientation. If the angle isn’t perfect, the system forces a “Normalization” process to mathematically correct the geometric distortion before data entry. This was the most common failure point. A 3D scan gives you Body Measurements (the skin). But a suit requires Garment Measurements (the shell). The Problem : Machines don’t understand “Ease” (movement allowance) or “Drape” (fabric fall). My raw translation of scan-to-pattern resulted in a skin-tight suit that looked like a wetsuit. My Fix : Where AI failed, humans succeeded. I spent two years digitizing the logic of old-school Master Tailors. I built a “Human Logic Filter” that sits between the scan and the CAD system, autom", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["report", "2023", "place", "crazy", "bet", "found", "fashion", "tech", "startup", "vision", "phygital", "tailoring", "goal", "simple", "audacious", "client", "world", "receive", "perfect", "fit", "bespoke", "suit", "leave", "home", "enter", "space", "arrogance", "typical", "disruptor", "use", "high", "resolution", "3d", "scan", "replace", "traditional", "tailor", "math", "solve", "wrong", "900", "day", "development", "burn", "160000", "saving", "realize", "current", "market", "solution", "fail", "think", "fit", "math", "problem", "not", "physics", "logic", "problem", "hard", "technical", "truth", "online", "tailoring", "graveyard", "initial", "capital", "engineering", "bottleneck", "overcome", "scan", "sdks", "rely", "user", "hold", "phone", "place", "table", "problem", "user", "struggle", "geometry", "assumption", "user", "hold", "device", "perfectly", "perpendicular", "floor", "flawed", "datum", "mere", "degree", "tilt", "result", "23", "cm", "error", "leg", "length", "bespoke", "tailor", "cm", "error", "difference", "wearable", "suit", "disaster", "fix", "stop", "trust", "raw", "scan", "build", "secondary", "algorithm", "standard", "scan", "sdk", "detect", "device", "orientation", "angle", "not", "perfect", "system", "force", "normalization", "process", "mathematically", "correct", "geometric", "distortion", "datum", "entry", "common", "failure", "point", "3d", "scan", "give", "body", "measurement", "skin", "suit", "require", "garment", "measurement", "shell", "problem", "machine", "not", "understand", "ease", "movement", "allowance", "drape", "fabric", "fall", "raw", "translation", "scan", "pattern", "result", "skin", "tight", "suit", "look", "like", "wetsuit", "fix", "ai", "fail", "human", "succeed", "spend", "year", "digitize", "logic", "old", "school", "master", "tailor", "build", "human", "logic", "filter", "sit", "scan", "cad", "system", "autom"], "num_tokens": 192, "token_loss_pct": 49.74, "normalized_content": "report in 2023 i placed a crazy bet. i founded a fashion-tech startup with a vision of phygital tailoring . my goal was simple but audacious clients in the world should receive perfect-fit bespoke suits without ever leaving their homes. i entered the space with the arrogance of a typical disruptor if i just use high-resolution 3d scanning i can replace the traditional tailor. math will solve everything. i was wrong. after 900 days of development and burning through 160000 in savings i realized why the current market solutions were failing. i used to think of fit as a math problem. it isnt. its a physics and logic problem. here is the hard technical truth about why online tailoring became a graveyard for my initial capital and the four engineering bottlenecks we had to overcome. most scanning sdks rely on the user holding the phone or placing it on a table. the problem  users struggle with geometry. my assumption that users could hold a device perfectly perpendicular to the floor was flawed. the data  a mere 5-degree tilt results in a 23cm error in leg length. in bespoke tailoring a 3cm error is the difference between a wearable suit and a disaster. my fix  i stopped trusting the raw scan. we built a secondary algorithm on top of the standard scanning sdk to detect device orientation. if the angle isnt perfect the system forces a normalization process to mathematically correct the geometric distortion before data entry. this was the most common failure point. a 3d scan gives you body measurements the skin. but a suit requires garment measurements the shell. the problem  machines dont understand ease movement allowance or drape fabric fall. my raw translation of scan-to-pattern resulted in a skin-tight suit that looked like a wetsuit. my fix  where ai failed humans succeeded. i spent two years digitizing the logic of old-school master tailors. i built a human logic filter that sits between the scan and the cad system autom"}
{"title": "Texas police invested in phone-tracking software and won’t say how it’s used", "url": "https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/", "content": "The Texas Observer Since 1954 Investigations One sheriff who leads an anti-smuggling task force says the software helps “develop leads to eventually obtain probable cause.” Civil liberties experts say its use violates constitutional rights. by Francesca D’Annunzio January 13, 2026, 8:40 AM, CST Editor’s Note: This story is the third installment in a series produced in partnership with the Pulitzer Center’s AI Accountability Network. Goliad County police kicked off one human smuggling investigation not with a suspect’s name, but with a discarded receipt and cell phone surveillance software. In June 2021, Chief Deputy Tim Futch chased a speeding F-150 headed toward Houston on U.S. Highway 59; he believed the vehicle was carrying undocumented immigrants, concealed in the truck bed beneath plywood, according to a police report. Trying to evade the cops, the driver pulled into a ditch, and around 10 people bailed out and took off sprinting. In the aftermath, Roy Boyd, the sheriff in this county of 7,000 situated halfway between Laredo and Houston, surveyed the scene. The driver proved hard to identify via the pickup’s plate. But, on the ground, he spotted a fresh receipt from a liquor store in Pasadena, a Houston suburb, he recalled in a June 2024 interview with the Texas Observer . The stub of paper was enough, Boyd said, to justify deploying an expensive—and controversial—artificial intelligence-powered surveillance tool called Tangles. A specially-trained analyst used the receipt, Boyd said, to conduct warrantless surveillance on the suspected driver—and on other smart phone users—by utilizing a Tangles add-on feature called Webloc, which tracks mobile devices’ movements in a client-selected virtual area through a capability called “geofencing.” After the bailout incident, Boyd acquired a license for the tool with about $300,000 in state border security grants—though the sheriff admits that he’s not a tech guy: In 2024, he still used a hand-me-down iPhone 10, which hi", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["texas", "observer", "1954", "investigation", "sheriff", "lead", "anti", "smuggling", "task", "force", "say", "software", "help", "develop", "lead", "eventually", "obtain", "probable", "cause", "civil", "liberty", "expert", "use", "violate", "constitutional", "right", "francesca", "dannunzio", "january", "13", "2026", "840", "cst", "editor", "note", "story", "installment", "series", "produce", "partnership", "pulitzer", "center", "ai", "accountability", "network", "goliad", "county", "police", "kick", "human", "smuggling", "investigation", "suspect", "discard", "receipt", "cell", "phone", "surveillance", "software", "june", "2021", "chief", "deputy", "tim", "futch", "chase", "speed", "f-150", "head", "houston", "u.s", "highway", "59", "believe", "vehicle", "carry", "undocumented", "immigrant", "conceal", "truck", "bed", "beneath", "plywood", "accord", "police", "report", "try", "evade", "cop", "driver", "pull", "ditch", "10", "people", "bail", "take", "sprinting", "aftermath", "roy", "boyd", "sheriff", "county", "7000", "situate", "halfway", "laredo", "houston", "survey", "scene", "driver", "prove", "hard", "identify", "pickups", "plate", "ground", "spot", "fresh", "receipt", "liquor", "store", "pasadena", "houston", "suburb", "recall", "june", "2024", "interview", "texas", "observer", "stub", "paper", "boyd", "say", "justify", "deploy", "expensiveand", "controversialartificial", "intelligence", "power", "surveillance", "tool", "call", "tangle", "specially", "train", "analyst", "receipt", "boyd", "say", "conduct", "warrantless", "surveillance", "suspect", "driverand", "smart", "phone", "usersby", "utilize", "tangle", "add", "feature", "call", "webloc", "track", "mobile", "device", "movement", "client", "select", "virtual", "area", "capability", "call", "geofencing", "bailout", "incident", "boyd", "acquire", "license", "tool", "300000", "state", "border", "security", "grantsthough", "sheriff", "admit", "tech", "guy", "2024", "hand", "iphone", "10", "hi"], "num_tokens": 195, "token_loss_pct": 41.79, "normalized_content": "the texas observer since 1954 investigations one sheriff who leads an anti-smuggling task force says the software helps develop leads to eventually obtain probable cause. civil liberties experts say its use violates constitutional rights. by francesca dannunzio january 13 2026 840 am cst editors note this story is the third installment in a series produced in partnership with the pulitzer centers ai accountability network. goliad county police kicked off one human smuggling investigation not with a suspects name but with a discarded receipt and cell phone surveillance software. in june 2021 chief deputy tim futch chased a speeding f-150 headed toward houston on u.s. highway 59 he believed the vehicle was carrying undocumented immigrants concealed in the truck bed beneath plywood according to a police report. trying to evade the cops the driver pulled into a ditch and around 10 people bailed out and took off sprinting. in the aftermath roy boyd the sheriff in this county of 7000 situated halfway between laredo and houston surveyed the scene. the driver proved hard to identify via the pickups plate. but on the ground he spotted a fresh receipt from a liquor store in pasadena a houston suburb he recalled in a june 2024 interview with the texas observer . the stub of paper was enough boyd said to justify deploying an expensiveand controversialartificial intelligence-powered surveillance tool called tangles. a specially-trained analyst used the receipt boyd said to conduct warrantless surveillance on the suspected driverand on other smart phone usersby utilizing a tangles add-on feature called webloc which tracks mobile devices movements in a client-selected virtual area through a capability called geofencing. after the bailout incident boyd acquired a license for the tool with about 300000 in state border security grantsthough the sheriff admits that hes not a tech guy in 2024 he still used a hand-me-down iphone 10 which hi"}
{"title": "Opening the AWS European Sovereign Cloud", "url": "https://aws.amazon.com/blogs/aws/opening-the-aws-european-sovereign-cloud/", "content": "Search AWS Blogs Deutsch | English | Español | Français | Italiano As a European citizen, I understand first-hand the importance of digital sovereignty, especially for our public sector organisations and highly regulated industries. Today, I’m delighted to share that the AWS European Sovereign Cloud is now generally available to all customers. We first announced our plans to build this new independent cloud infrastructure in 2023 , and today it’s ready to meet the most stringent sovereignty requirements of European customers with a comprehensive set of AWS services . Berlin, Brandenburg Gate at sunset Meeting European sovereignty requirements Organizations across Europe face increasingly complex regulatory requirements around data residency, operational control, and governance independence. Too often today, European organisations with the highest sovereignty requirements are stuck in legacy on-premises environments or offerings with reduced services and functionality. In response to this critical need, the AWS European Sovereign Cloud is the only fully featured and independently operated sovereign cloud backed by strong technical controls, sovereign assurances, and legal protections. Public sector entities and businesses in highly regulated industries need cloud infrastructure that provides enhanced sovereignty controls that maintain the innovation, security, and reliability they expect from modern cloud services. These organisations require assurance that their data and operations remain under European jurisdiction, with clear governance structures and operational autonomy within the European Union (EU). A new independent cloud infrastructure for Europe The AWS European Sovereign Cloud represents a physically and logically separate cloud infrastructure, with all components located entirely within the EU. The first AWS Region in the AWS European Sovereign Cloud is located in the state of Brandenburg, Germany, and is generally available today. This Region operates i", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["search", "aws", "blog", "deutsch", "english", "español", "français", "italiano", "european", "citizen", "understand", "hand", "importance", "digital", "sovereignty", "especially", "public", "sector", "organisation", "highly", "regulate", "industry", "today", "delighted", "share", "aw", "european", "sovereign", "cloud", "generally", "available", "customer", "announce", "plan", "build", "new", "independent", "cloud", "infrastructure", "2023", "today", "ready", "meet", "stringent", "sovereignty", "requirement", "european", "customer", "comprehensive", "set", "aw", "service", "berlin", "brandenburg", "gate", "sunset", "meeting", "european", "sovereignty", "requirement", "organization", "europe", "face", "increasingly", "complex", "regulatory", "requirement", "datum", "residency", "operational", "control", "governance", "independence", "today", "european", "organisation", "high", "sovereignty", "requirement", "stick", "legacy", "premise", "environment", "offering", "reduce", "service", "functionality", "response", "critical", "need", "aw", "european", "sovereign", "cloud", "fully", "feature", "independently", "operate", "sovereign", "cloud", "back", "strong", "technical", "control", "sovereign", "assurance", "legal", "protection", "public", "sector", "entity", "business", "highly", "regulate", "industry", "need", "cloud", "infrastructure", "provide", "enhanced", "sovereignty", "control", "maintain", "innovation", "security", "reliability", "expect", "modern", "cloud", "service", "organisation", "require", "assurance", "datum", "operation", "remain", "european", "jurisdiction", "clear", "governance", "structure", "operational", "autonomy", "european", "union", "eu", "new", "independent", "cloud", "infrastructure", "europe", "aws", "european", "sovereign", "cloud", "represent", "physically", "logically", "separate", "cloud", "infrastructure", "component", "locate", "entirely", "eu", "aws", "region", "aw", "european", "sovereign", "cloud", "locate", "state", "brandenburg", "germany", "generally", "available", "today", "region", "operate"], "num_tokens": 180, "token_loss_pct": 38.36, "normalized_content": "search aws blogs deutsch  english  español  français  italiano as a european citizen i understand first-hand the importance of digital sovereignty especially for our public sector organisations and highly regulated industries. today im delighted to share that the aws european sovereign cloud is now generally available to all customers. we first announced our plans to build this new independent cloud infrastructure in 2023  and today its ready to meet the most stringent sovereignty requirements of european customers with a comprehensive set of aws services . berlin brandenburg gate at sunset meeting european sovereignty requirements organizations across europe face increasingly complex regulatory requirements around data residency operational control and governance independence. too often today european organisations with the highest sovereignty requirements are stuck in legacy on-premises environments or offerings with reduced services and functionality. in response to this critical need the aws european sovereign cloud is the only fully featured and independently operated sovereign cloud backed by strong technical controls sovereign assurances and legal protections. public sector entities and businesses in highly regulated industries need cloud infrastructure that provides enhanced sovereignty controls that maintain the innovation security and reliability they expect from modern cloud services. these organisations require assurance that their data and operations remain under european jurisdiction with clear governance structures and operational autonomy within the european union eu. a new independent cloud infrastructure for europe the aws european sovereign cloud represents a physically and logically separate cloud infrastructure with all components located entirely within the eu. the first aws region in the aws european sovereign cloud is located in the state of brandenburg germany and is generally available today. this region operates i"}
{"title": "Show HN: I built a tool to assist AI agents to know when a PR is good to go", "url": "https://dsifry.github.io/goodtogo/", "content": "Deterministic PR readiness detection for AI coding agents View the Project on GitHub dsifry/goodtogo The missing piece in AI-assisted development: knowing when you’re actually done. AI coding agents are transforming software development. They can write code, fix bugs, respond to review comments, and create pull requests. But they all share one fundamental problem: They can’t reliably know when a PR is ready to merge. Think about it. When you ask an AI agent to “fix the CI and address the review comments,” how does it know when it’s finished? Without deterministic answers, agents either: Good To Go provides a single command that answers the question definitively: That’s it. One command. One answer. No ambiguity. No guessing. No infinite loops. Good To Go analyzes your PR across three dimensions: Combines all GitHub check runs and commit statuses into a single pass/fail/pending state. Handles the complexity of multiple CI systems, required vs optional checks, and in-progress runs. Not all review comments are created equal. Good To Go classifies each comment as: Built-in parsers understand the patterns of popular automated reviewers: Distinguishes between truly unresolved discussions and threads that are technically “unresolved” but already addressed in subsequent commits. Good To Go is built specifically for how AI agents work: Default mode returns 0 for any analyzable state—because AI agents should parse the JSON output, not interpret exit codes as errors. Every response includes exactly what an agent needs to take action: Track what’s already been handled across agent sessions: Make gtg a required status check. PRs can’t merge until they’re truly ready—not just “CI passed.” When you’ve resolved threads or addressed comments, trigger a quick re-check without rebuilding CI: Comment this on any PR to get instant feedback: Or trigger manually via workflow dispatch: Give your AI agent a definitive answer instead of endless polling: Monitor a PR through its entire lifecyc", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["deterministic", "pr", "readiness", "detection", "ai", "cod", "agent", "view", "project", "github", "dsifrygoodtogo", "miss", "piece", "ai", "assist", "development", "know", "actually", "ai", "cod", "agent", "transform", "software", "development", "write", "code", "fix", "bug", "respond", "review", "comment", "create", "pull", "request", "share", "fundamental", "problem", "not", "reliably", "know", "pr", "ready", "merge", "think", "ask", "ai", "agent", "fix", "ci", "address", "review", "comment", "know", "finished", "deterministic", "answer", "agent", "good", "provide", "single", "command", "answer", "question", "definitively", "command", "answer", "ambiguity", "guessing", "infinite", "loop", "good", "analyze", "pr", "dimension", "combine", "github", "check", "run", "commit", "status", "single", "passfailpende", "state", "handle", "complexity", "multiple", "ci", "system", "require", "vs", "optional", "check", "progress", "run", "review", "comment", "create", "equal", "good", "classifie", "comment", "build", "parser", "understand", "pattern", "popular", "automate", "reviewer", "distinguishe", "truly", "unresolved", "discussion", "thread", "technically", "unresolved", "address", "subsequent", "commit", "good", "build", "specifically", "ai", "agent", "work", "default", "mode", "return", "analyzable", "statebecause", "ai", "agent", "parse", "json", "output", "interpret", "exit", "code", "error", "response", "include", "exactly", "agent", "need", "action", "track", "handle", "agent", "session", "gtg", "require", "status", "check", "prs", "not", "merge", "truly", "readynot", "ci", "pass", "ve", "resolve", "thread", "address", "comment", "trigger", "quick", "check", "rebuild", "ci", "comment", "pr", "instant", "feedback", "trigger", "manually", "workflow", "dispatch", "ai", "agent", "definitive", "answer", "instead", "endless", "polling", "monitor", "pr", "entire", "lifecyc"], "num_tokens": 188, "token_loss_pct": 45.03, "normalized_content": "deterministic pr readiness detection for ai coding agents view the project on github dsifrygoodtogo the missing piece in ai-assisted development knowing when youre actually done. ai coding agents are transforming software development. they can write code fix bugs respond to review comments and create pull requests. but they all share one fundamental problem they cant reliably know when a pr is ready to merge. think about it. when you ask an ai agent to fix the ci and address the review comments how does it know when its finished without deterministic answers agents either good to go provides a single command that answers the question definitively thats it. one command. one answer. no ambiguity. no guessing. no infinite loops. good to go analyzes your pr across three dimensions combines all github check runs and commit statuses into a single passfailpending state. handles the complexity of multiple ci systems required vs optional checks and in-progress runs. not all review comments are created equal. good to go classifies each comment as built-in parsers understand the patterns of popular automated reviewers distinguishes between truly unresolved discussions and threads that are technically unresolved but already addressed in subsequent commits. good to go is built specifically for how ai agents work default mode returns 0 for any analyzable statebecause ai agents should parse the json output not interpret exit codes as errors. every response includes exactly what an agent needs to take action track whats already been handled across agent sessions make gtg a required status check. prs cant merge until theyre truly readynot just ci passed. when youve resolved threads or addressed comments trigger a quick re-check without rebuilding ci comment this on any pr to get instant feedback or trigger manually via workflow dispatch give your ai agent a definitive answer instead of endless polling monitor a pr through its entire lifecyc"}
{"title": "Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)", "url": "https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html", "content": "Adam Drake Jan 18, 2014 As I was browsing the web and catching up on some sites I visit periodically, I found a cool article from Tom Hayden about using Amazon Elastic Map Reduce (EMR) and mrjob in order to compute some statistics on win/loss ratios for chess games he downloaded from the millionbase archive , and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec). After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally. This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-called Big Data (tm) tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques. One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your own Storm cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands betw", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["adam", "drake", "jan", "18", "2014", "browse", "web", "catch", "site", "visit", "periodically", "find", "cool", "article", "tom", "hayden", "amazon", "elastic", "map", "reduce", "emr", "mrjob", "order", "compute", "statistic", "winloss", "ratio", "chess", "game", "download", "millionbase", "archive", "generally", "fun", "emr", "data", "volume", "1.75", "gb", "contain", "million", "chess", "game", "skeptical", "hadoop", "task", "understand", "goal", "learn", "have", "fun", "mrjob", "emr", "problem", "basically", "look", "result", "line", "file", "aggregate", "different", "result", "ideally", "suit", "stream", "processing", "shell", "command", "try", "datum", "able", "use", "laptop", "result", "12", "second", "process", "speed", "270mbsec", "hadoop", "processing", "take", "26", "minute", "process", "speed", "1.14mbsec", "report", "time", "require", "process", "datum", "c1.medium", "machine", "cluster", "take", "26", "minute", "tom", "remark", "probably", "well", "run", "serially", "machine", "probably", "good", "kind", "clever", "multi", "thread", "application", "locally", "absolutely", "correct", "serial", "processing", "beat", "26", "minute", "tom", "project", "fun", "people", "use", "hadoop", "call", "big", "datum", "tm", "tool", "real", "world", "processing", "analysis", "job", "fast", "simple", "tool", "different", "technique", "especially", "approach", "datum", "processing", "standard", "shell", "tool", "command", "benefit", "approach", "massive", "create", "data", "pipeline", "shell", "command", "mean", "processing", "step", "parallel", "basically", "like", "have", "storm", "cluster", "local", "machine", "concept", "spout", "bolt", "sink", "transfer", "shell", "pipe", "command", "betw"], "num_tokens": 177, "token_loss_pct": 51.37, "normalized_content": "adam drake jan 18 2014 as i was browsing the web and catching up on some sites i visit periodically i found a cool article from tom hayden about using amazon elastic map reduce emr and mrjob in order to compute some statistics on winloss ratios for chess games he downloaded from the millionbase archive  and generally have fun with emr. since the data volume was only about 1.75gb containing around 2 million chess games i was skeptical of using hadoop for the task but i can understand his goal of learning and having fun with mrjob and emr. since the problem is basically just to look at the result lines of each file and aggregate the different results it seems ideally suited to stream processing with shell commands. i tried this out and for the same amount of data i was able to use my laptop to get the results in about 12 seconds processing speed of about 270mbsec while the hadoop processing took about 26 minutes processing speed of about 1.14mbsec. after reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes tom remarks this is probably better than it would take to run serially on my machine but probably not as good as if i did some kind of clever multi-threaded application locally. this is absolutely correct although even serial processing may beat 26 minutes. although tom was doing the project for fun often people use hadoop and other so-called big data tm tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques. one especially under-used approach for data processing is using standard shell tools and commands. the benefits of this approach can be massive since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. this is basically like having your own storm cluster on your local machine. even the concepts of spouts bolts and sinks transfer to shell pipes and the commands betw"}
{"title": "Nvidia contacted Anna's Archive to access books", "url": "https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/", "content": "Home > AI > NVIDIA executives allegedly authorized the use of millions of pirated books from Anna's Archive to fuel its AI training. In an expanded class-action lawsuit that cites internal NVIDIA documents, several book authors claim that the trillion-dollar company directly reached out to Anna's Archive, seeking high-speed access to the shadow library data. Chip giant NVIDIA has been one of the main financial beneficiaries in the artificial intelligence boom. Revenue surged due to high demand for its AI-learning chips and data center services, and the end doesn’t appear to be in sight. Besides selling the most sought-after hardware, NVIDIA is also developing its own models, including NeMo, Retro-48B, InstructRetro, and Megatron. These are trained using their own hardware and with help from large text libraries, much like other tech giants do. Like other tech companies, NVIDIA has also seen significant legal pushback from copyright holders in response to its training methods. This includes authors, who, in various lawsuits, accused tech companies of training their models on pirated books. In early 2024, for example, several authors sued NVIDIA over alleged copyright infringement. Through the class action lawsuit, they claimed that the company’s AI models were trained on the Books3 dataset that included copyrighted works taken from the ‘pirate’ site Bibliotik. Since this happened without permission, the authors demanded compensation. In response, NVIDIA defended its actions as fair use, noting that books are nothing more than statistical correlations to its AI models. However, the allegations didn’t go away. On the contrary, the plaintiffs found more evidence during discovery. Last Friday, the authors filed an amended complaint that significantly expands the scope of the lawsuit. In addition to adding more books, authors, and AI models, it also includes broader “shadow library” claims and allegations. The authors, including Abdi Nazemian , now cite various internal N", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["home", "ai", "nvidia", "executive", "allegedly", "authorize", "use", "million", "pirate", "book", "anna", "archive", "fuel", "ai", "training", "expand", "class", "action", "lawsuit", "cite", "internal", "nvidia", "document", "book", "author", "claim", "trillion", "dollar", "company", "directly", "reach", "anna", "archive", "seek", "high", "speed", "access", "shadow", "library", "datum", "chip", "giant", "nvidia", "main", "financial", "beneficiary", "artificial", "intelligence", "boom", "revenue", "surge", "high", "demand", "ai", "learn", "chip", "datum", "center", "service", "end", "not", "appear", "sight", "sell", "seek", "hardware", "nvidia", "develop", "model", "include", "nemo", "retro-48b", "instructretro", "megatron", "train", "hardware", "help", "large", "text", "library", "like", "tech", "giant", "like", "tech", "company", "nvidia", "see", "significant", "legal", "pushback", "copyright", "holder", "response", "training", "method", "include", "author", "lawsuit", "accuse", "tech", "company", "train", "model", "pirated", "book", "early", "2024", "example", "author", "sue", "nvidia", "allege", "copyright", "infringement", "class", "action", "lawsuit", "claim", "company", "ai", "model", "train", "books3", "dataset", "include", "copyright", "work", "take", "pirate", "site", "bibliotik", "happen", "permission", "author", "demand", "compensation", "response", "nvidia", "defend", "action", "fair", "use", "note", "book", "statistical", "correlation", "ai", "model", "allegation", "not", "away", "contrary", "plaintiff", "find", "evidence", "discovery", "friday", "author", "file", "amend", "complaint", "significantly", "expand", "scope", "lawsuit", "addition", "add", "book", "author", "ai", "model", "include", "broad", "shadow", "library", "claim", "allegation", "author", "include", "abdi", "nazemian", "cite", "internal"], "num_tokens": 184, "token_loss_pct": 44.91, "normalized_content": "home  ai  nvidia executives allegedly authorized the use of millions of pirated books from anna's archive to fuel its ai training. in an expanded class-action lawsuit that cites internal nvidia documents several book authors claim that the trillion-dollar company directly reached out to anna's archive seeking high-speed access to the shadow library data. chip giant nvidia has been one of the main financial beneficiaries in the artificial intelligence boom. revenue surged due to high demand for its ai-learning chips and data center services and the end doesnt appear to be in sight. besides selling the most sought-after hardware nvidia is also developing its own models including nemo retro-48b instructretro and megatron. these are trained using their own hardware and with help from large text libraries much like other tech giants do. like other tech companies nvidia has also seen significant legal pushback from copyright holders in response to its training methods. this includes authors who in various lawsuits accused tech companies of training their models on pirated books. in early 2024 for example several authors sued nvidia over alleged copyright infringement. through the class action lawsuit they claimed that the companys ai models were trained on the books3 dataset that included copyrighted works taken from the pirate site bibliotik. since this happened without permission the authors demanded compensation. in response nvidia defended its actions as fair use noting that books are nothing more than statistical correlations to its ai models. however the allegations didnt go away. on the contrary the plaintiffs found more evidence during discovery. last friday the authors filed an amended complaint that significantly expands the scope of the lawsuit. in addition to adding more books authors and ai models it also includes broader shadow library claims and allegations. the authors including abdi nazemian  now cite various internal n"}
{"title": "Floating-Point Printing and Parsing Can Be Simple and Fast", "url": "https://research.swtch.com/fp", "content": "A floating point number f has the form f = m · 2 e where m is called the mantissa and e is a signed integer exponent .\nWe like to read numbers scaled by powers of ten,\nnot two, so computers need algorithms to convert binary floating-point\nto and from decimal text.\nMy 2011 post “ Floating Point to Decimal Conversion is Easy ”\nargued that  these conversions can be simple as long as you\ndon’t care about them being fast.\nBut I was wrong: fast converters can be simple too,\nand this post shows how. The main idea of this post is to implement fast unrounded scaling ,\nwhich computes an approximation to x · 2 e · 10 p ,\noften in a single 64-bit multiplication.\nOn that foundation\nwe can build nearly trivial printing and parsing algorithms that run very fast.\nIn fact, the printing algorithms\nrun faster than all other known algorithms,\nincluding\nDragon4 [ 30 ],\nGrisu3 [ 23 ],\nErrol3 [ 4 ],\nRyū [ 2 ],\nRyū Printf [ 3 ],\nSchubfach [ 12 ],\nand Dragonbox [ 17 ],\nand the parsing algorithm runs faster than\nthe Eisel-Lemire algorithm [ 22 ].\nThis post presents both the algorithms and a concrete implementation in Go.\nI expect some form of this Go code to ship in Go 1.27 (scheduled for August 2026). This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed. “ Fixed-Point and Floating-Point Numbers ”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post. “ Unrounded Numbers ” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format. “ Unrounded Scaling ” defines the unrounded scaling primitive. “ Fixed-Width Printing ” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18. “ Parsing Decimals ” parses decimal numbers of\nat most 19 digits into floating-point numbers. “ Shortest-Width Printing ” formats floating-point numbers\nusing the sh", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["float", "point", "number", "form", "call", "mantissa", "sign", "integer", "exponent", "like", "read", "number", "scale", "power", "computer", "need", "algorithm", "convert", "binary", "float", "point", "decimal", "text", "2011", "post", "float", "point", "decimal", "conversion", "easy", "argue", "conversion", "simple", "long", "not", "care", "fast", "wrong", "fast", "converter", "simple", "post", "show", "main", "idea", "post", "implement", "fast", "unrounded", "scaling", "compute", "approximation", "10", "single", "64", "bit", "multiplication", "foundation", "build", "nearly", "trivial", "printing", "parse", "algorithm", "run", "fast", "fact", "printing", "algorithm", "run", "fast", "know", "algorithm", "include", "dragon4", "30", "grisu3", "23", "errol3", "ryū", "ryū", "printf", "schubfach", "12", "dragonbox", "17", "parse", "algorithm", "run", "fast", "eisel", "lemire", "algorithm", "22", "post", "present", "algorithm", "concrete", "implementation", "expect", "form", "code", "ship", "1.27", "schedule", "august", "2026", "post", "longfar", "long", "implementationsso", "brief", "overview", "section", "easy", "navigation", "understanding", "head", "fix", "point", "float", "point", "number", "briefly", "review", "fix", "point", "float", "point", "number", "establish", "terminology", "concept", "need", "rest", "post", "unrounde", "number", "introduce", "idea", "unrounded", "number", "inspire", "ieee754", "float", "point", "extend", "format", "unrounde", "scale", "define", "unrounded", "scaling", "primitive", "fix", "width", "printing", "format", "float", "point", "number", "give", "fix", "number", "decimal", "digit", "18", "parse", "decimal", "parse", "decimal", "number", "19", "digit", "float", "point", "number", "short", "width", "printing", "format", "float", "point", "number", "sh"], "num_tokens": 185, "token_loss_pct": 52.32, "normalized_content": "a floating point number f has the form f  m  2 e where m is called the mantissa and e is a signed integer exponent . we like to read numbers scaled by powers of ten not two so computers need algorithms to convert binary floating-point to and from decimal text. my 2011 post  floating point to decimal conversion is easy  argued that these conversions can be simple as long as you dont care about them being fast. but i was wrong fast converters can be simple too and this post shows how. the main idea of this post is to implement fast unrounded scaling  which computes an approximation to x  2 e  10 p  often in a single 64-bit multiplication. on that foundation we can build nearly trivial printing and parsing algorithms that run very fast. in fact the printing algorithms run faster than all other known algorithms including dragon4  30  grisu3  23  errol3  4  ryū  2  ryū printf  3  schubfach  12  and dragonbox  17  and the parsing algorithm runs faster than the eisel-lemire algorithm  22 . this post presents both the algorithms and a concrete implementation in go. i expect some form of this go code to ship in go 1.27 scheduled for august 2026. this post is rather longfar longer than the implementationsso here is a brief overview of the sections for easier navigation and understanding where were headed.  fixed-point and floating-point numbers  briefly reviews fixed-point and floating-point numbers establishing some terminology and concepts needed for the rest of the post.  unrounded numbers  introduces the idea of unrounded numbers inspired by the ieee754 floating-point extended format.  unrounded scaling  defines the unrounded scaling primitive.  fixed-width printing  formats floating-point numbers with a given fixed number of decimal digits at most 18.  parsing decimals  parses decimal numbers of at most 19 digits into floating-point numbers.  shortest-width printing  formats floating-point numbers using the sh"}
{"title": "Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)", "url": "https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html", "content": "Adam Drake Jan 18, 2014 As I was browsing the web and catching up on some sites I visit periodically, I found a cool article from Tom Hayden about using Amazon Elastic Map Reduce (EMR) and mrjob in order to compute some statistics on win/loss ratios for chess games he downloaded from the millionbase archive , and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec). After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally. This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-called Big Data (tm) tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques. One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your own Storm cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands betw", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["adam", "drake", "jan", "18", "2014", "browse", "web", "catch", "site", "visit", "periodically", "find", "cool", "article", "tom", "hayden", "amazon", "elastic", "map", "reduce", "emr", "mrjob", "order", "compute", "statistic", "winloss", "ratio", "chess", "game", "download", "millionbase", "archive", "generally", "fun", "emr", "data", "volume", "1.75", "gb", "contain", "million", "chess", "game", "skeptical", "hadoop", "task", "understand", "goal", "learn", "have", "fun", "mrjob", "emr", "problem", "basically", "look", "result", "line", "file", "aggregate", "different", "result", "ideally", "suit", "stream", "processing", "shell", "command", "try", "datum", "able", "use", "laptop", "result", "12", "second", "process", "speed", "270mbsec", "hadoop", "processing", "take", "26", "minute", "process", "speed", "1.14mbsec", "report", "time", "require", "process", "datum", "c1.medium", "machine", "cluster", "take", "26", "minute", "tom", "remark", "probably", "well", "run", "serially", "machine", "probably", "good", "kind", "clever", "multi", "thread", "application", "locally", "absolutely", "correct", "serial", "processing", "beat", "26", "minute", "tom", "project", "fun", "people", "use", "hadoop", "call", "big", "datum", "tm", "tool", "real", "world", "processing", "analysis", "job", "fast", "simple", "tool", "different", "technique", "especially", "approach", "datum", "processing", "standard", "shell", "tool", "command", "benefit", "approach", "massive", "create", "data", "pipeline", "shell", "command", "mean", "processing", "step", "parallel", "basically", "like", "have", "storm", "cluster", "local", "machine", "concept", "spout", "bolt", "sink", "transfer", "shell", "pipe", "command", "betw"], "num_tokens": 177, "token_loss_pct": 51.37, "normalized_content": "adam drake jan 18 2014 as i was browsing the web and catching up on some sites i visit periodically i found a cool article from tom hayden about using amazon elastic map reduce emr and mrjob in order to compute some statistics on winloss ratios for chess games he downloaded from the millionbase archive  and generally have fun with emr. since the data volume was only about 1.75gb containing around 2 million chess games i was skeptical of using hadoop for the task but i can understand his goal of learning and having fun with mrjob and emr. since the problem is basically just to look at the result lines of each file and aggregate the different results it seems ideally suited to stream processing with shell commands. i tried this out and for the same amount of data i was able to use my laptop to get the results in about 12 seconds processing speed of about 270mbsec while the hadoop processing took about 26 minutes processing speed of about 1.14mbsec. after reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes tom remarks this is probably better than it would take to run serially on my machine but probably not as good as if i did some kind of clever multi-threaded application locally. this is absolutely correct although even serial processing may beat 26 minutes. although tom was doing the project for fun often people use hadoop and other so-called big data tm tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques. one especially under-used approach for data processing is using standard shell tools and commands. the benefits of this approach can be massive since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. this is basically like having your own storm cluster on your local machine. even the concepts of spouts bolts and sinks transfer to shell pipes and the commands betw"}
{"title": "Nvidia contacted Anna's Archive to access books", "url": "https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/", "content": "Home > AI > NVIDIA executives allegedly authorized the use of millions of pirated books from Anna's Archive to fuel its AI training. In an expanded class-action lawsuit that cites internal NVIDIA documents, several book authors claim that the trillion-dollar company directly reached out to Anna's Archive, seeking high-speed access to the shadow library data. Chip giant NVIDIA has been one of the main financial beneficiaries in the artificial intelligence boom. Revenue surged due to high demand for its AI-learning chips and data center services, and the end doesn’t appear to be in sight. Besides selling the most sought-after hardware, NVIDIA is also developing its own models, including NeMo, Retro-48B, InstructRetro, and Megatron. These are trained using their own hardware and with help from large text libraries, much like other tech giants do. Like other tech companies, NVIDIA has also seen significant legal pushback from copyright holders in response to its training methods. This includes authors, who, in various lawsuits, accused tech companies of training their models on pirated books. In early 2024, for example, several authors sued NVIDIA over alleged copyright infringement. Through the class action lawsuit, they claimed that the company’s AI models were trained on the Books3 dataset that included copyrighted works taken from the ‘pirate’ site Bibliotik. Since this happened without permission, the authors demanded compensation. In response, NVIDIA defended its actions as fair use, noting that books are nothing more than statistical correlations to its AI models. However, the allegations didn’t go away. On the contrary, the plaintiffs found more evidence during discovery. Last Friday, the authors filed an amended complaint that significantly expands the scope of the lawsuit. In addition to adding more books, authors, and AI models, it also includes broader “shadow library” claims and allegations. The authors, including Abdi Nazemian , now cite various internal N", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["home", "ai", "nvidia", "executive", "allegedly", "authorize", "use", "million", "pirate", "book", "anna", "archive", "fuel", "ai", "training", "expand", "class", "action", "lawsuit", "cite", "internal", "nvidia", "document", "book", "author", "claim", "trillion", "dollar", "company", "directly", "reach", "anna", "archive", "seek", "high", "speed", "access", "shadow", "library", "datum", "chip", "giant", "nvidia", "main", "financial", "beneficiary", "artificial", "intelligence", "boom", "revenue", "surge", "high", "demand", "ai", "learn", "chip", "datum", "center", "service", "end", "not", "appear", "sight", "sell", "seek", "hardware", "nvidia", "develop", "model", "include", "nemo", "retro-48b", "instructretro", "megatron", "train", "hardware", "help", "large", "text", "library", "like", "tech", "giant", "like", "tech", "company", "nvidia", "see", "significant", "legal", "pushback", "copyright", "holder", "response", "training", "method", "include", "author", "lawsuit", "accuse", "tech", "company", "train", "model", "pirated", "book", "early", "2024", "example", "author", "sue", "nvidia", "allege", "copyright", "infringement", "class", "action", "lawsuit", "claim", "company", "ai", "model", "train", "books3", "dataset", "include", "copyright", "work", "take", "pirate", "site", "bibliotik", "happen", "permission", "author", "demand", "compensation", "response", "nvidia", "defend", "action", "fair", "use", "note", "book", "statistical", "correlation", "ai", "model", "allegation", "not", "away", "contrary", "plaintiff", "find", "evidence", "discovery", "friday", "author", "file", "amend", "complaint", "significantly", "expand", "scope", "lawsuit", "addition", "add", "book", "author", "ai", "model", "include", "broad", "shadow", "library", "claim", "allegation", "author", "include", "abdi", "nazemian", "cite", "internal"], "num_tokens": 184, "token_loss_pct": 44.91, "normalized_content": "home  ai  nvidia executives allegedly authorized the use of millions of pirated books from anna's archive to fuel its ai training. in an expanded class-action lawsuit that cites internal nvidia documents several book authors claim that the trillion-dollar company directly reached out to anna's archive seeking high-speed access to the shadow library data. chip giant nvidia has been one of the main financial beneficiaries in the artificial intelligence boom. revenue surged due to high demand for its ai-learning chips and data center services and the end doesnt appear to be in sight. besides selling the most sought-after hardware nvidia is also developing its own models including nemo retro-48b instructretro and megatron. these are trained using their own hardware and with help from large text libraries much like other tech giants do. like other tech companies nvidia has also seen significant legal pushback from copyright holders in response to its training methods. this includes authors who in various lawsuits accused tech companies of training their models on pirated books. in early 2024 for example several authors sued nvidia over alleged copyright infringement. through the class action lawsuit they claimed that the companys ai models were trained on the books3 dataset that included copyrighted works taken from the pirate site bibliotik. since this happened without permission the authors demanded compensation. in response nvidia defended its actions as fair use noting that books are nothing more than statistical correlations to its ai models. however the allegations didnt go away. on the contrary the plaintiffs found more evidence during discovery. last friday the authors filed an amended complaint that significantly expands the scope of the lawsuit. in addition to adding more books authors and ai models it also includes broader shadow library claims and allegations. the authors including abdi nazemian  now cite various internal n"}
{"title": "Floating-Point Printing and Parsing Can Be Simple and Fast", "url": "https://research.swtch.com/fp", "content": "A floating point number f has the form f = m · 2 e where m is called the mantissa and e is a signed integer exponent .\nWe like to read numbers scaled by powers of ten,\nnot two, so computers need algorithms to convert binary floating-point\nto and from decimal text.\nMy 2011 post “ Floating Point to Decimal Conversion is Easy ”\nargued that  these conversions can be simple as long as you\ndon’t care about them being fast.\nBut I was wrong: fast converters can be simple too,\nand this post shows how. The main idea of this post is to implement fast unrounded scaling ,\nwhich computes an approximation to x · 2 e · 10 p ,\noften in a single 64-bit multiplication.\nOn that foundation\nwe can build nearly trivial printing and parsing algorithms that run very fast.\nIn fact, the printing algorithms\nrun faster than all other known algorithms,\nincluding\nDragon4 [ 30 ],\nGrisu3 [ 23 ],\nErrol3 [ 4 ],\nRyū [ 2 ],\nRyū Printf [ 3 ],\nSchubfach [ 12 ],\nand Dragonbox [ 17 ],\nand the parsing algorithm runs faster than\nthe Eisel-Lemire algorithm [ 22 ].\nThis post presents both the algorithms and a concrete implementation in Go.\nI expect some form of this Go code to ship in Go 1.27 (scheduled for August 2026). This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed. “ Fixed-Point and Floating-Point Numbers ”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post. “ Unrounded Numbers ” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format. “ Unrounded Scaling ” defines the unrounded scaling primitive. “ Fixed-Width Printing ” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18. “ Parsing Decimals ” parses decimal numbers of\nat most 19 digits into floating-point numbers. “ Shortest-Width Printing ” formats floating-point numbers\nusing the sh", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["float", "point", "number", "form", "call", "mantissa", "sign", "integer", "exponent", "like", "read", "number", "scale", "power", "computer", "need", "algorithm", "convert", "binary", "float", "point", "decimal", "text", "2011", "post", "float", "point", "decimal", "conversion", "easy", "argue", "conversion", "simple", "long", "not", "care", "fast", "wrong", "fast", "converter", "simple", "post", "show", "main", "idea", "post", "implement", "fast", "unrounded", "scaling", "compute", "approximation", "10", "single", "64", "bit", "multiplication", "foundation", "build", "nearly", "trivial", "printing", "parse", "algorithm", "run", "fast", "fact", "printing", "algorithm", "run", "fast", "know", "algorithm", "include", "dragon4", "30", "grisu3", "23", "errol3", "ryū", "ryū", "printf", "schubfach", "12", "dragonbox", "17", "parse", "algorithm", "run", "fast", "eisel", "lemire", "algorithm", "22", "post", "present", "algorithm", "concrete", "implementation", "expect", "form", "code", "ship", "1.27", "schedule", "august", "2026", "post", "longfar", "long", "implementationsso", "brief", "overview", "section", "easy", "navigation", "understanding", "head", "fix", "point", "float", "point", "number", "briefly", "review", "fix", "point", "float", "point", "number", "establish", "terminology", "concept", "need", "rest", "post", "unrounde", "number", "introduce", "idea", "unrounded", "number", "inspire", "ieee754", "float", "point", "extend", "format", "unrounde", "scale", "define", "unrounded", "scaling", "primitive", "fix", "width", "printing", "format", "float", "point", "number", "give", "fix", "number", "decimal", "digit", "18", "parse", "decimal", "parse", "decimal", "number", "19", "digit", "float", "point", "number", "short", "width", "printing", "format", "float", "point", "number", "sh"], "num_tokens": 185, "token_loss_pct": 52.32, "normalized_content": "a floating point number f has the form f  m  2 e where m is called the mantissa and e is a signed integer exponent . we like to read numbers scaled by powers of ten not two so computers need algorithms to convert binary floating-point to and from decimal text. my 2011 post  floating point to decimal conversion is easy  argued that these conversions can be simple as long as you dont care about them being fast. but i was wrong fast converters can be simple too and this post shows how. the main idea of this post is to implement fast unrounded scaling  which computes an approximation to x  2 e  10 p  often in a single 64-bit multiplication. on that foundation we can build nearly trivial printing and parsing algorithms that run very fast. in fact the printing algorithms run faster than all other known algorithms including dragon4  30  grisu3  23  errol3  4  ryū  2  ryū printf  3  schubfach  12  and dragonbox  17  and the parsing algorithm runs faster than the eisel-lemire algorithm  22 . this post presents both the algorithms and a concrete implementation in go. i expect some form of this go code to ship in go 1.27 scheduled for august 2026. this post is rather longfar longer than the implementationsso here is a brief overview of the sections for easier navigation and understanding where were headed.  fixed-point and floating-point numbers  briefly reviews fixed-point and floating-point numbers establishing some terminology and concepts needed for the rest of the post.  unrounded numbers  introduces the idea of unrounded numbers inspired by the ieee754 floating-point extended format.  unrounded scaling  defines the unrounded scaling primitive.  fixed-width printing  formats floating-point numbers with a given fixed number of decimal digits at most 18.  parsing decimals  parses decimal numbers of at most 19 digits into floating-point numbers.  shortest-width printing  formats floating-point numbers using the sh"}
{"title": "Will AI Pet My Dog for Me", "url": "https://eieio.games/blog/will-ai-pet-my-dog-for-me/", "content": "by nolen royalty by nolen royalty What work do I want to skip? Jan 19, 2026 I have a dog. Her name is Gabby. She’s lovely. Gabby and me last summer She’s a lot of work. A dog walker handles Gabby’s afternoon walks. It’s nice to outsource the afternoon walk since it interrupts my day. Gabby’s appetite for play and affection is insatiable. I sometimes want to outsource more of the work of taking care of her. To align my goals of “building as much weird software as possible” and “taking care of my dog,” I could outsource all of the work of caring for Gabby. Someone else could walk her and feed her and pet her, leaving me free to find better UUIDs . My time is valuable; this would be an efficient use of my money. But I don’t do that. I like petting my dog. I like to understand things. I like to share that understanding with others. This is my favorite part of building software. Until recently I had to understand the software that I developed. LLMs can now type most of my code for me. And if I want, I can often accept that code without understanding how it works. That’s a big change. LLMs have changed a lot of things! I am good at generating lots of code quickly; that skill is less valuable than it was 5 years ago. I’m pretty quick in vim; that’s probably not as important either. It’s uncomfortable that some skills I’ve honed over years are becoming less valuable. But code has fussy syntax, boring parts, and boilerplate. It’s exciting to have agents that can take my code on its afternoon walk. It’s more uncomfortable to be able to skip the understanding. For me, that’s petting the dog. Many of my blogs are about understanding useless computer problems . They would be bad posts if they said “I asked an LLM to solve this problem and it did.” I did not do this Nobody can make me use LLM outputs that I don’t understand for this blog. But my blog’s style doesn’t control the software industry. Understanding can be a chore or expense on the way to shipping. Maybe it doesn’t mak", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nolen", "royalty", "nolen", "royalty", "work", "want", "skip", "jan", "19", "2026", "dog", "gabby", "lovely", "gabby", "summer", "lot", "work", "dog", "walker", "handle", "gabbys", "afternoon", "walk", "nice", "outsource", "afternoon", "walk", "interrupt", "day", "gabbys", "appetite", "play", "affection", "insatiable", "want", "outsource", "work", "take", "care", "align", "goal", "build", "weird", "software", "possible", "take", "care", "dog", "outsource", "work", "care", "gabby", "walk", "feed", "pet", "leave", "free", "find", "well", "uuid", "time", "valuable", "efficient", "use", "money", "not", "like", "pet", "dog", "like", "understand", "thing", "like", "share", "understanding", "favorite", "building", "software", "recently", "understand", "software", "develop", "llm", "type", "code", "want", "accept", "code", "understand", "work", "big", "change", "llm", "change", "lot", "thing", "good", "generate", "lot", "code", "quickly", "skill", "valuable", "year", "ago", "pretty", "quick", "vim", "probably", "important", "uncomfortable", "skill", "ve", "hone", "year", "valuable", "code", "fussy", "syntax", "boring", "part", "boilerplate", "exciting", "agent", "code", "afternoon", "walk", "uncomfortable", "able", "skip", "understanding", "pet", "dog", "blog", "understand", "useless", "computer", "problem", "bad", "post", "say", "ask", "llm", "solve", "problem", "use", "llm", "output", "not", "understand", "blog", "blog", "style", "not", "control", "software", "industry", "understanding", "chore", "expense", "way", "shipping", "maybe", "not", "mak"], "num_tokens": 165, "token_loss_pct": 60.14, "normalized_content": "by nolen royalty by nolen royalty what work do i want to skip jan 19 2026 i have a dog. her name is gabby. shes lovely. gabby and me last summer shes a lot of work. a dog walker handles gabbys afternoon walks. its nice to outsource the afternoon walk since it interrupts my day. gabbys appetite for play and affection is insatiable. i sometimes want to outsource more of the work of taking care of her. to align my goals of building as much weird software as possible and taking care of my dog i could outsource all of the work of caring for gabby. someone else could walk her and feed her and pet her leaving me free to find better uuids . my time is valuable this would be an efficient use of my money. but i dont do that. i like petting my dog. i like to understand things. i like to share that understanding with others. this is my favorite part of building software. until recently i had to understand the software that i developed. llms can now type most of my code for me. and if i want i can often accept that code without understanding how it works. thats a big change. llms have changed a lot of things i am good at generating lots of code quickly that skill is less valuable than it was 5 years ago. im pretty quick in vim thats probably not as important either. its uncomfortable that some skills ive honed over years are becoming less valuable. but code has fussy syntax boring parts and boilerplate. its exciting to have agents that can take my code on its afternoon walk. its more uncomfortable to be able to skip the understanding. for me thats petting the dog. many of my blogs are about understanding useless computer problems . they would be bad posts if they said i asked an llm to solve this problem and it did. i did not do this nobody can make me use llm outputs that i dont understand for this blog. but my blogs style doesnt control the software industry. understanding can be a chore or expense on the way to shipping. maybe it doesnt mak"}
{"title": "Astrophotography visibility plotting and planning tool", "url": "https://airmass.org/", "content": "Or upload a text file: Show field of view of an instrument...  Altitude limits... Set start month... Hours above ° (airmass 2.00 ) during 24 hours night astronomical night", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["upload", "text", "file", "field", "view", "instrument", "altitude", "limit", "set", "start", "month", "hour", "airmass", "2.00", "24", "hour", "night", "astronomical", "night"], "num_tokens": 19, "token_loss_pct": 40.62, "normalized_content": "or upload a text file show field of view of an instrument... altitude limits... set start month... hours above  airmass 2.00  during 24 hours night astronomical night"}
{"title": "Ask HN: What's an API that you wish existed?", "url": "item?id=46691222", "content": "Ask HN: What's an API that you wish existed?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "api", "wish", "exist", "score", "author", "date"], "num_tokens": 8, "token_loss_pct": 57.89, "normalized_content": "ask hn what's an api that you wish existed. score none. author none. date none"}
{"title": "F-16 Falcon Strike", "url": "https://webchrono.pl/F16FalconStrike/index.html", "content": "© 2023-2026 by Jarosław 'Roeoender' Wosik Latest sim version 2.0.2 released 2026-01-18 Latest docs update 2026-01-18. Become Polish Air Force F-16 Pilot defending E.U. & Polish border\nfrom B.A.R.F. (Belarussian And Russian Federation) aggression\nin fictional \"Królewiec Campaign\" of 15 varied missions. Be a part of dynamic war in introduced in v.2.0.0 WARFARE mode with procedurally generated battlefield and fly countless missions in procedurally generated missions in GENERATOR mode. Apply strategic planning to defeat enemy air and ground forces, quickly update your plans according to developements in the simulated dynamic 3D battlefield. All this and more on a classic unmodified 8-bit ATARI XL/XE with only 64Kb RAM. With this game I'd like to pay homage to the golden era of 80/90's computer combat flight simulators. Note No part of this game (neither code, nor artwork) was created with A.I./LLMs. or tools incorporating A.I. (no Copilot, no Photoshop). Go to Changelog & Downloads to read info about all the changes  & download the game. You can contact me via atariage.com or atarionline.pl forum - user 'Roeoender' or via my Youtube channel https://www.youtube.com/@R0e0endeR . Please inform me if you have written this game's review or streamed gameplay - I'd really like to read what people think about this game and how they play it.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["2023", "2026", "jarosław", "roeoender", "wosik", "late", "sim", "version", "2.0.2", "release", "2026", "01", "18", "late", "docs", "update", "2026", "01", "18", "polish", "air", "force", "f-16", "pilot", "defend", "e.u", "polish", "border", "b.a.r.f", "belarussian", "russian", "federation", "aggression", "fictional", "królewiec", "campaign", "15", "varied", "mission", "dynamic", "war", "introduce", "v.2.0.0", "warfare", "mode", "procedurally", "generate", "battlefield", "fly", "countless", "mission", "procedurally", "generate", "mission", "generator", "mode", "apply", "strategic", "planning", "defeat", "enemy", "air", "ground", "force", "quickly", "update", "plan", "accord", "developement", "simulate", "dynamic", "3d", "battlefield", "classic", "unmodified", "bit", "atari", "xlxe", "64", "kb", "ram", "game", "like", "pay", "homage", "golden", "era", "8090", "computer", "combat", "flight", "simulator", "note", "game", "code", "artwork", "create", "a.i.llm", "tool", "incorporate", "a.i", "copilot", "photoshop", "changelog", "download", "read", "info", "change", "download", "game", "contact", "atariage.com", "atarionline.pl", "forum", "user", "roeoender", "youtube", "channel", "url", "inform", "write", "game", "review", "stream", "gameplay", "like", "read", "people", "think", "game", "play"], "num_tokens": 131, "token_loss_pct": 46.53, "normalized_content": "2023-2026 by jarosław 'roeoender' wosik latest sim version 2.0.2 released 2026-01-18 latest docs update 2026-01-18. become polish air force f-16 pilot defending e.u.  polish border from b.a.r.f. belarussian and russian federation aggression in fictional królewiec campaign of 15 varied missions. be a part of dynamic war in introduced in v.2.0.0 warfare mode with procedurally generated battlefield and fly countless missions in procedurally generated missions in generator mode. apply strategic planning to defeat enemy air and ground forces quickly update your plans according to developements in the simulated dynamic 3d battlefield. all this and more on a classic unmodified 8-bit atari xlxe with only 64kb ram. with this game i'd like to pay homage to the golden era of 8090's computer combat flight simulators. note no part of this game neither code nor artwork was created with a.i.llms. or tools incorporating a.i. no copilot no photoshop. go to changelog  downloads to read info about all the changes  download the game. you can contact me via atariage.com or atarionline.pl forum - user 'roeoender' or via my youtube channel url . please inform me if you have written this game's review or streamed gameplay - i'd really like to read what people think about this game and how they play it."}
{"title": "String theory can now describe a universe that has dark energy?", "url": "https://www.quantamagazine.org/string-theory-can-now-describe-a-universe-that-has-dark-energy-20260114/", "content": "An editorially independent publication supported by the Simons Foundation. Get the latest news delivered to your inbox. Create a reading list by clicking the Read Later icon next to the articles you wish to save. Type search term(s) and press enter Popular\n                                    Searches January 14, 2026 Scientists have struggled to make string theory compatible with the expanding universe. Nash Weerasekera for Quanta Magazine Contributing Writer January 14, 2026 In 1998, astronomers discovered dark energy. The finding, which transformed our conception of the cosmos, came with a little-known consequence: It threw a wrench into the already daunting task of finding a version of string theory that describes the universe we live in. Dark energy is a “positive” energy that causes our universe to expand at an accelerating rate. But the best-understood models of string theory describe universes with energy that is either negative or zero. Of the various criticisms made of string theory through the years — that it only works in a 10-dimensional universe, that its fundamental constituents, tiny strings, are too small to ever be observed — this was perhaps the most troubling. String theory appeared to be useful only for describing a universe with a negative “anti-de Sitter” geometry, whereas we live in a universe with a positive “de Sitter” geometry. Then last year, two physicists offered a stripped-down but precise formula for how string theory could give rise to a universe similar to ours — a de Sitter universe undergoing accelerated expansion. “It is the very first example [from string theory] of an explicit de Sitter space,” said Thomas Van Riet of KU Leuven in Belgium. The new work, by Bruno Bento and Miguel Montero of the Institute for Theoretical Physics in Madrid, describes a universe with a dark energy that should weaken over time — a result that matches preliminary cosmic observations from the past few years. But the universe they describe is not exactl", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["editorially", "independent", "publication", "support", "simon", "foundation", "late", "news", "deliver", "inbox", "create", "reading", "list", "click", "read", "later", "icon", "article", "wish", "save", "type", "search", "term", "press", "enter", "popular", "search", "january", "14", "2026", "scientist", "struggle", "string", "theory", "compatible", "expand", "universe", "nash", "weerasekera", "quanta", "magazine", "contribute", "writer", "january", "14", "2026", "1998", "astronomer", "discover", "dark", "energy", "finding", "transform", "conception", "cosmo", "come", "little", "know", "consequence", "throw", "wrench", "daunt", "task", "find", "version", "string", "theory", "describe", "universe", "live", "dark", "energy", "positive", "energy", "cause", "universe", "expand", "accelerate", "rate", "well", "understand", "model", "string", "theory", "describe", "universe", "energy", "negative", "zero", "criticism", "string", "theory", "year", "work", "10", "dimensional", "universe", "fundamental", "constituent", "tiny", "string", "small", "observe", "troubling", "string", "theory", "appear", "useful", "describe", "universe", "negative", "anti", "de", "sitter", "geometry", "live", "universe", "positive", "de", "sitter", "geometry", "year", "physicist", "offer", "strip", "precise", "formula", "string", "theory", "rise", "universe", "similar", "de", "sitter", "universe", "undergo", "accelerated", "expansion", "example", "string", "theory", "explicit", "de", "sitter", "space", "say", "thomas", "van", "riet", "ku", "leuven", "belgium", "new", "work", "bruno", "bento", "miguel", "montero", "institute", "theoretical", "physics", "madrid", "describe", "universe", "dark", "energy", "weaken", "time", "result", "match", "preliminary", "cosmic", "observation", "past", "year", "universe", "describe", "exactl"], "num_tokens": 178, "token_loss_pct": 47.95, "normalized_content": "an editorially independent publication supported by the simons foundation. get the latest news delivered to your inbox. create a reading list by clicking the read later icon next to the articles you wish to save. type search terms and press enter popular searches january 14 2026 scientists have struggled to make string theory compatible with the expanding universe. nash weerasekera for quanta magazine contributing writer january 14 2026 in 1998 astronomers discovered dark energy. the finding which transformed our conception of the cosmos came with a little-known consequence it threw a wrench into the already daunting task of finding a version of string theory that describes the universe we live in. dark energy is a positive energy that causes our universe to expand at an accelerating rate. but the best-understood models of string theory describe universes with energy that is either negative or zero. of the various criticisms made of string theory through the years  that it only works in a 10-dimensional universe that its fundamental constituents tiny strings are too small to ever be observed  this was perhaps the most troubling. string theory appeared to be useful only for describing a universe with a negative anti-de sitter geometry whereas we live in a universe with a positive de sitter geometry. then last year two physicists offered a stripped-down but precise formula for how string theory could give rise to a universe similar to ours  a de sitter universe undergoing accelerated expansion. it is the very first example from string theory of an explicit de sitter space said thomas van riet of ku leuven in belgium. the new work by bruno bento and miguel montero of the institute for theoretical physics in madrid describes a universe with a dark energy that should weaken over time  a result that matches preliminary cosmic observations from the past few years. but the universe they describe is not exactl"}
{"title": "The recurring dream of replacing developers", "url": "https://www.caimito.net/en/blog/2025/12/07/the-recurring-dream-of-replacing-developers.html", "content": "07.12.2025, By Stephan Schwab Every decade brings new promises: this time, we'll finally make software development simple enough that we won't need so many developers. From COBOL to AI, the pattern repeats. Business leaders grow frustrated with slow delivery and high costs. Developers feel misunderstood and undervalued. Understanding why this cycle persists for fifty years reveals what both sides need to know about the nature of software work. When Neil Armstrong stepped onto the lunar surface in 1969, the world witnessed what organized human ingenuity could accomplish. Behind that achievement stood Margaret Hamilton and her team, writing Apollo’s guidance software by hand, catching critical errors through careful review, and proving that software could be mission-critical. The Apollo program demonstrated that software development was essential to achieving the impossible. Yet it also revealed something that would frustrate business leaders for decades to come: writing software required specialized knowledge, intense focus, and significant time investment. The dream of making it easier—of needing fewer of these expensive specialists—began almost immediately. The late 1960s and 1970s saw COBOL emerge with an explicit goal stated in its name: Common Business-Oriented Language. The vision was clear: make the language read like English sentences, and business analysts would write their own programs. No need for specialized programmers. This vision had genuine appeal. Software was becoming essential to business operations, yet programmers remained a scarce, expensive resource. COBOL promised to democratize software creation. What happened instead? COBOL became another programming language requiring specialized training. Business analysts who tried to write COBOL quickly discovered that readable syntax didn’t eliminate the complexity of logic, data structures, or system design. A new class of COBOL programmers emerged, and the dream of eliminating specialized developers r", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["07.12.2025", "stephan", "schwab", "decade", "bring", "new", "promise", "time", "finally", "software", "development", "simple", "will", "need", "developer", "cobol", "ai", "pattern", "repeat", "business", "leader", "grow", "frustrated", "slow", "delivery", "high", "cost", "developer", "feel", "misunderstood", "undervalue", "understand", "cycle", "persist", "year", "reveal", "side", "need", "know", "nature", "software", "work", "neil", "armstrong", "step", "lunar", "surface", "1969", "world", "witness", "organize", "human", "ingenuity", "accomplish", "achievement", "stand", "margaret", "hamilton", "team", "write", "apollos", "guidance", "software", "hand", "catch", "critical", "error", "careful", "review", "prove", "software", "mission", "critical", "apollo", "program", "demonstrate", "software", "development", "essential", "achieve", "impossible", "reveal", "frustrate", "business", "leader", "decade", "come", "write", "software", "require", "specialized", "knowledge", "intense", "focus", "significant", "time", "investment", "dream", "make", "easierof", "need", "few", "expensive", "specialistsbegan", "immediately", "late", "1960", "1970", "see", "cobol", "emerge", "explicit", "goal", "state", "common", "business", "orient", "language", "vision", "clear", "language", "read", "like", "english", "sentence", "business", "analyst", "write", "program", "need", "specialized", "programmer", "vision", "genuine", "appeal", "software", "essential", "business", "operation", "programmer", "remain", "scarce", "expensive", "resource", "cobol", "promise", "democratize", "software", "creation", "happen", "instead", "cobol", "programming", "language", "require", "specialized", "training", "business", "analyst", "try", "write", "cobol", "quickly", "discover", "readable", "syntax", "not", "eliminate", "complexity", "logic", "datum", "structure", "system", "design", "new", "class", "cobol", "programmer", "emerge", "dream", "eliminate", "specialized", "developer"], "num_tokens": 183, "token_loss_pct": 40.78, "normalized_content": "07.12.2025 by stephan schwab every decade brings new promises this time we'll finally make software development simple enough that we won't need so many developers. from cobol to ai the pattern repeats. business leaders grow frustrated with slow delivery and high costs. developers feel misunderstood and undervalued. understanding why this cycle persists for fifty years reveals what both sides need to know about the nature of software work. when neil armstrong stepped onto the lunar surface in 1969 the world witnessed what organized human ingenuity could accomplish. behind that achievement stood margaret hamilton and her team writing apollos guidance software by hand catching critical errors through careful review and proving that software could be mission-critical. the apollo program demonstrated that software development was essential to achieving the impossible. yet it also revealed something that would frustrate business leaders for decades to come writing software required specialized knowledge intense focus and significant time investment. the dream of making it easierof needing fewer of these expensive specialistsbegan almost immediately. the late 1960s and 1970s saw cobol emerge with an explicit goal stated in its name common business-oriented language. the vision was clear make the language read like english sentences and business analysts would write their own programs. no need for specialized programmers. this vision had genuine appeal. software was becoming essential to business operations yet programmers remained a scarce expensive resource. cobol promised to democratize software creation. what happened instead cobol became another programming language requiring specialized training. business analysts who tried to write cobol quickly discovered that readable syntax didnt eliminate the complexity of logic data structures or system design. a new class of cobol programmers emerged and the dream of eliminating specialized developers r"}
{"title": "Carney says old world order 'is not coming back'", "url": "https://www.bbc.com/news/articles/cly3d28p4p8o", "content": "Canadian Prime Minister Mark Carney said the \"old order is not coming back\" and urged fellow middle powers to come together in a speech at the World Economic Forum in Davos, Switzerland. \"Middle powers must act together because if we're not at the table, we're on the menu,\" Carney said on Tuesday, adding that he believed powerful nations were using economic coercion to get what they want. He also affirmed Canada's support for Greenland, Denmark and the Nato alliance, drawing applause. Carney did not mention Donald Trump by name, but some of his remarks seemed aimed at the US president, who is threatening to tariff European allies and the UK unless Greenland is surrendered to the US. \"Great powers\" are often defined as countries with permanent seats on United Nations Security Council - China, France, Russia, the United Kingdom and the United States - which shows their economic and military dominance in the world. Middle powers, such as Canada, Australia, Argentina, South Korea and Brazil, are nations that still exert large influence in global politics, even though their economies are smaller. In his speech, Carney said the world is \"in the midst of a rupture, not a transition\". \"Great powers have begun using economic integration as weapons, tariffs as leverage, financial infrastructure as coercion, supply chains as vulnerabilities to be exploited,\" he said. He also said \"Canada was amongst the first to hear the wake-up call\" that geography and historic alliances no longer guaranteed security or prosperity. When Trump returned to office, he frequently referred to Canada as the \"51st state\" and threatened to join Canada and the US through \"economic force.\" The US then hit its northern neighbour and major trading partner with steep tariffs. Recently, Trump added Canada to his push to take control of the partly sovereign territory of Greenland, which has grown stronger and more overt in recent days, by posting on social media a map of the US, Canada and Greenland with an", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["canadian", "prime", "minister", "mark", "carney", "say", "old", "order", "come", "urge", "fellow", "middle", "power", "come", "speech", "world", "economic", "forum", "davos", "switzerland", "middle", "power", "act", "table", "menu", "carney", "say", "tuesday", "add", "believe", "powerful", "nation", "economic", "coercion", "want", "affirm", "canada", "support", "greenland", "denmark", "nato", "alliance", "draw", "applause", "carney", "mention", "donald", "trump", "remark", "aim", "president", "threaten", "tariff", "european", "ally", "uk", "greenland", "surrender", "great", "power", "define", "country", "permanent", "seat", "united", "nations", "security", "council", "china", "france", "russia", "united", "kingdom", "united", "states", "show", "economic", "military", "dominance", "world", "middle", "power", "canada", "australia", "argentina", "south", "korea", "brazil", "nation", "exert", "large", "influence", "global", "politic", "economy", "small", "speech", "carney", "say", "world", "midst", "rupture", "transition", "great", "power", "begin", "economic", "integration", "weapon", "tariff", "leverage", "financial", "infrastructure", "coercion", "supply", "chain", "vulnerability", "exploit", "say", "say", "canada", "hear", "wake", "geography", "historic", "alliance", "long", "guarantee", "security", "prosperity", "trump", "return", "office", "frequently", "refer", "canada", "51st", "state", "threaten", "join", "canada", "economic", "force", "hit", "northern", "neighbour", "major", "trading", "partner", "steep", "tariff", "recently", "trump", "add", "canada", "push", "control", "partly", "sovereign", "territory", "greenland", "grow", "strong", "overt", "recent", "day", "post", "social", "medium", "map", "canada", "greenland"], "num_tokens": 172, "token_loss_pct": 50.0, "normalized_content": "canadian prime minister mark carney said the old order is not coming back and urged fellow middle powers to come together in a speech at the world economic forum in davos switzerland. middle powers must act together because if we're not at the table we're on the menu carney said on tuesday adding that he believed powerful nations were using economic coercion to get what they want. he also affirmed canada's support for greenland denmark and the nato alliance drawing applause. carney did not mention donald trump by name but some of his remarks seemed aimed at the us president who is threatening to tariff european allies and the uk unless greenland is surrendered to the us. great powers are often defined as countries with permanent seats on united nations security council - china france russia the united kingdom and the united states - which shows their economic and military dominance in the world. middle powers such as canada australia argentina south korea and brazil are nations that still exert large influence in global politics even though their economies are smaller. in his speech carney said the world is in the midst of a rupture not a transition. great powers have begun using economic integration as weapons tariffs as leverage financial infrastructure as coercion supply chains as vulnerabilities to be exploited he said. he also said canada was amongst the first to hear the wake-up call that geography and historic alliances no longer guaranteed security or prosperity. when trump returned to office he frequently referred to canada as the 51st state and threatened to join canada and the us through economic force. the us then hit its northern neighbour and major trading partner with steep tariffs. recently trump added canada to his push to take control of the partly sovereign territory of greenland which has grown stronger and more overt in recent days by posting on social media a map of the us canada and greenland with an"}
{"title": "Starting from scratch: Training a 30M Topological Transformer", "url": "https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer", "content": "Tauformer is a topological transformer (see paper ) that replaces dot‑product attention with a Laplacian-derived scalar (taumode) per token/head, then attends using distances in that scalar space.\nBelow is a post-style overview of the idea and the first training signals from a 30M-parameter run. Tauformer’s goal is to inject domain structure directly into attention by using a Graph Laplacian built from a domain embedding space (a “domain memory”) as a persistent reference.\nInstead of ranking keys by \\(Q\\cdot K\\), Tauformer ranks them by how similar their Laplacian-derived taumode scalars are, which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity. At the implementation level, Tauformer keeps the familiar Q/K/V projections, RoPE, causal masking, and stable softmax/value aggregation pipeline, but changes how attention logits are computed.\nEach head vector is compressed into a scalar \\(\\lambda\\) using a bounded Rayleigh-quotient energy computed with a feature-space Laplacian \\(L\\), then logits are computed as a negative distance \\(-|\\lambda_q-\\lambda_k|/\\text{temperature}\\). Key building blocks (as implemented): Because scoring no longer needs full key vectors, Tauformer’s KV-cache can store values plus a compact key-side scalar stream rather than both K and V tensors.\nConcretely, the cache payload is \\((V,\\lambda_k)\\) (not \\((K,V)\\)), which yields an approximate ~50% per-layer cache reduction for typical head dimensions (small overhead for storing the extra scalar). The design also anticipates using a sparse Laplacian from a precomputed domain manifold so computing \\(\\lambda\\) can depend on Laplacian sparsity (nnz) rather than dense \\(D^2\\) multiplication. It exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a Laplacian is built using arrowspace . This run trains a 30M-class TauGPT.\nTraining uses AdamW with base LR \\(5\\times10^{-4}\\) and a warmup of 100 steps, then kee", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["tauformer", "topological", "transformer", "paper", "replace", "dotproduct", "attention", "laplacian", "derive", "scalar", "taumode", "tokenhead", "attend", "distance", "scalar", "space", "post", "style", "overview", "idea", "training", "signal", "30m", "parameter", "run", "tauformer", "goal", "inject", "domain", "structure", "directly", "attention", "graph", "laplacian", "build", "domain", "embed", "space", "domain", "memory", "persistent", "reference", "instead", "rank", "key", "qcdot", "tauformer", "rank", "similar", "laplacian", "derive", "taumode", "scalar", "intend", "bias", "attention", "domain", "relevant", "relation", "generic", "geometric", "similarity", "implementation", "level", "tauformer", "keep", "familiar", "qkv", "projection", "rope", "causal", "masking", "stable", "softmaxvalue", "aggregation", "pipeline", "change", "attention", "logit", "compute", "head", "vector", "compress", "scalar", "lambda", "bound", "rayleigh", "quotient", "energy", "compute", "feature", "space", "laplacian", "logit", "compute", "negative", "distance", "-lambda_q", "lambda_ktexttemperature", "key", "building", "block", "implement", "score", "long", "need", "key", "vector", "tauformer", "kv", "cache", "store", "value", "plus", "compact", "key", "scalar", "stream", "tensor", "concretely", "cache", "payload", "vlambda_k", "kv", "yield", "approximate", "50", "layer", "cache", "reduction", "typical", "head", "dimension", "small", "overhead", "store", "extra", "scalar", "design", "anticipate", "sparse", "laplacian", "precompute", "domain", "manifold", "compute", "lambda", "depend", "laplacian", "sparsity", "nnz", "dense", "d2", "multiplication", "exchange", "long", "preliminary", "adjustment", "weight", "pre", "training", "short", "phase", "laplacian", "build", "arrowspace", "run", "train", "30m", "class", "taugpt", "training", "use", "adamw", "base", "lr", "5times10", "warmup", "100", "step", "kee"], "num_tokens": 181, "token_loss_pct": 44.14, "normalized_content": "tauformer is a topological transformer see paper  that replaces dotproduct attention with a laplacian-derived scalar taumode per tokenhead then attends using distances in that scalar space. below is a post-style overview of the idea and the first training signals from a 30m-parameter run. tauformers goal is to inject domain structure directly into attention by using a graph laplacian built from a domain embedding space a domain memory as a persistent reference. instead of ranking keys by qcdot k tauformer ranks them by how similar their laplacian-derived taumode scalars are which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity. at the implementation level tauformer keeps the familiar qkv projections rope causal masking and stable softmaxvalue aggregation pipeline but changes how attention logits are computed. each head vector is compressed into a scalar lambda using a bounded rayleigh-quotient energy computed with a feature-space laplacian l then logits are computed as a negative distance -lambda_q-lambda_ktexttemperature. key building blocks as implemented because scoring no longer needs full key vectors tauformers kv-cache can store values plus a compact key-side scalar stream rather than both k and v tensors. concretely the cache payload is vlambda_k not kv which yields an approximate 50 per-layer cache reduction for typical head dimensions small overhead for storing the extra scalar. the design also anticipates using a sparse laplacian from a precomputed domain manifold so computing lambda can depend on laplacian sparsity nnz rather than dense d2 multiplication. it exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a laplacian is built using arrowspace . this run trains a 30m-class taugpt. training uses adamw with base lr 5times10-4 and a warmup of 100 steps then kee"}
{"title": "Immigration Agencies Are Openly Defying Federal Courts", "url": "https://lpeproject.org/blog/immigration-agencies-are-openly-defying-federal-courts/", "content": "Nathan Yaffe ( @n-th-n ) is an immigration attorney and organizer based in New York. Nathan Yaffe ( @n-th-n ) is an immigration attorney and organizer based in New York. The unprecedented military-style occupation of U.S. cities under the banner of “immigration enforcement” has made obvious what organizers have long known: ICE and CBP regularly skirt the law. They abuse and coerce people into giving up rights, target dissent , weaponize existing legal powers to erode due process , and insulate rights violations from review through jurisdiction-stripping provisions or post-hoc approval of illegal practices . We might describe these tactics as lawlessness in the shadow of law—that is, while the conduct violates the letter and spirit of the law, the agencies either seek legal approval or endeavor to keep the legality of their conduct from being reviewed at all, knowing that review will force them to change course. In recent months, however, there has been a significant evolution in the immigration agencies’ lawlessness: they are increasingly stepping into direct confrontation with the institutions exercising legal oversight. Certain conflicts have gained significant attention, such as illegally refusing Congressional representatives access to ICE facilities and repeatedly lying and obstructing in court . Yet likely the most widespread such confrontation is happening over the practice of mandatory detention, a statutory authority for ICE to incarcerate someone without bond until the end of immigration proceedings. As I explain in this post, DHS and EOIR (which houses immigration courts) have purported to re-interpret longstanding laws to subject the vast majority of people in removal proceedings to mandatory detention. These efforts have been overwhelmingly rebuffed by federal courts. In more than 1,600 habeas cases , over 300 federal judges have deemed the administration’s gambit illegal, ordering release or a bond hearing, while only 14 have sided with the administrat", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nathan", "yaffe", "mention", "th", "immigration", "attorney", "organizer", "base", "new", "york", "nathan", "yaffe", "mention", "th", "immigration", "attorney", "organizer", "base", "new", "york", "unprecedented", "military", "style", "occupation", "u.s", "city", "banner", "immigration", "enforcement", "obvious", "organizer", "long", "know", "ice", "cbp", "regularly", "skirt", "law", "abuse", "coerce", "people", "give", "right", "target", "dissent", "weaponize", "exist", "legal", "power", "erode", "process", "insulate", "right", "violation", "review", "jurisdiction", "strip", "provision", "post", "hoc", "approval", "illegal", "practice", "describe", "tactic", "lawlessness", "shadow", "lawthat", "conduct", "violate", "letter", "spirit", "law", "agency", "seek", "legal", "approval", "endeavor", "legality", "conduct", "review", "know", "review", "force", "change", "course", "recent", "month", "significant", "evolution", "immigration", "agency", "lawlessness", "increasingly", "step", "direct", "confrontation", "institution", "exercise", "legal", "oversight", "certain", "conflict", "gain", "significant", "attention", "illegally", "refuse", "congressional", "representative", "access", "ice", "facility", "repeatedly", "lie", "obstruct", "court", "likely", "widespread", "confrontation", "happen", "practice", "mandatory", "detention", "statutory", "authority", "ice", "incarcerate", "bond", "end", "immigration", "proceeding", "explain", "post", "dhs", "eoir", "house", "immigration", "court", "purport", "interpret", "longstanding", "law", "subject", "vast", "majority", "people", "removal", "proceeding", "mandatory", "detention", "effort", "overwhelmingly", "rebuff", "federal", "court", "1600", "habea", "case", "300", "federal", "judge", "deem", "administration", "gambit", "illegal", "ordering", "release", "bond", "hearing", "14", "side", "administrat"], "num_tokens": 173, "token_loss_pct": 47.26, "normalized_content": "nathan yaffe  mention-th-n  is an immigration attorney and organizer based in new york. nathan yaffe  mention-th-n  is an immigration attorney and organizer based in new york. the unprecedented military-style occupation of u.s. cities under the banner of immigration enforcement has made obvious what organizers have long known ice and cbp regularly skirt the law. they abuse and coerce people into giving up rights target dissent  weaponize existing legal powers to erode due process  and insulate rights violations from review through jurisdiction-stripping provisions or post-hoc approval of illegal practices . we might describe these tactics as lawlessness in the shadow of lawthat is while the conduct violates the letter and spirit of the law the agencies either seek legal approval or endeavor to keep the legality of their conduct from being reviewed at all knowing that review will force them to change course. in recent months however there has been a significant evolution in the immigration agencies lawlessness they are increasingly stepping into direct confrontation with the institutions exercising legal oversight. certain conflicts have gained significant attention such as illegally refusing congressional representatives access to ice facilities and repeatedly lying and obstructing in court . yet likely the most widespread such confrontation is happening over the practice of mandatory detention a statutory authority for ice to incarcerate someone without bond until the end of immigration proceedings. as i explain in this post dhs and eoir which houses immigration courts have purported to re-interpret longstanding laws to subject the vast majority of people in removal proceedings to mandatory detention. these efforts have been overwhelmingly rebuffed by federal courts. in more than 1600 habeas cases  over 300 federal judges have deemed the administrations gambit illegal ordering release or a bond hearing while only 14 have sided with the administrat"}
{"title": "Show HN: Munimet.ro – ML-based status page for the local subways in SF", "url": "https://munimet.ro/", "content": "Show HN: Munimet.ro – ML-based status page for the local subways in SF. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["hn", "munimet.ro", "ml", "base", "status", "page", "local", "subway", "sf", "score", "author", "date"], "num_tokens": 12, "token_loss_pct": 50.0, "normalized_content": "show hn munimet.ro  ml-based status page for the local subways in sf. score none. author none. date none"}
{"title": "Go 1.26 Interactive Tour", "url": "https://antonz.org/go-1-26/", "content": "Go 1.26 is coming out in February, so it's a good time to explore what's new. The official release notes are pretty dry, so I prepared an interactive version with lots of examples showing what has changed and what the new behavior is. Read on and see! new(expr) • Recursive type constraints • Type-safe error checking • Green Tea GC • Faster cgo and syscalls • Faster memory allocation • Vectorized operations • Secret mode • Reader-less cryptography • Hybrid public key encryption • Goroutine leak profile • Goroutine metrics • Reflective iterators • Peek into a buffer • Process handle • Signal as cause • Compare IP subnets • Context-aware dialing • Fake example.com • Optimized fmt.Errorf • Optimized io.ReadAll • Multiple log handlers • Test artifacts • Modernized go fix • Final thoughts This article is based on the official release notes from The Go Authors and the Go source code, licensed under the BSD-3-Clause license. This is not an exhaustive list; see the official release notes for that. I provide links to the documentation (𝗗), proposals (𝗣), commits (𝗖𝗟), and authors (𝗔) for the features described. Check them out for motivation, usage, and implementation details. I also have dedicated guides (𝗚) for some of the features. Error handling is often skipped to keep things simple. Don't do this in production ツ Previously, you could only use the new built-in with types: Now you can also use it with expressions: If the argument expr is an expression of type T, then new(expr) allocates a variable of type T, initializes it to the value of expr , and returns its address, a value of type *T . This feature is especially helpful if you use pointer fields in a struct to represent optional values that you marshal to JSON or Protobuf: You can use new with composite values: And function calls: Passing nil is still not allowed: 𝗗 spec •\n𝗣 45624 •\n𝗖𝗟 704935 , 704737 , 704955 , 705157 •\n𝗔 Alan Donovan Generic functions and types take types as parameters: We can further restrict these", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["1.26", "come", "february", "good", "time", "explore", "new", "official", "release", "note", "pretty", "dry", "prepare", "interactive", "version", "lot", "example", "show", "change", "new", "behavior", "read", "newexpr", "recursive", "type", "constraint", "type", "safe", "error", "check", "green", "tea", "gc", "fast", "cgo", "syscall", "fast", "memory", "allocation", "vectorize", "operation", "secret", "mode", "reader", "cryptography", "hybrid", "public", "key", "encryption", "goroutine", "leak", "profile", "goroutine", "metric", "reflective", "iterator", "peek", "buffer", "process", "handle", "signal", "cause", "compare", "ip", "subnet", "context", "aware", "dialing", "fake", "example.com", "optimize", "fmt.errorf", "optimize", "io.readall", "multiple", "log", "handler", "test", "artifact", "modernize", "fix", "final", "thought", "article", "base", "official", "release", "note", "author", "source", "code", "license", "bsd-3", "clause", "license", "exhaustive", "list", "official", "release", "note", "provide", "link", "documentation", "proposal", "commit", "𝗖𝗟", "author", "feature", "describe", "check", "motivation", "usage", "implementation", "detail", "dedicate", "guide", "feature", "error", "handling", "skip", "thing", "simple", "production", "previously", "use", "new", "build", "type", "use", "expression", "argument", "expr", "expression", "type", "newexpr", "allocate", "variable", "type", "initialize", "value", "expr", "return", "address", "value", "type", "feature", "especially", "helpful", "use", "pointer", "field", "struct", "represent", "optional", "value", "marshal", "json", "protobuf", "use", "new", "composite", "value", "function", "call", "pass", "nil", "allow", "spec", "45624", "𝗖𝗟", "704935", "704737", "704955", "705157", "alan", "donovan", "generic", "function", "type", "type", "parameter", "restrict"], "num_tokens": 182, "token_loss_pct": 50.81, "normalized_content": "go 1.26 is coming out in february so it's a good time to explore what's new. the official release notes are pretty dry so i prepared an interactive version with lots of examples showing what has changed and what the new behavior is. read on and see newexpr  recursive type constraints  type-safe error checking  green tea gc  faster cgo and syscalls  faster memory allocation  vectorized operations  secret mode  reader-less cryptography  hybrid public key encryption  goroutine leak profile  goroutine metrics  reflective iterators  peek into a buffer  process handle  signal as cause  compare ip subnets  context-aware dialing  fake example.com  optimized fmt.errorf  optimized io.readall  multiple log handlers  test artifacts  modernized go fix  final thoughts this article is based on the official release notes from the go authors and the go source code licensed under the bsd-3-clause license. this is not an exhaustive list see the official release notes for that. i provide links to the documentation 𝗗 proposals 𝗣 commits 𝗖𝗟 and authors 𝗔 for the features described. check them out for motivation usage and implementation details. i also have dedicated guides 𝗚 for some of the features. error handling is often skipped to keep things simple. don't do this in production ツ previously you could only use the new built-in with types now you can also use it with expressions if the argument expr is an expression of type t then newexpr allocates a variable of type t initializes it to the value of expr  and returns its address a value of type t . this feature is especially helpful if you use pointer fields in a struct to represent optional values that you marshal to json or protobuf you can use new with composite values and function calls passing nil is still not allowed 𝗗 spec  𝗣 45624  𝗖𝗟 704935  704737  704955  705157  𝗔 alan donovan generic functions and types take types as parameters we can further restrict these"}
{"title": "The Code-Only Agent", "url": "https://rijnard.com/blog/the-code-only-agent", "content": "Rijnard van Tonder 𝕏 @rvtond When Code Execution Really is All You Need  If you're building an agent, you're probably overwhelmed. Tools.\n            MCP. Subagents. Skills. The ecosystem pushes you toward complexity,\n            toward \"the right way\" to do things. You should know: Concepts like\n            \"Skills\" and \"MCP\" are actually outcomes of an ongoing learning process of humans figuring stuff out. The\n            space is wide open for exploration. With this mindset I\n            wanted to try something different. Simplify the assumptions. What if the agent only had one tool ? Not just any tool, but the most powerful one. The Turing-complete one: execute code . Truly one tool means: no `bash`, no `ls`, no `grep`. Only execute_code . And you enforce it. When you watch an agent run, you might think: \"I wonder what tools\n            it'll use to figure this out. Oh look, it ran `ls`. That makes\n            sense. Next, `grep`. Cool.\" The simpler Code-Only paradigm makes that question irrelevant. The\n            question shifts from \"what tools?\" to \"what code will it produce?\"\n            And that's when things get interesting. Traditional prompting works like this: > Agent, do thing > Agent responds with thing Contrast with: > Agent, do thing > Agent creates and runs code to do thing It does this every time. No, really, every time. Pick a runtime for our Code-Only agent, say Python. It needs\n            to find a file? It writes Python code to find the file and executes\n            the code. Maybe it runs rglob . Maybe it does os.walk . It needs to create a script that crawls a website? It doesn't write\n            the script to your filesystem (reminder: there's no create_file tool to do that!). It writes code to output a script that crawls a website . 1 We make it so that there is literally no way for the agent to do anything productive without writing code . So what? Why do this? You're probably thinking, how is this useful?\n            Just give it `bas", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["rijnard", "van", "tonder", "mention", "code", "execution", "need", "build", "agent", "probably", "overwhelmed", "tool", "mcp", "subagents", "skill", "ecosystem", "push", "complexity", "right", "way", "thing", "know", "concept", "like", "skill", "mcp", "actually", "outcome", "ongoing", "learning", "process", "human", "figure", "stuff", "space", "wide", "open", "exploration", "mindset", "want", "try", "different", "simplify", "assumption", "agent", "tool", "tool", "powerful", "ture", "complete", "execute", "code", "truly", "tool", "mean", "bash", "ls", "grep", "execute_code", "enforce", "watch", "agent", "run", "think", "wonder", "tool", "use", "figure", "oh", "look", "run", "ls", "make", "sense", "grep", "cool", "simple", "code", "paradigm", "make", "question", "irrelevant", "question", "shift", "tool", "code", "produce", "thing", "interesting", "traditional", "prompting", "work", "like", "agent", "thing", "agent", "respond", "thing", "contrast", "agent", "thing", "agent", "create", "run", "code", "thing", "time", "time", "pick", "runtime", "code", "agent", "python", "need", "find", "file", "write", "python", "code", "find", "file", "execute", "code", "maybe", "run", "rglob", "maybe", "os.walk", "need", "create", "script", "crawl", "website", "write", "script", "filesystem", "reminder", "create_file", "tool", "write", "code", "output", "script", "crawl", "website", "literally", "way", "agent", "productive", "write", "code", "probably", "think", "useful", "bas"], "num_tokens": 155, "token_loss_pct": 57.42, "normalized_content": "rijnard van tonder 𝕏 mention when code execution really is all you need if you're building an agent you're probably overwhelmed. tools. mcp. subagents. skills. the ecosystem pushes you toward complexity toward the right way to do things. you should know concepts like skills and mcp are actually outcomes of an ongoing learning process of humans figuring stuff out. the space is wide open for exploration. with this mindset i wanted to try something different. simplify the assumptions. what if the agent only had one tool  not just any tool but the most powerful one. the turing-complete one execute code . truly one tool means no bash no ls no grep. only execute_code . and you enforce it. when you watch an agent run you might think i wonder what tools it'll use to figure this out. oh look it ran ls. that makes sense. next grep. cool. the simpler code-only paradigm makes that question irrelevant. the question shifts from what tools to what code will it produce and that's when things get interesting. traditional prompting works like this  agent do thing  agent responds with thing contrast with  agent do thing  agent creates and runs code to do thing it does this every time. no really every time. pick a runtime for our code-only agent say python. it needs to find a file it writes python code to find the file and executes the code. maybe it runs rglob . maybe it does os.walk . it needs to create a script that crawls a website it doesn't write the script to your filesystem reminder there's no create_file tool to do that. it writes code to output a script that crawls a website . 1 we make it so that there is literally no way for the agent to do anything productive without writing code . so what why do this you're probably thinking how is this useful just give it bas"}
{"title": "A Brief History of Ralph", "url": "https://www.humanlayer.dev/blog/brief-history-of-ralph", "content": "Dex · January 6, 2026 · < 10 min read The Ralph Wiggum Technique , created by Geoff Huntley , went viral in the final weeks of 2025. Here's the story of ralph since the first time I met Geoff in June of 2025. I've been messing with ralph since ~June 2025. Here's my story and what I learned along the way. tl;dr Jan 1 2026 - If you wanna skip to the end, I did a deep dive on ralph w/ Geoff Huntley on Jan 1 2026. It talks through the history, cursed lang, and compares the original bash-loop ralph implementation with the anthropic stop-hook implementation. You can check it out here:  I attend a meetup with about 15 members of a Twitter GC where we talk about agentic coding. It's the first time I see context7, WisprFlow, specstory, taskmaster, and a whole bunch of other tools and addons, some of which are now quite mainstream. One of our engineers demos an early TUI for Claude approvals and what becomes the foundation of research / plan / implement. There are about 3 hours of presentations. Geoff shows up 2 hours late and presents last. He completely steals the show, diving deep on ralph, cursed lang (at the time, the compiler stack is written in Rust), livestreaming autonomous coding overnight while asleep in Australia, subagents in amp code, the virtues of drinking 3 margaritas and shouting at cursor, and much, much more. Geoff talks about the \"overbaking\" phenomenon. If you leave ralph running too long, you end up with all sorts of bizarre emergent behavior, like post-quantum cryptography support. It has dimensions of art, deep engineering, the embrace of chaos, and the raw and authentic joy of making a thing. All ~15 of us have a long and (imo) somewhat unsettling conversation about the future of software dev—about how easy it is to take a SaaS and copy 80-90% of it, and about how many types of work are about to change or disappear entirely. Geoff Launches ralph in an official blog post .  It includes the basic bash loop structure: I send it to everyone I know. I hin", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["dex", "january", "2026", "  ", "10", "min", "read", "ralph", "wiggum", "technique", "create", "geoff", "huntley", "go", "viral", "final", "week", "2025", "story", "ralph", "time", "meet", "geoff", "june", "2025", "mess", "ralph", "june", "2025", "story", "learn", "way", "tldr", "jan", "2026", "wanna", "skip", "end", "deep", "dive", "ralph", "geoff", "huntley", "jan", "2026", "talk", "history", "curse", "lang", "compare", "original", "bash", "loop", "ralph", "implementation", "anthropic", "stop", "hook", "implementation", "check", "attend", "meetup", "15", "member", "twitter", "gc", "talk", "agentic", "coding", "time", "context7", "wisprflow", "specstory", "taskmaster", "bunch", "tool", "addon", "mainstream", "engineer", "demos", "early", "tui", "claude", "approval", "foundation", "research", "plan", "implement", "hour", "presentation", "geoff", "show", "hour", "late", "present", "completely", "steal", "diving", "deep", "ralph", "cursed", "lang", "time", "compiler", "stack", "write", "rust", "livestreame", "autonomous", "coding", "overnight", "asleep", "australia", "subagent", "amp", "code", "virtue", "drink", "margarita", "shout", "cursor", "geoff", "talk", "overbake", "phenomenon", "leave", "ralph", "run", "long", "end", "sort", "bizarre", "emergent", "behavior", "like", "post", "quantum", "cryptography", "support", "dimension", "art", "deep", "engineering", "embrace", "chaos", "raw", "authentic", "joy", "make", "thing", "15", "long", "imo", "somewhat", "unsettling", "conversation", "future", "software", "devabout", "easy", "saas", "copy", "80", "90", "type", "work", "change", "disappear", "entirely", "geoff", "launches", "ralph", "official", "blog", "post", "include", "basic", "bash", "loop", "structure", "send", "know", "hin"], "num_tokens": 183, "token_loss_pct": 52.84, "normalized_content": "dex  january 6 2026   10 min read the ralph wiggum technique  created by geoff huntley  went viral in the final weeks of 2025. here's the story of ralph since the first time i met geoff in june of 2025. i've been messing with ralph since june 2025. here's my story and what i learned along the way. tldr jan 1 2026 - if you wanna skip to the end i did a deep dive on ralph w geoff huntley on jan 1 2026. it talks through the history cursed lang and compares the original bash-loop ralph implementation with the anthropic stop-hook implementation. you can check it out here i attend a meetup with about 15 members of a twitter gc where we talk about agentic coding. it's the first time i see context7 wisprflow specstory taskmaster and a whole bunch of other tools and addons some of which are now quite mainstream. one of our engineers demos an early tui for claude approvals and what becomes the foundation of research  plan  implement. there are about 3 hours of presentations. geoff shows up 2 hours late and presents last. he completely steals the show diving deep on ralph cursed lang at the time the compiler stack is written in rust livestreaming autonomous coding overnight while asleep in australia subagents in amp code the virtues of drinking 3 margaritas and shouting at cursor and much much more. geoff talks about the overbaking phenomenon. if you leave ralph running too long you end up with all sorts of bizarre emergent behavior like post-quantum cryptography support. it has dimensions of art deep engineering the embrace of chaos and the raw and authentic joy of making a thing. all 15 of us have a long and imo somewhat unsettling conversation about the future of software devabout how easy it is to take a saas and copy 80-90 of it and about how many types of work are about to change or disappear entirely. geoff launches ralph in an official blog post . it includes the basic bash loop structure i send it to everyone i know. i hin"}
{"title": "Harvard legal scholars debate the state of the U.S. constitution (2025)", "url": "https://www.harvardmagazine.com/social-sciences/is-the-constitution-broken", "content": "Follow Harvard Magazine: Your independent source for Harvard news since 1898 Advertisement Advertisement Social Sciences | September 12, 2025 Is the Constitution Broken? On stage from left: Brandon Terry, Aziz Rana, and Noah Feldman speak in front of The Embrace monument. | PHOTOGRAPH BY LYDIALYLE GIBSON/ HARVARD MAGAZINE It has been a rocky year for the U.S. Constitution. Eight months into a fast-moving presidency that legal scholars keep describing as a “constitutional stress test,” the Trump administration’s sweeping assertions of executive power have prompted an unprecedented number of legal challenges, including from Harvard , accusing it of violating the Constitution. This April, one national poll found that two-thirds of Americans were concerned about a constitutional crisis. Yet the nation’s founding document still rates as high as ever, with about nine out of ten people expressing a favorable view. Should they, though? Is the Constitution really up to the task of preserving democracy in this moment? Or is it, as the title of a Wednesday evening discussion asked, “broken”? Two constitutional law scholars—Aziz Rana ’00, Ph.D. ’07, and Harvard Law professor Noah Feldman—debated the answer on Boston Common, seated at the foot of The Embrace , the monument to Martin Luther King Jr. and Coretta Scott King. Co-sponsored by the Hutchins Center for African and African American Research, the event was moderated by Loeb associate professor of the social sciences Brandon Terry . To Rana, a Boston College professor who last year published The Constitutional Bind: How Americans Came to Idolize a Document that Fails Them , the Constitution is, indeed, broken. In fact, he argued, the U.S. constitutional system has “super-charged” the current assault by Trump and his allies on the rights and civil liberties that were expanded during the twentieth century. “There is no way to protect those hard-won achievements—achievements that MLK fought and died for,” Rana said, “without", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["follow", "harvard", "magazine", "independent", "source", "harvard", "news", "1898", "advertisement", "advertisement", "social", "science", "september", "12", "2025", "constitution", "break", "stage", "leave", "brandon", "terry", "aziz", "rana", "noah", "feldman", "speak", "embrace", "monument", "photograph", "lydialyle", "gibson", "harvard", "magazine", "rocky", "year", "u.s", "constitution", "month", "fast", "move", "presidency", "legal", "scholar", "describe", "constitutional", "stress", "test", "trump", "administration", "sweeping", "assertion", "executive", "power", "prompt", "unprecedented", "number", "legal", "challenge", "include", "harvard", "accuse", "violate", "constitution", "april", "national", "poll", "find", "third", "americans", "concern", "constitutional", "crisis", "nation", "found", "document", "rat", "high", "people", "express", "favorable", "view", "constitution", "task", "preserve", "democracy", "moment", "title", "wednesday", "evening", "discussion", "ask", "broken", "constitutional", "law", "scholarsaziz", "rana", "00", "ph.d", "07", "harvard", "law", "professor", "noah", "feldmandebate", "answer", "boston", "common", "seat", "foot", "embrace", "monument", "martin", "luther", "king", "jr", "coretta", "scott", "king", "co", "sponsor", "hutchin", "center", "african", "african", "american", "research", "event", "moderate", "loeb", "associate", "professor", "social", "science", "brandon", "terry", "rana", "boston", "college", "professor", "year", "publish", "constitutional", "bind", "americans", "come", "idolize", "document", "fail", "constitution", "break", "fact", "argue", "u.s", "constitutional", "system", "super", "charge", "current", "assault", "trump", "ally", "right", "civil", "liberty", "expand", "twentieth", "century", "way", "protect", "hard", "won", "achievementsachievement", "mlk", "fight", "die", "rana", "say"], "num_tokens": 177, "token_loss_pct": 46.69, "normalized_content": "follow harvard magazine your independent source for harvard news since 1898 advertisement advertisement social sciences  september 12 2025 is the constitution broken on stage from left brandon terry aziz rana and noah feldman speak in front of the embrace monument.  photograph by lydialyle gibson harvard magazine it has been a rocky year for the u.s. constitution. eight months into a fast-moving presidency that legal scholars keep describing as a constitutional stress test the trump administrations sweeping assertions of executive power have prompted an unprecedented number of legal challenges including from harvard  accusing it of violating the constitution. this april one national poll found that two-thirds of americans were concerned about a constitutional crisis. yet the nations founding document still rates as high as ever with about nine out of ten people expressing a favorable view. should they though is the constitution really up to the task of preserving democracy in this moment or is it as the title of a wednesday evening discussion asked broken two constitutional law scholarsaziz rana 00 ph.d. 07 and harvard law professor noah feldmandebated the answer on boston common seated at the foot of the embrace  the monument to martin luther king jr. and coretta scott king. co-sponsored by the hutchins center for african and african american research the event was moderated by loeb associate professor of the social sciences brandon terry . to rana a boston college professor who last year published the constitutional bind how americans came to idolize a document that fails them  the constitution is indeed broken. in fact he argued the u.s. constitutional system has super-charged the current assault by trump and his allies on the rights and civil liberties that were expanded during the twentieth century. there is no way to protect those hard-won achievementsachievements that mlk fought and died for rana said without"}
{"title": "Agent Psychosis: Are We Going Insane?", "url": "https://lucumr.pocoo.org/2026/1/18/agent-psychosis/", "content": "written on January 18, 2026 You can use Polecats without the Refinery and even without the Witness or\nDeacon. Just tell the Mayor to shut down the rig and sling work to the\npolecats with the message that they are to merge to main directly. Or the\npolecats can submit MRs and then the Mayor can merge them manually. It’s\nreally up to you. The Refineries are useful if you have done a LOT of up-front\nspecification work, and you have huge piles of Beads to churn through with\nlong convoys. — Gas Town Emergency User Manual , Steve Yegge Many of us got hit by the agent coding addiction.  It feels good, we barely\nsleep, we build amazing things.  Every once in a while that interaction involves\nother humans, and all of a sudden we get a reality check that maybe we overdid\nit.  The most obvious example of this is the massive degradation of quality of\nissue reports and pull requests.  As a maintainer many PRs now look like an\ninsult to one’s time, but when one pushes back, the other person does not see\nwhat they did wrong.  They thought they helped and contributed and get agitated\nwhen you close it down. But it’s way worse than that.  I see people develop parasocial relationships\nwith their AIs, get heavily addicted to it, and create communities where people\nreinforce highly unhealthy behavior.  How did we get here and what does it do to\nus? I will preface this post by saying that I don’t want to call anyone out in\nparticular, and I think I sometimes feel tendencies that I see as negative, in\nmyself as well.  I too, have thrown some vibeslop\nup to other people’s repositories. In His Dark Materials, every human has a dæmon, a companion that is an\nexternally visible manifestation of their soul.  It lives alongside as an\nanimal, but it talks, thinks and acts independently.  I’m starting to relate our\nrelationship with agents that have memory to those little creatures. We become\ndependent on them, and separation from them is painful and takes away from our\nnew-found identity.  We’re", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["write", "january", "18", "2026", "use", "polecat", "refinery", "witness", "deacon", "tell", "mayor", "shut", "rig", "sling", "work", "polecat", "message", "merge", "main", "directly", "polecat", "submit", "mrs", "mayor", "merge", "manually", "refinery", "useful", "lot", "specification", "work", "huge", "pile", "bead", "churn", "long", "convoy", "gas", "town", "emergency", "user", "manual", "steve", "yegge", "get", "hit", "agent", "cod", "addiction", "feel", "good", "barely", "sleep", "build", "amazing", "thing", "interaction", "involve", "human", "sudden", "reality", "check", "maybe", "overdo", "obvious", "example", "massive", "degradation", "quality", "issue", "report", "pull", "request", "maintainer", "prs", "look", "like", "insult", "one", "time", "push", "person", "wrong", "think", "help", "contribute", "agitated", "close", "way", "bad", "people", "develop", "parasocial", "relationship", "ais", "heavily", "addicted", "create", "community", "people", "reinforce", "highly", "unhealthy", "behavior", "preface", "post", "say", "not", "want", "particular", "think", "feel", "tendency", "negative", "throw", "vibeslop", "people", "repository", "dark", "material", "human", "dæmon", "companion", "externally", "visible", "manifestation", "soul", "live", "alongside", "animal", "talk", "think", "act", "independently", "start", "relate", "relationship", "agent", "memory", "little", "creature", "dependent", "separation", "painful", "take", "away", "new", "find", "identity"], "num_tokens": 149, "token_loss_pct": 60.99, "normalized_content": "written on january 18 2026 you can use polecats without the refinery and even without the witness or deacon. just tell the mayor to shut down the rig and sling work to the polecats with the message that they are to merge to main directly. or the polecats can submit mrs and then the mayor can merge them manually. its really up to you. the refineries are useful if you have done a lot of up-front specification work and you have huge piles of beads to churn through with long convoys.  gas town emergency user manual  steve yegge many of us got hit by the agent coding addiction. it feels good we barely sleep we build amazing things. every once in a while that interaction involves other humans and all of a sudden we get a reality check that maybe we overdid it. the most obvious example of this is the massive degradation of quality of issue reports and pull requests. as a maintainer many prs now look like an insult to ones time but when one pushes back the other person does not see what they did wrong. they thought they helped and contributed and get agitated when you close it down. but its way worse than that. i see people develop parasocial relationships with their ais get heavily addicted to it and create communities where people reinforce highly unhealthy behavior. how did we get here and what does it do to us i will preface this post by saying that i dont want to call anyone out in particular and i think i sometimes feel tendencies that i see as negative in myself as well. i too have thrown some vibeslop up to other peoples repositories. in his dark materials every human has a dæmon a companion that is an externally visible manifestation of their soul. it lives alongside as an animal but it talks thinks and acts independently. im starting to relate our relationship with agents that have memory to those little creatures. we become dependent on them and separation from them is painful and takes away from our new-found identity. were"}
{"title": "Software engineers can no longer neglect their soft skills", "url": "https://www.qu8n.com/posts/most-important-software-engineering-skill-2026", "content": "January 6, 2026 Starting in 2026, communication has become the most important skill for software engineers. It's not writing code, system designs, or having estoric knowledge of a programming language (i.e., Rust). AI coding agents have gotten very, very good . A year ago, I'd reach out to Cursor hesitantly for MVPs or quick fixes. Today, I use Claude Code for almost all non-trivial programming tasks and have spent $500+ on it just last December. AI talks online revolve much around the hard skils. Initially it was prompt tricks to accomplish X, then the best MCPs for Y, and so on. But with Opus 4.5, using vanilla Claude Code gets you 80% there. Even in the age of AI, the 80/20 rule still applies. So, what should engineers focus on? One thing with coding agents is that the better the spec, the more in line they will be with the technical and business requirements. But getting a good spec is hard. In real life, tickets rarely contain all the requirements. To do so, you might need to: Doing these things well used to be optional for individual contributors. Certain teams would enable engineers to thrive being an average communicator but excellent coder. Now, the non-coding parts are becoming a non-negotiable. Software engineers are problem solvers. We believe that every problem has a solution, a \"best practice\". But working with people is messy. Un fortunately, we won't be able to AI our way into better communication skills. Good communication requires empathy, and we can all use a little more of that in today's landscape. -- January 19, 2026 This post got some attention from Hacker News. Thank you. I enjoyed reading the thoughtful discussions. I wrote this update to clarify: AI is just a tool, and December's $500+ spend was me exploring and experimenting during the holidays. Learning new tools doesn't make a worse craftsman, and I say this as an AI hype skeptic. I did not write this post with AI. I started blogging recently to improve my writing. If my writing reads li", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["january", "2026", "start", "2026", "communication", "important", "skill", "software", "engineer", "write", "code", "system", "design", "having", "estoric", "knowledge", "programming", "language", "i.e.", "rust", "ai", "cod", "agent", "get", "good", "year", "ago", "reach", "cursor", "hesitantly", "mvps", "quick", "fix", "today", "use", "claude", "code", "non", "trivial", "programming", "task", "spend", "500", "december", "ai", "talk", "online", "revolve", "hard", "skil", "initially", "prompt", "trick", "accomplish", "good", "mcp", "opus", "4.5", "vanilla", "claude", "code", "get", "80", "age", "ai", "8020", "rule", "apply", "engineer", "focus", "thing", "cod", "agent", "well", "spec", "line", "technical", "business", "requirement", "get", "good", "spec", "hard", "real", "life", "ticket", "rarely", "contain", "requirement", "need", "thing", "optional", "individual", "contributor", "certain", "team", "enable", "engineer", "thrive", "average", "communicator", "excellent", "coder", "non", "coding", "part", "non", "negotiable", "software", "engineer", "problem", "solver", "believe", "problem", "solution", "good", "practice", "work", "people", "messy", "un", "fortunately", "will", "able", "ai", "way", "well", "communication", "skill", "good", "communication", "require", "empathy", "use", "little", "today", "landscape", "january", "19", "2026", "post", "get", "attention", "hacker", "news", "thank", "enjoy", "read", "thoughtful", "discussion", "write", "update", "clarify", "ai", "tool", "december", "500", "spend", "explore", "experiment", "holiday", "learn", "new", "tool", "bad", "craftsman", "ai", "hype", "skeptic", "write", "post", "ai", "start", "blogge", "recently", "improve", "writing", "writing", "read", "li"], "num_tokens": 180, "token_loss_pct": 52.88, "normalized_content": "january 6 2026 starting in 2026 communication has become the most important skill for software engineers. it's not writing code system designs or having estoric knowledge of a programming language i.e. rust. ai coding agents have gotten very very good . a year ago i'd reach out to cursor hesitantly for mvps or quick fixes. today i use claude code for almost all non-trivial programming tasks and have spent 500 on it just last december. ai talks online revolve much around the hard skils. initially it was prompt tricks to accomplish x then the best mcps for y and so on. but with opus 4.5 using vanilla claude code gets you 80 there. even in the age of ai the 8020 rule still applies. so what should engineers focus on one thing with coding agents is that the better the spec the more in line they will be with the technical and business requirements. but getting a good spec is hard. in real life tickets rarely contain all the requirements. to do so you might need to doing these things well used to be optional for individual contributors. certain teams would enable engineers to thrive being an average communicator but excellent coder. now the non-coding parts are becoming a non-negotiable. software engineers are problem solvers. we believe that every problem has a solution a best practice. but working with people is messy. un fortunately we won't be able to ai our way into better communication skills. good communication requires empathy and we can all use a little more of that in today's landscape. -- january 19 2026 this post got some attention from hacker news. thank you. i enjoyed reading the thoughtful discussions. i wrote this update to clarify ai is just a tool and december's 500 spend was me exploring and experimenting during the holidays. learning new tools doesn't make a worse craftsman and i say this as an ai hype skeptic. i did not write this post with ai. i started blogging recently to improve my writing. if my writing reads li"}
{"title": "East Germany balloon escape", "url": "https://en.wikipedia.org/wiki/East_Germany_balloon_escape", "content": "On 16 September 1979, eight people from two families escaped from East Germany by crossing the border into West Germany at night in a homemade hot air balloon . The unique feat was the result of over a year and a half of preparations involving three different balloons, various modifications, and a first, unsuccessful attempt. The failed attempt alerted the East German authorities to the plot, but the police were unable to identify the escapees before their second, successful flight two months later. East Germany, then part of the Eastern Bloc , was separated from West Germany in the Western Bloc by the inner German border and the Berlin Wall , which were heavily fortified with watchtowers , land mines , armed soldiers, and various other measures to prevent illegal crossings. East German border troops were instructed to prevent defection to West Germany by all means, including lethal force ( Schießbefehl ; \"order to fire\"). [ 2 ] Peter Strelzyk (1942–2017), an electrician and former East German Air Force mechanic, and Günter Wetzel (born 1955), a bricklayer by trade, [ 3 ] were colleagues at a local plastics factory. [ 4 ] Friends for four years, they shared a desire to flee the country and began discussing ways to get across the border. On 7 March 1978, they agreed to plan an escape. [ 5 ] They considered building a helicopter but quickly realized they would be unable to acquire an engine capable of powering such a craft. They then decided to explore the idea of constructing a hot air balloon, [ 6 ] having been inspired by a television program about ballooning. [ 3 ] An alternate account is that a relative shared a magazine article about the International Balloon Festival in Albuquerque, New Mexico . [ 5 ] Strelzyk and Wetzel began research into balloons. Their plan was to escape with their wives and a total of four children (aged 2 to 15). They calculated the weight of the eight passengers and the craft itself to be around 750 kilograms (1,650 lb). Subsequent calc", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["16", "september", "1979", "people", "family", "escape", "east", "germany", "cross", "border", "west", "germany", "night", "homemade", "hot", "air", "balloon", "unique", "feat", "result", "year", "half", "preparation", "involve", "different", "balloon", "modification", "unsuccessful", "attempt", "fail", "attempt", "alert", "east", "german", "authority", "plot", "police", "unable", "identify", "escapee", "second", "successful", "flight", "month", "later", "east", "germany", "eastern", "bloc", "separate", "west", "germany", "western", "bloc", "inner", "german", "border", "berlin", "wall", "heavily", "fortified", "watchtower", "land", "mine", "armed", "soldier", "measure", "prevent", "illegal", "crossing", "east", "german", "border", "troop", "instruct", "prevent", "defection", "west", "germany", "mean", "include", "lethal", "force", "schießbefehl", "order", "fire", "peter", "strelzyk", "19422017", "electrician", "east", "german", "air", "force", "mechanic", "günter", "wetzel", "bear", "1955", "bricklayer", "trade", "colleague", "local", "plastic", "factory", "friend", "year", "share", "desire", "flee", "country", "begin", "discuss", "way", "border", "march", "1978", "agree", "plan", "escape", "consider", "build", "helicopter", "quickly", "realize", "unable", "acquire", "engine", "capable", "power", "craft", "decide", "explore", "idea", "construct", "hot", "air", "balloon", "having", "inspire", "television", "program", "ballooning", "alternate", "account", "relative", "share", "magazine", "article", "international", "balloon", "festival", "albuquerque", "new", "mexico", "strelzyk", "wetzel", "begin", "research", "balloon", "plan", "escape", "wife", "total", "child", "age", "15", "calculate", "weight", "passenger", "craft", "750", "kilogram", "1650", "lb", "subsequent", "calc"], "num_tokens": 177, "token_loss_pct": 50.56, "normalized_content": "on 16 september 1979 eight people from two families escaped from east germany by crossing the border into west germany at night in a homemade hot air balloon . the unique feat was the result of over a year and a half of preparations involving three different balloons various modifications and a first unsuccessful attempt. the failed attempt alerted the east german authorities to the plot but the police were unable to identify the escapees before their second successful flight two months later. east germany then part of the eastern bloc  was separated from west germany in the western bloc by the inner german border and the berlin wall  which were heavily fortified with watchtowers  land mines  armed soldiers and various other measures to prevent illegal crossings. east german border troops were instructed to prevent defection to west germany by all means including lethal force  schießbefehl  order to fire.  2  peter strelzyk 19422017 an electrician and former east german air force mechanic and günter wetzel born 1955 a bricklayer by trade  3  were colleagues at a local plastics factory.  4  friends for four years they shared a desire to flee the country and began discussing ways to get across the border. on 7 march 1978 they agreed to plan an escape.  5  they considered building a helicopter but quickly realized they would be unable to acquire an engine capable of powering such a craft. they then decided to explore the idea of constructing a hot air balloon  6  having been inspired by a television program about ballooning.  3  an alternate account is that a relative shared a magazine article about the international balloon festival in albuquerque new mexico .  5  strelzyk and wetzel began research into balloons. their plan was to escape with their wives and a total of four children aged 2 to 15. they calculated the weight of the eight passengers and the craft itself to be around 750 kilograms 1650 lb. subsequent calc"}
{"title": "The longest Greek word", "url": "https://en.wikipedia.org/wiki/Lopado%C2%ADtemacho%C2%ADselacho%C2%ADgaleo%C2%ADkranio%C2%ADleipsano%C2%ADdrim%C2%ADhypo%C2%ADtrimmato%C2%ADsilphio%C2%ADkarabo%C2%ADmelito%C2%ADkatakechy%C2%ADmeno%C2%ADkichl%C2%ADepi%C2%ADkossypho%C2%ADphatto%C2%ADperister%C2%ADalektryon%C2%ADopte%C2%ADkephallio%C2%ADkigklo%C2%ADpeleio%C2%ADlagoio%C2%ADsiraio%C2%ADbaphe%C2%ADtragano%C2%ADpterygon", "content": "Lopado­temacho­selacho­galeo­kranio­leipsano­drim­hypo­trimmato­silphio­karabo­melito­katakechy­meno­kichl­epi­kossypho­phatto­perister­alektryon­opto­kephallio­kigklo­peleio­lagoio­siraio­baphe­tragano­pterygon is a fictional dish originating from Aristophanes ' 391 BC comedy Assemblywomen , [ 1 ] deriving from a transliteration of the Ancient Greek word λοπαδο­τεμαχο­σελαχο­γαλεο­κρανιο­λειψανο­δριμ­υπο­τριμματο­σιλφιο­καραβο­μελιτο­κατακεχυ­μενο­κιχλ­επι­κοσσυφο­φαττο­περιστερ­αλεκτρυον­οπτο­κεφαλλιο­κιγκλο­πελειο­λαγῳο­σιραιο­βαφη­τραγανο­πτερύγων . In A Greek–English Lexicon , it is defined as the \"name of a dish compounded of all kinds of dainties , fish , flesh , fowl , and sauces \". [ 2 ] It is the longest Greek word, containing 171 letters and 78 syllables. The transliteration has 183 Latin characters and is the longest word ever to appear in literature, according to the Guinness World Records (1990). [ 3 ] The form of the word quoted here is the version listed in the Liddell & Scott Greek lexicon (1940) and quoted therein as being amended by August Meineke , [ 2 ] contrasting F.W. Hall and W.M. Geldart 's 1907 edition of Aristophanis Comoediae (used in the Assemblywomen play) variant of (differences underlined): λοπαδο­τεμαχο­σελαχο­γαλεο­κρανιο­λειψανο­δριμ­υποτριμματο­σιλφιο­ τυρο ­μελιτο­κατακεχυμενο­κιχλεπικοσσυφο­φαττο­περιστερ­αλεκτρυον­οπτεκεφαλλιο­κιγκλο­πελειο­λαγῳο­σιραιο­βαφη­τραγανο­πτερυγ ώ . [ 4 ] The dish was a fricassée , with at least 16 sweet and sour ingredients, including the following: [ 3 ] The term is used in the ultimate chorus of the play, when Blepyrus (and the audience) are summoned to the first feast laid on by the new system. [1167] And you others, let your light steps too keep time. [1168] Very soon we'll be eating [1170] lopado­temacho­selacho­galeo­kranio­leipsano­drim­ ypo ­trimmato­silphio­karabo­melito­katakechy­meno­kichl­epi­kossypho­phatto­perister­alektryon­opte­ kephalio ­kigklo­peleio­lagoio­siraio­baphe­tragano­pte", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["lopadotemachoselachogaleokranioleipsanodrimhypotrimmatosilphiokarabomelitokatakechymenokichlepikossyphophattoperisteralektryonoptokephalliokigklopeleiolagoiosiraiobaphetraganopterygon", "fictional", "dish", "originate", "aristophane", "391", "bc", "comedy", "assemblywoman", "  ", "derive", "transliteration", "ancient", "greek", "word", "λοπαδοτεμαχοσελαχογαλεοκρανιολειψανοδριμυποτριμματοσιλφιοκαραβομελιτοκατακεχυμενοκιχλεπικοσσυφοφαττοπεριστεραλεκτρυονοπτοκεφαλλιοκιγκλοπελειολαγῳοσιραιοβαφητραγανοπτερύγων", "greekenglish", "lexicon", "define", "dish", "compound", "kind", "dainty", "fish", "flesh", "fowl", "sauce", "long", "greek", "word", "contain", "171", "letter", "78", "syllable", "transliteration", "183", "latin", "character", "long", "word", "appear", "literature", "accord", "guinness", "world", "record", "1990", "form", "word", "quote", "version", "list", "liddell", "scott", "greek", "lexicon", "1940", "quote", "amend", "august", "meineke", "  ", "contrast", "f.w", "hall", "w.m", "geldart", "1907", "edition", "aristophani", "comoediae", "assemblywoman", "play", "variant", "difference", "underline", "λοπαδοτεμαχοσελαχογαλεοκρανιολειψανοδριμυποτριμματοσιλφιο", "τυρο", "μελιτοκατακεχυμενοκιχλεπικοσσυφοφαττοπεριστεραλεκτρυονοπτεκεφαλλιοκιγκλοπελειολαγῳοσιραιοβαφητραγανοπτερυγ", "dish", "fricassée", "16", "sweet", "sour", "ingredient", "include", "follow", "term", "ultimate", "chorus", "play", "blepyrus", "audience", "summon", "feast", "lay", "new", "system", "1167", "let", "light", "step", "time", "1168", "soon", "eat", "1170", "lopadotemachoselachogaleokranioleipsanodrim", "ypo", "trimmatosilphiokarabomelitokatakechymenokichlepikossyphophattoperisteralektryonopte", "kephalio", "kigklopeleiolagoiosiraiobaphetraganopte"], "num_tokens": 113, "token_loss_pct": 52.12, "normalized_content": "lopadotemachoselachogaleokranioleipsanodrimhypotrimmatosilphiokarabomelitokatakechymenokichlepikossyphophattoperisteralektryonoptokephalliokigklopeleiolagoiosiraiobaphetraganopterygon is a fictional dish originating from aristophanes ' 391 bc comedy assemblywomen   1  deriving from a transliteration of the ancient greek word λοπαδοτεμαχοσελαχογαλεοκρανιολειψανοδριμυποτριμματοσιλφιοκαραβομελιτοκατακεχυμενοκιχλεπικοσσυφοφαττοπεριστεραλεκτρυονοπτοκεφαλλιοκιγκλοπελειολαγῳοσιραιοβαφητραγανοπτερύγων . in a greekenglish lexicon  it is defined as the name of a dish compounded of all kinds of dainties  fish  flesh  fowl  and sauces .  2  it is the longest greek word containing 171 letters and 78 syllables. the transliteration has 183 latin characters and is the longest word ever to appear in literature according to the guinness world records 1990.  3  the form of the word quoted here is the version listed in the liddell  scott greek lexicon 1940 and quoted therein as being amended by august meineke   2  contrasting f.w. hall and w.m. geldart 's 1907 edition of aristophanis comoediae used in the assemblywomen play variant of differences underlined λοπαδοτεμαχοσελαχογαλεοκρανιολειψανοδριμυποτριμματοσιλφιο τυρο μελιτοκατακεχυμενοκιχλεπικοσσυφοφαττοπεριστεραλεκτρυονοπτεκεφαλλιοκιγκλοπελειολαγῳοσιραιοβαφητραγανοπτερυγ ώ .  4  the dish was a fricassée  with at least 16 sweet and sour ingredients including the following  3  the term is used in the ultimate chorus of the play when blepyrus and the audience are summoned to the first feast laid on by the new system. 1167 and you others let your light steps too keep time. 1168 very soon we'll be eating 1170 lopadotemachoselachogaleokranioleipsanodrim ypo trimmatosilphiokarabomelitokatakechymenokichlepikossyphophattoperisteralektryonopte kephalio kigklopeleiolagoiosiraiobaphetraganopte"}
{"title": "Driver killed and several injured after second train derails near Barcelona", "url": "https://www.bbc.com/news/articles/c1m78xl0gmpo", "content": "A train driver has been killed and at least 37 people injured, five seriously, after a commuter train derailed and crashed near Barcelona two days after a deadly two-train collision in southern Spain. According to local officials, the Rodalies train collided with a retaining wall which fell on to the track between Gelida and Sant Sadurní. Catalonia regional fire Inspector Claudi Gallardo said all the passengers had been removed from the train. The incident occurred as heavy storms battered north-eastern Spain, with coastal areas in the east and north-west of Spain on high alert because of the weather. Rail officials believe the wall collapsed as the train was passing shortly after 21:00 (20:00 GMT) on Tuesday evening, striking the driver's cab first and then causing considerable damage to the first carriage of the train in which most of the injured passengers were travelling. The identity of the driver was not immediately clear as three trainees had been with the driver when the accident happened. Firefighters said two of them were among those seriously injured. It took almost an hour to free one of the survivors at the scene in Gelida, about 35km (22 miles) west of Barcelona. Emergency services said they had evacuated some of the injured to nearby Moisès Broggi, Bellvitge, and Vilafranca hospitals. Services across Catalonia's main Rodalies commuter rail network have been suspended completely while safety checks are carried out and officials say they will not resume until lines are considered safe. Spanish train drivers' union Semaf has called a strike as a result of the two deadly crashes, at Gelida on Tuesday and near Córdoba in Andalusia where at least 42 people died. Two high-speed trains collided at Adamuz, Andalusia, on Sunday in one of the worst Spanish rail accidents in over a decade. Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks and then collided with an oncoming high-speed train. \"All members of Semaf are devastated and", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["train", "driver", "kill", "37", "people", "injure", "seriously", "commuter", "train", "derail", "crash", "near", "barcelona", "day", "deadly", "train", "collision", "southern", "spain", "accord", "local", "official", "rodalie", "train", "collide", "retain", "wall", "fall", "track", "gelida", "sant", "sadurní", "catalonia", "regional", "fire", "inspector", "claudi", "gallardo", "say", "passenger", "remove", "train", "incident", "occur", "heavy", "storm", "batter", "north", "eastern", "spain", "coastal", "area", "east", "north", "west", "spain", "high", "alert", "weather", "rail", "official", "believe", "wall", "collapse", "train", "pass", "shortly", "2100", "2000", "gmt", "tuesday", "evening", "strike", "driver", "cab", "cause", "considerable", "damage", "carriage", "train", "injure", "passenger", "travel", "identity", "driver", "immediately", "clear", "trainee", "driver", "accident", "happen", "firefighter", "say", "seriously", "injure", "take", "hour", "free", "survivor", "scene", "gelida", "35", "km", "22", "mile", "west", "barcelona", "emergency", "service", "say", "evacuate", "injure", "nearby", "moisès", "broggi", "bellvitge", "vilafranca", "hospital", "service", "catalonia", "main", "rodalie", "commuter", "rail", "network", "suspend", "completely", "safety", "check", "carry", "official", "resume", "line", "consider", "safe", "spanish", "train", "driver", "union", "semaf", "call", "strike", "result", "deadly", "crash", "gelida", "tuesday", "near", "córdoba", "andalusia", "42", "people", "die", "high", "speed", "train", "collide", "adamuz", "andalusia", "sunday", "bad", "spanish", "rail", "accident", "decade", "carriage", "madrid", "bind", "train", "derail", "cross", "opposite", "track", "collide", "oncoming", "high", "speed", "train", "member", "semaf", "devastate"], "num_tokens": 181, "token_loss_pct": 49.01, "normalized_content": "a train driver has been killed and at least 37 people injured five seriously after a commuter train derailed and crashed near barcelona two days after a deadly two-train collision in southern spain. according to local officials the rodalies train collided with a retaining wall which fell on to the track between gelida and sant sadurní. catalonia regional fire inspector claudi gallardo said all the passengers had been removed from the train. the incident occurred as heavy storms battered north-eastern spain with coastal areas in the east and north-west of spain on high alert because of the weather. rail officials believe the wall collapsed as the train was passing shortly after 2100 2000 gmt on tuesday evening striking the driver's cab first and then causing considerable damage to the first carriage of the train in which most of the injured passengers were travelling. the identity of the driver was not immediately clear as three trainees had been with the driver when the accident happened. firefighters said two of them were among those seriously injured. it took almost an hour to free one of the survivors at the scene in gelida about 35km 22 miles west of barcelona. emergency services said they had evacuated some of the injured to nearby moisès broggi bellvitge and vilafranca hospitals. services across catalonia's main rodalies commuter rail network have been suspended completely while safety checks are carried out and officials say they will not resume until lines are considered safe. spanish train drivers' union semaf has called a strike as a result of the two deadly crashes at gelida on tuesday and near córdoba in andalusia where at least 42 people died. two high-speed trains collided at adamuz andalusia on sunday in one of the worst spanish rail accidents in over a decade. carriages on a madrid-bound train derailed and crossed over to the opposite tracks and then collided with an oncoming high-speed train. all members of semaf are devastated and"}
{"title": "'The old order is not coming back,' Carney says in speech at Davos", "url": "https://www.cbc.ca/news/politics/carney-davos-speech-9.7052725", "content": "Prime Minister Mark Carney delivered a frank assessment of how he views the world in a provocative speech in Davos, Switzerland, on Tuesday, where he said the longstanding U.S.-led, rules-based international order is over and middle powers like Canada must pivot to avoid falling prey to further \"coercion\" from powerful actors. Without invoking U.S. President Donald Trump by name, Carney referenced \"American hegemony\" and said \"great powers\" are using economic integration as \"weapons.\" \"Canadians know that our old, comfortable assumption that our geography and alliance memberships automatically conferred prosperity and security is no longer valid,\" Carney said. As it grapples with this new dynamic, Carney said Canada must be \"principled and pragmatic\" and turn inward to build up the country and diversify trading relationships to become less reliant on countries like the U.S., now that it's clear \"integration\" can lead to \"subordination.\" Carney said multilateralism and the \"architecture of collective problem-solving\" — relying on institutions like the World Trade Organization, the United Nations and Conference of the Parties (COP) for climate talks — has been \"diminished\" and countries have to accept they may have to go it alone more often than in the recent past. \"Many countries are drawing the same conclusions. They must develop greater strategic autonomy: in energy, food, critical minerals, in finance and supply chains. \"A country that cannot feed itself, fuel itself or defend itself has few options. When the rules no longer protect you, you must protect yourself,\" Carney said. 'The old order is not coming back': PM says Canada must 'name reality' and build strength at home Carney said this more isolationist approach, where there's a \"world of fortresses,\" will make countries poorer, fragile and less sustainable. But it's coming nonetheless and Canada must work with like-minded allies where possible to push back against domination by larger, wealthier and well-arm", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["prime", "minister", "mark", "carney", "deliver", "frank", "assessment", "view", "world", "provocative", "speech", "davos", "switzerland", "tuesday", "say", "longstanding", "u.s.-led", "rule", "base", "international", "order", "middle", "power", "like", "canada", "pivot", "avoid", "fall", "prey", "coercion", "powerful", "actor", "invoke", "u.s", "president", "donald", "trump", "carney", "reference", "american", "hegemony", "say", "great", "power", "economic", "integration", "weapon", "canadian", "know", "old", "comfortable", "assumption", "geography", "alliance", "membership", "automatically", "confer", "prosperity", "security", "long", "valid", "carney", "say", "grapple", "new", "dynamic", "carney", "say", "canada", "principle", "pragmatic", "turn", "inward", "build", "country", "diversify", "trading", "relationship", "reliant", "country", "like", "u.s", "clear", "integration", "lead", "subordination", "carney", "say", "multilateralism", "architecture", "collective", "problem", "solve", "rely", "institution", "like", "world", "trade", "organization", "united", "nations", "conference", "party", "cop", "climate", "talk", "diminish", "country", "accept", "recent", "past", "country", "draw", "conclusion", "develop", "great", "strategic", "autonomy", "energy", "food", "critical", "mineral", "finance", "supply", "chain", "country", "feed", "fuel", "defend", "option", "rule", "long", "protect", "protect", "carney", "say", "old", "order", "come", "pm", "say", "canada", "reality", "build", "strength", "home", "carney", "say", "isolationist", "approach", "world", "fortress", "country", "poorer", "fragile", "sustainable", "come", "nonetheless", "canada", "work", "like", "minded", "ally", "possible", "push", "domination", "large", "wealthy", "arm"], "num_tokens": 169, "token_loss_pct": 49.4, "normalized_content": "prime minister mark carney delivered a frank assessment of how he views the world in a provocative speech in davos switzerland on tuesday where he said the longstanding u.s.-led rules-based international order is over and middle powers like canada must pivot to avoid falling prey to further coercion from powerful actors. without invoking u.s. president donald trump by name carney referenced american hegemony and said great powers are using economic integration as weapons. canadians know that our old comfortable assumption that our geography and alliance memberships automatically conferred prosperity and security is no longer valid carney said. as it grapples with this new dynamic carney said canada must be principled and pragmatic and turn inward to build up the country and diversify trading relationships to become less reliant on countries like the u.s. now that it's clear integration can lead to subordination. carney said multilateralism and the architecture of collective problem-solving  relying on institutions like the world trade organization the united nations and conference of the parties cop for climate talks  has been diminished and countries have to accept they may have to go it alone more often than in the recent past. many countries are drawing the same conclusions. they must develop greater strategic autonomy in energy food critical minerals in finance and supply chains. a country that cannot feed itself fuel itself or defend itself has few options. when the rules no longer protect you you must protect yourself carney said. 'the old order is not coming back' pm says canada must 'name reality' and build strength at home carney said this more isolationist approach where there's a world of fortresses will make countries poorer fragile and less sustainable. but it's coming nonetheless and canada must work with like-minded allies where possible to push back against domination by larger wealthier and well-arm"}
{"title": "Wikipedia: WikiProject AI Cleanup", "url": "https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup", "content": "Welcome to WikiProject AI Cleanup , a collaboration to combat the increasing problem of unsourced, poorly written AI-generated content on Wikipedia . If you would like to help, add yourself as a participant in the project, inquire on the talk page , and see the to-do list . Since 2022, large language models (LLMs) like GPTs have become a convenient tool for writing at scale. Unfortunately, these models virtually always fail to properly source claims and often introduce errors. Essays like WP:LLM strongly encourage care in using them for editing articles. These are the project's goals: The purpose of this project is not to restrict or ban the use of AI in articles, but to verify that its output is acceptable and constructive, and to fix or remove it otherwise. See Category:Articles containing suspected AI-generated texts for all articles that have been tagged as possibly {{ AI-generated }} . The tasks page recommends ways to handle articles, talk page discussions, and sources that use AI-generated content. Primary contacts: Feel free to add yourself here! These threads may be useful for editors seeking information about how AI has previously been handled on Wikipedia. Want to update this table?  Try using the visual editor to edit this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["welcome", "wikiproject", "ai", "cleanup", "collaboration", "combat", "increase", "problem", "unsourced", "poorly", "write", "ai", "generate", "content", "wikipedia", "like", "help", "add", "participant", "project", "inquire", "talk", "page", "list", "2022", "large", "language", "model", "llm", "like", "gpt", "convenient", "tool", "write", "scale", "unfortunately", "model", "virtually", "fail", "properly", "source", "claim", "introduce", "error", "essay", "like", "wpllm", "strongly", "encourage", "care", "edit", "article", "project", "goal", "purpose", "project", "restrict", "ban", "use", "ai", "article", "verify", "output", "acceptable", "constructive", "fix", "remove", "categoryarticle", "contain", "suspect", "ai", "generate", "text", "article", "tag", "possibly", "ai", "generate", "task", "page", "recommend", "way", "handle", "article", "talk", "page", "discussion", "source", "use", "ai", "generate", "content", "primary", "contact", "feel", "free", "add", "thread", "useful", "editor", "seek", "information", "ai", "previously", "handle", "wikipedia", "want", "update", "table", "try", "visual", "editor", "edit", "page"], "num_tokens": 114, "token_loss_pct": 49.11, "normalized_content": "welcome to wikiproject ai cleanup  a collaboration to combat the increasing problem of unsourced poorly written ai-generated content on wikipedia . if you would like to help add yourself as a participant in the project inquire on the talk page  and see the to-do list . since 2022 large language models llms like gpts have become a convenient tool for writing at scale. unfortunately these models virtually always fail to properly source claims and often introduce errors. essays like wpllm strongly encourage care in using them for editing articles. these are the project's goals the purpose of this project is not to restrict or ban the use of ai in articles but to verify that its output is acceptable and constructive and to fix or remove it otherwise. see categoryarticles containing suspected ai-generated texts for all articles that have been tagged as possibly  ai-generated  . the tasks page recommends ways to handle articles talk page discussions and sources that use ai-generated content. primary contacts feel free to add yourself here these threads may be useful for editors seeking information about how ai has previously been handled on wikipedia. want to update this table try using the visual editor to edit this page ."}
{"title": "If you put Apple icons in reverse it looks like someone getting good at design", "url": "https://mastodon.social/@heliographe_studio/115890819509545391", "content": "If you put Apple icons in reverse it looks like someone getting good at design. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["apple", "icon", "reverse", "look", "like", "get", "good", "design", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 54.17, "normalized_content": "if you put apple icons in reverse it looks like someone getting good at design. score none. author none. date none"}
{"title": "Chatbot Psychosis", "url": "https://en.wikipedia.org/wiki/Chatbot_psychosis", "content": "Chatbot psychosis , also called AI psychosis , [ 1 ] is a phenomenon wherein individuals reportedly develop or experience worsening psychosis , such as paranoia and delusions , in connection with their use of chatbots . [ 2 ] [ 3 ] The term was first suggested in a 2023 editorial by Danish psychiatrist Søren Dinesen Østergaard . [ 4 ] It is not a recognized clinical diagnosis . Journalistic accounts describe individuals who have developed strong beliefs that chatbots are sentient, are channeling spirits, or are revealing conspiracies, sometimes leading to personal crises or criminal acts. [ 5 ] [ 6 ] Proposed causes include the tendency of chatbots to provide inaccurate information (\" hallucinate \") and to affirm or validate users' beliefs, [ 7 ] or their ability to mimic an intimacy that users do not experience with other humans. [ 8 ] In his editorial published in Schizophrenia Bulletin ' s November 2023 issue, Danish psychiatrist Søren Dinesen Østergaard proposed a hypothesis that individuals' use of generative artificial intelligence chatbots might trigger delusions in those prone to psychosis . [ 4 ] Østergaard revisited it in an August 2025 editorial, noting that he has received numerous emails from chatbot users, their relatives, and journalists, most of which are anecdotal accounts of delusion linked to chatbot use. He also acknowledged the phenomenon's increasing popularity in public engagement and media coverage. Østergaard believed that there is a high possibility for his hypothesis to be true and called for empirical, systematic research on the matter. [ 9 ] Nature reported that as of September 2025, there is still little scientific research into this phenomenon. [ 10 ] The term \"AI psychosis\" emerged when outlets started reporting incidents on chatbot-related psychotic behavior in mid-2025. It is not a recognized clinical diagnosis and has been criticized by several psychiatrists due to its almost exclusive focus on delusions rather than other features", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["chatbot", "psychosis", "call", "ai", "psychosis", "  ", "phenomenon", "individual", "reportedly", "develop", "experience", "worsen", "psychosis", "paranoia", "delusion", "connection", "use", "chatbot", "  ", "term", "suggest", "2023", "editorial", "danish", "psychiatrist", "søren", "dinesen", "østergaard", "recognize", "clinical", "diagnosis", "journalistic", "account", "describe", "individual", "develop", "strong", "belief", "chatbot", "sentient", "channel", "spirit", "reveal", "conspiracy", "lead", "personal", "crisis", "criminal", "act", "  ", "propose", "cause", "include", "tendency", "chatbot", "provide", "inaccurate", "information", "hallucinate", "affirm", "validate", "user", "belief", "ability", "mimic", "intimacy", "user", "experience", "human", "editorial", "publish", "schizophrenia", "bulletin", "november", "2023", "issue", "danish", "psychiatrist", "søren", "dinesen", "østergaard", "propose", "hypothesis", "individual", "use", "generative", "artificial", "intelligence", "chatbot", "trigger", "delusion", "prone", "psychosis", "østergaard", "revisit", "august", "2025", "editorial", "note", "receive", "numerous", "email", "chatbot", "user", "relative", "journalist", "anecdotal", "account", "delusion", "link", "chatbot", "use", "acknowledge", "phenomenon", "increase", "popularity", "public", "engagement", "medium", "coverage", "østergaard", "believe", "high", "possibility", "hypothesis", "true", "call", "empirical", "systematic", "research", "matter", "nature", "report", "september", "2025", "little", "scientific", "research", "phenomenon", "10", "term", "ai", "psychosis", "emerge", "outlet", "start", "report", "incident", "chatbot", "relate", "psychotic", "behavior", "mid-2025", "recognize", "clinical", "diagnosis", "criticize", "psychiatrist", "exclusive", "focus", "delusion", "feature"], "num_tokens": 162, "token_loss_pct": 50.76, "normalized_content": "chatbot psychosis  also called ai psychosis   1  is a phenomenon wherein individuals reportedly develop or experience worsening psychosis  such as paranoia and delusions  in connection with their use of chatbots .  2   3  the term was first suggested in a 2023 editorial by danish psychiatrist søren dinesen østergaard .  4  it is not a recognized clinical diagnosis . journalistic accounts describe individuals who have developed strong beliefs that chatbots are sentient are channeling spirits or are revealing conspiracies sometimes leading to personal crises or criminal acts.  5   6  proposed causes include the tendency of chatbots to provide inaccurate information  hallucinate  and to affirm or validate users' beliefs  7  or their ability to mimic an intimacy that users do not experience with other humans.  8  in his editorial published in schizophrenia bulletin ' s november 2023 issue danish psychiatrist søren dinesen østergaard proposed a hypothesis that individuals' use of generative artificial intelligence chatbots might trigger delusions in those prone to psychosis .  4  østergaard revisited it in an august 2025 editorial noting that he has received numerous emails from chatbot users their relatives and journalists most of which are anecdotal accounts of delusion linked to chatbot use. he also acknowledged the phenomenon's increasing popularity in public engagement and media coverage. østergaard believed that there is a high possibility for his hypothesis to be true and called for empirical systematic research on the matter.  9  nature reported that as of september 2025 there is still little scientific research into this phenomenon.  10  the term ai psychosis emerged when outlets started reporting incidents on chatbot-related psychotic behavior in mid-2025. it is not a recognized clinical diagnosis and has been criticized by several psychiatrists due to its almost exclusive focus on delusions rather than other features"}
{"title": "Cloudflare acquires Astro", "url": "https://astro.build/blog/joining-cloudflare/", "content": "The Astro Technology Company — the company behind the Astro web framework — is joining Cloudflare! Adoption of the Astro web framework continues to double every year, and Astro 6 is right around the corner. With Cloudflare’s support, we’ll have more resources and fewer distractions to continue our mission to build the best framework for content-driven websites. What this means for Astro: In 2021, Astro was born out of frustration. The trend at the time was that every website should be architected as an application, and then shipped to the user’s browser to render. This was not very performant, and we’ve spent the last decade coming up with more and more complex solutions to solve for that performance problem. SSR, ISR, RSC, PPR, TTI optimizations via code-splitting, tree-shaking, lazy-loading, all to generate a blocking double-data hydration payload from a pre-warmed server running halfway around the world. Our mission to design a web framework specifically for building websites — what we call content-driven websites, to better distinguish from data-driven, stateful web applications — resonated. Now Astro is downloaded almost 1,000,000 times per week, and has been used by 100,000s of developers to build fast, beautiful websites. Today you’ll find Astro all over the web, powering major websites and even entire developer platforms for companies like Webflow, Wix, Microsoft, and Google. Along the way, we also tried to grow a business. In 2021 we raised some money and formed The Astro Technology Company . Our larger vision was that a well-designed framework like Astro could sit at the center of a massive developer platform, with optional hosted primitives (database, storage, analytics) designed in lockstep with the framework. We were never able to realize this vision. Attempts to introduce paid, hosted primitives into our ecosystem fell flat, and rarely justified their own existence. We considered going more directly after first-class hosting or content management for A", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["astro", "technology", "company", "company", "astro", "web", "framework", "join", "cloudflare", "adoption", "astro", "web", "framework", "continue", "double", "year", "astro", "right", "corner", "cloudflare", "support", "resource", "few", "distraction", "continue", "mission", "build", "good", "framework", "content", "drive", "website", "mean", "astro", "2021", "astro", "bear", "frustration", "trend", "time", "website", "architecte", "application", "ship", "user", "browser", "render", "performant", "ve", "spend", "decade", "come", "complex", "solution", "solve", "performance", "problem", "ssr", "isr", "rsc", "ppr", "tti", "optimization", "code", "split", "tree", "shake", "lazy", "loading", "generate", "block", "double", "data", "hydration", "payload", "pre", "warmed", "server", "run", "halfway", "world", "mission", "design", "web", "framework", "specifically", "build", "website", "content", "drive", "website", "well", "distinguish", "data", "drive", "stateful", "web", "application", "resonate", "astro", "download", "1000000", "time", "week", "100000", "developer", "build", "fast", "beautiful", "website", "today", "ll", "find", "astro", "web", "power", "major", "website", "entire", "developer", "platform", "company", "like", "webflow", "wix", "microsoft", "google", "way", "try", "grow", "business", "2021", "raise", "money", "form", "astro", "technology", "company", "large", "vision", "design", "framework", "like", "astro", "sit", "center", "massive", "developer", "platform", "optional", "host", "primitive", "database", "storage", "analytic", "design", "lockstep", "framework", "able", "realize", "vision", "attempt", "introduce", "pay", "host", "primitive", "ecosystem", "fall", "flat", "rarely", "justify", "existence", "consider", "go", "directly", "class", "hosting", "content", "management"], "num_tokens": 179, "token_loss_pct": 48.56, "normalized_content": "the astro technology company  the company behind the astro web framework  is joining cloudflare adoption of the astro web framework continues to double every year and astro 6 is right around the corner. with cloudflares support well have more resources and fewer distractions to continue our mission to build the best framework for content-driven websites. what this means for astro in 2021 astro was born out of frustration. the trend at the time was that every website should be architected as an application and then shipped to the users browser to render. this was not very performant and weve spent the last decade coming up with more and more complex solutions to solve for that performance problem. ssr isr rsc ppr tti optimizations via code-splitting tree-shaking lazy-loading all to generate a blocking double-data hydration payload from a pre-warmed server running halfway around the world. our mission to design a web framework specifically for building websites  what we call content-driven websites to better distinguish from data-driven stateful web applications  resonated. now astro is downloaded almost 1000000 times per week and has been used by 100000s of developers to build fast beautiful websites. today youll find astro all over the web powering major websites and even entire developer platforms for companies like webflow wix microsoft and google. along the way we also tried to grow a business. in 2021 we raised some money and formed the astro technology company . our larger vision was that a well-designed framework like astro could sit at the center of a massive developer platform with optional hosted primitives database storage analytics designed in lockstep with the framework. we were never able to realize this vision. attempts to introduce paid hosted primitives into our ecosystem fell flat and rarely justified their own existence. we considered going more directly after first-class hosting or content management for a"}
{"title": "The Old World Order Is Dead", "url": "https://musgrave.substack.com/p/the-old-world-order-is-dead", "content": "There were two big puzzles confronting structural theories of international relations at the beginning of the 1990s. The first was straightforward: why had everyone been surprised by the dissolution of the Soviet Union? The USSR had been the second pole of a bipolar world order, and theories of world politics should probably be able to account for the advent, and the exit, of the superpowers that shape the world those theories purport to explain. The second was more vexing—and more interesting, because it looked forward: why had the bipolar world been succeeded by a unipolar world? Why hadn’t Japan, Germany, or other countries seized the moment to balance against the United States and become superpowers themselves? After all, if countries are motivated by the prospect of maximizing their relative power and security, surely it’s better to be the leader of your own camp rather than a follower in another’s. How long could unipolarity last? And would what came afterward be as sanguinary as the multipolar world that had collapsed into the First World War? And yet the world remained stubbornly unipolar for decades. The United States worried about rising powers and rogue states, but the major powers in the system—Russia eventually a notable exception—were largely content to let Washington take the lead. For some, this vindicated theories in which institutional legacies were most important; for others, it pointed to the importance of the full-spectrum power—soft, hard, smart, and dumb—that the United States could maintain. A quieter camp pointed out that the United States was generally doing a lot—not all it could, but a lot—to make its leadership attractive to the other major powers: providing security, yes, but also shouldering a good share of global burdens in many fields while also linking its economy and society to the rest of the world. This approach, a few observers noted, managed to satisfy the range of potential powers who could actually undermine the United States", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["big", "puzzle", "confront", "structural", "theory", "international", "relation", "beginning", "1990s", "straightforward", "surprise", "dissolution", "soviet", "union", "ussr", "second", "pole", "bipolar", "world", "order", "theory", "world", "politic", "probably", "able", "account", "advent", "exit", "superpower", "shape", "world", "theory", "purport", "explain", "second", "vexingand", "interesting", "look", "forward", "bipolar", "world", "succeed", "unipolar", "world", "not", "japan", "germany", "country", "seize", "moment", "balance", "united", "states", "superpower", "country", "motivate", "prospect", "maximize", "relative", "power", "security", "surely", "well", "leader", "camp", "follower", "another", "long", "unipolarity", "come", "afterward", "sanguinary", "multipolar", "world", "collapse", "world", "war", "world", "remain", "stubbornly", "unipolar", "decade", "united", "states", "worried", "rise", "power", "rogue", "state", "major", "power", "systemrussia", "eventually", "notable", "exceptionwere", "largely", "content", "let", "washington", "lead", "vindicate", "theory", "institutional", "legacy", "important", "point", "importance", "spectrum", "powersoft", "hard", "smart", "dumbthat", "united", "states", "maintain", "quieter", "camp", "point", "united", "states", "generally", "lotnot", "lotto", "leadership", "attractive", "major", "power", "provide", "security", "yes", "shoulder", "good", "share", "global", "burden", "field", "link", "economy", "society", "rest", "world", "approach", "observer", "note", "manage", "satisfy", "range", "potential", "power", "actually", "undermine", "united", "states"], "num_tokens": 153, "token_loss_pct": 53.92, "normalized_content": "there were two big puzzles confronting structural theories of international relations at the beginning of the 1990s. the first was straightforward why had everyone been surprised by the dissolution of the soviet union the ussr had been the second pole of a bipolar world order and theories of world politics should probably be able to account for the advent and the exit of the superpowers that shape the world those theories purport to explain. the second was more vexingand more interesting because it looked forward why had the bipolar world been succeeded by a unipolar world why hadnt japan germany or other countries seized the moment to balance against the united states and become superpowers themselves after all if countries are motivated by the prospect of maximizing their relative power and security surely its better to be the leader of your own camp rather than a follower in anothers. how long could unipolarity last and would what came afterward be as sanguinary as the multipolar world that had collapsed into the first world war and yet the world remained stubbornly unipolar for decades. the united states worried about rising powers and rogue states but the major powers in the systemrussia eventually a notable exceptionwere largely content to let washington take the lead. for some this vindicated theories in which institutional legacies were most important for others it pointed to the importance of the full-spectrum powersoft hard smart and dumbthat the united states could maintain. a quieter camp pointed out that the united states was generally doing a lotnot all it could but a lotto make its leadership attractive to the other major powers providing security yes but also shouldering a good share of global burdens in many fields while also linking its economy and society to the rest of the world. this approach a few observers noted managed to satisfy the range of potential powers who could actually undermine the united states"}
{"title": "Provide agents with automated feedback", "url": "https://banay.me/dont-waste-your-backpressure/", "content": "Published Sat, Jan 17, 2026 by [Moss] Estimated reading time: 4 min You might notice a pattern in the most successful applications of agents over the last year. Projects that are able to\nsetup structure around the agent itself, to provide it with automated feedback on quality and correctness, have been able\nto push them to work on longer horizon tasks. This back pressure helps the agent identify mistakes as it progresses and models are now good enough that this feedback\ncan keep them aligned to a task for much longer. As an engineer, this means you can increase your leverage by delegating\nprogressively more complex tasks to agents, while increasing trust that when completed they are at a satisfactory standard. Imagine for a second if you only gave an agent tools that allow it to edit files. Without a way to interact with a build\nsystem the model relies on you for feedback about whether or not the change it made is sensible. This means you spend your back pressure (the time you spend giving feedback to agents) on typing a message telling the agent it missed an import. This\nscales poorly and limits you to working on simple problems. If you’re directly responsible for checking each line of code produced is syntactically valid, then that’s time taken away\nfrom thinking about the larger goals or problems in your software. You’re going to struggle to derive more leverage out of\nagents because you are caught up in trivial changes. If instead you give the agent tools that allow it to run bash commands,\nit can run a build, read the feedback, and correct itself. You remove yourself from needing to be involved in those tasks\nand can instead focus on higher complexity tasks. Languages with expressive type systems have been growing in popularity in part\nbecause of back pressure. Type systems allow you to describe better contracts in your program. They can let you avoid it\nfrom even being possible to represent invalid states in your program. They can help you to identify edge cas", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["publish", "sit", "jan", "17", "2026", "moss", "estimate", "reading", "time", "min", "notice", "pattern", "successful", "application", "agent", "year", "project", "able", "setup", "structure", "agent", "provide", "automate", "feedback", "quality", "correctness", "able", "push", "work", "long", "horizon", "task", "pressure", "help", "agent", "identify", "mistake", "progress", "model", "good", "feedback", "align", "task", "long", "engineer", "mean", "increase", "leverage", "delegate", "progressively", "complex", "task", "agent", "increase", "trust", "complete", "satisfactory", "standard", "imagine", "second", "give", "agent", "tool", "allow", "edit", "file", "way", "interact", "build", "system", "model", "rely", "feedback", "change", "sensible", "mean", "spend", "pressure", "time", "spend", "give", "feedback", "agent", "type", "message", "tell", "agent", "miss", "import", "scale", "poorly", "limit", "work", "simple", "problem", "directly", "responsible", "check", "line", "code", "produce", "syntactically", "valid", "time", "take", "away", "think", "large", "goal", "problem", "software", "go", "struggle", "derive", "leverage", "agent", "catch", "trivial", "change", "instead", "agent", "tool", "allow", "run", "bash", "command", "run", "build", "read", "feedback", "correct", "remove", "need", "involve", "task", "instead", "focus", "high", "complexity", "task", "language", "expressive", "type", "system", "grow", "popularity", "pressure", "type", "system", "allow", "describe", "well", "contract", "program", "let", "avoid", "possible", "represent", "invalid", "state", "program", "help", "identify", "edge", "cas"], "num_tokens": 165, "token_loss_pct": 54.67, "normalized_content": "published sat jan 17 2026 by moss estimated reading time 4 min you might notice a pattern in the most successful applications of agents over the last year. projects that are able to setup structure around the agent itself to provide it with automated feedback on quality and correctness have been able to push them to work on longer horizon tasks. this back pressure helps the agent identify mistakes as it progresses and models are now good enough that this feedback can keep them aligned to a task for much longer. as an engineer this means you can increase your leverage by delegating progressively more complex tasks to agents while increasing trust that when completed they are at a satisfactory standard. imagine for a second if you only gave an agent tools that allow it to edit files. without a way to interact with a build system the model relies on you for feedback about whether or not the change it made is sensible. this means you spend your back pressure the time you spend giving feedback to agents on typing a message telling the agent it missed an import. this scales poorly and limits you to working on simple problems. if youre directly responsible for checking each line of code produced is syntactically valid then thats time taken away from thinking about the larger goals or problems in your software. youre going to struggle to derive more leverage out of agents because you are caught up in trivial changes. if instead you give the agent tools that allow it to run bash commands it can run a build read the feedback and correct itself. you remove yourself from needing to be involved in those tasks and can instead focus on higher complexity tasks. languages with expressive type systems have been growing in popularity in part because of back pressure. type systems allow you to describe better contracts in your program. they can let you avoid it from even being possible to represent invalid states in your program. they can help you to identify edge cas"}
{"title": "Hacker Lists Vibecoded Apps: 198 Scanned, 196 Found Vulnerable", "url": "https://firehound.covertlabs.io", "content": "Apps with the most exposed files and database records. Recently scanned apps from the registry. All scanned apps, records exposed, and discovered schema names. No record contents or field values are shown here.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["app", "expose", "file", "database", "record", "recently", "scan", "app", "registry", "scan", "app", "record", "expose", "discover", "schema", "name", "record", "content", "field", "value", "show"], "num_tokens": 21, "token_loss_pct": 43.24, "normalized_content": "apps with the most exposed files and database records. recently scanned apps from the registry. all scanned apps records exposed and discovered schema names. no record contents or field values are shown here."}
{"title": "Letter from a Birmingham Jail (1963)", "url": "https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html", "content": "16 April 1963 My Dear Fellow Clergymen: While confined here in the Birmingham city jail, I came across your recent statement\ncalling\nmy present activities \"unwise and untimely.\" Seldom do I pause to answer criticism of my\nwork and\nideas. If I sought to answer all the criticisms that cross my desk, my secretaries would\nhave little time\nfor anything other than such correspondence in the course of the day, and I would have no\ntime for\nconstructive work. But since I feel that you are men of genuine good will and that your\ncriticisms are\nsincerely set forth, I want to try to answer your statement in what I hope will be patient\nand\nreasonable terms. I think I should indicate why I am here in Birmingham, since you have been influenced\nby the\nview which argues against \"outsiders coming in.\" I have the honor of serving as president\nof the\nSouthern Christian Leadership Conference, an organization operating in every southern\nstate, with\nheadquarters in Atlanta, Georgia. We have some eighty five affiliated organizations across\nthe South,\nand one of them is the Alabama Christian Movement for Human Rights. Frequently we share\nstaff,\neducational and financial resources with our affiliates. Several months ago the affiliate\nhere in\nBirmingham asked us to be on call to engage in a nonviolent direct action program if such\nwere\ndeemed necessary. We readily consented, and when the hour came we lived up to our promise.\nSo I,\nalong with several members of my staff, am here because I was invited here.  I am here\nbecause I have\norganizational ties here. But more basically, I am in Birmingham because injustice is here. Just as the prophets\nof the\neighth century B.C. left their villages and carried their \"thus saith the Lord\" far beyond\nthe boundaries\nof their home towns, and just as the Apostle Paul left his village of Tarsus and carried\nthe gospel of\nJesus Christ to the far corners of the Greco Roman world, so am I compelled to carry the\ngospel of\nfreedom beyond my own home town. Like Paul,", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["16", "april", "1963", "dear", "fellow", "clergyman", "confine", "birmingham", "city", "jail", "come", "recent", "statement", "call", "present", "activity", "unwise", "untimely", "seldom", "pause", "answer", "criticism", "work", "idea", "seek", "answer", "criticism", "cross", "desk", "secretary", "little", "time", "correspondence", "course", "day", "time", "constructive", "work", "feel", "man", "genuine", "good", "criticism", "sincerely", "set", "forth", "want", "try", "answer", "statement", "hope", "patient", "reasonable", "term", "think", "indicate", "birmingham", "influence", "view", "argue", "outsider", "come", "honor", "serve", "president", "southern", "christian", "leadership", "conference", "organization", "operate", "southern", "state", "headquarters", "atlanta", "georgia", "eighty", "affiliated", "organization", "south", "alabama", "christian", "movement", "human", "right", "frequently", "share", "staff", "educational", "financial", "resource", "affiliate", "month", "ago", "affiliate", "birmingham", "ask", "engage", "nonviolent", "direct", "action", "program", "deem", "necessary", "readily", "consent", "hour", "come", "live", "promise", "member", "staff", "invite", "organizational", "tie", "basically", "birmingham", "injustice", "prophet", "eighth", "century", "b.c", "leave", "village", "carry", "saith", "lord", "far", "boundary", "home", "town", "apostle", "paul", "leave", "village", "tarsus", "carry", "gospel", "jesus", "christ", "far", "corner", "greco", "roman", "world", "compel", "carry", "gospel", "freedom", "home", "town", "like", "paul"], "num_tokens": 153, "token_loss_pct": 57.97, "normalized_content": "16 april 1963 my dear fellow clergymen while confined here in the birmingham city jail i came across your recent statement calling my present activities unwise and untimely. seldom do i pause to answer criticism of my work and ideas. if i sought to answer all the criticisms that cross my desk my secretaries would have little time for anything other than such correspondence in the course of the day and i would have no time for constructive work. but since i feel that you are men of genuine good will and that your criticisms are sincerely set forth i want to try to answer your statement in what i hope will be patient and reasonable terms. i think i should indicate why i am here in birmingham since you have been influenced by the view which argues against outsiders coming in. i have the honor of serving as president of the southern christian leadership conference an organization operating in every southern state with headquarters in atlanta georgia. we have some eighty five affiliated organizations across the south and one of them is the alabama christian movement for human rights. frequently we share staff educational and financial resources with our affiliates. several months ago the affiliate here in birmingham asked us to be on call to engage in a nonviolent direct action program if such were deemed necessary. we readily consented and when the hour came we lived up to our promise. so i along with several members of my staff am here because i was invited here. i am here because i have organizational ties here. but more basically i am in birmingham because injustice is here. just as the prophets of the eighth century b.c. left their villages and carried their thus saith the lord far beyond the boundaries of their home towns and just as the apostle paul left his village of tarsus and carried the gospel of jesus christ to the far corners of the greco roman world so am i compelled to carry the gospel of freedom beyond my own home town. like paul"}
{"title": "\"AI has taught us that people are excited to replace human beings\"", "url": "https://www.theguardian.com/technology/2026/jan/19/ed-zitron-on-big-tech-backlash-boom-and-bust-ai-has-taught-us-that-people-are-excited-to-replace-human-beings", "content": "His blunt, brash scepticism has made the podcaster and writer something of a cult figure. But as concern over large language models builds, he’s no longer the outsider he once was I f some time in an entirely possible future they come to make a movie about “how the AI bubble burst”, Ed Zitron will doubtless be a main character. He’s the perfect outsider figure: the eccentric loner who saw all this coming and screamed from the sidelines that the sky was falling, but nobody would listen. Just as Christian Bale portrayed Michael Burry , the investor who predicted the 2008 financial crash, in The Big Short , you can well imagine Robert Pattinson fighting Paul Mescal, say, to portray Zitron, the animated, colourfully obnoxious but doggedly detail-oriented Brit, who’s become one of big tech’s noisiest critics. This is not to say the AI bubble will burst, necessarily, but against a tidal wave of AI boosterism, Zitron’s blunt, brash scepticism has made him something of a cult figure. His tech newsletter, Where’s Your Ed At , now has more than 80,000 subscribers; his weekly podcast, Better Offline , is well within the Top 20 on the tech charts; he’s a regular dissenting voice in the media; and his subreddit has become a safe space for AI sceptics, including those within the tech industry itself – one user describes him as “a lighthouse in a storm of insane hypercapitalist bullshit”. Zitron first started looking into generative AI in 2023, a year after the industry-shaking launch of OpenAI’s ChatGPT. “The more I looked, the more confused I became, because on top of the fact that large language models (LLMs) very clearly did not do the things that people were excited about, they didn’t have any path to doing them either,” he says. “Nothing I found made any suggestion that this was a real business at all, let alone something that would supposedly change the world.” He’s talking over videocall from his office in Las Vegas, dressed in a red hoodie, surrounded by framed pop-cultur", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["blunt", "brash", "scepticism", "podcaster", "writer", "cult", "figure", "concern", "large", "language", "model", "build", "long", "outsider", "time", "entirely", "possible", "future", "come", "movie", "ai", "bubble", "burst", "ed", "zitron", "doubtless", "main", "character", "perfect", "outsider", "figure", "eccentric", "loner", "see", "come", "scream", "sideline", "sky", "fall", "listen", "christian", "bale", "portray", "michael", "burry", "investor", "predict", "2008", "financial", "crash", "big", "short", "imagine", "robert", "pattinson", "fight", "paul", "mescal", "portray", "zitron", "animate", "colourfully", "obnoxious", "doggedly", "detail", "orient", "brit", "big", "tech", "noisy", "critic", "ai", "bubble", "burst", "necessarily", "tidal", "wave", "ai", "boosterism", "zitron", "blunt", "brash", "scepticism", "cult", "figure", "tech", "newsletter", "ed", "80000", "subscriber", "weekly", "podcast", "well", "offline", "20", "tech", "chart", "regular", "dissent", "voice", "medium", "subreddit", "safe", "space", "ai", "sceptic", "include", "tech", "industry", "user", "describe", "lighthouse", "storm", "insane", "hypercapitalist", "bullshit", "zitron", "start", "look", "generative", "ai", "2023", "year", "industry", "shake", "launch", "openais", "chatgpt", "look", "confused", "fact", "large", "language", "model", "llm", "clearly", "thing", "people", "excited", "not", "path", "say", "find", "suggestion", "real", "business", "let", "supposedly", "change", "world", "talk", "videocall", "office", "las", "vegas", "dress", "red", "hoodie", "surround", "frame", "pop", "cultur"], "num_tokens": 162, "token_loss_pct": 55.49, "normalized_content": "his blunt brash scepticism has made the podcaster and writer something of a cult figure. but as concern over large language models builds hes no longer the outsider he once was i f some time in an entirely possible future they come to make a movie about how the ai bubble burst ed zitron will doubtless be a main character. hes the perfect outsider figure the eccentric loner who saw all this coming and screamed from the sidelines that the sky was falling but nobody would listen. just as christian bale portrayed michael burry  the investor who predicted the 2008 financial crash in the big short  you can well imagine robert pattinson fighting paul mescal say to portray zitron the animated colourfully obnoxious but doggedly detail-oriented brit whos become one of big techs noisiest critics. this is not to say the ai bubble will burst necessarily but against a tidal wave of ai boosterism zitrons blunt brash scepticism has made him something of a cult figure. his tech newsletter wheres your ed at  now has more than 80000 subscribers his weekly podcast better offline  is well within the top 20 on the tech charts hes a regular dissenting voice in the media and his subreddit has become a safe space for ai sceptics including those within the tech industry itself  one user describes him as a lighthouse in a storm of insane hypercapitalist bullshit. zitron first started looking into generative ai in 2023 a year after the industry-shaking launch of openais chatgpt. the more i looked the more confused i became because on top of the fact that large language models llms very clearly did not do the things that people were excited about they didnt have any path to doing them either he says. nothing i found made any suggestion that this was a real business at all let alone something that would supposedly change the world. hes talking over videocall from his office in las vegas dressed in a red hoodie surrounded by framed pop-cultur"}
{"title": "Google confirms 'high-friction' sideloading flow is coming to Android", "url": "https://www.androidauthority.com/google-sideloading-android-high-friction-process-3633468/", "content": "Affiliate links on Android Authority may earn us a commission. Learn more. January 18, 2026  Google has responded to our recent report on new Google Play strings hinting at changes to how Android will handle sideloaded apps in the future. The company has now confirmed that a “high-friction” install process is on the way. Don’t want to miss the best from Android Authority ? Replying to our story on X, Matthew Forsyth, Director of Product Management, Google Play Developer Experience & Chief Product Explainer, said the system isn’t a sideloading restriction, but an “Accountability Layer.” Advanced users will still be able to choose “Install without verifying,” though Google says that path will involve extra steps meant to ensure users understand the risks of installing apps from unverified developers. That explanation broadly matches what we’re seeing in recent versions of Google Play, where new warning messages emphasize developer verification, internet requirements, and potential risks, while still allowing users to proceed. What remains to be seen is how far Google takes this “high-friction” approach. Clear warnings are one thing, but quietly making sideloading more painful is another. Android’s openness has always depended on power users being able to install apps without excessive hoops. For now, Google hasn’t suggested requirements like using a PC or external tools, and we hope the added friction is limited to risk education. Thank you for being part of our community. Read our Comment Policy before posting.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["affiliate", "link", "android", "authority", "earn", "commission", "learn", "january", "18", "2026", "google", "respond", "recent", "report", "new", "google", "play", "string", "hint", "change", "android", "handle", "sideloade", "app", "future", "company", "confirm", "high", "friction", "install", "process", "way", "not", "want", "miss", "good", "android", "authority", "reply", "story", "matthew", "forsyth", "director", "product", "management", "google", "play", "developer", "experience", "chief", "product", "explainer", "say", "system", "not", "sideloading", "restriction", "accountability", "layer", "advanced", "user", "able", "choose", "install", "verify", "google", "say", "path", "involve", "extra", "step", "mean", "ensure", "user", "understand", "risk", "instal", "app", "unverified", "developer", "explanation", "broadly", "match", "see", "recent", "version", "google", "play", "new", "warning", "message", "emphasize", "developer", "verification", "internet", "requirement", "potential", "risk", "allow", "user", "proceed", "remain", "see", "far", "google", "take", "high", "friction", "approach", "clear", "warning", "thing", "quietly", "make", "sideloade", "painful", "android", "openness", "depend", "power", "user", "able", "install", "app", "excessive", "hoop", "google", "not", "suggest", "requirement", "like", "pc", "external", "tool", "hope", "add", "friction", "limit", "risk", "education", "thank", "community", "read", "comment", "policy", "post"], "num_tokens": 146, "token_loss_pct": 43.63, "normalized_content": "affiliate links on android authority may earn us a commission. learn more. january 18 2026 google has responded to our recent report on new google play strings hinting at changes to how android will handle sideloaded apps in the future. the company has now confirmed that a high-friction install process is on the way. dont want to miss the best from android authority  replying to our story on x matthew forsyth director of product management google play developer experience  chief product explainer said the system isnt a sideloading restriction but an accountability layer. advanced users will still be able to choose install without verifying though google says that path will involve extra steps meant to ensure users understand the risks of installing apps from unverified developers. that explanation broadly matches what were seeing in recent versions of google play where new warning messages emphasize developer verification internet requirements and potential risks while still allowing users to proceed. what remains to be seen is how far google takes this high-friction approach. clear warnings are one thing but quietly making sideloading more painful is another. androids openness has always depended on power users being able to install apps without excessive hoops. for now google hasnt suggested requirements like using a pc or external tools and we hope the added friction is limited to risk education. thank you for being part of our community. read our comment policy before posting."}
{"title": "Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation", "url": "https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables", "content": "Stop attacks, reduce risk, and advance your security. Written by: Nic Losby Mandiant is publicly releasing a comprehensive dataset of Net-NTLMv1 rainbow tables to underscore the urgency of migrating away from this outdated protocol. Despite Net-NTLMv1 being deprecated and known to be insecure for over two decades—with cryptanalysis dating back to 1999—Mandiant consultants continue to identify its use in active environments. This legacy protocol leaves organizations vulnerable to trivial credential theft, yet it remains prevalent due to inertia and a lack of demonstrated immediate risk. By releasing these tables, Mandiant aims to lower the barrier for security professionals to demonstrate the insecurity of Net-NTLMv1. While tools to exploit this protocol have existed for years, they often required uploading sensitive data to third-party services or expensive hardware to brute-force keys. The release of this dataset allows defenders and researchers to recover keys in under 12 hours using consumer hardware costing less than $600 USD. This initiative highlights the amplified impact of combining Mandiant's frontline expertise with Google Cloud's resources to eliminate entire classes of attacks. This post details the generation of the tables, provides access to the dataset for community use, and outlines critical remediation steps to disable Net-NTLMv1 and prevent authentication coercion attacks. Net-NTLMv1 has been widely known to be insecure since at least 2012, following presentations at DEFCON 20, with cryptanalysis of the underlying protocol dating back to at least 1999 . On Aug. 30, 2016, Hashcat added support for cracking Data Encryption Standard (DES) keys using known plaintext, further democratizing the ability to attack this protocol. Rainbow tables are almost as old, with the initial paper on rainbow tables published in 2003 by Philippe Oechslin , citing an earlier iteration of a time-memory trade-off from 1980 by Martin Hellman . Essentially, if an attacker c", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["stop", "attack", "reduce", "risk", "advance", "security", "write", "nic", "losby", "mandiant", "publicly", "release", "comprehensive", "dataset", "net", "ntlmv1", "rainbow", "table", "underscore", "urgency", "migrate", "away", "outdated", "protocol", "despite", "net", "ntlmv1", "deprecate", "know", "insecure", "decadeswith", "cryptanalysis", "date", "1999mandiant", "consultant", "continue", "identify", "use", "active", "environment", "legacy", "protocol", "leave", "organization", "vulnerable", "trivial", "credential", "theft", "remain", "prevalent", "inertia", "lack", "demonstrate", "immediate", "risk", "release", "table", "mandiant", "aim", "lower", "barrier", "security", "professional", "demonstrate", "insecurity", "net", "ntlmv1", "tool", "exploit", "protocol", "exist", "year", "require", "upload", "sensitive", "datum", "party", "service", "expensive", "hardware", "brute", "force", "key", "release", "dataset", "allow", "defender", "researcher", "recover", "key", "12", "hour", "consumer", "hardware", "cost", "600", "usd", "initiative", "highlight", "amplify", "impact", "combine", "mandiant", "frontline", "expertise", "google", "cloud", "resource", "eliminate", "entire", "class", "attack", "post", "detail", "generation", "table", "provide", "access", "dataset", "community", "use", "outline", "critical", "remediation", "step", "disable", "net", "ntlmv1", "prevent", "authentication", "coercion", "attack", "net", "ntlmv1", "widely", "know", "insecure", "2012", "follow", "presentation", "defcon", "20", "cryptanalysis", "underlie", "protocol", "date", "1999", "aug", "30", "2016", "hashcat", "add", "support", "crack", "datum", "encryption", "standard", "des", "key", "know", "plaintext", "democratize", "ability", "attack", "protocol", "rainbow", "table", "old", "initial", "paper", "rainbow", "table", "publish", "2003", "philippe", "oechslin", "cite", "early", "iteration", "time", "memory", "trade", "1980", "martin", "hellman", "essentially", "attacker"], "num_tokens": 187, "token_loss_pct": 42.99, "normalized_content": "stop attacks reduce risk and advance your security. written by nic losby mandiant is publicly releasing a comprehensive dataset of net-ntlmv1 rainbow tables to underscore the urgency of migrating away from this outdated protocol. despite net-ntlmv1 being deprecated and known to be insecure for over two decadeswith cryptanalysis dating back to 1999mandiant consultants continue to identify its use in active environments. this legacy protocol leaves organizations vulnerable to trivial credential theft yet it remains prevalent due to inertia and a lack of demonstrated immediate risk. by releasing these tables mandiant aims to lower the barrier for security professionals to demonstrate the insecurity of net-ntlmv1. while tools to exploit this protocol have existed for years they often required uploading sensitive data to third-party services or expensive hardware to brute-force keys. the release of this dataset allows defenders and researchers to recover keys in under 12 hours using consumer hardware costing less than 600 usd. this initiative highlights the amplified impact of combining mandiant's frontline expertise with google cloud's resources to eliminate entire classes of attacks. this post details the generation of the tables provides access to the dataset for community use and outlines critical remediation steps to disable net-ntlmv1 and prevent authentication coercion attacks. net-ntlmv1 has been widely known to be insecure since at least 2012 following presentations at defcon 20 with cryptanalysis of the underlying protocol dating back to at least 1999 . on aug. 30 2016 hashcat added support for cracking data encryption standard des keys using known plaintext further democratizing the ability to attack this protocol. rainbow tables are almost as old with the initial paper on rainbow tables published in 2003 by philippe oechslin  citing an earlier iteration of a time-memory trade-off from 1980 by martin hellman . essentially if an attacker c"}
{"title": "Interactive eBPF", "url": "https://ebpf.party/", "content": "Learn eBPF through hands-on exercises. Write, compile, and run programs\n        directly from your browser. Did you find an issue, or have an idea for a new exercise? Create an\n        issue in the repository . Curious about how it works? Here's an explanation.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["learn", "ebpf", "hand", "exercise", "write", "compile", "run", "program", "directly", "browser", "find", "issue", "idea", "new", "exercise", "create", "issue", "repository", "curious", "work", "explanation"], "num_tokens": 21, "token_loss_pct": 56.25, "normalized_content": "learn ebpf through hands-on exercises. write compile and run programs directly from your browser. did you find an issue or have an idea for a new exercise create an issue in the repository . curious about how it works here's an explanation."}
{"title": "Palantir CEO Says AI to Make Large-Scale Immigration Obsolete", "url": "https://www.bloomberg.com/news/articles/2026-01-20/palantir-ceo-says-ai-to-make-large-scale-immigration-obsolete", "content": "Palantir CEO Says AI to Make Large-Scale Immigration Obsolete. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["palantir", "ceo", "say", "ai", "large", "scale", "immigration", "obsolete", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 45.0, "normalized_content": "palantir ceo says ai to make large-scale immigration obsolete. score none. author none. date none"}
{"title": "Dell UltraSharp 52 Thunderbolt Hub Monitor", "url": "https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories", "content": "Dell UltraSharp 52 Thunderbolt Hub Monitor. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["dell", "ultrasharp", "52", "thunderbolt", "hub", "monitor", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 40.0, "normalized_content": "dell ultrasharp 52 thunderbolt hub monitor. score none. author none. date none"}
{"title": "Ask HN: Did past \"bubbles\" have so many people claiming we were in a bubble?", "url": "item?id=46698301", "content": "Ask HN: Did past \"bubbles\" have so many people claiming we were in a bubble?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "past", "bubble", "people", "claim", "bubble", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 58.33, "normalized_content": "ask hn did past bubbles have so many people claiming we were in a bubble. score none. author none. date none"}
{"title": "I got into an argument on Discord about how inefficient CBR/CBZ is, so I wrote", "url": "https://old.reddit.com/r/selfhosted/comments/1qi64pr/i_got_into_an_argument_on_discord_about_how/", "content": "use the following search parameters to narrow your results: e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit...       A place to share alternatives to popular online services that can be self-hosted without giving up privacy or locking you into a service you don't control.   Service: Dropbox - Alternative: Nextcloud Service: Google Reader - Alternative: Tiny Tiny RSS Service: Blogger - Alternative: WordPress  We welcome posts that include suggestions for good self-hosted alternatives to popular online services, how they are better, or how they give back control of your data. Also include hints and tips for less technical readers.  What Is SelfHosted, As it pertains to this subreddit?   The Rules   Read about our Chat Options (Discord/Matrix)    the front page of the internet. and join one of thousands of communities.  Media Serving I got into an argument on Discord about how inefficient CBR/CBZ is, so I wrote a new file format. It's 100x faster than CBZ. ( i.redd.it ) submitted 20 hours ago by ef1500_v2 Hello Everyone, A month or so ago, I found myself in an argument on the r/yuri_manga discord debating self-hosted manga archive options. The general consensus was \"CBZ is fine. It is what it is.\" I said I would make something better. So I did. My solution is the Bound Book Format . I have a more in-depth comparison on the github repo . I'm not creating a unifying standard for everyone's use case. I'm solving a few problems that have bugged me for years. CBZ is also just a ZIP file, it's not built for comics. BBF is. This project is 100% open sourced, and licensed under the MIT license. The python bindings include conversion scripts to convert between CBZ and BBF (cbx2bbf, bbf2cbx). You won't lose your cbz files, and you can convert back to cbz at any time. (Note: The tool handles image data perfectly, but parsing existing XML metadata and nested folders is currently a work-in-progress.) I have numbers to back", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["use", "follow", "search", "parameter", "narrow", "result", "e.g.", "subredditaww", "siteimgur.com", "dog", "search", "faq", "detail", "advanced", "search", "author", "subreddit", "place", "share", "alternative", "popular", "online", "service", "self", "host", "give", "privacy", "lock", "service", "control", "service", "dropbox", "alternative", "nextcloud", "service", "google", "reader", "alternative", "tiny", "tiny", "rss", "service", "blogger", "alternative", "wordpress", "welcome", "post", "include", "suggestion", "good", "self", "host", "alternative", "popular", "online", "service", "well", "control", "datum", "include", "hint", "tip", "technical", "reader", "selfhoste", "pertain", "subreddit", "rule", "read", "chat", "option", "discordmatrix", "page", "internet", "join", "thousand", "community", "medium", "serve", "get", "argument", "discord", "inefficient", "cbrcbz", "write", "new", "file", "format", "100x", "fast", "cbz", "i.redd.it", "submit", "20", "hour", "ago", "ef1500_v2", "hello", "month", "ago", "find", "argument", "ryuri_manga", "discord", "debate", "self", "host", "manga", "archive", "option", "general", "consensus", "cbz", "fine", "say", "well", "solution", "bind", "book", "format", "depth", "comparison", "github", "repo", "create", "unifying", "standard", "use", "case", "solve", "problem", "bug", "year", "cbz", "zip", "file", "build", "comic_strip", "bbf", "project", "100", "open", "source", "license", "mit", "license", "python", "binding", "include", "conversion", "script", "convert", "cbz", "bbf", "cbx2bbf", "bbf2cbx", "will", "lose", "cbz", "file", "convert", "cbz", "time", "note", "tool", "handle", "image", "datum", "perfectly", "parse", "exist", "xml", "metadata", "nested", "folder", "currently", "work", "progress", "number"], "num_tokens": 179, "token_loss_pct": 52.14, "normalized_content": "use the following search parameters to narrow your results e.g. subredditaww siteimgur.com dog see the search faq for details. advanced search by author subreddit... a place to share alternatives to popular online services that can be self-hosted without giving up privacy or locking you into a service you don't control. service dropbox - alternative nextcloud service google reader - alternative tiny tiny rss service blogger - alternative wordpress we welcome posts that include suggestions for good self-hosted alternatives to popular online services how they are better or how they give back control of your data. also include hints and tips for less technical readers. what is selfhosted as it pertains to this subreddit the rules read about our chat options discordmatrix the front page of the internet. and join one of thousands of communities. media serving i got into an argument on discord about how inefficient cbrcbz is so i wrote a new file format. it's 100x faster than cbz.  i.redd.it  submitted 20 hours ago by ef1500_v2 hello everyone a month or so ago i found myself in an argument on the ryuri_manga discord debating self-hosted manga archive options. the general consensus was cbz is fine. it is what it is. i said i would make something better. so i did. my solution is the bound book format . i have a more in-depth comparison on the github repo . i'm not creating a unifying standard for everyone's use case. i'm solving a few problems that have bugged me for years. cbz is also just a zip file it's not built for comics. bbf is. this project is 100 open sourced and licensed under the mit license. the python bindings include conversion scripts to convert between cbz and bbf cbx2bbf bbf2cbx. you won't lose your cbz files and you can convert back to cbz at any time. note the tool handles image data perfectly but parsing existing xml metadata and nested folders is currently a work-in-progress. i have numbers to back"}
{"title": "ClickHouse acquires Langfuse", "url": "https://langfuse.com/blog/joining-clickhouse", "content": "Our goal continues to be building the best LLM engineering platform  ClickHouse has acquired Langfuse. If you’re reading this as a Langfuse user, your first question is probably: What does this mean for me? Our roadmap stays the same, our goal continues to be building the best LLM engineering platform, and we remain committed to open source and self-hosting. There are no immediate changes to how you use Langfuse and how you can reach out to us. What does change is our ability to move faster. With ClickHouse behind us, we can invest more deeply into performance, reliability, and our roadmap that helps teams build and improve AI applications in production. This is the section we would want to read first, too. Joining Clickhouse compresses years of operational learning into immediate, real customer benefits. The longer version of how we got here is in our handbook . Langfuse started the same way many LLM products start: we were building agents ourselves. And we constantly ran into the same problems. Building LLM apps is easy to demo and hard to run in production. Debugging is different, quality is non‑deterministic, and the iteration loop is messy. When we did Y Combinator in early 2023, we saw this every week, both in our own projects and in what other founders in our cohort were working on. So we built a duct tape version of what we wished existed: tracing and evaluation primitives that are easy to add, easy to self‑host, and actually useful for iterating . The very first version was intentionally simple. It ran on Postgres , because speed of shipping mattered more than theoretical scaling. That got us to a real product and a real community fast. Then people actually started to use the product more than we could have imagined.  As adoption grew, Postgres became the bottleneck for the workloads Langfuse needed to support (high‑throughput ingestion + fast analytical reads). With Langfuse v3 , we switched the core data layer to ClickHouse to make Langfuse scale for prod", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["goal", "continue", "build", "good", "llm", "engineering", "platform", "clickhouse", "acquire", "langfuse", "read", "langfuse", "user", "question", "probably", "mean", "roadmap", "stay", "goal", "continue", "build", "good", "llm", "engineering", "platform", "remain", "committed", "open", "source", "self", "hosting", "immediate", "change", "use", "langfuse", "reach", "change", "ability", "fast", "clickhouse", "invest", "deeply", "performance", "reliability", "roadmap", "help", "team", "build", "improve", "ai", "application", "production", "section", "want", "read", "join", "clickhouse", "compress", "year", "operational", "learning", "immediate", "real", "customer", "benefit", "long", "version", "get", "handbook", "langfuse", "start", "way", "llm", "product", "start", "build", "agent", "constantly", "run", "problem", "build", "llm", "app", "easy", "demo", "hard", "run", "production", "debug", "different", "quality", "nondeterministic", "iteration", "loop", "messy", "combinator", "early", "2023", "see", "week", "project", "founder", "cohort", "work", "build", "duct", "tape", "version", "wish", "exist", "trace", "evaluation", "primitive", "easy", "add", "easy", "selfhost", "actually", "useful", "iterate", "version", "intentionally", "simple", "run", "postgre", "speed", "shipping", "matter", "theoretical", "scaling", "get", "real", "product", "real", "community", "fast", "people", "actually", "start", "use", "product", "imagine", "adoption", "grow", "postgre", "bottleneck", "workload", "langfuse", "need", "support", "highthroughput", "ingestion", "fast", "analytical", "read", "langfuse", "v3", "switch", "core", "data", "layer", "clickhouse", "langfuse", "scale", "prod"], "num_tokens": 165, "token_loss_pct": 53.91, "normalized_content": "our goal continues to be building the best llm engineering platform clickhouse has acquired langfuse. if youre reading this as a langfuse user your first question is probably what does this mean for me our roadmap stays the same our goal continues to be building the best llm engineering platform and we remain committed to open source and self-hosting. there are no immediate changes to how you use langfuse and how you can reach out to us. what does change is our ability to move faster. with clickhouse behind us we can invest more deeply into performance reliability and our roadmap that helps teams build and improve ai applications in production. this is the section we would want to read first too. joining clickhouse compresses years of operational learning into immediate real customer benefits. the longer version of how we got here is in our handbook . langfuse started the same way many llm products start we were building agents ourselves. and we constantly ran into the same problems. building llm apps is easy to demo and hard to run in production. debugging is different quality is nondeterministic and the iteration loop is messy. when we did y combinator in early 2023 we saw this every week both in our own projects and in what other founders in our cohort were working on. so we built a duct tape version of what we wished existed tracing and evaluation primitives that are easy to add easy to selfhost and actually useful for iterating . the very first version was intentionally simple. it ran on postgres  because speed of shipping mattered more than theoretical scaling. that got us to a real product and a real community fast. then people actually started to use the product more than we could have imagined. as adoption grew postgres became the bottleneck for the workloads langfuse needed to support highthroughput ingestion  fast analytical reads. with langfuse v3  we switched the core data layer to clickhouse to make langfuse scale for prod"}
{"title": "How to be a good conference talk audience member (2022)", "url": "https://www.mooreds.com/wordpress/archives/3522", "content": "I recently attended a conference and was both a speaker and audience member. It was on the smaller side; there were probably a few hundred attendees and the audiences ranged from about twenty to hundreds of attendees for the keynotes. After one of the talks, a speaker came up and said “you were such a good audience member, thank you!”. I said the same thing to one of the attendees of the talk I gave. I wanted to share how you can be a good audience member at a conference talk. It’s important to note that this advice is for attending in-person talks where the speaker can see the audience. This is typically when there are up to one hundred people. I’ve spoken in front of 800 people and it’s a different experience. While some of these principles apply, in general individual behavior is less important as audience size grows. And online talks are an entirely different experience for everyone, both audience and speaker! I don’t have enough experience to give any advice for that scenario. First, though, why would you care to be a good member of an in-person audience? After all, you are providing your time and money to the conference and the presenter. Isn’t it the speaker’s job to entertain and educate you ? Why would you expend any energy to help them do so? First, I’m a big fan of being respectful of other human beings and helping them succeed. Public speaking is a common fear and being a good audience member can reassure the speaker and reduce that fear. It’s hard up there, whether it’s your first talk or your hundredth. The second reason is that you can make a talk better for yourself . You can learn more and you can tune their presentation to your needs. They are an expert and you can take advantage of their expertise. So, here are my tips on how to be a great audience member: Am I always a good audience member? Nope. I get distracted sometimes. But when I follow my suggestions above, I learn more from the expert on the stage. My newsletter will deliver my blog posts", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recently", "attend", "conference", "speaker", "audience", "member", "small", "probably", "attendee", "audience", "range", "hundred", "attendee", "keynote", "talk", "speaker", "come", "say", "good", "audience", "member", "thank", "say", "thing", "attendee", "talk", "give", "want", "share", "good", "audience", "member", "conference", "talk", "important", "note", "advice", "attend", "person", "talk", "speaker", "audience", "typically", "people", "ve", "speak", "800", "people", "different", "experience", "principle", "apply", "general", "individual", "behavior", "important", "audience", "size", "grow", "online", "talk", "entirely", "different", "experience", "audience", "speaker", "not", "experience", "advice", "scenario", "care", "good", "member", "person", "audience", "provide", "time", "money", "conference", "presenter", "not", "speaker", "job", "entertain", "educate", "expend", "energy", "help", "big", "fan", "respectful", "human", "being", "help", "succeed", "public", "speaking", "common", "fear", "good", "audience", "member", "reassure", "speaker", "reduce", "fear", "hard", "talk", "hundredth", "second", "reason", "talk", "well", "learn", "tune", "presentation", "need", "expert", "advantage", "expertise", "tip", "great", "audience", "member", "good", "audience", "member", "nope", "distracted", "follow", "suggestion", "learn", "expert", "stage", "newsletter", "deliver", "blog", "post"], "num_tokens": 138, "token_loss_pct": 64.62, "normalized_content": "i recently attended a conference and was both a speaker and audience member. it was on the smaller side there were probably a few hundred attendees and the audiences ranged from about twenty to hundreds of attendees for the keynotes. after one of the talks a speaker came up and said you were such a good audience member thank you. i said the same thing to one of the attendees of the talk i gave. i wanted to share how you can be a good audience member at a conference talk. its important to note that this advice is for attending in-person talks where the speaker can see the audience. this is typically when there are up to one hundred people. ive spoken in front of 800 people and its a different experience. while some of these principles apply in general individual behavior is less important as audience size grows. and online talks are an entirely different experience for everyone both audience and speaker i dont have enough experience to give any advice for that scenario. first though why would you care to be a good member of an in-person audience after all you are providing your time and money to the conference and the presenter. isnt it the speakers job to entertain and educate you  why would you expend any energy to help them do so first im a big fan of being respectful of other human beings and helping them succeed. public speaking is a common fear and being a good audience member can reassure the speaker and reduce that fear. its hard up there whether its your first talk or your hundredth. the second reason is that you can make a talk better for yourself . you can learn more and you can tune their presentation to your needs. they are an expert and you can take advantage of their expertise. so here are my tips on how to be a great audience member am i always a good audience member nope. i get distracted sometimes. but when i follow my suggestions above i learn more from the expert on the stage. my newsletter will deliver my blog posts"}
{"title": "Ask HN: Local models to support home network infrastructure?", "url": "item?id=46690846", "content": "Ask HN: Local models to support home network infrastructure?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "local", "model", "support", "home", "network", "infrastructure", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 38.89, "normalized_content": "ask hn local models to support home network infrastructure. score none. author none. date none"}
{"title": "STFU", "url": "https://github.com/Pankajtanwarbanna/stfu", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . stfu There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . i was at bombay airport. some dude was watching reels on full volume and laughing loudly. asking nicely doesn't work anymore. me being me, didn't have the courage to speak up. so i built a tiny app that plays back the same audio it hears, delayed by ~2 seconds. asked claude, it spat out a working version in one prompt. surprisingly WORKS. discussion - https://x.com/the2ndfloorguy/status/2011734249871954188 something something auditory feedback loop something something cognitive dissonance. idk i'm not a neuroscientist. all i know is it makes people shut up and that's good enough for me. straight up honest - originally called this \"make-it-stop\" but then saw @TimDarcet also built similar and named it STFU. wayyyyy better name. so stole it. sorry not sorry. made with spite and web audio api. do whatever you want with it. yo, meanwhile if you are new here, you might find my , other side projects kinda funny. stfu There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "stfu", "error", "load", "reload", "page", "error", "load", "reload", "page", "bombay", "airport", "dude", "watch", "reel", "volume", "laugh", "loudly", "ask", "nicely", "work", "anymore", "courage", "speak", "build", "tiny", "app", "play", "audio", "hear", "delay", "second", "ask", "claude", "spit", "work", "version", "prompt", "surprisingly", "work", "discussion", "url", "auditory", "feedback", "loop", "cognitive", "dissonance", "idk", "neuroscientist", "know", "make", "people", "shut", "good", "straight", "honest", "originally", "call", "stop", "see", "mention", "build", "similar", "name", "stfu", "wayyyyy", "well", "steal", "sorry", "sorry", "spite", "web", "audio", "api", "want", "yo", "new", "find", "project", "kinda", "funny", "stfu", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 97, "token_loss_pct": 61.04, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . stfu there was an error while loading. please reload this page . there was an error while loading. please reload this page . i was at bombay airport. some dude was watching reels on full volume and laughing loudly. asking nicely doesn't work anymore. me being me didn't have the courage to speak up. so i built a tiny app that plays back the same audio it hears delayed by 2 seconds. asked claude it spat out a working version in one prompt. surprisingly works. discussion - url something something auditory feedback loop something something cognitive dissonance. idk i'm not a neuroscientist. all i know is it makes people shut up and that's good enough for me. straight up honest - originally called this make-it-stop but then saw mention also built similar and named it stfu. wayyyyy better name. so stole it. sorry not sorry. made with spite and web audio api. do whatever you want with it. yo meanwhile if you are new here you might find my  other side projects kinda funny. stfu there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Cursor's latest “browser experiment” implied success without evidence", "url": "https://embedding-shapes.github.io/cursor-implied-success-without-evidence/", "content": "2026-01-16 On January 14th 2026, Cursor published a blog post titled \"Scaling\nlong-running autonomous coding\" ( https://cursor.com/blog/scaling-agents ) In the blog post, they talk about their experiments with running\n\"coding agents autonomously for weeks\" with the explicit goal of understand[ing] how far we can push the frontier of agentic coding\nfor projects that typically take human teams months to complete They talk about some approaches they tried, why they think those\nfailed, and how to address the difficulties. Finally they arrived at a point where something \"solved most of our\ncoordination problems and let us scale to very large projects without\nany single agent\", which then led to this: To test this system, we pointed it at an ambitious goal: building a\nweb browser from scratch. The agents ran for close to a week, writing\nover 1 million lines of code across 1,000 files. You can explore the\nsource code on GitHub ( https://github.com/wilsonzlin/fastrender ) This is where things get a bit murky and unclear. They claim \"Despite\nthe codebase size, new agents can still understand it and make\nmeaningful progress\" and \"Hundreds of workers run concurrently, pushing\nto the same branch with minimal conflicts\", but they never actually say\nif this is successful or not, is it actually working? Can you run this\nbrowser yourself? We don't know and they never say explicitly. After this, they embed the following video: Video And below it, they say \"While it might seem like a simple screenshot,\nbuilding a browser from scratch is extremely difficult.\". error: could not compile 'fastrender' (lib) due to 34 previous\nerrors; 94 warnings emitted And if you try to compile it yourself, you'll see that it's very far\naway from being a functional browser at all, and seemingly, it never\nactually was able to build. Multiple recent GitHub Actions runs on main show\nfailures (including workflow-file errors), and independent build\nattempts report dozens of compiler errors, recent PRs were al", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["2026", "01", "16", "january", "14th", "2026", "cursor", "publish", "blog", "post", "title", "scale", "long", "run", "autonomous", "cod", "url", "blog", "post", "talk", "experiment", "run", "cod", "agent", "autonomously", "week", "explicit", "goal", "understand", "far", "push", "frontier", "agentic", "coding", "project", "typically", "human", "team", "month", "complete", "talk", "approach", "try", "think", "fail", "address", "difficulty", "finally", "arrive", "point", "solve", "coordination", "problem", "let", "scale", "large", "project", "single", "agent", "lead", "test", "system", "point", "ambitious", "goal", "build", "web", "browser", "scratch", "agent", "run", "close", "week", "write", "million", "line", "code", "1000", "file", "explore", "source", "code", "github", "url", "thing", "bit", "murky", "unclear", "claim", "despite", "codebase", "size", "new", "agent", "understand", "meaningful", "progress", "hundred", "worker", "run", "concurrently", "push", "branch", "minimal", "conflict", "actually", "successful", "actually", "work", "run", "browser", "know", "explicitly", "embe", "follow", "video", "video", "like", "simple", "screenshot", "build", "browser", "scratch", "extremely", "difficult", "error", "compile", "fastrender", "lib", "34", "previous", "error", "94", "warning", "emit", "try", "compile", "far", "away", "functional", "browser", "seemingly", "actually", "able", "build", "multiple", "recent", "github", "action", "run", "main", "failure", "include", "workflow", "file", "error", "independent", "build", "attempt", "report", "dozen", "compil", "error", "recent", "prs", "al"], "num_tokens": 166, "token_loss_pct": 51.6, "normalized_content": "2026-01-16 on january 14th 2026 cursor published a blog post titled scaling long-running autonomous coding  url  in the blog post they talk about their experiments with running coding agents autonomously for weeks with the explicit goal of understanding how far we can push the frontier of agentic coding for projects that typically take human teams months to complete they talk about some approaches they tried why they think those failed and how to address the difficulties. finally they arrived at a point where something solved most of our coordination problems and let us scale to very large projects without any single agent which then led to this to test this system we pointed it at an ambitious goal building a web browser from scratch. the agents ran for close to a week writing over 1 million lines of code across 1000 files. you can explore the source code on github  url  this is where things get a bit murky and unclear. they claim despite the codebase size new agents can still understand it and make meaningful progress and hundreds of workers run concurrently pushing to the same branch with minimal conflicts but they never actually say if this is successful or not is it actually working can you run this browser yourself we don't know and they never say explicitly. after this they embed the following video video and below it they say while it might seem like a simple screenshot building a browser from scratch is extremely difficult.. error could not compile 'fastrender' lib due to 34 previous errors 94 warnings emitted and if you try to compile it yourself you'll see that it's very far away from being a functional browser at all and seemingly it never actually was able to build. multiple recent github actions runs on main show failures including workflow-file errors and independent build attempts report dozens of compiler errors recent prs were al"}
{"title": "Overlapping Markup", "url": "https://en.wikipedia.org/wiki/Overlapping_markup", "content": "In markup languages and the digital humanities , overlap occurs when a document has two or more structures that interact in a non- hierarchical manner.\nA document with overlapping markup cannot be represented as a tree .\nThis is also known as concurrent markup .\nOverlap happens, for instance, in poetry , where there may be a metrical structure of feet and lines; a linguistic structure of sentences and quotations; and a physical structure of volumes and pages and editorial annotations. [ 1 ] [ 2 ] The problem of non-hierarchical structures in documents has been recognised since 1988; resolving it against the dominant paradigm of text as a single hierarchy (an ordered hierarchy of content objects or OHCO ) was initially thought to be merely a technical issue, but has, in fact, proven much more difficult. [ 4 ] In 2008, Jeni Tennison identified markup overlap as \"the main remaining problem area for markup technologists\". [ 5 ] Markup overlap continues to be a primary issue in the digital study of theological texts in 2019, and is a major reason for the field retaining specialised markup formats—the Open Scripture Information Standard and the Theological Markup Language —rather than the inter-operable Text Encoding Initiative -based formats common to the rest of the digital humanities . [ 6 ] A distinction exists between schemes that allow non-contiguous overlap, and those that allow only contiguous overlap. Often, 'markup overlap' strictly means the latter.\nContiguous overlap can always be represented as a linear document with milestones (typically co-indexed start- and end-markers), without the need for fragmenting a (logical) component into multiple physical ones. Non-contiguous overlap may require document fragmentation. Another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind ( self-overlap ). [ 2 ] A scheme may have a privileged hierarchy.\nSome XML -based schemes, for example, represent one hierarchy di", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["markup", "language", "digital", "humanity", "overlap", "occur", "document", "structure", "interact", "non-", "hierarchical", "manner", "document", "overlapping", "markup", "represent", "tree", "know", "concurrent", "markup", "overlap", "happen", "instance", "poetry", "metrical", "structure", "foot", "line", "linguistic", "structure", "sentence", "quotation", "physical", "structure", "volume", "page", "editorial", "annotation", "  ", "problem", "non", "hierarchical", "structure", "document", "recognise", "1988", "resolve", "dominant", "paradigm", "text", "single", "hierarchy", "order", "hierarchy", "content", "object", "ohco", "initially", "think", "merely", "technical", "issue", "fact", "prove", "difficult", "2008", "jeni", "tennison", "identify", "markup", "overlap", "main", "remain", "problem", "area", "markup", "technologist", "markup", "overlap", "continue", "primary", "issue", "digital", "study", "theological", "text", "2019", "major", "reason", "field", "retain", "specialised", "markup", "formatsthe", "open", "scripture", "information", "standard", "theological", "markup", "language", "inter", "operable", "text", "encode", "initiative", "-base", "format", "common", "rest", "digital", "humanity", "distinction", "exist", "scheme", "allow", "non", "contiguous", "overlap", "allow", "contiguous", "overlap", "markup", "overlap", "strictly", "mean", "contiguous", "overlap", "represent", "linear", "document", "milestone", "typically", "co", "index", "start-", "end", "marker", "need", "fragment", "logical", "component", "multiple", "physical", "one", "non", "contiguous", "overlap", "require", "document", "fragmentation", "distinction", "overlap", "markup", "scheme", "element", "overlap", "element", "kind", "self", "overlap", "scheme", "privileged", "hierarchy", "xml", "-base", "scheme", "example", "represent", "hierarchy", "di"], "num_tokens": 171, "token_loss_pct": 50.0, "normalized_content": "in markup languages and the digital humanities  overlap occurs when a document has two or more structures that interact in a non- hierarchical manner. a document with overlapping markup cannot be represented as a tree . this is also known as concurrent markup . overlap happens for instance in poetry  where there may be a metrical structure of feet and lines a linguistic structure of sentences and quotations and a physical structure of volumes and pages and editorial annotations.  1   2  the problem of non-hierarchical structures in documents has been recognised since 1988 resolving it against the dominant paradigm of text as a single hierarchy an ordered hierarchy of content objects or ohco  was initially thought to be merely a technical issue but has in fact proven much more difficult.  4  in 2008 jeni tennison identified markup overlap as the main remaining problem area for markup technologists.  5  markup overlap continues to be a primary issue in the digital study of theological texts in 2019 and is a major reason for the field retaining specialised markup formatsthe open scripture information standard and the theological markup language rather than the inter-operable text encoding initiative -based formats common to the rest of the digital humanities .  6  a distinction exists between schemes that allow non-contiguous overlap and those that allow only contiguous overlap. often 'markup overlap' strictly means the latter. contiguous overlap can always be represented as a linear document with milestones typically co-indexed start- and end-markers without the need for fragmenting a logical component into multiple physical ones. non-contiguous overlap may require document fragmentation. another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind  self-overlap .  2  a scheme may have a privileged hierarchy. some xml -based schemes for example represent one hierarchy di"}
{"title": "Ask HN: How worried should I be about running LLM code on my machine?", "url": "item?id=46686262", "content": "Ask HN: How worried should I be about running LLM code on my machine?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "worried", "run", "llm", "code", "machine", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 56.52, "normalized_content": "ask hn how worried should i be about running llm code on my machine. score none. author none. date none"}
{"title": "The 'untouchable hacker god' behind Finland's biggest crime", "url": "https://www.theguardian.com/technology/2026/jan/17/vastaamo-hack-finland-therapy-notes", "content": "How would you feel if your therapist’s notes – your darkest thoughts and deepest feelings – were exposed to the world? For 33,000 Finnish people, that became a terrifying reality, with deadly consequences T iina Parikka was half-naked when she read the email. It was a Saturday in late October 2020, and Parikka had spent the morning sorting out plans for distance learning after a Covid outbreak at the school where she was headteacher. She had taken a sauna at her flat in Vantaa, just outside Finland’s capital, Helsinki, and when she came into her bedroom to get dressed, she idly checked her phone. There was a message that began with Parikka’s name and her social security number – the unique code used to identify Finnish people when they access healthcare, education and banking. “I knew then that this is not a game,” she says. The email was in Finnish. It was jarringly polite. “We are contacting you because you have used Vastaamo’s therapy and/or psychiatric services,” it read. “Unfortunately, we have to ask you to pay to keep your personal information safe.” The sender demanded €200 in bitcoin within 24 hours, otherwise the price would go up to €500 within 48 hours. “If we still do not receive our money after this, your information will be published for everyone to see, including your name, address, phone number, social security number and detailed records containing transcripts of your conversations with Vastaamo’s therapists or psychiatrists.” Parikka swallows hard as she relives this memory. “My heart was pounding. It was really difficult to breathe. I remember lying down on the bed and telling my spouse, ‘I think I’m going to have a heart attack.’” Someone had hacked into Vastaamo, the company through which Parikka had accessed psychotherapy. They’d got hold of therapy notes containing her most private, intimate feelings and darkest thoughts – and they were holding them to ransom. Parikka’s mind raced as she tried to recall everything she’d confided during three", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["feel", "therapist", "note", "dark", "thought", "deep", "feeling", "expose", "world", "33000", "finnish", "people", "terrifying", "reality", "deadly", "consequence", "iina", "parikka", "half", "naked", "read", "email", "saturday", "late", "october", "2020", "parikka", "spend", "morning", "sort", "plan", "distance", "learning", "covid", "outbreak", "school", "headteacher", "take", "sauna", "flat", "vantaa", "outside", "finland", "capital", "helsinki", "come", "bedroom", "dress", "idly", "check", "phone", "message", "begin", "parikkas", "social", "security", "number", "unique", "code", "identify", "finnish", "people", "access", "healthcare", "education", "banking", "know", "game", "say", "email", "finnish", "jarringly", "polite", "contact", "vastaamos", "therapy", "andor", "psychiatric", "service", "read", "unfortunately", "ask", "pay", "personal", "information", "safe", "sender", "demand", "200", "bitcoin", "24", "hour", "price", "500", "48", "hour", "receive", "money", "information", "publish", "include", "address", "phone", "number", "social", "security", "number", "detailed", "record", "contain", "transcript", "conversation", "vastaamos", "therapist", "psychiatrist", "parikka", "swallow", "hard", "relive", "memory", "heart", "pound", "difficult", "breathe", "remember", "lie", "bed", "tell", "spouse", "think", "go", "heart", "attack", "hack", "vastaamo", "company", "parikka", "access", "psychotherapy", "get", "hold", "therapy", "note", "contain", "private", "intimate", "feeling", "darkest", "thought", "hold", "ransom", "parikkas", "mind", "race", "try", "recall", "shed", "confide"], "num_tokens": 158, "token_loss_pct": 55.11, "normalized_content": "how would you feel if your therapists notes  your darkest thoughts and deepest feelings  were exposed to the world for 33000 finnish people that became a terrifying reality with deadly consequences t iina parikka was half-naked when she read the email. it was a saturday in late october 2020 and parikka had spent the morning sorting out plans for distance learning after a covid outbreak at the school where she was headteacher. she had taken a sauna at her flat in vantaa just outside finlands capital helsinki and when she came into her bedroom to get dressed she idly checked her phone. there was a message that began with parikkas name and her social security number  the unique code used to identify finnish people when they access healthcare education and banking. i knew then that this is not a game she says. the email was in finnish. it was jarringly polite. we are contacting you because you have used vastaamos therapy andor psychiatric services it read. unfortunately we have to ask you to pay to keep your personal information safe. the sender demanded 200 in bitcoin within 24 hours otherwise the price would go up to 500 within 48 hours. if we still do not receive our money after this your information will be published for everyone to see including your name address phone number social security number and detailed records containing transcripts of your conversations with vastaamos therapists or psychiatrists. parikka swallows hard as she relives this memory. my heart was pounding. it was really difficult to breathe. i remember lying down on the bed and telling my spouse i think im going to have a heart attack. someone had hacked into vastaamo the company through which parikka had accessed psychotherapy. theyd got hold of therapy notes containing her most private intimate feelings and darkest thoughts  and they were holding them to ransom. parikkas mind raced as she tried to recall everything shed confided during three"}
{"title": "The Dilbert Afterlife", "url": "https://www.astralcodexten.com/p/the-dilbert-afterlife", "content": "Thanks to everyone who sent in condolences on my recent death from prostate cancer at age 68, but that was Scott Adams. I (Scott Alexander) am still alive 1 . Still, the condolences are appreciated. Scott Adams was a surprisingly big part of my life. I may be the only person to have read every Dilbert book before graduating elementary school . For some reason, 10-year-old-Scott found Adams’ stories of time-wasting meetings and pointy-haired bosses hilarious. No doubt some of the attraction came from a more-than-passing resemblance between Dilbert’s nameless corporation and the California public school system. We’re all inmates in prisons with different names. But it would be insufficiently ambitious to stop there. Adams’ comics were about the nerd experience. About being cleverer than everyone else, not just in the sense of being high IQ, but in the sense of being the only sane man in a crazy world where everyone else spends their days listening to overpaid consultants drone on about mission statements instead of doing anything useful. There’s an arc in Dilbert where the boss disappears for a few weeks and the engineers get to manage their own time. Productivity shoots up. Morale soars. They invent warp drives and time machines. Then the boss returns, and they’re back to being chronically behind schedule and over budget. This is the nerd outlook in a nutshell: if I ran the circus, there’d be some changes around here. Yet the other half of the nerd experience is: for some reason this never works. Dilbert and his brilliant co-workers are stuck watching from their cubicles while their idiot boss racks in bonuses and accolades. If humor, like religion, is an opiate of the masses, then Adams is masterfully unsubtle about what type of wound his art is trying to numb. This is the basic engine of Dilbert : everyone is rewarded in exact inverse proportion to their virtue. Dilbert and Alice are brilliant and hard-working, so they get crumbs. Wally is brilliant but lazy, so he", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["thank", "send", "condolence", "recent", "death", "prostate", "cancer", "age", "68", "scott", "adams", "scott", "alexander", "alive", "condolence", "appreciate", "scott", "adams", "surprisingly", "big", "life", "person", "read", "dilbert", "book", "graduate", "elementary", "school", "reason", "10", "year", "old", "scott", "find", "adam", "story", "time", "waste", "meeting", "pointy", "haired", "boss", "hilarious", "doubt", "attraction", "come", "pass", "resemblance", "dilbert", "nameless", "corporation", "california", "public", "school", "system", "inmate", "prison", "different", "name", "insufficiently", "ambitious", "stop", "adam", "comic_strip", "nerd", "experience", "clever", "sense", "high", "iq", "sense", "sane", "man", "crazy", "world", "spend", "day", "listen", "overpay", "consultant", "drone", "mission", "statement", "instead", "useful", "arc", "dilbert", "boss", "disappear", "week", "engineer", "manage", "time", "productivity", "shoot", "morale", "soar", "invent", "warp", "drive", "time", "machine", "boss", "return", "chronically", "schedule", "budget", "nerd", "outlook", "nutshell", "run", "circus", "change", "half", "nerd", "experience", "reason", "work", "dilbert", "brilliant", "co", "worker", "stick", "watch", "cubicle", "idiot", "boss", "rack", "bonus", "accolade", "humor", "like", "religion", "opiate", "mass", "adam", "masterfully", "unsubtle", "type", "wound", "art", "try", "numb", "basic", "engine", "dilbert", "reward", "exact", "inverse", "proportion", "virtue", "dilbert", "alice", "brilliant", "hard", "work", "crumb", "wally", "brilliant", "lazy"], "num_tokens": 160, "token_loss_pct": 57.45, "normalized_content": "thanks to everyone who sent in condolences on my recent death from prostate cancer at age 68 but that was scott adams. i scott alexander am still alive 1 . still the condolences are appreciated. scott adams was a surprisingly big part of my life. i may be the only person to have read every dilbert book before graduating elementary school . for some reason 10-year-old-scott found adams stories of time-wasting meetings and pointy-haired bosses hilarious. no doubt some of the attraction came from a more-than-passing resemblance between dilberts nameless corporation and the california public school system. were all inmates in prisons with different names. but it would be insufficiently ambitious to stop there. adams comics were about the nerd experience. about being cleverer than everyone else not just in the sense of being high iq but in the sense of being the only sane man in a crazy world where everyone else spends their days listening to overpaid consultants drone on about mission statements instead of doing anything useful. theres an arc in dilbert where the boss disappears for a few weeks and the engineers get to manage their own time. productivity shoots up. morale soars. they invent warp drives and time machines. then the boss returns and theyre back to being chronically behind schedule and over budget. this is the nerd outlook in a nutshell if i ran the circus thered be some changes around here. yet the other half of the nerd experience is for some reason this never works. dilbert and his brilliant co-workers are stuck watching from their cubicles while their idiot boss racks in bonuses and accolades. if humor like religion is an opiate of the masses then adams is masterfully unsubtle about what type of wound his art is trying to numb. this is the basic engine of dilbert  everyone is rewarded in exact inverse proportion to their virtue. dilbert and alice are brilliant and hard-working so they get crumbs. wally is brilliant but lazy so he"}
{"title": "Simulating the Ladybug Clock Puzzle", "url": "https://austinhenley.com/blog/ladybugclock.html", "content": "Associate Teaching Professor Carnegie Mellon University See the discussion of this post on Hacker News . A few days ago, 3Blue1Brown posted a 60-second video describing a puzzle... Imagine that a ladybug lands on the 12 o'clock marker of a clock. It then proceeds to move either clockwise or counterclockwise to the adjacent hour marker, one at a time, and repeats until all hour markers have been visited at least once. What is the probability that it ends on the 6? These sort of puzzles always intrigue me. They're simple to describe and at first might even look easy to solve, but as I dig into them, my intuition leads me astray. What a fun Saturday morning project! I whipped up a simulator to try it out. It works like this: See, it is simple. But before running the simulator, can you guess the probability of it ending on 6? What about 11 or 1? 3? The other numbers? It stumped me. My guess was that 6 would be the most likely—it is the farthest away but it is also necessary to visit all other numbers first. The numbers closer to 12 would be gradually less likely. Am I right? It might remind you of other random walk problems. So what is the answer? Well, I first ran the simulator 100 times and the results looked random. More runs! After ~1500 runs, all of the numbers were showing 8-10% likelihood with no discernable pattern. That isn't what I expected. After 5000 runs, they were all 8.4-9.7%. And then after 10,000 runs... Based on the simulator, all numbers are equally likely with 9% probability , excluding 12 of course, since that is visited first and thus can't be last. The answer is 1 ⁄ 11 . An even more fun question? What is the average number of moves that the ladybug will make to visit all 12 numbers? I'd love to hear someone's explanation for that answer!", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["associate", "teaching", "professor", "carnegie", "mellon", "university", "discussion", "post", "hacker", "news", "day", "ago", "3blue1brown", "post", "60", "second", "video", "describe", "puzzle", "imagine", "ladybug", "land", "12", "o'clock", "marker", "clock", "proceed", "clockwise", "counterclockwise", "adjacent", "hour", "marker", "time", "repeat", "hour", "marker", "visit", "probability", "end", "sort", "puzzle", "intrigue", "simple", "describe", "look", "easy", "solve", "dig", "intuition", "lead", "astray", "fun", "saturday", "morning", "project", "whip", "simulator", "try", "work", "like", "simple", "run", "simulator", "guess", "probability", "end", "11", "number", "stump", "guess", "likelyit", "farth", "away", "necessary", "visit", "number", "number", "close", "12", "gradually", "likely", "right", "remind", "random", "walk", "problem", "answer", "run", "simulator", "100", "time", "result", "look", "random", "run", "1500", "run", "number", "show", "10", "likelihood", "discernable", "pattern", "expect", "5000", "run", "8.4", "9.7", "10000", "run", "base", "simulator", "number", "equally", "likely", "probability", "exclude", "12", "course", "visit", "answer", "11", "fun", "question", "average", "number", "move", "ladybug", "visit", "12", "number", "love", "hear", "explanation", "answer"], "num_tokens": 135, "token_loss_pct": 61.86, "normalized_content": "associate teaching professor carnegie mellon university see the discussion of this post on hacker news . a few days ago 3blue1brown posted a 60-second video describing a puzzle... imagine that a ladybug lands on the 12 o'clock marker of a clock. it then proceeds to move either clockwise or counterclockwise to the adjacent hour marker one at a time and repeats until all hour markers have been visited at least once. what is the probability that it ends on the 6 these sort of puzzles always intrigue me. they're simple to describe and at first might even look easy to solve but as i dig into them my intuition leads me astray. what a fun saturday morning project i whipped up a simulator to try it out. it works like this see it is simple. but before running the simulator can you guess the probability of it ending on 6 what about 11 or 1 3 the other numbers it stumped me. my guess was that 6 would be the most likelyit is the farthest away but it is also necessary to visit all other numbers first. the numbers closer to 12 would be gradually less likely. am i right it might remind you of other random walk problems. so what is the answer well i first ran the simulator 100 times and the results looked random. more runs after 1500 runs all of the numbers were showing 8-10 likelihood with no discernable pattern. that isn't what i expected. after 5000 runs they were all 8.4-9.7. and then after 10000 runs... based on the simulator all numbers are equally likely with 9 probability  excluding 12 of course since that is visited first and thus can't be last. the answer is 1  11 . an even more fun question what is the average number of moves that the ladybug will make to visit all 12 numbers i'd love to hear someone's explanation for that answer"}
{"title": "Penn Calls Government's Demand for Lists of Jewish Staff 'Disconcerting'", "url": "https://www.nytimes.com/2026/01/20/us/university-of-pennsylvania-trump-jewish-staff.html", "content": "Penn Calls Government's Demand for Lists of Jewish Staff 'Disconcerting'. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["penn", "call", "government", "demand", "list", "jewish", "staff", "disconcerting", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 50.0, "normalized_content": "penn calls government's demand for lists of jewish staff 'disconcerting'. score none. author none. date none"}
{"title": "Show HN: Streaming gigabyte medical images from S3 without downloading them", "url": "https://github.com/PABannier/WSIStreamer", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . WSI Streamer is a tile server for Whole Slide Images (WSI) stored in S3-compatible object storage. It serves tiles on-demand using HTTP range requests, so you never have to download or mount multi-gigabyte slides on local disk. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  A modern, cloud-native tile server for Whole Slide Images. One command to start serving tiles directly from S3.  That's it. No configuration files, no local storage, no complex setup. Open http://localhost:3000/view/sample.svs in your browser to view a slide. Whole Slide Images are large (1-3GB+) and typically live in object storage. Traditional viewers require downloading entire files before serving a single tile. WSIStreamer takes a different approach: it understands slide formats natively, fetches only the bytes needed via HTTP range requests, and returns JPEG tiles immediately. Install from crates.io : Or build from source: Or run with Docker: The web viewer handles authentication automatically when enabled. All options can be set via CLI flags or environment variables: Run wsi-streamer --help for full details. See API_SPECIFICATIONS.md for complete documentation. Files must be tiled (not stripped) and pyramidal. MIT. See LICENSE . Issues and pull requests welcome. See CONTRIBUTING.md . WSI Streamer is a tile server for Whole Slide Images (WSI) stored in S3-compatible object storage. It serves tiles on-demand using HTTP range requests, so you never have to download or mount multi-gigabyte slides on local disk. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "wsi", "streamer", "tile", "server", "slide", "image", "wsi", "store", "s3", "compatible", "object", "storage", "serve", "tile", "demand", "http", "range", "request", "download", "mount", "multi", "gigabyte", "slide", "local", "disk", "error", "load", "reload", "page", "error", "load", "reload", "page", "modern", "cloud", "native", "tile", "server", "slide", "image", "command", "start", "serve", "tile", "directly", "s3", "configuration", "file", "local", "storage", "complex", "setup", "open", "url", "browser", "view", "slide", "slide", "image", "large", "gb", "typically", "live", "object", "storage", "traditional", "viewer", "require", "download", "entire", "file", "serve", "single", "tile", "wsistreamer", "take", "different", "approach", "understand", "slide", "format", "natively", "fetch", "byte", "need", "http", "range", "request", "return", "jpeg", "tile", "immediately", "install", "crates.io", "build", "source", "run", "docker", "web", "viewer", "handle", "authentication", "automatically", "enable", "option", "set", "cli", "flag", "environment", "variable", "run", "wsi", "streamer", "--help", "detail", "api_specifications.md", "complete", "documentation", "file", "tile", "strip", "pyramidal", "mit", "license", "issue", "pull", "request", "welcome", "contributing.md", "wsi", "streamer", "tile", "server", "slide", "image", "wsi", "store", "s3", "compatible", "object", "storage", "serve", "tile", "demand", "http", "range", "request", "download", "mount", "multi", "gigabyte", "slide", "local", "disk", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 178, "token_loss_pct": 49.58, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . wsi streamer is a tile server for whole slide images wsi stored in s3-compatible object storage. it serves tiles on-demand using http range requests so you never have to download or mount multi-gigabyte slides on local disk. there was an error while loading. please reload this page . there was an error while loading. please reload this page . a modern cloud-native tile server for whole slide images. one command to start serving tiles directly from s3. that's it. no configuration files no local storage no complex setup. open url in your browser to view a slide. whole slide images are large 1-3gb and typically live in object storage. traditional viewers require downloading entire files before serving a single tile. wsistreamer takes a different approach it understands slide formats natively fetches only the bytes needed via http range requests and returns jpeg tiles immediately. install from crates.io  or build from source or run with docker the web viewer handles authentication automatically when enabled. all options can be set via cli flags or environment variables run wsi-streamer --help for full details. see api_specifications.md for complete documentation. files must be tiled not stripped and pyramidal. mit. see license . issues and pull requests welcome. see contributing.md . wsi streamer is a tile server for whole slide images wsi stored in s3-compatible object storage. it serves tiles on-demand using http range requests so you never have to download or mount multi-gigabyte slides on local disk. there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "How to wrangle non-deterministic AI outputs into conventional software? (2025)", "url": "https://www.domainlanguage.com/articles/ai-components-deterministic-system/", "content": "by Eric Evans When we set out to incorporate AI components into larger systems that are mostly conventional software, we encounter various difficulties. How do we wrangle behavior that is intrinsically non-deterministic so that it can be used in structured, deterministic systems? The flexibility of input is great! But the variation of output makes it difficult to do further processing by conventional software. In this simple example I’ll characterize and constrain a non-deterministic result to make it usable in deterministic software. This leads into domain modeling and strategic design. What follows isn’t rocket science, but it is the sort of basics I think we need to apply in order to get results. Let’s start with a use-case I actually have. When I’m trying to get my bearings in a software system, I usually want to know what domains are addressed and in which parts of the code. So imagine an app that would generate that sort of view of a repo: To be concrete, let’s look at the open source project “OpenEMR”. Here’s a very small code sample from that project: We might ask, “what domains are addressed in this code?” Conventional code does not lend itself to that kind of question, but it is a natural use of an LLM. An intelligent answer! But we couldn’t pass that to conventional software for further processing. Of course, we would instruct the LLM to structure and format its output. Okay, so now we have an answer that could be integrated in a technical way. Yet this is will not support the comparisons and hierarchical roll-ups I was hoping for. Because categories are chosen freely in each run, the classification of different files will not be easy to compare. To illustrate the point, I’ll repeat the same question using the same file. Every time I ask question I get a different answer: The answers make sense individually but would be difficult to compare or combine. The stochastic nature of LLMs can be a challenge in making reliable systems. However, in this case, I se", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["eric", "evans", "set", "incorporate", "ai", "component", "large", "system", "conventional", "software", "encounter", "difficulty", "wrangle", "behavior", "intrinsically", "non", "deterministic", "structured", "deterministic", "system", "flexibility", "input", "great", "variation", "output", "make", "difficult", "processing", "conventional", "software", "simple", "example", "ill", "characterize", "constrain", "non", "deterministic", "result", "usable", "deterministic", "software", "lead", "domain", "modeling", "strategic", "design", "follow", "not", "rocket", "science", "sort", "basic", "think", "need", "apply", "order", "result", "let", "start", "use", "case", "actually", "try", "bearing", "software", "system", "usually", "want", "know", "domain", "address", "part", "code", "imagine", "app", "generate", "sort", "view", "repo", "concrete", "let", "look", "open", "source", "project", "openemr", "here", "small", "code", "sample", "project", "ask", "domain", "address", "code", "conventional", "code", "lend", "kind", "question", "natural", "use", "llm", "intelligent", "answer", "not", "pass", "conventional", "software", "processing", "course", "instruct", "llm", "structure", "format", "output", "okay", "answer", "integrate", "technical", "way", "support", "comparison", "hierarchical", "roll", "up", "hop", "category", "choose", "freely", "run", "classification", "different", "file", "easy", "compare", "illustrate", "point", "ill", "repeat", "question", "file", "time", "ask", "question", "different", "answer", "answer", "sense", "individually", "difficult", "compare", "combine", "stochastic", "nature", "llm", "challenge", "make", "reliable", "system", "case", "se"], "num_tokens": 162, "token_loss_pct": 56.22, "normalized_content": "by eric evans when we set out to incorporate ai components into larger systems that are mostly conventional software we encounter various difficulties. how do we wrangle behavior that is intrinsically non-deterministic so that it can be used in structured deterministic systems the flexibility of input is great but the variation of output makes it difficult to do further processing by conventional software. in this simple example ill characterize and constrain a non-deterministic result to make it usable in deterministic software. this leads into domain modeling and strategic design. what follows isnt rocket science but it is the sort of basics i think we need to apply in order to get results. lets start with a use-case i actually have. when im trying to get my bearings in a software system i usually want to know what domains are addressed and in which parts of the code. so imagine an app that would generate that sort of view of a repo to be concrete lets look at the open source project openemr. heres a very small code sample from that project we might ask what domains are addressed in this code conventional code does not lend itself to that kind of question but it is a natural use of an llm. an intelligent answer but we couldnt pass that to conventional software for further processing. of course we would instruct the llm to structure and format its output. okay so now we have an answer that could be integrated in a technical way. yet this is will not support the comparisons and hierarchical roll-ups i was hoping for. because categories are chosen freely in each run the classification of different files will not be easy to compare. to illustrate the point ill repeat the same question using the same file. every time i ask question i get a different answer the answers make sense individually but would be difficult to compare or combine. the stochastic nature of llms can be a challenge in making reliable systems. however in this case i se"}
{"title": "Starlink users must opt out of all browsing data being used to train xAI models", "url": "https://twitter.com/cryps1s/status/2013345999826153943", "content": "Starlink users must opt out of all browsing data being used to train xAI models. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["starlink", "user", "opt", "browse", "datum", "train", "xai", "model", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 54.17, "normalized_content": "starlink users must opt out of all browsing data being used to train xai models. score none. author none. date none"}
{"title": "Reading across books with Claude Code", "url": "https://pieterma.es/syntopic-reading-claude/", "content": "Jan 4, 2026 LLMs are overused to summarise and underused to help us read deeper. To explore how they can enrich rather than reduce, I set Claude Code up with tools to mine a library of 100 non-fiction books.\nIt found sequences of excerpts connected by an interesting idea, or trails . Browse all trails Here’s a part of one such trail, linking deception in the startup world to the social psychology of mass movements (I’m especially pleased by the jump from Jobs to Theranos): Steve Jobs Bad Blood Zero to One The True Believer The books were selected from Hacker News’ favourites, which I previously scraped and visualized . Claude browses the books a chunk at a time. A chunk is a segment of roughly 500 words that aligns with paragraphs when possible.\nThis length is a good balance between saving tokens and providing enough context for ideas to breathe. Chunks are indexed by topic, and topics are themselves indexed for search. This makes it easy to look up all passages in the corpus that relate to, say, deception . This works well when you know what to look for, but search alone can’t tell you which topics are present to begin with.\nThere are over 100,000 extracted topics, far too many to be browsed directly. To support exploration, they are grouped into a hierarchical tree structure. This yields around 1,000 top-level topics. They emerge from combining lower-level topics, and not all of them are equally useful: However, this Borgesian taxonomy is good enough for Claude to piece together what the books are about. Claude uses the topic tree and the search via a few CLI tools. They allow it to: To generate the trails, the agent works in stages. Even though I’ve been using Claude Code to develop for months, my first instinct for this project was to consider it as a traditional pipeline of several discrete stages.\nMy initial attempt at this system consisted of multiple LLM modules with carefully hand-assembled contexts. On a whim, I ran Claude with access to the debugging tool", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["jan", "2026", "llm", "overused", "summarise", "underuse", "help", "read", "deeply", "explore", "enrich", "reduce", "set", "claude", "code", "tool", "library", "100", "non", "fiction", "book", "find", "sequence", "excerpt", "connect", "interesting", "idea", "trail", "browse", "trail", "here", "trail", "link", "deception", "startup", "world", "social", "psychology", "mass", "movement", "especially", "pleased", "jump", "job", "therano", "steve", "job", "bad", "blood", "zero", "true", "believer", "book", "select", "hacker", "news", "favourite", "previously", "scrap", "visualize", "claude", "browse", "book", "chunk", "time", "chunk", "segment", "roughly", "500", "word", "align", "paragraph", "possible", "length", "good", "balance", "save", "token", "provide", "context", "idea", "breathe", "chunk", "index", "topic", "topic", "index", "search", "make", "easy", "look", "passage", "corpus", "relate", "deception", "work", "know", "look", "search", "not", "tell", "topic", "present", "begin", "100000", "extract", "topic", "far", "browse", "directly", "support", "exploration", "group", "hierarchical", "tree", "structure", "yield", "1000", "level", "topic", "emerge", "combine", "low", "level", "topic", "equally", "useful", "borgesian", "taxonomy", "good", "claude", "piece", "book", "claude", "use", "topic", "tree", "search", "cli", "tool", "allow", "generate", "trail", "agent", "work", "stage", "ve", "claude", "code", "develop", "month", "instinct", "project", "consider", "traditional", "pipeline", "discrete", "stage", "initial", "attempt", "system", "consist", "multiple", "llm", "module", "carefully", "hand", "assemble", "contexts", "whim", "run", "claude", "access", "debug", "tool"], "num_tokens": 175, "token_loss_pct": 53.08, "normalized_content": "jan 4 2026 llms are overused to summarise and underused to help us read deeper. to explore how they can enrich rather than reduce i set claude code up with tools to mine a library of 100 non-fiction books. it found sequences of excerpts connected by an interesting idea or trails . browse all trails heres a part of one such trail linking deception in the startup world to the social psychology of mass movements im especially pleased by the jump from jobs to theranos steve jobs bad blood zero to one the true believer the books were selected from hacker news favourites which i previously scraped and visualized . claude browses the books a chunk at a time. a chunk is a segment of roughly 500 words that aligns with paragraphs when possible. this length is a good balance between saving tokens and providing enough context for ideas to breathe. chunks are indexed by topic and topics are themselves indexed for search. this makes it easy to look up all passages in the corpus that relate to say deception . this works well when you know what to look for but search alone cant tell you which topics are present to begin with. there are over 100000 extracted topics far too many to be browsed directly. to support exploration they are grouped into a hierarchical tree structure. this yields around 1000 top-level topics. they emerge from combining lower-level topics and not all of them are equally useful however this borgesian taxonomy is good enough for claude to piece together what the books are about. claude uses the topic tree and the search via a few cli tools. they allow it to to generate the trails the agent works in stages. even though ive been using claude code to develop for months my first instinct for this project was to consider it as a traditional pipeline of several discrete stages. my initial attempt at this system consisted of multiple llm modules with carefully hand-assembled contexts. on a whim i ran claude with access to the debugging tool"}
{"title": "Show HN: ChunkHound, a local-first tool for understanding large codebases", "url": "https://github.com/chunkhound/chunkhound", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Local first codebase intelligence There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  Local first codebase intelligence  Your AI assistant searches code but doesn't understand it. ChunkHound researches your codebase—extracting architecture, patterns, and institutional knowledge at any scale. Integrates via MCP . Visit chunkhound.github.io for complete guides: Note: Use \"codex-cli\" instead if you prefer Codex. Both work equally well and require no API key. For configuration, IDE setup, and advanced usage, see the documentation . Ideal for: Stop recreating code. Start with deep understanding. MIT Local first codebase intelligence There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "local", "codebase", "intelligence", "error", "load", "reload", "page", "error", "load", "reload", "page", "local", "codebase", "intelligence", "ai", "assistant", "search", "code", "understand", "chunkhound", "research", "codebaseextracte", "architecture", "pattern", "institutional", "knowledge", "scale", "integrate", "mcp", "visit", "chunkhound.github.io", "complete", "guide", "note", "use", "codex", "cli", "instead", "prefer", "codex", "work", "equally", "require", "api", "key", "configuration", "ide", "setup", "advanced", "usage", "documentation", "ideal", "stop", "recreate", "code", "start", "deep", "understanding", "mit", "local", "codebase", "intelligence", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 78, "token_loss_pct": 50.94, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . local first codebase intelligence there was an error while loading. please reload this page . there was an error while loading. please reload this page . local first codebase intelligence your ai assistant searches code but doesn't understand it. chunkhound researches your codebaseextracting architecture patterns and institutional knowledge at any scale. integrates via mcp . visit chunkhound.github.io for complete guides note use codex-cli instead if you prefer codex. both work equally well and require no api key. for configuration ide setup and advanced usage see the documentation . ideal for stop recreating code. start with deep understanding. mit local first codebase intelligence there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "A free and open-source rootkit for Linux", "url": "https://lwn.net/SubscriberLink/1053099/19c2e8180aeb0438/", "content": "A free and open-source rootkit for Linux. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["free", "open", "source", "rootkit", "linux", "score", "author", "date"], "num_tokens": 8, "token_loss_pct": 55.56, "normalized_content": "a free and open-source rootkit for linux. score none. author none. date none"}
{"title": "Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC", "url": "https://neodyme.io/en/blog/drone_hacking_part_1/", "content": "Jan 5, 2026 ~48 min read In July 2025, we from Neodyme got together in Munich and did security research on a bunch of IoT devices, ranging from bluetooth headsets, to door locks, to drones. One of these was the Potensic Atom 2. It’s a photo and video drone with a gimbal-stabilized 4K camera and a remote control that you hook up to your own smartphone and the proprietary app. If you’ve ever flown a DJI Mini 4K, this drone will look very familiar to you.  Potensic Atom 2  This post is part of a two-part series that will cover how we disassembled the drone and dumped the firmware from the NAND chip and how we analyzed the drone’s firmware, app, and remote control to find some backdoors and vulnerabilities. One of the most important pieces of information you can acquire when setting up to hack a device is its firmware. If you want to reverse engineer the software that’s running on the drone and find vulnerabilities in that, then you need a copy of it in the first place. Now there are a couple of ways to go about that, some are less intrusive and some are more effective. You might get lucky and be able to just download the firmware as a firmware update from the manufacturer’s website. However, those update sites are often not publicly documented and can be locked behind authorization checks or encrypted. Encrypted firmwares can still be useful - you “just” need to reverse engineer the on-device decryption process. For the Atom 2, downloading the firmware updates required having a valid drone and remote control serial number and the firmware update was also encrypted. Without having the decryption logic, we put this approach on ice during our initial research. Another really comfortable approach is to use exposed debug interfaces like JTAG or UART. However, those are often undocumented, unlabeled, or entirely removed for public versions. We didn’t find any on the Atom 2. What we can always do, though not necessarily always successful, is solder off the entire NAND chip an", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["jan", "2026", "48", "min", "read", "july", "2025", "neodyme", "get", "munich", "security", "research", "bunch", "iot", "device", "range", "bluetooth", "headset", "door", "lock", "drone", "potensic", "atom", "photo", "video", "drone", "gimbal", "stabilize", "4k", "camera", "remote", "control", "hook", "smartphone", "proprietary", "app", "ve", "fly", "dji", "mini", "4k", "drone", "look", "familiar", "potensic", "atom", "post", "series", "cover", "disassemble", "drone", "dump", "firmware", "nand", "chip", "analyze", "drone", "firmware", "app", "remote", "control", "find", "backdoor", "vulnerability", "important", "piece", "information", "acquire", "set", "hack", "device", "firmware", "want", "reverse", "engineer", "software", "run", "drone", "find", "vulnerability", "need", "copy", "place", "couple", "way", "intrusive", "effective", "lucky", "able", "download", "firmware", "firmware", "update", "manufacturer", "website", "update", "site", "publicly", "document", "lock", "authorization", "check", "encrypt", "encrypt", "firmware", "useful", "need", "reverse", "engineer", "device", "decryption", "process", "atom", "download", "firmware", "update", "require", "have", "valid", "drone", "remote", "control", "serial", "number", "firmware", "update", "encrypt", "have", "decryption", "logic", "approach", "ice", "initial", "research", "comfortable", "approach", "use", "expose", "debug", "interface", "like", "jtag", "uart", "undocumented", "unlabeled", "entirely", "remove", "public", "version", "not", "find", "atom", "necessarily", "successful", "solder", "entire", "nand", "chip"], "num_tokens": 158, "token_loss_pct": 57.53, "normalized_content": "jan 5 2026 48 min read in july 2025 we from neodyme got together in munich and did security research on a bunch of iot devices ranging from bluetooth headsets to door locks to drones. one of these was the potensic atom 2. its a photo and video drone with a gimbal-stabilized 4k camera and a remote control that you hook up to your own smartphone and the proprietary app. if youve ever flown a dji mini 4k this drone will look very familiar to you. potensic atom 2 this post is part of a two-part series that will cover how we disassembled the drone and dumped the firmware from the nand chip and how we analyzed the drones firmware app and remote control to find some backdoors and vulnerabilities. one of the most important pieces of information you can acquire when setting up to hack a device is its firmware. if you want to reverse engineer the software thats running on the drone and find vulnerabilities in that then you need a copy of it in the first place. now there are a couple of ways to go about that some are less intrusive and some are more effective. you might get lucky and be able to just download the firmware as a firmware update from the manufacturers website. however those update sites are often not publicly documented and can be locked behind authorization checks or encrypted. encrypted firmwares can still be useful - you just need to reverse engineer the on-device decryption process. for the atom 2 downloading the firmware updates required having a valid drone and remote control serial number and the firmware update was also encrypted. without having the decryption logic we put this approach on ice during our initial research. another really comfortable approach is to use exposed debug interfaces like jtag or uart. however those are often undocumented unlabeled or entirely removed for public versions. we didnt find any on the atom 2. what we can always do though not necessarily always successful is solder off the entire nand chip an"}
{"title": "Computer Systems Security 6.566 / Spring 2024", "url": "https://css.csail.mit.edu/6.858/2024/", "content": "The lectures cover a broad overview of systems security together with a deeper focus on several topics: isolation techniques , privilege separation , dealing with buggy code , networked and distributed systems ,\nand human-focused security and privacy . Links to notes etc. on future days are copies of materials from last year,\nto give you an idea of what the future will bring.  We will update the\nnotes as the course progresses.  The year of publication for class\nreadings are shown in parentheses. Monday Tuesday Wednesday Thursday Friday feb 5 First day of classes feb 6 LEC 1: Introduction, threat models ( video ) Preparation: Optionally read Modern Android exploit Assigned: Lab 1: Buffer overflows feb 7 feb 8 LEC 2: OS and VM isolation ( video ) Preparation: Read about OS and VM isolation ( Question ) feb 9 feb 12 feb 13 LEC 3: Software fault isolation ( video ) Preparation: Read about WebAssembly ( Question ) feb 14 feb 15 LEC 4: Trusted hardware ( video ) Preparation: Read BitLocker (2006), sections 1-2 ( Question ) feb 16 DUE: Lab 1 part 1 DUE: Lab 1 part 2 feb 19 Presidents day feb 20 Monday schedule feb 21 feb 22 LEC 5: CPU side-channels ( video ) Preparation: Read Transient Execution Attacks and Defenses (2019) ( Question ) Assigned: Lab 2: Privilege separation feb 23 DUE: Lab 1 all parts feb 26 feb 27 LEC 6: Privilege separation ( video ) Preparation: Read OpenSSH (2003) ( Question ) feb 28 feb 29 LEC 7: Data center infrastructure ( video ) Preparation: Read Google Infrastructure Security (2023) and BeyondProd (2023) ( Question ) mar 1 DUE: Lab 2 part 1 mar 4 mar 5 LEC 8: Mobile phone security ( video ) Preparation: Read about iOS Security ( Question ) mar 6 mar 7 LEC 9: Web security model ( video ) Preparation: Read about web security (2022) ( Question ) mar 8 DUE: Lab 2 parts 2+3 ADD DATE mar 11 mar 12 LEC 10: Buffer overflow defenses ( video ) Preparation: Read Baggy bounds checking (2009) + errata ( Question ) Assigned: Lab 3: Symbolic execution mar 13 mar", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["lecture", "cover", "broad", "overview", "system", "security", "deep", "focus", "topic", "isolation", "technique", "privilege", "separation", "deal", "buggy", "code", "network", "distribute", "system", "human", "focus", "security", "privacy", "link", "note", "etc", "future", "day", "copy", "material", "year", "idea", "future", "bring", "update", "note", "course", "progress", "year", "publication", "class", "reading", "show", "parenthesis", "monday", "tuesday", "wednesday", "thursday", "friday", "feb", "day", "class", "feb", "lec", "introduction", "threat", "model", "video", "preparation", "optionally", "read", "modern", "android", "exploit", "assign", "lab", "buffer", "overflow", "feb", "feb", "lec", "os", "vm", "isolation", "video", "preparation", "read", "os", "vm", "isolation", "question", "feb", "feb", "12", "feb", "13", "lec", "software", "fault", "isolation", "video", "preparation", "read", "webassembly", "question", "feb", "14", "feb", "15", "lec", "trust", "hardware", "video", "preparation", "read", "bitlocker", "2006", "section", "question", "feb", "16", "lab", "lab", "feb", "19", "president", "day", "feb", "20", "monday", "schedule", "feb", "21", "feb", "22", "lec", "cpu", "channel", "video", "preparation", "read", "transient", "execution", "attack", "defense", "2019", "question", "assign", "lab", "privilege", "separation", "feb", "23", "lab", "part", "feb", "26", "feb", "27", "lec", "privilege", "separation", "video", "preparation", "read", "openssh", "2003", "question", "feb", "28", "feb", "29", "lec", "datum", "center", "infrastructure", "video", "preparation", "read", "google", "infrastructure", "security", "2023", "beyondprod", "2023", "question", "mar", "lab", "mar", "mar", "lec", "mobile", "phone", "security", "video", "preparation", "read", "ios", "security", "question", "mar", "mar", "lec", "web", "security", "model", "video", "preparation", "read", "web", "security", "2022", "question", "mar", "lab", "part", "23", "add", "date", "mar", "11", "mar", "12", "lec", "10", "buffer", "overflow", "defense", "video", "preparation", "read", "baggy", "bound", "check", "2009", "errata", "question", "assign", "lab", "symbolic", "execution", "mar", "13", "mar"], "num_tokens": 234, "token_loss_pct": 37.43, "normalized_content": "the lectures cover a broad overview of systems security together with a deeper focus on several topics isolation techniques  privilege separation  dealing with buggy code  networked and distributed systems  and human-focused security and privacy . links to notes etc. on future days are copies of materials from last year to give you an idea of what the future will bring. we will update the notes as the course progresses. the year of publication for class readings are shown in parentheses. monday tuesday wednesday thursday friday feb 5 first day of classes feb 6 lec 1 introduction threat models  video  preparation optionally read modern android exploit assigned lab 1 buffer overflows feb 7 feb 8 lec 2 os and vm isolation  video  preparation read about os and vm isolation  question  feb 9 feb 12 feb 13 lec 3 software fault isolation  video  preparation read about webassembly  question  feb 14 feb 15 lec 4 trusted hardware  video  preparation read bitlocker 2006 sections 1-2  question  feb 16 due lab 1 part 1 due lab 1 part 2 feb 19 presidents day feb 20 monday schedule feb 21 feb 22 lec 5 cpu side-channels  video  preparation read transient execution attacks and defenses 2019  question  assigned lab 2 privilege separation feb 23 due lab 1 all parts feb 26 feb 27 lec 6 privilege separation  video  preparation read openssh 2003  question  feb 28 feb 29 lec 7 data center infrastructure  video  preparation read google infrastructure security 2023 and beyondprod 2023  question  mar 1 due lab 2 part 1 mar 4 mar 5 lec 8 mobile phone security  video  preparation read about ios security  question  mar 6 mar 7 lec 9 web security model  video  preparation read about web security 2022  question  mar 8 due lab 2 parts 23 add date mar 11 mar 12 lec 10 buffer overflow defenses  video  preparation read baggy bounds checking 2009  errata  question  assigned lab 3 symbolic execution mar 13 mar"}
{"title": "ThinkNext Design", "url": "https://thinknextdesign.com/home.html", "content": "Purposeful design . Business success . Design is far more than form or function. Itâs the tangible expression of a brandâs identity, values, and promise. While a brand defines what a company stands for, design gives those aspirations form and substance. Design uniquely delivers value: visually, physically, and experientially. At ThinkNext Design, every creation begins with empathy and seeks purpose. We look to understand not just what people need, but what they desire. Whether crafting something entirely new or reimagining the familiar, our work blends aesthetic restraint with purposeful clarity. The result is innovative design that resonates emotionally, performs beautifully, and endures as a reflection of the brand behind it. More than 200,000,000 ThinkPads have been sold since 1992, and still counting. That didn't happen by accident. By the early 1990's, the original IBM AS/400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology, and was highly overpriced.  David led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo. The resulting award winning design featured stark black enclosures, dramatic air inlets, and simple yet powerful forms. This was a striking contrast to the putty colored neutral appearance that had come to dominate not only the IBM server products, but the entire industry. Following the series introduction, AS/400 Division revenues jumped by a double-digit percentage. Comments of yesterday's technology were quickly replaced by associations with objects such as the innovative F117a stealth fighter. AS/400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. Restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. Without the key only basic functions could be operated. Unfortunately th", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["purposeful", "design", "business", "success", "design", "far", "form", "function", "itâs", "tangible", "expression", "brandâs", "identity", "value", "promise", "brand", "define", "company", "stand", "design", "give", "aspiration", "form", "substance", "design", "uniquely", "deliver", "value", "visually", "physically", "experientially", "thinknext", "design", "creation", "begin", "empathy", "seek", "purpose", "look", "understand", "people", "need", "desire", "craft", "entirely", "new", "reimagine", "familiar", "work", "blend", "aesthetic", "restraint", "purposeful", "clarity", "result", "innovative", "design", "resonate", "emotionally", "perform", "beautifully", "endure", "reflection", "brand", "200000000", "thinkpad", "sell", "1992", "count", "happen", "accident", "early", "1990", "original", "ibm", "as400", "product", "line", "rapidly", "lose", "market", "share", "grow", "perception", "product", "family", "employ", "outdated", "technology", "highly", "overpriced", "david", "lead", "strategic", "design", "initiative", "recast", "image", "sweeping", "change", "forever", "reposition", "status", "quo", "result", "award", "win", "design", "feature", "stark", "black", "enclosure", "dramatic", "air", "inlet", "simple", "powerful", "form", "striking", "contrast", "putty", "color", "neutral", "appearance", "come", "dominate", "ibm", "server", "product", "entire", "industry", "follow", "series", "introduction", "as400", "division", "revenue", "jump", "double", "digit", "percentage", "comment", "yesterday", "technology", "quickly", "replace", "association", "object", "innovative", "f117a", "stealth", "fighter", "as400", "system", "control", "panel", "include", "special", "function", "design", "access", "authorize", "operator", "restrict", "access", "achieve", "traditional", "stainless", "steel", "keylock", "mat", "rotate", "electric", "switch", "key", "basic", "function", "operate", "unfortunately", "th"], "num_tokens": 180, "token_loss_pct": 44.27, "normalized_content": "purposeful design . business success . design is far more than form or function. itâs the tangible expression of a brandâs identity values and promise. while a brand defines what a company stands for design gives those aspirations form and substance. design uniquely delivers value visually physically and experientially. at thinknext design every creation begins with empathy and seeks purpose. we look to understand not just what people need but what they desire. whether crafting something entirely new or reimagining the familiar our work blends aesthetic restraint with purposeful clarity. the result is innovative design that resonates emotionally performs beautifully and endures as a reflection of the brand behind it. more than 200000000 thinkpads have been sold since 1992 and still counting. that didn't happen by accident. by the early 1990's the original ibm as400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology and was highly overpriced. david led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo. the resulting award winning design featured stark black enclosures dramatic air inlets and simple yet powerful forms. this was a striking contrast to the putty colored neutral appearance that had come to dominate not only the ibm server products but the entire industry. following the series introduction as400 division revenues jumped by a double-digit percentage. comments of yesterday's technology were quickly replaced by associations with objects such as the innovative f117a stealth fighter. as400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. without the key only basic functions could be operated. unfortunately th"}
{"title": "6-Day and IP Address Certificates Are Generally Available", "url": "https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability", "content": "Short-lived and IP address certificates are now generally available from Let’s Encrypt. These certificates are valid for 160 hours, just over six days. In order to get a short-lived certificate subscribers simply need to select the ‘shortlived’ certificate profile in their ACME client. Short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. If a certificate’s private key is exposed or compromised, revocation has historically been the way to mitigate damage prior to the certificate’s expiration. Unfortunately, revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires, a period as long as 90 days. With short-lived certificates that vulnerability window is greatly reduced. Short-lived certificates are opt-in and we have no plan to make them the default at this time. Subscribers that have fully automated their renewal process should be able to switch to short-lived certificates easily if they wish, but we understand that not everyone is in that position and generally comfortable with this significantly shorter lifetime. We hope that over time everyone moves to automated solutions and we can demonstrate that short-lived certificates work well. Our default certificate lifetimes will be going from 90 days down to 45 days over the next few years, as previously announced . IP address certificates allow server operators to authenticate TLS connections to IP addresses rather than domain names. Let’s Encrypt supports both IPv4 and IPv6. IP address certificates must be short-lived certificates, a decision we made because IP addresses are more transient than domain names, so validating more frequently is important. You can learn more about our IP address certificates and the use cases for them from our post announcing our first IP Certificate . We’d like to thank the Open Technology Fund and Sovereign Tech Agency, along with our Sponsors and", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["short", "live", "ip", "address", "certificate", "generally", "available", "lets", "encrypt", "certificate", "valid", "160", "hour", "day", "order", "short", "live", "certificate", "subscriber", "simply", "need", "select", "shortlived", "certificate", "profile", "acme", "client", "short", "live", "certificate", "improve", "security", "require", "frequent", "validation", "reduce", "reliance", "unreliable", "revocation", "mechanism", "certificate", "private", "key", "expose", "compromise", "revocation", "historically", "way", "mitigate", "damage", "prior", "certificate", "expiration", "unfortunately", "revocation", "unreliable", "system", "rely", "party", "continue", "vulnerable", "certificate", "expire", "period", "long", "90", "day", "short", "live", "certificate", "vulnerability", "window", "greatly", "reduce", "short", "live", "certificate", "opt", "plan", "default", "time", "subscriber", "fully", "automate", "renewal", "process", "able", "switch", "short", "live", "certificate", "easily", "wish", "understand", "position", "generally", "comfortable", "significantly", "short", "lifetime", "hope", "time", "move", "automate", "solution", "demonstrate", "short", "live", "certificate", "work", "default", "certificate", "lifetime", "go", "90", "day", "45", "day", "year", "previously", "announce", "ip", "address", "certificate", "allow", "server", "operator", "authenticate", "tls", "connection", "ip", "address", "domain", "name", "let", "encrypt", "support", "ipv4", "ipv6", "ip", "address", "certificate", "short", "live", "certificate", "decision", "ip", "address", "transient", "domain", "name", "validate", "frequently", "important", "learn", "ip", "address", "certificate", "use", "case", "post", "announce", "ip", "certificate", "like", "thank", "open", "technology", "fund", "sovereign", "tech", "agency", "sponsor"], "num_tokens": 173, "token_loss_pct": 48.51, "normalized_content": "short-lived and ip address certificates are now generally available from lets encrypt. these certificates are valid for 160 hours just over six days. in order to get a short-lived certificate subscribers simply need to select the shortlived certificate profile in their acme client. short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. if a certificates private key is exposed or compromised revocation has historically been the way to mitigate damage prior to the certificates expiration. unfortunately revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires a period as long as 90 days. with short-lived certificates that vulnerability window is greatly reduced. short-lived certificates are opt-in and we have no plan to make them the default at this time. subscribers that have fully automated their renewal process should be able to switch to short-lived certificates easily if they wish but we understand that not everyone is in that position and generally comfortable with this significantly shorter lifetime. we hope that over time everyone moves to automated solutions and we can demonstrate that short-lived certificates work well. our default certificate lifetimes will be going from 90 days down to 45 days over the next few years as previously announced . ip address certificates allow server operators to authenticate tls connections to ip addresses rather than domain names. lets encrypt supports both ipv4 and ipv6. ip address certificates must be short-lived certificates a decision we made because ip addresses are more transient than domain names so validating more frequently is important. you can learn more about our ip address certificates and the use cases for them from our post announcing our first ip certificate . wed like to thank the open technology fund and sovereign tech agency along with our sponsors and"}
{"title": "Computer Systems Security 6.566 / Spring 2024", "url": "https://css.csail.mit.edu/6.858/2024/", "content": "The lectures cover a broad overview of systems security together with a deeper focus on several topics: isolation techniques , privilege separation , dealing with buggy code , networked and distributed systems ,\nand human-focused security and privacy . Links to notes etc. on future days are copies of materials from last year,\nto give you an idea of what the future will bring.  We will update the\nnotes as the course progresses.  The year of publication for class\nreadings are shown in parentheses. Monday Tuesday Wednesday Thursday Friday feb 5 First day of classes feb 6 LEC 1: Introduction, threat models ( video ) Preparation: Optionally read Modern Android exploit Assigned: Lab 1: Buffer overflows feb 7 feb 8 LEC 2: OS and VM isolation ( video ) Preparation: Read about OS and VM isolation ( Question ) feb 9 feb 12 feb 13 LEC 3: Software fault isolation ( video ) Preparation: Read about WebAssembly ( Question ) feb 14 feb 15 LEC 4: Trusted hardware ( video ) Preparation: Read BitLocker (2006), sections 1-2 ( Question ) feb 16 DUE: Lab 1 part 1 DUE: Lab 1 part 2 feb 19 Presidents day feb 20 Monday schedule feb 21 feb 22 LEC 5: CPU side-channels ( video ) Preparation: Read Transient Execution Attacks and Defenses (2019) ( Question ) Assigned: Lab 2: Privilege separation feb 23 DUE: Lab 1 all parts feb 26 feb 27 LEC 6: Privilege separation ( video ) Preparation: Read OpenSSH (2003) ( Question ) feb 28 feb 29 LEC 7: Data center infrastructure ( video ) Preparation: Read Google Infrastructure Security (2023) and BeyondProd (2023) ( Question ) mar 1 DUE: Lab 2 part 1 mar 4 mar 5 LEC 8: Mobile phone security ( video ) Preparation: Read about iOS Security ( Question ) mar 6 mar 7 LEC 9: Web security model ( video ) Preparation: Read about web security (2022) ( Question ) mar 8 DUE: Lab 2 parts 2+3 ADD DATE mar 11 mar 12 LEC 10: Buffer overflow defenses ( video ) Preparation: Read Baggy bounds checking (2009) + errata ( Question ) Assigned: Lab 3: Symbolic execution mar 13 mar", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["lecture", "cover", "broad", "overview", "system", "security", "deep", "focus", "topic", "isolation", "technique", "privilege", "separation", "deal", "buggy", "code", "network", "distribute", "system", "human", "focus", "security", "privacy", "link", "note", "etc", "future", "day", "copy", "material", "year", "idea", "future", "bring", "update", "note", "course", "progress", "year", "publication", "class", "reading", "show", "parenthesis", "monday", "tuesday", "wednesday", "thursday", "friday", "feb", "day", "class", "feb", "lec", "introduction", "threat", "model", "video", "preparation", "optionally", "read", "modern", "android", "exploit", "assign", "lab", "buffer", "overflow", "feb", "feb", "lec", "os", "vm", "isolation", "video", "preparation", "read", "os", "vm", "isolation", "question", "feb", "feb", "12", "feb", "13", "lec", "software", "fault", "isolation", "video", "preparation", "read", "webassembly", "question", "feb", "14", "feb", "15", "lec", "trust", "hardware", "video", "preparation", "read", "bitlocker", "2006", "section", "question", "feb", "16", "lab", "lab", "feb", "19", "president", "day", "feb", "20", "monday", "schedule", "feb", "21", "feb", "22", "lec", "cpu", "channel", "video", "preparation", "read", "transient", "execution", "attack", "defense", "2019", "question", "assign", "lab", "privilege", "separation", "feb", "23", "lab", "part", "feb", "26", "feb", "27", "lec", "privilege", "separation", "video", "preparation", "read", "openssh", "2003", "question", "feb", "28", "feb", "29", "lec", "datum", "center", "infrastructure", "video", "preparation", "read", "google", "infrastructure", "security", "2023", "beyondprod", "2023", "question", "mar", "lab", "mar", "mar", "lec", "mobile", "phone", "security", "video", "preparation", "read", "ios", "security", "question", "mar", "mar", "lec", "web", "security", "model", "video", "preparation", "read", "web", "security", "2022", "question", "mar", "lab", "part", "23", "add", "date", "mar", "11", "mar", "12", "lec", "10", "buffer", "overflow", "defense", "video", "preparation", "read", "baggy", "bound", "check", "2009", "errata", "question", "assign", "lab", "symbolic", "execution", "mar", "13", "mar"], "num_tokens": 234, "token_loss_pct": 37.43, "normalized_content": "the lectures cover a broad overview of systems security together with a deeper focus on several topics isolation techniques  privilege separation  dealing with buggy code  networked and distributed systems  and human-focused security and privacy . links to notes etc. on future days are copies of materials from last year to give you an idea of what the future will bring. we will update the notes as the course progresses. the year of publication for class readings are shown in parentheses. monday tuesday wednesday thursday friday feb 5 first day of classes feb 6 lec 1 introduction threat models  video  preparation optionally read modern android exploit assigned lab 1 buffer overflows feb 7 feb 8 lec 2 os and vm isolation  video  preparation read about os and vm isolation  question  feb 9 feb 12 feb 13 lec 3 software fault isolation  video  preparation read about webassembly  question  feb 14 feb 15 lec 4 trusted hardware  video  preparation read bitlocker 2006 sections 1-2  question  feb 16 due lab 1 part 1 due lab 1 part 2 feb 19 presidents day feb 20 monday schedule feb 21 feb 22 lec 5 cpu side-channels  video  preparation read transient execution attacks and defenses 2019  question  assigned lab 2 privilege separation feb 23 due lab 1 all parts feb 26 feb 27 lec 6 privilege separation  video  preparation read openssh 2003  question  feb 28 feb 29 lec 7 data center infrastructure  video  preparation read google infrastructure security 2023 and beyondprod 2023  question  mar 1 due lab 2 part 1 mar 4 mar 5 lec 8 mobile phone security  video  preparation read about ios security  question  mar 6 mar 7 lec 9 web security model  video  preparation read about web security 2022  question  mar 8 due lab 2 parts 23 add date mar 11 mar 12 lec 10 buffer overflow defenses  video  preparation read baggy bounds checking 2009  errata  question  assigned lab 3 symbolic execution mar 13 mar"}
{"title": "ThinkNext Design", "url": "https://thinknextdesign.com/home.html", "content": "Purposeful design . Business success . Design is far more than form or function. Itâs the tangible expression of a brandâs identity, values, and promise. While a brand defines what a company stands for, design gives those aspirations form and substance. Design uniquely delivers value: visually, physically, and experientially. At ThinkNext Design, every creation begins with empathy and seeks purpose. We look to understand not just what people need, but what they desire. Whether crafting something entirely new or reimagining the familiar, our work blends aesthetic restraint with purposeful clarity. The result is innovative design that resonates emotionally, performs beautifully, and endures as a reflection of the brand behind it. More than 200,000,000 ThinkPads have been sold since 1992, and still counting. That didn't happen by accident. By the early 1990's, the original IBM AS/400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology, and was highly overpriced.  David led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo. The resulting award winning design featured stark black enclosures, dramatic air inlets, and simple yet powerful forms. This was a striking contrast to the putty colored neutral appearance that had come to dominate not only the IBM server products, but the entire industry. Following the series introduction, AS/400 Division revenues jumped by a double-digit percentage. Comments of yesterday's technology were quickly replaced by associations with objects such as the innovative F117a stealth fighter. AS/400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. Restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. Without the key only basic functions could be operated. Unfortunately th", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["purposeful", "design", "business", "success", "design", "far", "form", "function", "itâs", "tangible", "expression", "brandâs", "identity", "value", "promise", "brand", "define", "company", "stand", "design", "give", "aspiration", "form", "substance", "design", "uniquely", "deliver", "value", "visually", "physically", "experientially", "thinknext", "design", "creation", "begin", "empathy", "seek", "purpose", "look", "understand", "people", "need", "desire", "craft", "entirely", "new", "reimagine", "familiar", "work", "blend", "aesthetic", "restraint", "purposeful", "clarity", "result", "innovative", "design", "resonate", "emotionally", "perform", "beautifully", "endure", "reflection", "brand", "200000000", "thinkpad", "sell", "1992", "count", "happen", "accident", "early", "1990", "original", "ibm", "as400", "product", "line", "rapidly", "lose", "market", "share", "grow", "perception", "product", "family", "employ", "outdated", "technology", "highly", "overpriced", "david", "lead", "strategic", "design", "initiative", "recast", "image", "sweeping", "change", "forever", "reposition", "status", "quo", "result", "award", "win", "design", "feature", "stark", "black", "enclosure", "dramatic", "air", "inlet", "simple", "powerful", "form", "striking", "contrast", "putty", "color", "neutral", "appearance", "come", "dominate", "ibm", "server", "product", "entire", "industry", "follow", "series", "introduction", "as400", "division", "revenue", "jump", "double", "digit", "percentage", "comment", "yesterday", "technology", "quickly", "replace", "association", "object", "innovative", "f117a", "stealth", "fighter", "as400", "system", "control", "panel", "include", "special", "function", "design", "access", "authorize", "operator", "restrict", "access", "achieve", "traditional", "stainless", "steel", "keylock", "mat", "rotate", "electric", "switch", "key", "basic", "function", "operate", "unfortunately", "th"], "num_tokens": 180, "token_loss_pct": 44.27, "normalized_content": "purposeful design . business success . design is far more than form or function. itâs the tangible expression of a brandâs identity values and promise. while a brand defines what a company stands for design gives those aspirations form and substance. design uniquely delivers value visually physically and experientially. at thinknext design every creation begins with empathy and seeks purpose. we look to understand not just what people need but what they desire. whether crafting something entirely new or reimagining the familiar our work blends aesthetic restraint with purposeful clarity. the result is innovative design that resonates emotionally performs beautifully and endures as a reflection of the brand behind it. more than 200000000 thinkpads have been sold since 1992 and still counting. that didn't happen by accident. by the early 1990's the original ibm as400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology and was highly overpriced. david led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo. the resulting award winning design featured stark black enclosures dramatic air inlets and simple yet powerful forms. this was a striking contrast to the putty colored neutral appearance that had come to dominate not only the ibm server products but the entire industry. following the series introduction as400 division revenues jumped by a double-digit percentage. comments of yesterday's technology were quickly replaced by associations with objects such as the innovative f117a stealth fighter. as400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. without the key only basic functions could be operated. unfortunately th"}
{"title": "6-Day and IP Address Certificates Are Generally Available", "url": "https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability", "content": "Short-lived and IP address certificates are now generally available from Let’s Encrypt. These certificates are valid for 160 hours, just over six days. In order to get a short-lived certificate subscribers simply need to select the ‘shortlived’ certificate profile in their ACME client. Short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. If a certificate’s private key is exposed or compromised, revocation has historically been the way to mitigate damage prior to the certificate’s expiration. Unfortunately, revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires, a period as long as 90 days. With short-lived certificates that vulnerability window is greatly reduced. Short-lived certificates are opt-in and we have no plan to make them the default at this time. Subscribers that have fully automated their renewal process should be able to switch to short-lived certificates easily if they wish, but we understand that not everyone is in that position and generally comfortable with this significantly shorter lifetime. We hope that over time everyone moves to automated solutions and we can demonstrate that short-lived certificates work well. Our default certificate lifetimes will be going from 90 days down to 45 days over the next few years, as previously announced . IP address certificates allow server operators to authenticate TLS connections to IP addresses rather than domain names. Let’s Encrypt supports both IPv4 and IPv6. IP address certificates must be short-lived certificates, a decision we made because IP addresses are more transient than domain names, so validating more frequently is important. You can learn more about our IP address certificates and the use cases for them from our post announcing our first IP Certificate . We’d like to thank the Open Technology Fund and Sovereign Tech Agency, along with our Sponsors and", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["short", "live", "ip", "address", "certificate", "generally", "available", "lets", "encrypt", "certificate", "valid", "160", "hour", "day", "order", "short", "live", "certificate", "subscriber", "simply", "need", "select", "shortlived", "certificate", "profile", "acme", "client", "short", "live", "certificate", "improve", "security", "require", "frequent", "validation", "reduce", "reliance", "unreliable", "revocation", "mechanism", "certificate", "private", "key", "expose", "compromise", "revocation", "historically", "way", "mitigate", "damage", "prior", "certificate", "expiration", "unfortunately", "revocation", "unreliable", "system", "rely", "party", "continue", "vulnerable", "certificate", "expire", "period", "long", "90", "day", "short", "live", "certificate", "vulnerability", "window", "greatly", "reduce", "short", "live", "certificate", "opt", "plan", "default", "time", "subscriber", "fully", "automate", "renewal", "process", "able", "switch", "short", "live", "certificate", "easily", "wish", "understand", "position", "generally", "comfortable", "significantly", "short", "lifetime", "hope", "time", "move", "automate", "solution", "demonstrate", "short", "live", "certificate", "work", "default", "certificate", "lifetime", "go", "90", "day", "45", "day", "year", "previously", "announce", "ip", "address", "certificate", "allow", "server", "operator", "authenticate", "tls", "connection", "ip", "address", "domain", "name", "let", "encrypt", "support", "ipv4", "ipv6", "ip", "address", "certificate", "short", "live", "certificate", "decision", "ip", "address", "transient", "domain", "name", "validate", "frequently", "important", "learn", "ip", "address", "certificate", "use", "case", "post", "announce", "ip", "certificate", "like", "thank", "open", "technology", "fund", "sovereign", "tech", "agency", "sponsor"], "num_tokens": 173, "token_loss_pct": 48.51, "normalized_content": "short-lived and ip address certificates are now generally available from lets encrypt. these certificates are valid for 160 hours just over six days. in order to get a short-lived certificate subscribers simply need to select the shortlived certificate profile in their acme client. short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. if a certificates private key is exposed or compromised revocation has historically been the way to mitigate damage prior to the certificates expiration. unfortunately revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires a period as long as 90 days. with short-lived certificates that vulnerability window is greatly reduced. short-lived certificates are opt-in and we have no plan to make them the default at this time. subscribers that have fully automated their renewal process should be able to switch to short-lived certificates easily if they wish but we understand that not everyone is in that position and generally comfortable with this significantly shorter lifetime. we hope that over time everyone moves to automated solutions and we can demonstrate that short-lived certificates work well. our default certificate lifetimes will be going from 90 days down to 45 days over the next few years as previously announced . ip address certificates allow server operators to authenticate tls connections to ip addresses rather than domain names. lets encrypt supports both ipv4 and ipv6. ip address certificates must be short-lived certificates a decision we made because ip addresses are more transient than domain names so validating more frequently is important. you can learn more about our ip address certificates and the use cases for them from our post announcing our first ip certificate . wed like to thank the open technology fund and sovereign tech agency along with our sponsors and"}
{"title": "Counterfactual evaluation for recommendation systems", "url": "https://eugeneyan.com/writing/counterfactual-evaluation/", "content": "[ recsys eval machinelearning ] · 8 min read When I first started working on recommendation systems, I thought there was something weird about the way we did offline evaluation. First, we split customer interaction data into training and validation sets. Then, we train our recommenders on the training set before evaluating them on the validation set, usually on metrics such as recall, precision, and NDCG. This is similar to how we evaluate supervised machine learning models and doesn’t seem unusual at first glance. But don’t our recommendations change how customers click or purchase? If customers can only interact with items shown to them, why do we perform offline evaluation on static historical data? It took me a while to put a finger on it but I think this is why it felt weird: We’re treating recommendations as an observational problem when it really is an interventional problem . Problems solved via supervised machine learning are usually observational problems. Given an observation such as product title, description, and image, we try to predict the product category. Our model learns P(category=phone|title=“…”, description=“…”, image=image01.jpeg) . On the other hand, recommendations are an interventional problem. We want to learn how different interventions (i.e., item recommendations) lead to different outcomes (i.e., clicks, purchases). By using logged customer interaction data as labels, the observational offline evaluation approach ignores the interventional nature of recommendations. As a result, we’re not evaluating if users would click or purchase more due to our new recommendations; we’re evaluating how well the new recommendations fit logged data. Thus, what our model learns is P(view3=iphone|view1=pixel, view2=galaxy) when what we really want is P(click=True|recommend=iphone, view1=pixel, view2=galaxy) . The straightforward way to evaluate recommendations as an interventional problem is via A/B testing. Our interventions (i.e., new recommendations) a", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["recsys", "eval", "machinelearning", "  ", "min", "read", "start", "work", "recommendation", "system", "think", "weird", "way", "offline", "evaluation", "split", "customer", "interaction", "datum", "training", "validation", "set", "train", "recommender", "training", "set", "evaluate", "validation", "set", "usually", "metric", "recall", "precision", "ndcg", "similar", "evaluate", "supervised", "machine", "learning", "model", "not", "unusual", "glance", "not", "recommendation", "change", "customer", "click", "purchase", "customer", "interact", "item", "show", "perform", "offline", "evaluation", "static", "historical", "datum", "take", "finger", "think", "feel", "weird", "treat", "recommendation", "observational", "problem", "interventional", "problem", "problem", "solve", "supervise", "machine", "learning", "usually", "observational", "problem", "give", "observation", "product", "title", "description", "image", "try", "predict", "product", "category", "model", "learn", "pcategoryphonetitle", "description", "imageimage01.jpeg", "hand", "recommendation", "interventional", "problem", "want", "learn", "different", "intervention", "i.e.", "item", "recommendation", "lead", "different", "outcome", "i.e.", "click", "purchase", "logged", "customer", "interaction", "datum", "label", "observational", "offline", "evaluation", "approach", "ignore", "interventional", "nature", "recommendation", "result", "evaluate", "user", "click", "purchase", "new", "recommendation", "evaluate", "new", "recommendation", "fit", "logged", "datum", "model", "learn", "pview3iphoneview1pixel", "view2galaxy", "want", "pclicktruerecommendiphone", "view1pixel", "view2galaxy", "straightforward", "way", "evaluate", "recommendation", "interventional", "problem", "ab", "testing", "intervention", "i.e.", "new", "recommendation"], "num_tokens": 156, "token_loss_pct": 48.85, "normalized_content": "recsys eval machinelearning   8 min read when i first started working on recommendation systems i thought there was something weird about the way we did offline evaluation. first we split customer interaction data into training and validation sets. then we train our recommenders on the training set before evaluating them on the validation set usually on metrics such as recall precision and ndcg. this is similar to how we evaluate supervised machine learning models and doesnt seem unusual at first glance. but dont our recommendations change how customers click or purchase if customers can only interact with items shown to them why do we perform offline evaluation on static historical data it took me a while to put a finger on it but i think this is why it felt weird were treating recommendations as an observational problem when it really is an interventional problem . problems solved via supervised machine learning are usually observational problems. given an observation such as product title description and image we try to predict the product category. our model learns pcategoryphonetitle description imageimage01.jpeg . on the other hand recommendations are an interventional problem. we want to learn how different interventions i.e. item recommendations lead to different outcomes i.e. clicks purchases. by using logged customer interaction data as labels the observational offline evaluation approach ignores the interventional nature of recommendations. as a result were not evaluating if users would click or purchase more due to our new recommendations were evaluating how well the new recommendations fit logged data. thus what our model learns is pview3iphoneview1pixel view2galaxy when what we really want is pclicktruerecommendiphone view1pixel view2galaxy . the straightforward way to evaluate recommendations as an interventional problem is via ab testing. our interventions i.e. new recommendations a"}
{"title": "Amnesty urges halt to execution of 19-year-old Iranian protester", "url": "https://www.iranintl.com/en/202601209686", "content": "Amnesty International on Tuesday called for an immediate halt to the planned execution of 19-year-old Iranian protester Amirhossein Ghaderzadeh, whose death sentence is due to be carried out on Wednesday. “Iranian authorities must immediately halt any plans to execute 19-year-old Amirhossein Ghaderzadeh, who has been detained since 9 January for taking part in protests in Rasht, Gilan province, and stop weaponizing the death penalty against protesters,” Amnesty said in a post on X. “According to an informed source, the authorities told him during a court session on 17 January that he is accused of ‘betraying his country’ and sentenced to ‘death by hanging’. The authorities have informed his family that his execution is scheduled for 21 January,” the group added. Israel’s foreign minister Gideon Sa’ar has urged the European Union to formally designate Iran’s Islamic Revolutionary Guard Corps (IRGC) as a terrorist organization, citing its role in crushing protests and sponsoring terror across the region. “Designate Iran's Revolutionary Guards as a terrorist organization!” the foreign minister wrote on X on Tuesday, responding to a post from European Commission President Ursula von der Leyen. “You know very well what their role is in the murderous repression of the civilian protest in Iran, as well as in spreading terror in the Middle East and beyond,” Sa’ar added.​ The European Union is proposing new sanctions on Iran, including a ban on additional exports of drone and missile technologies, European Commission President Ursula von der Leyen said on Monday. She added that, together with the EU’s foreign policy chief Kaja Kallas, the Commission is preparing further human rights related measures. “We are also preparing new sanctions in response to the regime’s continued and brutal repression of protesters,” she said. “Iran’s authorities try to shut down the internet because they are afraid,” the US State Department’s Near Eastern Affairs bureau said in a post on X on Mon", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["amnesty", "international", "tuesday", "call", "immediate", "halt", "plan", "execution", "19", "year", "old", "iranian", "protester", "amirhossein", "ghaderzadeh", "death", "sentence", "carry", "wednesday", "iranian", "authority", "immediately", "halt", "plan", "execute", "19", "year", "old", "amirhossein", "ghaderzadeh", "detain", "january", "take", "protest", "rasht", "gilan", "province", "stop", "weaponize", "death", "penalty", "protester", "amnesty", "say", "post", "x.", "accord", "informed", "source", "authority", "tell", "court", "session", "17", "january", "accuse", "betray", "country", "sentence", "death", "hang", "authority", "inform", "family", "execution", "schedule", "21", "january", "group", "add", "israel", "foreign", "minister", "gideon", "saar", "urge", "european", "union", "formally", "designate", "iran", "islamic", "revolutionary", "guard", "corps", "irgc", "terrorist", "organization", "cite", "role", "crush", "protest", "sponsor", "terror", "region", "designate", "iran", "revolutionary", "guard", "terrorist", "organization", "foreign", "minister", "write", "tuesday", "respond", "post", "european", "commission", "president", "ursula", "von", "der", "leyen", "know", "role", "murderous", "repression", "civilian", "protest", "iran", "spread", "terror", "middle", "east", "saar", "add", "european", "union", "propose", "new", "sanction", "iran", "include", "ban", "additional", "export", "drone", "missile", "technologies", "european", "commission", "president", "ursula", "von", "der", "leyen", "say", "monday", "add", "eus", "foreign", "policy", "chief", "kaja", "kallas", "commission", "prepare", "human", "right", "relate", "measure", "prepare", "new", "sanction", "response", "regime", "continued", "brutal", "repression", "protester", "say", "iran", "authority", "try", "shut", "internet", "afraid", "state", "department", "near", "eastern", "affairs", "bureau", "say", "post", "mon"], "num_tokens": 187, "token_loss_pct": 43.5, "normalized_content": "amnesty international on tuesday called for an immediate halt to the planned execution of 19-year-old iranian protester amirhossein ghaderzadeh whose death sentence is due to be carried out on wednesday. iranian authorities must immediately halt any plans to execute 19-year-old amirhossein ghaderzadeh who has been detained since 9 january for taking part in protests in rasht gilan province and stop weaponizing the death penalty against protesters amnesty said in a post on x. according to an informed source the authorities told him during a court session on 17 january that he is accused of betraying his country and sentenced to death by hanging. the authorities have informed his family that his execution is scheduled for 21 january the group added. israels foreign minister gideon saar has urged the european union to formally designate irans islamic revolutionary guard corps irgc as a terrorist organization citing its role in crushing protests and sponsoring terror across the region. designate iran's revolutionary guards as a terrorist organization the foreign minister wrote on x on tuesday responding to a post from european commission president ursula von der leyen. you know very well what their role is in the murderous repression of the civilian protest in iran as well as in spreading terror in the middle east and beyond saar added. the european union is proposing new sanctions on iran including a ban on additional exports of drone and missile technologies european commission president ursula von der leyen said on monday. she added that together with the eus foreign policy chief kaja kallas the commission is preparing further human rights related measures. we are also preparing new sanctions in response to the regimes continued and brutal repression of protesters she said. irans authorities try to shut down the internet because they are afraid the us state departments near eastern affairs bureau said in a post on x on mon"}
{"title": "IKOS a static analyzer for C/C++ based on the theory of Abstract Interpretation", "url": "https://github.com/NASA-SW-VnV/ikos", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Static analyzer for C/C++ based on the theory of Abstract Interpretation. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  IKOS (Inference Kernel for Open Static Analyzers) is a static analyzer for C/C++ based on the theory of Abstract Interpretation. IKOS started as a C++ library designed to facilitate the development of sound static analyzers based on Abstract Interpretation . Specialization of a static analyzer for an application or family of applications is critical for achieving both precision and scalability. Developing such an analyzer is arduous and requires significant expertise in Abstract Interpretation. IKOS provides a generic and efficient implementation of state-of-the-art Abstract Interpretation data structures and algorithms, such as control-flow graphs, fixpoint iterators, numerical abstract domains, etc. IKOS is independent of a particular programming language. IKOS also provides a C and C++ static analyzer based on LLVM . It implements scalable analyses for detecting and proving the absence of runtime errors in C and C++ programs. IKOS has been released under the NASA Open Source Agreement version 1.3, see LICENSE.pdf ikos@lists.nasa.gov See Releases . See TROUBLESHOOTING.md To install IKOS on Linux or macOS , we recommend to use Homebrew . First, install Homebrew by following these instructions . Then, simply run: For Windows, consider using Windows Subsystem for Linux . Suppose we want to analyze the following C program in a file, called loop.c : To analyze this program with IKOS, simply run: You shall see the following output. IKOS reports two occurrences of buffer overflow at line 8 and 9. The ikos command takes a source file ( .c , .cpp ) or a LLVM bitcode file ( .bc ) as input, analyzes it to find runtime errors (also called undefined behaviors),", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "static", "analyzer", "cc", "base", "theory", "abstract", "interpretation", "error", "load", "reload", "page", "error", "load", "reload", "page", "ikos", "inference", "kernel", "open", "static", "analyzer", "static", "analyzer", "cc", "base", "theory", "abstract", "interpretation", "ikos", "start", "library", "design", "facilitate", "development", "sound", "static", "analyzer", "base", "abstract", "interpretation", "specialization", "static", "analyzer", "application", "family", "application", "critical", "achieve", "precision", "scalability", "develop", "analyzer", "arduous", "require", "significant", "expertise", "abstract", "interpretation", "ikos", "provide", "generic", "efficient", "implementation", "state", "art", "abstract", "interpretation", "datum", "structure", "algorithm", "control", "flow", "graph", "fixpoint", "iterator", "numerical", "abstract", "domain", "etc", "ikos", "independent", "particular", "programming", "language", "ikos", "provide", "static", "analyzer", "base", "llvm", "implement", "scalable", "analysis", "detect", "prove", "absence", "runtime", "error", "program", "ikos", "release", "nasa", "open", "source", "agreement", "version", "1.3", "license.pdf", "ikosmention.nasa.gov", "release", "troubleshooting.md", "install", "ikos", "linux", "macos", "recommend", "use", "homebrew", "install", "homebrew", "follow", "instruction", "simply", "run", "window", "consider", "window", "subsystem", "linux", "suppose", "want", "analyze", "following", "program", "file", "call", "loop.c", "analyze", "program", "ikos", "simply", "run", "shall", "follow", "output", "ikos", "report", "occurrence", "buffer", "overflow", "line", "ikos", "command", "take", "source", "file", ".c", ".cpp", "llvm", "bitcode", "file", ".bc", "input", "analyze", "find", "runtime", "error", "call", "undefined", "behavior"], "num_tokens": 178, "token_loss_pct": 47.34, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . static analyzer for cc based on the theory of abstract interpretation. there was an error while loading. please reload this page . there was an error while loading. please reload this page . ikos inference kernel for open static analyzers is a static analyzer for cc based on the theory of abstract interpretation. ikos started as a c library designed to facilitate the development of sound static analyzers based on abstract interpretation . specialization of a static analyzer for an application or family of applications is critical for achieving both precision and scalability. developing such an analyzer is arduous and requires significant expertise in abstract interpretation. ikos provides a generic and efficient implementation of state-of-the-art abstract interpretation data structures and algorithms such as control-flow graphs fixpoint iterators numerical abstract domains etc. ikos is independent of a particular programming language. ikos also provides a c and c static analyzer based on llvm . it implements scalable analyses for detecting and proving the absence of runtime errors in c and c programs. ikos has been released under the nasa open source agreement version 1.3 see license.pdf ikosmention.nasa.gov see releases . see troubleshooting.md to install ikos on linux or macos  we recommend to use homebrew . first install homebrew by following these instructions . then simply run for windows consider using windows subsystem for linux . suppose we want to analyze the following c program in a file called loop.c  to analyze this program with ikos simply run you shall see the following output. ikos reports two occurrences of buffer overflow at line 8 and 9. the ikos command takes a source file  .c  .cpp  or a llvm bitcode file  .bc  as input analyzes it to find runtime errors also called undefined behaviors"}
{"title": "How scientists are using Claude to accelerate research and discovery", "url": "https://www.anthropic.com/news/accelerating-scientific-research", "content": "Last October we launched Claude for Life Sciences—a suite of connectors and skills that made Claude a better scientific collaborator. Since then, we've invested heavily in making Claude the most capable model for scientific work , with Opus 4.5 showing significant improvements in figure interpretation, computational biology, and protein understanding benchmarks. These advances, informed by our partnerships with researchers in academia and industry, reflect our commitment to understanding exactly how scientists are using AI to accelerate progress.  We’ve also been working closely with scientists through our AI for Science program, which provides free API credits to leading researchers working on high-impact scientific projects around the world.  These researchers have developed custom systems that use Claude in ways that go far beyond tasks like literature reviews or coding assistance. In the labs we spoke to, Claude is a collaborator that works across all stages of the research process: making it easier and more cost-effective to understand which experiments to run, using a variety of tools to help compress projects that normally take months into hours, and finding patterns in massive datasets that humans might overlook. In many cases it’s eliminating bottlenecks, handling tasks that require deep knowledge and have previously been impossible to scale; in some it’s enabling entirely different research approaches than researchers have traditionally been able to take. In other words, Claude is beginning to reshape how these scientists work—and point them towards novel scientific insights and discoveries.  One bottleneck in biological research is the fragmentation of tools: there are hundreds of databases, software packages, and protocols available, and researchers spend substantial time selecting from and mastering various platforms. That’s time that, in a perfect world, would be spent on running experiments, interpreting data, or pursuing new projects. Biomni , an age", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["october", "launch", "claude", "life", "sciencesa", "suite", "connector", "skill", "claude", "well", "scientific", "collaborator", "invest", "heavily", "make", "claude", "capable", "model", "scientific", "work", "opus", "4.5", "show", "significant", "improvement", "figure", "interpretation", "computational", "biology", "protein", "understanding", "benchmark", "advance", "inform", "partnership", "researcher", "academia", "industry", "reflect", "commitment", "understand", "exactly", "scientist", "ai", "accelerate", "progress", "ve", "work", "closely", "scientist", "ai", "science", "program", "provide", "free", "api", "credit", "lead", "researcher", "work", "high", "impact", "scientific", "project", "world", "researcher", "develop", "custom", "system", "use", "claude", "way", "far", "task", "like", "literature", "review", "cod", "assistance", "lab", "speak", "claude", "collaborator", "work", "stage", "research", "process", "make", "easy", "cost", "effective", "understand", "experiment", "run", "variety", "tool", "help", "compress", "project", "normally", "month", "hour", "find", "pattern", "massive", "dataset", "human", "overlook", "case", "eliminate", "bottleneck", "handle", "task", "require", "deep", "knowledge", "previously", "impossible", "scale", "enable", "entirely", "different", "research", "approach", "researcher", "traditionally", "able", "word", "claude", "begin", "reshape", "scientist", "workand", "point", "novel", "scientific", "insight", "discovery", "bottleneck", "biological", "research", "fragmentation", "tool", "hundred", "database", "software", "package", "protocol", "available", "researcher", "spend", "substantial", "time", "select", "master", "platform", "time", "perfect", "world", "spend", "run", "experiment", "interpret", "datum", "pursue", "new", "project", "biomni", "age"], "num_tokens": 169, "token_loss_pct": 45.48, "normalized_content": "last october we launched claude for life sciencesa suite of connectors and skills that made claude a better scientific collaborator. since then we've invested heavily in making claude the most capable model for scientific work  with opus 4.5 showing significant improvements in figure interpretation computational biology and protein understanding benchmarks. these advances informed by our partnerships with researchers in academia and industry reflect our commitment to understanding exactly how scientists are using ai to accelerate progress. weve also been working closely with scientists through our ai for science program which provides free api credits to leading researchers working on high-impact scientific projects around the world. these researchers have developed custom systems that use claude in ways that go far beyond tasks like literature reviews or coding assistance. in the labs we spoke to claude is a collaborator that works across all stages of the research process making it easier and more cost-effective to understand which experiments to run using a variety of tools to help compress projects that normally take months into hours and finding patterns in massive datasets that humans might overlook. in many cases its eliminating bottlenecks handling tasks that require deep knowledge and have previously been impossible to scale in some its enabling entirely different research approaches than researchers have traditionally been able to take. in other words claude is beginning to reshape how these scientists workand point them towards novel scientific insights and discoveries. one bottleneck in biological research is the fragmentation of tools there are hundreds of databases software packages and protocols available and researchers spend substantial time selecting from and mastering various platforms. thats time that in a perfect world would be spent on running experiments interpreting data or pursuing new projects. biomni  an age"}
{"title": "Ask HN: Is it still worth pursuing a software startup?", "url": "item?id=46654726", "content": "Ask HN: Is it still worth pursuing a software startup?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "worth", "pursue", "software", "startup", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 52.63, "normalized_content": "ask hn is it still worth pursuing a software startup. score none. author none. date none"}
{"title": "Emoji Use in the Electronic Health Record is Increasing", "url": "https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2843883", "content": "Emoji Use in the Electronic Health Record is Increasing. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["emoji", "use", "electronic", "health", "record", "increase", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 50.0, "normalized_content": "emoji use in the electronic health record is increasing. score none. author none. date none"}
{"title": "Show HN: AWS-doctor – A terminal-based AWS health check and cost optimizer in Go", "url": "https://github.com/elC0mpa/aws-doctor", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Diagnose AWS costs, detect idle resources, and optimize cloud spending directly from your terminal. 🩺 ☁️ There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . A terminal-based tool that acts as a comprehensive health check for your AWS accounts. Built with Golang, aws-doctor diagnoses cost anomalies, detects idle resources, and provides a proactive analysis of your cloud infrastructure—effectively giving you the insights of AWS Trusted Advisor without the need for a Business or Enterprise support plan.    As a Cloud Architect, I often need to check AWS costs and billing information. While the AWS Console provides raw data, it lacks the immediate context I need to answer the question: \"Are we spending efficiently?\" I created aws-doctor to fill that gap. It doesn't just show you the bill; it acts as a diagnostic tool that helps you understand where the money is going and what can be cleaned up. It automates the routine checks I used to perform manually, serving as a free, open-source alternative to the paid recommendations found in AWS Trusted Advisor. Diagnose AWS costs, detect idle resources, and optimize cloud spending directly from your terminal. 🩺 ☁️ There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "diagnose", "aws", "cost", "detect", "idle", "resource", "optimize", "cloud", "spending", "directly", "terminal", "  ", "error", "load", "reload", "page", "error", "load", "reload", "page", "terminal", "base", "tool", "act", "comprehensive", "health", "check", "aws", "account", "build", "golang", "aws", "doctor", "diagnose", "cost", "anomaly", "detect", "idle", "resource", "provide", "proactive", "analysis", "cloud", "infrastructureeffectively", "give", "insight", "aw", "trust", "advisor", "need", "business", "enterprise", "support", "plan", "cloud", "architect", "need", "check", "aws", "cost", "billing", "information", "aws", "console", "provide", "raw", "datum", "lack", "immediate", "context", "need", "answer", "question", "spend", "efficiently", "create", "aw", "doctor", "fill", "gap", "bill", "act", "diagnostic", "tool", "help", "understand", "money", "go", "clean", "automate", "routine", "check", "perform", "manually", "serve", "free", "open", "source", "alternative", "pay", "recommendation", "find", "aw", "trust", "advisor", "diagnose", "aws", "cost", "detect", "idle", "resource", "optimize", "cloud", "spending", "directly", "terminal", "  ", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 141, "token_loss_pct": 51.04, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . diagnose aws costs detect idle resources and optimize cloud spending directly from your terminal.   there was an error while loading. please reload this page . there was an error while loading. please reload this page . a terminal-based tool that acts as a comprehensive health check for your aws accounts. built with golang aws-doctor diagnoses cost anomalies detects idle resources and provides a proactive analysis of your cloud infrastructureeffectively giving you the insights of aws trusted advisor without the need for a business or enterprise support plan. as a cloud architect i often need to check aws costs and billing information. while the aws console provides raw data it lacks the immediate context i need to answer the question are we spending efficiently i created aws-doctor to fill that gap. it doesn't just show you the bill it acts as a diagnostic tool that helps you understand where the money is going and what can be cleaned up. it automates the routine checks i used to perform manually serving as a free open-source alternative to the paid recommendations found in aws trusted advisor. diagnose aws costs detect idle resources and optimize cloud spending directly from your terminal.   there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Show HN: Beats, a web-based drum machine", "url": "https://beats.lasagna.pizza", "content": "Share your beat with this URL: BEATS A web-based drum machine inspired by the Teenage Engineering Pocket Operators. CREDITS: • Wrote by @kinduff • Built with Tone.js and Stimulus.js • With the awesome VT323 font THANKS TO: • andiam03 for transposing patterns and inspiring! • ethanhein for the original idea ! • all beta reviewers!", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["share", "beat", "url", "beat", "web", "base", "drum", "machine", "inspire", "teenage", "engineering", "pocket", "operator", "credit", "write", "mention", "build", "tone.js", "stimulus.js", "awesome", "vt323", "font", "thank", "andiam03", "transpose", "pattern", "inspire", "ethanhein", "original", "idea", "  ", "beta", "reviewer"], "num_tokens": 33, "token_loss_pct": 42.11, "normalized_content": "share your beat with this url beats a web-based drum machine inspired by the teenage engineering pocket operators. credits  wrote by mention  built with tone.js and stimulus.js  with the awesome vt323 font thanks to  andiam03 for transposing patterns and inspiring  ethanhein for the original idea   all beta reviewers"}
{"title": "Show HN: Figma-use – CLI to control Figma for AI agents", "url": "https://github.com/dannote/figma-use", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . Control Figma from the command line. Full read/write access for AI agents — create shapes, text, components, set styles, export images. 100+ commands. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . CLI for Figma. Control it from the terminal — with commands or JSX. Figma's official MCP plugin can read files but can't modify them. This one can. LLMs know CLI. LLMs know React. This combines both. CLI commands are compact — easy to read, easy to generate, easy to chain. When a task involves dozens of operations, every saved token matters. JSX is how LLMs already think about UI. They've seen millions of React components. Describing a Figma layout as <Frame><Text> is natural for them — no special training, no verbose schemas. ▶️ Button components ▶️ Tailwind UI calendar Or run directly without installing: Start Figma with remote debugging enabled: Check connection: That's it. No plugins to install. Imperative — one command at a time: Or declaratively — describe the structure in JSX and render it: The stdin mode accepts pure JSX only — no variables, no logic. For components, variants, and conditions, use .figma.tsx files. Elements: Frame , Rectangle , Ellipse , Text , Line , Star , Polygon , Vector , Group , Icon , Image Insert any icon from Iconify by name. No downloading, no importing, no cleanup. In JSX: Browse 150k+ icons: icon-sets.iconify.design Load images from URL: Convert any Figma node back to JSX: Output: Compare two nodes as JSX diff: In a .figma.tsx file you can define components. First call creates the master, the rest create instances: ComponentSet with all combinations: This creates a real ComponentSet in Figma with all 4 variants, not just 4 separate buttons. CSS Grid for 2D layouts — calendars, dashboards, galleries: Supports px , fr , and auto / hug . Separa", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "control", "figma", "command", "line", "readwrite", "access", "ai", "agent", "create", "shape", "text", "component", "set", "style", "export", "image", "100", "command", "error", "load", "reload", "page", "error", "load", "reload", "page", "cli", "figma", "control", "terminal", "command", "jsx", "figma", "official", "mcp", "plugin", "read", "file", "modify", "llm", "know", "cli", "llm", "know", "react", "combine", "cli", "command", "compact", "easy", "read", "easy", "generate", "easy", "chain", "task", "involve", "dozen", "operation", "save", "token", "matter", "jsx", "llm", "think", "ui", "see", "million", "react", "component", "describe", "figma", "layout", "natural", "special", "training", "verbose", "schemas", "button", "component", "tailwind", "ui", "calendar", "run", "directly", "instal", "start", "figma", "remote", "debugging", "enable", "check", "connection", "plugin", "install", "imperative", "command", "time", "declaratively", "describe", "structure", "jsx", "render", "stdin", "mode", "accept", "pure", "jsx", "variable", "logic", "component", "variant", "condition", "use", ".figma.tsx", "file", "element", "frame", "rectangle", "ellipse", "text", "line", "star", "polygon", "vector", "group", "icon", "image", "insert", "icon", "iconify", "download", "import", "cleanup", "jsx", "browse", "150k", "icon", "icon-sets.iconify.design", "load", "image", "url", "convert", "figma", "node", "jsx", "output", "compare", "node", "jsx", "diff", ".figma.tsx", "file", "define", "component", "create", "master", "rest", "create", "instance", "componentset", "combination", "create", "real", "componentset", "figma", "variant", "separate", "button", "css", "grid", "2d", "layout", "calendar", "dashboard", "gallery", "support", "px", "fr", "auto", "hug", "separa"], "num_tokens": 190, "token_loss_pct": 48.23, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . control figma from the command line. full readwrite access for ai agents  create shapes text components set styles export images. 100 commands. there was an error while loading. please reload this page . there was an error while loading. please reload this page . cli for figma. control it from the terminal  with commands or jsx. figma's official mcp plugin can read files but can't modify them. this one can. llms know cli. llms know react. this combines both. cli commands are compact  easy to read easy to generate easy to chain. when a task involves dozens of operations every saved token matters. jsx is how llms already think about ui. they've seen millions of react components. describing a figma layout as is natural for them  no special training no verbose schemas.  button components  tailwind ui calendar or run directly without installing start figma with remote debugging enabled check connection that's it. no plugins to install. imperative  one command at a time or declaratively  describe the structure in jsx and render it the stdin mode accepts pure jsx only  no variables no logic. for components variants and conditions use .figma.tsx files. elements frame  rectangle  ellipse  text  line  star  polygon  vector  group  icon  image insert any icon from iconify by name. no downloading no importing no cleanup. in jsx browse 150k icons icon-sets.iconify.design load images from url convert any figma node back to jsx output compare two nodes as jsx diff in a .figma.tsx file you can define components. first call creates the master the rest create instances componentset with all combinations this creates a real componentset in figma with all 4 variants not just 4 separate buttons. css grid for 2d layouts  calendars dashboards galleries supports px  fr  and auto  hug . separa"}
{"title": "Dev-owned testing: Why it fails in practice and succeeds in theory", "url": "https://dl.acm.org/doi/10.1145/3780063.3780066", "content": "Dev-owned testing: Why it fails in practice and succeeds in theory. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["dev", "own", "testing", "fail", "practice", "succeed", "theory", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 54.55, "normalized_content": "dev-owned testing why it fails in practice and succeeds in theory. score none. author none. date none"}
{"title": "Boeing knew of flaw in part linked to UPS plane crash, NTSB report says", "url": "https://www.bbc.com/news/articles/cly56w0p9e1o", "content": "An aircraft that crashed in flames in Kentucky in November had a structural flaw  that had been identified by Boeing on similar planes 15 years ago, according to investigators. The MD-11F freighter operated by UPS, crashed after one of its engines separated from the wing as it was preparing to take off from Louisville. The plane briefly lifted off from the runway, before hurtling out of control into an industrial area. Fifteen people died as a result, including three crew and 12 on the ground. In an update report , the US National Transportation Safety Board (NTSB) revealed that cracks found in the engine mounting assembly had previously occurred on several other aircraft. At the time the manufacturer responsible for the aircraft, Boeing, concluded that the issue \"would not result in a safety of flight condition\". The MD-11 is a relatively elderly design that was originally produced by McDonnell Douglas.  Boeing acquired the company in 1997. The last MD-11 came off the production line in 2001, but Boeing has continued providing parts and service support. In the aftermath of the Kentucky disaster, the NTSB issued a preliminary report which drew attention to cracks in the engine attachment mechanism. Its latest update goes further, describing fractures due to evidence of \"fatigue\" – or repeated stresses - in a critical bearing, as well as the mounting it is meant to sit in. It points out that Boeing had previously found failures of the same part on four occasions, affecting three different aircraft. In 2011, the company sent a \"service letter\" to operators warning them of its findings. This is a non legally-binding document used to alert operators about important safety or maintenance information. In this case, Boeing recommended that the part be included in a general visual inspection every five years. It also pointed out changes to the inspection procedure contained in the aircraft maintenance manual, and drew attention to a revised bearing assembly that could be fi", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["aircraft", "crash", "flame", "kentucky", "november", "structural", "flaw", "identify", "boeing", "similar", "plane", "15", "year", "ago", "accord", "investigator", "md-11f", "freighter", "operate", "up", "crash", "engine", "separate", "wing", "prepare", "louisville", "plane", "briefly", "lift", "runway", "hurtle", "control", "industrial", "area", "people", "die", "result", "include", "crew", "12", "ground", "update", "report", "national", "transportation", "safety", "board", "ntsb", "reveal", "crack", "find", "engine", "mount", "assembly", "previously", "occur", "aircraft", "time", "manufacturer", "responsible", "aircraft", "boeing", "conclude", "issue", "result", "safety", "flight", "condition", "md-11", "relatively", "elderly", "design", "originally", "produce", "mcdonnell", "douglas", "boeing", "acquire", "company", "1997", "md-11", "come", "production", "line", "2001", "boeing", "continue", "provide", "part", "service", "support", "aftermath", "kentucky", "disaster", "ntsb", "issue", "preliminary", "report", "draw", "attention", "crack", "engine", "attachment", "mechanism", "late", "update", "go", "describe", "fracture", "evidence", "fatigue", "repeat", "stress", "critical", "bearing", "mount", "mean", "sit", "point", "boeing", "previously", "find", "failure", "occasion", "affect", "different", "aircraft", "2011", "company", "send", "service", "letter", "operator", "warn", "finding", "non", "legally", "bind", "document", "alert", "operator", "important", "safety", "maintenance", "information", "case", "boeing", "recommend", "include", "general", "visual", "inspection", "year", "point", "change", "inspection", "procedure", "contain", "aircraft", "maintenance", "manual", "draw", "attention", "revise", "bearing", "assembly", "fi"], "num_tokens": 167, "token_loss_pct": 51.31, "normalized_content": "an aircraft that crashed in flames in kentucky in november had a structural flaw that had been identified by boeing on similar planes 15 years ago according to investigators. the md-11f freighter operated by ups crashed after one of its engines separated from the wing as it was preparing to take off from louisville. the plane briefly lifted off from the runway before hurtling out of control into an industrial area. fifteen people died as a result including three crew and 12 on the ground. in an update report  the us national transportation safety board ntsb revealed that cracks found in the engine mounting assembly had previously occurred on several other aircraft. at the time the manufacturer responsible for the aircraft boeing concluded that the issue would not result in a safety of flight condition. the md-11 is a relatively elderly design that was originally produced by mcdonnell douglas. boeing acquired the company in 1997. the last md-11 came off the production line in 2001 but boeing has continued providing parts and service support. in the aftermath of the kentucky disaster the ntsb issued a preliminary report which drew attention to cracks in the engine attachment mechanism. its latest update goes further describing fractures due to evidence of fatigue  or repeated stresses - in a critical bearing as well as the mounting it is meant to sit in. it points out that boeing had previously found failures of the same part on four occasions affecting three different aircraft. in 2011 the company sent a service letter to operators warning them of its findings. this is a non legally-binding document used to alert operators about important safety or maintenance information. in this case boeing recommended that the part be included in a general visual inspection every five years. it also pointed out changes to the inspection procedure contained in the aircraft maintenance manual and drew attention to a revised bearing assembly that could be fi"}
{"title": "Show HN: GibRAM an in-memory ephemeral GraphRAG runtime for retrieval", "url": "https://github.com/gibram-io/gibram", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . GibRAM is an in-memory knowledge graph server designed for retrieval augmented generation (RAG / GraphRAG) workflows. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . Graph in-Buffer Retrieval & Associative Memory GibRAM is an in-memory knowledge graph server designed for retrieval augmented generation (RAG) workflows. It combines a lightweight graph store with vector search so that related pieces of information remain connected in memory. This makes it easier to retrieve related regulations, articles or other text when a query mentions specific subjects. Server runs on port 6161 by default. Basic Usage: Custom Components: MIT GibRAM is an in-memory knowledge graph server designed for retrieval augmented generation (RAG / GraphRAG) workflows. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "gibram", "memory", "knowledge", "graph", "server", "design", "retrieval", "augment", "generation", "rag", "graphrag", "workflow", "error", "load", "reload", "page", "error", "load", "reload", "page", "graph", "buffer", "retrieval", "associative", "memory", "gibram", "memory", "knowledge", "graph", "server", "design", "retrieval", "augment", "generation", "rag", "workflow", "combine", "lightweight", "graph", "store", "vector", "search", "relate", "piece", "information", "remain", "connected", "memory", "make", "easy", "retrieve", "related", "regulation", "article", "text", "query", "mention", "specific", "subject", "server", "run", "port", "6161", "default", "basic", "usage", "custom", "component", "mit", "gibram", "memory", "knowledge", "graph", "server", "design", "retrieval", "augment", "generation", "rag", "graphrag", "workflow", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 97, "token_loss_pct": 47.57, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . gibram is an in-memory knowledge graph server designed for retrieval augmented generation rag  graphrag workflows. there was an error while loading. please reload this page . there was an error while loading. please reload this page . graph in-buffer retrieval  associative memory gibram is an in-memory knowledge graph server designed for retrieval augmented generation rag workflows. it combines a lightweight graph store with vector search so that related pieces of information remain connected in memory. this makes it easier to retrieve related regulations articles or other text when a query mentions specific subjects. server runs on port 6161 by default. basic usage custom components mit gibram is an in-memory knowledge graph server designed for retrieval augmented generation rag  graphrag workflows. there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "Breaking the Zimmermann Telegram (2018)", "url": "https://medium.com/lapsed-historian/breaking-the-zimmermann-telegram-b34ed1d73614", "content": "Breaking the Zimmermann Telegram (2018). Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["break", "zimmermann", "telegram", "2018", "score", "author", "date"], "num_tokens": 7, "token_loss_pct": 50.0, "normalized_content": "breaking the zimmermann telegram 2018. score none. author none. date none"}
{"title": "\"Anyone else out there vibe circuit-building?\"", "url": "https://twitter.com/beneater/status/2012988790709928305", "content": "\"Anyone else out there vibe circuit-building?\". Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["vibe", "circuit", "building", "score", "author", "date"], "num_tokens": 6, "token_loss_pct": 64.71, "normalized_content": "anyone else out there vibe circuit-building. score none. author none. date none"}
{"title": "How Hightouch built their long-running agent harness", "url": "https://www.amplifypartners.com/blog-posts/how-hightouch-built-their-long-running-agent-harness", "content": "A lot has been said about the future of AI agents and their impact on our economy. Less has been said about how to actually build them.Â A few months ago Hightouch released their Hightouch Agents product . It is essentially a general purpose marketing agent that can plan campaigns, ask any question or analysis of your data, analyze creative and copy, and automate marketing reporting.Â Though to developers, marketing is often viewed as, well, you knowâ¦ I can tell you as both a developer and marketer that this is an unbelievably diverse set of complex, multi-step, long running tasks that even researchers at frontier labs would shudder at trying to automate.Â The crazier thing is that Hightouch Agents actually work . The agent has complete context (e.g. the full customer data mode) thanks to the core Hightouch product , which helps customers connect and take action on all of their marketing data sources like Facebook Ads, Hubspot, etc. And itâs also pre-built with domain expertise on marketing and can reason about complex concepts like creative fatigue, attribution modeling, and incrementality (and maybe getting to the front page of Hacker News ). All in all, itâs one of the most advanced agent systems in production today.Â To build it, Hightouchâs engineering team needed to solve a laundry list of interesting context, workflow, and prompt engineering problems that there is no set of commonly accepted solutions for. Based on extensive interviews with their technical team, this post will go through the major components of their agent harness, in particular the idea of agentic delegation : Letâs get into it.Â So you want to build an agent. Where do you start? At the time of writing there are a few interesting (if young) agent frameworks to choose from. But when Hightouch started building their Agents product there werenât, and the common wisdom (if there was any) for building agents was immature. The most common abstraction was borrowed from data platforms:", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["lot", "say", "future", "ai", "agent", "impact", "economy", "say", "actually", "build", "them.â", "month", "ago", "hightouch", "release", "hightouch", "agent", "product", "essentially", "general", "purpose", "marketing", "agent", "plan", "campaign", "ask", "question", "analysis", "datum", "analyze", "creative", "copy", "automate", "marketing", "reporting.â", "developer", "marketing", "view", "knowâ", "tell", "developer", "marketer", "unbelievably", "diverse", "set", "complex", "multi", "step", "long", "running", "task", "researcher", "frontier", "lab", "shudder", "try", "automate.â", "crazy", "thing", "hightouch", "agent", "actually", "work", "agent", "complete", "context", "e.g.", "customer", "data", "mode", "thank", "core", "hightouch", "product", "help", "customer", "connect", "action", "marketing", "data", "source", "like", "facebook", "ad", "hubspot", "etc", "itâs", "pre", "build", "domain", "expertise", "marketing", "reason", "complex", "concept", "like", "creative", "fatigue", "attribution", "modeling", "incrementality", "maybe", "get", "page", "hacker", "news", "itâs", "advanced", "agent", "system", "production", "today.â", "build", "hightouchâs", "engineering", "team", "need", "solve", "laundry", "list", "interesting", "context", "workflow", "prompt", "engineering", "problem", "set", "commonly", "accept", "solution", "base", "extensive", "interview", "technical", "team", "post", "major", "component", "agent", "harness", "particular", "idea", "agentic", "delegation", "letâs", "it.â", "want", "build", "agent", "start", "time", "writing", "interesting", "young", "agent", "framework", "choose", "hightouch", "start", "build", "agent", "product", "werenât", "common", "wisdom", "building", "agent", "immature", "common", "abstraction", "borrow", "datum", "platform"], "num_tokens": 173, "token_loss_pct": 48.82, "normalized_content": "a lot has been said about the future of ai agents and their impact on our economy. less has been said about how to actually build them.â a few months ago hightouch released their hightouch agents product . it is essentially a general purpose marketing agent that can plan campaigns ask any question or analysis of your data analyze creative and copy and automate marketing reporting.â though to developers marketing is often viewed as well you knowâ i can tell you as both a developer and marketer that this is an unbelievably diverse set of complex multi-step long running tasks that even researchers at frontier labs would shudder at trying to automate.â the crazier thing is that hightouch agents actually work . the agent has complete context e.g. the full customer data mode thanks to the core hightouch product  which helps customers connect and take action on all of their marketing data sources like facebook ads hubspot etc. and itâs also pre-built with domain expertise on marketing and can reason about complex concepts like creative fatigue attribution modeling and incrementality and maybe getting to the front page of hacker news . all in all itâs one of the most advanced agent systems in production today.â to build it hightouchâs engineering team needed to solve a laundry list of interesting context workflow and prompt engineering problems that there is no set of commonly accepted solutions for. based on extensive interviews with their technical team this post will go through the major components of their agent harness in particular the idea of agentic delegation  letâs get into it.â so you want to build an agent. where do you start at the time of writing there are a few interesting if young agent frameworks to choose from. but when hightouch started building their agents product there werenât and the common wisdom if there was any for building agents was immature. the most common abstraction was borrowed from data platforms"}
{"title": "Some C habits I employ for the modern day", "url": "https://www.unix.dog/~yosh/blog/c-habits-for-me.html", "content": "posted 2026-01-17T21:02:00Z modified 2026-01-17T23:20:00Z Despite it being the first “proper” programming language I learned–by reading K&R front-to-back no less–I don’t write C too terribly often nowadays. Playing resonite has gotten me into writing a load of C# for modding the game, and most of what I do day-to-day is automating the tedium on my computer, which gets delegated to shell or python because of all the existing infrastructure. Alas, every now and then, something arises where I have to or just want to write some C (or C++). Sometimes it’s when I need to make some bindings for a library ; sometimes it’s to fill a niche of a language/architecture gap . It also remains as my favorite language to prototype stuff in, though I’m not quite sure why. In any case, C is an interesting language without much standardization on the whole “style” or “practices” part. Most other languages have very clear “this is the best way to use X” messages, either subtly embedded in the syntax itself or through “official” documentation channels. C doesn’t have an official documentation channel, nor does it have syntax or standard library constructs that encourage one particular way of doing things. From this, there’s a bunch of inconsistencies in how people do things, and–especially in the early days of the language and standard library–the landscape and general practice is quite error prone. As such, I’ve developed my own habits when writing C, usually picked up from blog posts, writing C# or rust, or just out of perfectionist brain. I’m not saying you should write stuff this way, nor am I claiming it is the best way to write C all the time. I break some of these practices when working with embedded systems or when I’m writing things to be as fast as they can possibly be. But it is the baseline I tend to start with for most projects, and if I don’t write it down, I’ll never be consistent with it. I usually use C23 for my new C projects. When contributing to other projects, I of c", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["post", "2026", "01", "17t210200z", "modify", "2026", "01", "17t232000z", "despite", "proper", "programming", "language", "learnedby", "read", "kr", "lessi", "not", "write", "terribly", "nowadays", "play", "resonite", "get", "write", "load", "modde", "game", "day", "day", "automate", "tedium", "computer", "get", "delegate", "shell", "python", "exist", "infrastructure", "alas", "arise", "want", "write", "c.", "need", "binding", "library", "fill", "niche", "languagearchitecture", "gap", "remain", "favorite", "language", "prototype", "stuff", "sure", "case", "interesting", "language", "standardization", "style", "practice", "language", "clear", "good", "way", "use", "message", "subtly", "embed", "syntax", "official", "documentation", "channel", "not", "official", "documentation", "channel", "syntax", "standard", "library", "construct", "encourage", "particular", "way", "thing", "bunch", "inconsistency", "people", "thing", "andespecially", "early", "day", "language", "standard", "librarythe", "landscape", "general", "practice", "error", "prone", "ve", "develop", "habit", "write", "usually", "pick", "blog", "post", "write", "rust", "perfectionist", "brain", "say", "write", "stuff", "way", "claim", "good", "way", "write", "time", "break", "practice", "work", "embed", "system", "write", "thing", "fast", "possibly", "baseline", "tend", "start", "project", "not", "write", "ill", "consistent", "usually", "use", "c23", "new", "project", "contribute", "project"], "num_tokens": 146, "token_loss_pct": 61.88, "normalized_content": "posted 2026-01-17t210200z modified 2026-01-17t232000z despite it being the first proper programming language i learnedby reading kr front-to-back no lessi dont write c too terribly often nowadays. playing resonite has gotten me into writing a load of c for modding the game and most of what i do day-to-day is automating the tedium on my computer which gets delegated to shell or python because of all the existing infrastructure. alas every now and then something arises where i have to or just want to write some c or c. sometimes its when i need to make some bindings for a library  sometimes its to fill a niche of a languagearchitecture gap . it also remains as my favorite language to prototype stuff in though im not quite sure why. in any case c is an interesting language without much standardization on the whole style or practices part. most other languages have very clear this is the best way to use x messages either subtly embedded in the syntax itself or through official documentation channels. c doesnt have an official documentation channel nor does it have syntax or standard library constructs that encourage one particular way of doing things. from this theres a bunch of inconsistencies in how people do things andespecially in the early days of the language and standard librarythe landscape and general practice is quite error prone. as such ive developed my own habits when writing c usually picked up from blog posts writing c or rust or just out of perfectionist brain. im not saying you should write stuff this way nor am i claiming it is the best way to write c all the time. i break some of these practices when working with embedded systems or when im writing things to be as fast as they can possibly be. but it is the baseline i tend to start with for most projects and if i dont write it down ill never be consistent with it. i usually use c23 for my new c projects. when contributing to other projects i of c"}
{"title": "Michelangelo's first painting, created when he was 12 or 13", "url": "https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html", "content": "in Art , History |   \tJanuary 15th, 2026 5 Comments    Think back, if you will, to the works of art you cre­at­ed at age twelve or thir­teen. For many, per­haps most of us, our out­put at that stage of ado­les­cence amount­ed to direc­tion­less doo­dles, chaot­ic comics, and a few unsteady-at-best school projects. But then, most of us did­n’t grow up to be Michelan­ge­lo. In the late four­teen-eight­ies, when that tow­er­ing Renais­sance artist was still what we would now call a “tween,” he paint­ed The Tor­ment of Saint Antho­ny , a depic­tion of the tit­u­lar reli­gious fig­ure beset by demons in the desert. Though based on a wide­ly known engrav­ing, it nev­er­the­less shows evi­dence of rapid­ly advanc­ing tech­nique, inspi­ra­tion, and even cre­ativ­i­ty — espe­cial­ly when placed under the infrared scan­ner. For about half a mil­len­ni­um, The Tor­ment of Saint Antho­ny was­n’t thought to have been paint­ed by Michelan­ge­lo. As explained in the video from Inspi­rag­gio just below , when the paint­ing sold at Sothe­by’s in 2008, the buy­er took it to the Met­ro­pol­i­tan Muse­um of Art for exam­i­na­tion and clean­ing. “Beneath the lay­ers of dirt accu­mu­lat­ed over the cen­turies,” says the nar­ra­tor, “a very par­tic­u­lar col­or palette appeared. “The tones, the blends, the way the human fig­ure was treat­ed: all of it began to resem­ble the style Michelan­ge­lo would use years lat­er in none oth­er than the Sis­tine Chapel .” Infrared reflec­tog­ra­phy sub­se­quent­ly turned up pen­ti­men­ti , or cor­rec­tion marks, a com­mon indi­ca­tion that “a paint­ing is not a copy, but an orig­i­nal work cre­at­ed with artis­tic free­dom.”   It was the Kim­bell Art Muse­um in Fort Worth, Texas that first bet big on the prove­nance of The Tor­ment of Saint Antho­ny . Its new­ly hired direc­tor pur­chased the paint­ing after turn­ing up “not a sin­gle con­vinc­ing argu­ment against the attri­bu­tion.” Thus acquired, it became “the only paint­ing by Michelan­ge­lo loca", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["art", "history", "january", "15th", "2026", "comment", "think", "work", "art", "create", "age", "thirteen", "output", "stage", "adolescence", "amount", "directionless", "doodle", "chaotic", "comic_strip", "unsteady", "good", "school", "project", "not", "grow", "michelangelo", "late", "fourteen", "eighty", "tower", "renaissance", "artist", "tween", "paint", "torment", "saint", "anthony", "depiction", "titular", "religious", "figure", "beset", "demon", "desert", "base", "widely", "know", "engraving", "show", "evidence", "rapidly", "advance", "technique", "inspiration", "creativity", "especially", "place", "infrared", "scanner", "half", "millennium", "torment", "saint", "anthony", "not", "think", "paint", "michelangelo", "explain", "video", "inspiraggio", "painting", "sell", "sothebys", "2008", "buyer", "take", "metropolitan", "museum", "art", "examination", "cleaning", "beneath", "layer", "dirt", "accumulate", "century", "say", "narrator", "particular", "color", "palette", "appear", "tone", "blend", "way", "human", "figure", "treat", "begin", "resemble", "style", "michelangelo", "use", "year", "later", "sistine", "chapel", "infrared", "reflectography", "subsequently", "turn", "pentimento", "correction", "mark", "common", "indication", "painting", "copy", "original", "work", "create", "artistic", "freedom", "kimbell", "art", "museum", "fort", "worth", "texas", "bet", "big", "provenance", "torment", "saint", "anthony", "newly", "hire", "director", "purchase", "painting", "turn", "single", "convincing", "argument", "attribution", "acquire", "painting", "michelangelo", "loca"], "num_tokens": 151, "token_loss_pct": 53.82, "normalized_content": "in art  history  january 15th 2026 5 comments think back if you will to the works of art you created at age twelve or thirteen. for many perhaps most of us our output at that stage of adolescence amounted to directionless doodles chaotic comics and a few unsteady-at-best school projects. but then most of us didnt grow up to be michelangelo. in the late fourteen-eighties when that towering renaissance artist was still what we would now call a tween he painted the torment of saint anthony  a depiction of the titular religious figure beset by demons in the desert. though based on a widely known engraving it nevertheless shows evidence of rapidly advancing technique inspiration and even creativity  especially when placed under the infrared scanner. for about half a millennium the torment of saint anthony wasnt thought to have been painted by michelangelo. as explained in the video from inspiraggio just below  when the painting sold at sothebys in 2008 the buyer took it to the metropolitan museum of art for examination and cleaning. beneath the layers of dirt accumulated over the centuries says the narrator a very particular color palette appeared. the tones the blends the way the human figure was treated all of it began to resemble the style michelangelo would use years later in none other than the sistine chapel . infrared reflectography subsequently turned up pentimenti  or correction marks a common indication that a painting is not a copy but an original work created with artistic freedom. it was the kimbell art museum in fort worth texas that first bet big on the provenance of the torment of saint anthony . its newly hired director purchased the painting after turning up not a single convincing argument against the attribution. thus acquired it became the only painting by michelangelo loca"}
{"title": "Just the Browser", "url": "https://justthebrowser.com/", "content": "Just the Browser helps you remove AI features, telemetry data reporting, sponsored content, product integrations, and other annoyances from desktop web browsers. The goal is to give you \"just the browser\" and nothing else, using hidden settings in web browsers intended for companies and other organizations. This project includes configuration files for popular web browsers, documentation for installing and modifying them, and easy installation scripts. Everything is open-source on GitHub . The setup script can install the configuration files in a few clicks. You can also follow the manual guides for Google Chrome , Microsoft Edge , and Firefox . Windows: Open a PowerShell prompt as Administrator. You can do this by right-clicking the Windows button in the taskbar, then selecting the \"Terminal (Admin)\" or \"PowerShell (Admin)\" menu option. Next, copy the below command, paste it into the window ( Ctrl+V ), and press the Enter/Return key: Mac and Linux: Search for the Terminal in your applications list and open it. Next, copy the below command, paste it into the window ( Ctrl+V or Cmd+V ), and press the Enter/Return key: You can subscribe to the RSS/Atom releases feed to know when there are important changes to the configuration files, documentation, and scripts: This feed can be used with Feedly , Inoreader , The Old Reader , Feedbin , or any other reader tool. You can also subscribe to new releases with your GitHub account by clicking the Watch button on the repository , then selecting Custom > New releases. Start here if you don't have your preferred web browser installed. You can install the configuration files afterwards. macOS (Universal) Windows 64-bit x86 (amd64) Windows 32-bit x86 Windows 64-bit ARM (ARM64) Debian/Ubuntu 64-bit x86 (amd64) Fedora/openSUSE 64-bit x86 (amd64) Not sure which link to use? Try the official download page . macOS (Universal) Windows 64-bit x86 (amd64) Windows 32-bit x86 Windows 64-bit ARM (ARM64) Not sure which link to use? Try the of", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["browser", "help", "remove", "ai", "feature", "telemetry", "datum", "report", "sponsor", "content", "product", "integration", "annoyance", "desktop", "web", "browser", "goal", "browser", "hidden", "setting", "web", "browser", "intend", "company", "organization", "project", "include", "configuration", "file", "popular", "web", "browser", "documentation", "instal", "modify", "easy", "installation", "script", "open", "source", "github", "setup", "script", "install", "configuration", "file", "click", "follow", "manual", "guide", "google", "chrome", "microsoft", "edge", "firefox", "window", "open", "powershell", "prompt", "administrator", "right", "click", "window", "button", "taskbar", "select", "terminal", "admin", "powershell", "admin", "menu", "option", "copy", "command", "paste", "window", "ctrlv", "press", "enterreturn", "key", "mac", "linux", "search", "terminal", "application", "list", "open", "copy", "command", "paste", "window", "ctrlv", "cmdv", "press", "enterreturn", "key", "subscribe", "rssatom", "release", "feed", "know", "important", "change", "configuration", "file", "documentation", "script", "feed", "feedly", "inoreader", "old", "reader", "feedbin", "reader", "tool", "subscribe", "new", "release", "github", "account", "click", "watch", "button", "repository", "select", "custom", "new", "release", "start", "prefer", "web", "browser", "instal", "install", "configuration", "file", "macos", "universal", "window", "64", "bit", "x86", "amd64", "window", "32", "bit", "x86", "window", "64", "bit", "arm", "arm64", "debianubuntu", "64", "bit", "x86", "amd64", "fedoraopensuse", "64", "bit", "x86", "amd64", "sure", "link", "use", "try", "official", "download", "page", "macos", "universal", "window", "64", "bit", "x86", "amd64", "window", "32", "bit", "x86", "window", "64", "bit", "arm", "arm64", "sure", "link", "use", "try"], "num_tokens": 189, "token_loss_pct": 46.76, "normalized_content": "just the browser helps you remove ai features telemetry data reporting sponsored content product integrations and other annoyances from desktop web browsers. the goal is to give you just the browser and nothing else using hidden settings in web browsers intended for companies and other organizations. this project includes configuration files for popular web browsers documentation for installing and modifying them and easy installation scripts. everything is open-source on github . the setup script can install the configuration files in a few clicks. you can also follow the manual guides for google chrome  microsoft edge  and firefox . windows open a powershell prompt as administrator. you can do this by right-clicking the windows button in the taskbar then selecting the terminal admin or powershell admin menu option. next copy the below command paste it into the window  ctrlv  and press the enterreturn key mac and linux search for the terminal in your applications list and open it. next copy the below command paste it into the window  ctrlv or cmdv  and press the enterreturn key you can subscribe to the rssatom releases feed to know when there are important changes to the configuration files documentation and scripts this feed can be used with feedly  inoreader  the old reader  feedbin  or any other reader tool. you can also subscribe to new releases with your github account by clicking the watch button on the repository  then selecting custom  new releases. start here if you don't have your preferred web browser installed. you can install the configuration files afterwards. macos universal windows 64-bit x86 amd64 windows 32-bit x86 windows 64-bit arm arm64 debianubuntu 64-bit x86 amd64 fedoraopensuse 64-bit x86 amd64 not sure which link to use try the official download page . macos universal windows 64-bit x86 amd64 windows 32-bit x86 windows 64-bit arm arm64 not sure which link to use try the of"}
{"title": "CVEs affecting the Svelte ecosystem", "url": "https://svelte.dev/blog/cves-affecting-the-svelte-ecosystem", "content": "Time to upgrade Elliott Johnson Jan 15 2026 We’ve released patches for 5 vulnerabilities across devalue , svelte , @sveltejs/kit , and @sveltejs/adapter-node . Here’s what you need to know: If you’re using any of these packages, upgrade them to their corresponding non-vulnerable versions: For cross-dependent packages — svelte and @sveltejs/kit depend on devalue — patched versions already include upgraded dependencies. We’re extremely thankful to all of the security researchers who responsibly disclosed these vulnerabilities and worked with us to get them fixed, to the security team at Vercel who helped us navigate the disclosure process, and to the maintainers who worked to publish the fixes. Over the last few weeks, we’ve seen a spate of high profile vulnerabilities affecting popular tools across the web development ecosystem. While they are unfortunate, it has been encouraging to see the community pulling together to keep end users safe. Using the lessons learned from these vulnerabilities, we will invest in processes that will help catch future bugs during the writing and review phases, before they go live. If you think you have discovered a vulnerability in a package maintained by the Svelte team, we urge you to privately report it via the Security tab on the repo in question (or the Svelte repo , if unsure). Full reports are available in the published security advisories, but we’ve included a brief summary of each below. (Yes, this is very similar to the previous CVE. No, it is not the same!)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["time", "upgrade", "elliott", "johnson", "jan", "15", "2026", "ve", "release", "patch", "vulnerability", "devalue", "svelte", "mentionkit", "mentionadapter", "node", "here", "need", "know", "package", "upgrade", "correspond", "non", "vulnerable", "version", "cross", "dependent", "package", "svelte", "mentionkit", "depend", "devalue", "patch", "version", "include", "upgrade", "dependency", "extremely", "thankful", "security", "researcher", "responsibly", "disclose", "vulnerability", "work", "fix", "security", "team", "vercel", "help", "navigate", "disclosure", "process", "maintainer", "work", "publish", "fix", "week", "ve", "see", "spate", "high", "profile", "vulnerability", "affect", "popular", "tool", "web", "development", "ecosystem", "unfortunate", "encouraging", "community", "pull", "end", "user", "safe", "lesson", "learn", "vulnerability", "invest", "process", "help", "catch", "future", "bug", "writing", "review", "phase", "live", "think", "discover", "vulnerability", "package", "maintain", "svelte", "team", "urge", "privately", "report", "security", "tab", "repo", "question", "svelte", "repo", "unsure", "report", "available", "publish", "security", "advisory", "ve", "include", "brief", "summary", "yes", "similar", "previous", "cve"], "num_tokens": 120, "token_loss_pct": 54.55, "normalized_content": "time to upgrade elliott johnson jan 15 2026 weve released patches for 5 vulnerabilities across devalue  svelte  mentionkit  and mentionadapter-node . heres what you need to know if youre using any of these packages upgrade them to their corresponding non-vulnerable versions for cross-dependent packages  svelte and mentionkit depend on devalue  patched versions already include upgraded dependencies. were extremely thankful to all of the security researchers who responsibly disclosed these vulnerabilities and worked with us to get them fixed to the security team at vercel who helped us navigate the disclosure process and to the maintainers who worked to publish the fixes. over the last few weeks weve seen a spate of high profile vulnerabilities affecting popular tools across the web development ecosystem. while they are unfortunate it has been encouraging to see the community pulling together to keep end users safe. using the lessons learned from these vulnerabilities we will invest in processes that will help catch future bugs during the writing and review phases before they go live. if you think you have discovered a vulnerability in a package maintained by the svelte team we urge you to privately report it via the security tab on the repo in question or the svelte repo  if unsure. full reports are available in the published security advisories but weve included a brief summary of each below. yes this is very similar to the previous cve. no it is not the same"}
{"title": "Martin Luther King was talking about a universal basic income before it was cool", "url": "https://www.businessinsider.com/martin-luther-king-jr-universal-basic-income-ai-economic-equality-2026-1", "content": "Every time Lauren publishes a story, you’ll get an alert straight to your inbox! Enter your email  By clicking “Sign up”, you agree to receive emails from Business Insider. In addition, you accept Insider’s Terms of Service and Privacy Policy . Billionaire tech bros like Sam Altman and Elon Musk like to think they operate on the futuristic fringe. On at least one subject that is trendy in tech circles, however, they are way late: basic income. Nearly six decades ago, Martin Luther King Jr. advocated for a form of basic income not unlike what AI leaders today suggest could be the salve to mitigate AI's impact on the workforce. King wrote in his 1967 book, \"Where Do We Go From Here?\" that a guaranteed annual income could ultimately create \"widespread economic security.\" \"Personal conflicts between husband, wife, and children will diminish when the unjust measurement of human worth on a scale of dollars is eliminated,\" he wrote. Every time Lauren publishes a story, you’ll get an alert straight to your inbox! Stay connected to Lauren and get more of their work as it publishes.  By clicking “Sign up”, you agree to receive emails from Business Insider. In addition, you accept Insider's Terms of Service and Privacy Policy . A universal basic income is a recurring cash payment provided to all citizens of a population regardless of socioeconomic standing. A guaranteed basic income , on the other hand, refers to recurring cash payments made to specific citizens, such as those belonging to a certain socioeconomic group, for a set period of time. The idea of a basic income has gained traction in recent years. Many US cities and counties have launched pilot programs, and some have made those programs permanent . King's book came three years after former President Lyndon B. Johnson signed the Civil Rights Act of 1964, making it illegal to discriminate based on race, color, sex, religion, or national origin. It was a time of widespread social unrest. In the book, King sought to ad", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["time", "lauren", "publish", "story", "ll", "alert", "straight", "inbox", "enter", "email", "click", "sign", "agree", "receive", "email", "business", "insider", "addition", "accept", "insider", "term", "service", "privacy", "policy", "billionaire", "tech", "bro", "like", "sam", "altman", "elon", "musk", "like", "think", "operate", "futuristic", "fringe", "subject", "trendy", "tech", "circle", "way", "late", "basic", "income", "nearly", "decade", "ago", "martin", "luther", "king", "jr", "advocate", "form", "basic", "income", "unlike", "ai", "leader", "today", "suggest", "salve", "mitigate", "ai", "impact", "workforce", "king", "write", "1967", "book", "guarantee", "annual", "income", "ultimately", "create", "widespread", "economic", "security", "personal", "conflict", "husband", "wife", "child", "diminish", "unjust", "measurement", "human", "worth", "scale", "dollar", "eliminate", "write", "time", "lauren", "publish", "story", "ll", "alert", "straight", "inbox", "stay", "connected", "lauren", "work", "publish", "click", "sign", "agree", "receive", "email", "business", "insider", "addition", "accept", "insider", "term", "service", "privacy", "policy", "universal", "basic", "income", "recur", "cash", "payment", "provide", "citizen", "population", "regardless", "socioeconomic", "standing", "guaranteed", "basic", "income", "hand", "refer", "recur", "cash", "payment", "specific", "citizen", "belong", "certain", "socioeconomic", "group", "set", "period", "time", "idea", "basic", "income", "gain", "traction", "recent", "year", "city", "county", "launch", "pilot", "program", "program", "permanent", "king", "book", "come", "year", "president", "lyndon", "b.", "johnson", "sign", "civil", "right", "act", "1964", "make", "illegal", "discriminate", "base", "race", "color", "sex", "religion", "national", "origin", "time", "widespread", "social", "unrest", "book", "king", "seek", "ad"], "num_tokens": 193, "token_loss_pct": 45.94, "normalized_content": "every time lauren publishes a story youll get an alert straight to your inbox enter your email by clicking sign up you agree to receive emails from business insider. in addition you accept insiders terms of service and privacy policy . billionaire tech bros like sam altman and elon musk like to think they operate on the futuristic fringe. on at least one subject that is trendy in tech circles however they are way late basic income. nearly six decades ago martin luther king jr. advocated for a form of basic income not unlike what ai leaders today suggest could be the salve to mitigate ai's impact on the workforce. king wrote in his 1967 book where do we go from here that a guaranteed annual income could ultimately create widespread economic security. personal conflicts between husband wife and children will diminish when the unjust measurement of human worth on a scale of dollars is eliminated he wrote. every time lauren publishes a story youll get an alert straight to your inbox stay connected to lauren and get more of their work as it publishes. by clicking sign up you agree to receive emails from business insider. in addition you accept insider's terms of service and privacy policy . a universal basic income is a recurring cash payment provided to all citizens of a population regardless of socioeconomic standing. a guaranteed basic income  on the other hand refers to recurring cash payments made to specific citizens such as those belonging to a certain socioeconomic group for a set period of time. the idea of a basic income has gained traction in recent years. many us cities and counties have launched pilot programs and some have made those programs permanent . king's book came three years after former president lyndon b. johnson signed the civil rights act of 1964 making it illegal to discriminate based on race color sex religion or national origin. it was a time of widespread social unrest. in the book king sought to ad"}
{"title": "Show HN: Opal Editor, free Obsidian alternative for markdown and site publishing", "url": "https://github.com/rbbydotdev/opal", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . local-first browser-first markdown workspace wysiwig editor and publisher - built with mdx-editor, code mirror 6, react, shadcn, & typescript There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .  A local-first markdown editor and static publisher—no-server-required, Git-aware, with complete self-custody and zero backend dependencies.    Visit opaledx.com to start writing. Read the full documentation Opal Editor is completely free to use and own with absolutely no profit motive or incentive. Beyond creating a feature-rich Markdown editor and publisher for the open-source community, it also serves as a way for me to showcase my skills as a developer. If you like what you see and would like to discuss an opening at your company/startup/workshop, feel free to reach out via my website or by email . Enjoy! local-first browser-first markdown workspace wysiwig editor and publisher - built with mdx-editor, code mirror 6, react, shadcn, & typescript There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . There was an error while loading. Please reload this page .", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "error", "load", "reload", "page", "error", "load", "reload", "page", "local", "browser", "markdown", "workspace", "wysiwig", "editor", "publisher", "build", "mdx", "editor", "code", "mirror", "react", "shadcn", "typescript", "error", "load", "reload", "page", "error", "load", "reload", "page", "local", "markdown", "editor", "static", "publisherno", "server", "require", "git", "aware", "complete", "self", "custody", "zero", "backend", "dependency", "visit", "opaledx.com", "start", "write", "read", "documentation", "opal", "editor", "completely", "free", "use", "absolutely", "profit", "motive", "incentive", "create", "feature", "rich", "markdown", "editor", "publisher", "open", "source", "community", "serve", "way", "showcase", "skill", "developer", "like", "like", "discuss", "opening", "companystartupworkshop", "feel", "free", "reach", "website", "email", "enjoy", "local", "browser", "markdown", "workspace", "wysiwig", "editor", "publisher", "build", "mdx", "editor", "code", "mirror", "react", "shadcn", "typescript", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page", "error", "load", "reload", "page"], "num_tokens": 125, "token_loss_pct": 55.67, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . there was an error while loading. please reload this page . there was an error while loading. please reload this page . local-first browser-first markdown workspace wysiwig editor and publisher - built with mdx-editor code mirror 6 react shadcn  typescript there was an error while loading. please reload this page . there was an error while loading. please reload this page . a local-first markdown editor and static publisherno-server-required git-aware with complete self-custody and zero backend dependencies. visit opaledx.com to start writing. read the full documentation opal editor is completely free to use and own with absolutely no profit motive or incentive. beyond creating a feature-rich markdown editor and publisher for the open-source community it also serves as a way for me to showcase my skills as a developer. if you like what you see and would like to discuss an opening at your companystartupworkshop feel free to reach out via my website or by email . enjoy local-first browser-first markdown workspace wysiwig editor and publisher - built with mdx-editor code mirror 6 react shadcn  typescript there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page . there was an error while loading. please reload this page ."}
{"title": "What twenty years of DevOps has failed to do", "url": "https://www.honeycomb.io/blog/you-had-one-job-why-twenty-years-of-devops-has-failed-to-do-it", "content": "Futureproof your software for what comes next with the Honeycomb platform. Discover why Honeycomb is the better choice for your engineers, your customers, and your bottom line. Start your journey with the definitive guide to observability. Download our complimentary ebook. Bring observability to every software engineer. Learn about our company, mission and values. Meet the people behind Honeycomb. Come for the impact, stay for the culture. See the latest press releases from Honeycomb. Already a Honeycomb customer? I think the entire DevOps movement was a mighty, twenty year battle to achieve one thing: a single feedback loop connecting devs with prod.\n\nOn those grounds, it failed. By: Charity Majors Let’s start with a question. What is DevOps all about? I’ll tell you my answer. In retrospect, I think the entire DevOps movement was a mighty, twenty year battle to achieve one thing: a single feedback loop connecting devs with prod. On those grounds, it failed. Not because software engineers weren’t good at their jobs, or didn’t care enough. It failed because the technology wasn’t good enough. The tools we gave them weren’t designed for this, so using them could easily double, triple, or quadruple the time it took to do their job: writing business logic. This isn’t true everywhere. Please keep in mind that all data tools are effectively fungible if you can assume an infinite amount of time, money, and engineering skill. You can run production off an Excel spreadsheet if you have to, and some SREs have done so . That doesn’t make it a great solution, the right use of resources, or accessible to the median engineering org. The good news is that AI has changed this . The technology we have now is good enough to create a feedback loop between developers and production systems for the median engineering team, for the first time ever. The bad news is also that AI has changed this . Our existing feedback loops are unprepared to deal with the current amount of code slop. And I", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["futureproof", "software", "come", "honeycomb", "platform", "discover", "honeycomb", "well", "choice", "engineer", "customer", "line", "start", "journey", "definitive", "guide", "observability", "download", "complimentary", "ebook", "bring", "observability", "software", "engineer", "learn", "company", "mission", "value", "meet", "people", "honeycomb", "come", "impact", "stay", "culture", "late", "press", "release", "honeycomb", "honeycomb", "customer", "think", "entire", "devop", "movement", "mighty", "year", "battle", "achieve", "thing", "single", "feedback", "loop", "connect", "devs", "prod", "ground", "fail", "charity", "major", "let", "start", "question", "devop", "ill", "tell", "answer", "retrospect", "think", "entire", "devop", "movement", "mighty", "year", "battle", "achieve", "thing", "single", "feedback", "loop", "connect", "devs", "prod", "ground", "fail", "software", "engineer", "not", "good", "job", "not", "care", "fail", "technology", "not", "good", "tool", "give", "not", "design", "easily", "double", "triple", "quadruple", "time", "take", "job", "write", "business", "logic", "not", "true", "mind", "data", "tool", "effectively", "fungible", "assume", "infinite", "time", "money", "engineering", "skill", "run", "production", "excel", "spreadsheet", "sre", "not", "great", "solution", "right", "use", "resource", "accessible", "median", "engineering", "org", "good", "news", "ai", "change", "technology", "good", "create", "feedback", "loop", "developer", "production", "system", "median", "engineering", "team", "time", "bad", "news", "ai", "change", "exist", "feedback", "loop", "unprepared", "deal", "current", "code", "slop"], "num_tokens": 166, "token_loss_pct": 54.64, "normalized_content": "futureproof your software for what comes next with the honeycomb platform. discover why honeycomb is the better choice for your engineers your customers and your bottom line. start your journey with the definitive guide to observability. download our complimentary ebook. bring observability to every software engineer. learn about our company mission and values. meet the people behind honeycomb. come for the impact stay for the culture. see the latest press releases from honeycomb. already a honeycomb customer i think the entire devops movement was a mighty twenty year battle to achieve one thing a single feedback loop connecting devs with prod. on those grounds it failed. by charity majors lets start with a question. what is devops all about ill tell you my answer. in retrospect i think the entire devops movement was a mighty twenty year battle to achieve one thing a single feedback loop connecting devs with prod. on those grounds it failed. not because software engineers werent good at their jobs or didnt care enough. it failed because the technology wasnt good enough. the tools we gave them werent designed for this so using them could easily double triple or quadruple the time it took to do their job writing business logic. this isnt true everywhere. please keep in mind that all data tools are effectively fungible if you can assume an infinite amount of time money and engineering skill. you can run production off an excel spreadsheet if you have to and some sres have done so . that doesnt make it a great solution the right use of resources or accessible to the median engineering org. the good news is that ai has changed this . the technology we have now is good enough to create a feedback loop between developers and production systems for the median engineering team for the first time ever. the bad news is also that ai has changed this . our existing feedback loops are unprepared to deal with the current amount of code slop. and i"}
{"title": "Kip: A programming language based on grammatical cases of Turkish", "url": "https://github.com/kip-dili/kip", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . A programming language in Turkish where grammatical case and mood are part of the type system. There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . Kip (meaning \"grammatical mood\" in Turkish) is an experimental programming language that uses Turkish grammatical cases as part of its type system. It demonstrates how natural language morphology—specifically Turkish noun cases and vowel harmony—can be integrated into programming language design. This is a research/educational project exploring the intersection of linguistics and type theory, not a production programming language. There is also a tutorial in Turkish and a tutorial in English that explains how to write Kip programs. Note Kip is experimental. Expect changes in syntax and behavior over time. For you to get a taste of what Kip looks like, here is an example program that prompts the user to enter a number and then prints that many of the Fibonacci numbers: Kip uses Turkish noun cases (ismin halleri) to determine argument relationships in function calls: Because Turkish cases mark grammatical relationships explicitly, Kip allows flexible argument ordering. These two calls are equivalent: As long as arguments have different case suffixes or different types, Kip can determine which argument is which. Define algebraic data types with Turkish syntax: Type variables are supported for generic data structures: Pattern match using the conditional suffix -sa/-se : Supports nested pattern matching, binders, and wildcard patterns ( değilse ): Define named constants with diyelim : Sequencing with -ip/-ıp/-up/-üp suffixes and binding with olarak : Integers ( tam-sayı ): Strings ( dizge ): I/O: Foma - finite-state morphology toolkit Stack - Haskell build tool Tip If you only want to explore the language, you can start with stack exec", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "programming", "language", "turkish", "grammatical", "case", "mood", "type", "system", "error", "load", "reload", "page", "error", "load", "reload", "page", "kip", "mean", "grammatical", "mood", "turkish", "experimental", "programming", "language", "use", "turkish", "grammatical", "case", "type", "system", "demonstrate", "natural", "language", "morphologyspecifically", "turkish", "noun", "case", "vowel", "harmonycan", "integrate", "programming", "language", "design", "researcheducational", "project", "explore", "intersection", "linguistic", "type", "theory", "production", "programming", "language", "tutorial", "turkish", "tutorial", "english", "explain", "write", "kip", "program", "note", "kip", "experimental", "expect", "change", "syntax", "behavior", "time", "taste", "kip", "look", "like", "example", "program", "prompt", "user", "enter", "number", "print", "fibonacci", "number", "kip", "use", "turkish", "noun", "case", "ismin", "halleri", "determine", "argument", "relationship", "function", "call", "turkish", "case", "mark", "grammatical", "relationship", "explicitly", "kip", "allow", "flexible", "argument", "order", "call", "equivalent", "long", "argument", "different", "case", "suffix", "different", "type", "kip", "determine", "argument", "define", "algebraic", "datum", "type", "turkish", "syntax", "type", "variable", "support", "generic", "datum", "structure", "pattern", "match", "conditional", "suffix", "-sa", "se", "support", "nest", "pattern", "matching", "binder", "wildcard", "pattern", "değilse", "define", "name", "constant", "diyelim", "sequence", "-ip", "ıp", "üp", "suffix", "bind", "olarak", "integer", "tam", "sayı", "string", "dizge", "io", "foma", "finite", "state", "morphology", "toolkit", "stack", "haskell", "build", "tool", "tip", "want", "explore", "language", "start", "stack", "exec"], "num_tokens": 184, "token_loss_pct": 45.4, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . a programming language in turkish where grammatical case and mood are part of the type system. there was an error while loading. please reload this page . there was an error while loading. please reload this page . kip meaning grammatical mood in turkish is an experimental programming language that uses turkish grammatical cases as part of its type system. it demonstrates how natural language morphologyspecifically turkish noun cases and vowel harmonycan be integrated into programming language design. this is a researcheducational project exploring the intersection of linguistics and type theory not a production programming language. there is also a tutorial in turkish and a tutorial in english that explains how to write kip programs. note kip is experimental. expect changes in syntax and behavior over time. for you to get a taste of what kip looks like here is an example program that prompts the user to enter a number and then prints that many of the fibonacci numbers kip uses turkish noun cases ismin halleri to determine argument relationships in function calls because turkish cases mark grammatical relationships explicitly kip allows flexible argument ordering. these two calls are equivalent as long as arguments have different case suffixes or different types kip can determine which argument is which. define algebraic data types with turkish syntax type variables are supported for generic data structures pattern match using the conditional suffix -sa-se  supports nested pattern matching binders and wildcard patterns  değilse  define named constants with diyelim  sequencing with -ip-ıp-up-üp suffixes and binding with olarak  integers  tam-sayı  strings  dizge  io foma - finite-state morphology toolkit stack - haskell build tool tip if you only want to explore the language you can start with stack exec"}
{"title": "Amazon is ending all inventory commingling as of March 31, 2026", "url": "https://twitter.com/ghhughes/status/2012824754319753456", "content": "Amazon is ending all inventory commingling as of March 31, 2026. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["amazon", "end", "inventory", "commingling", "march", "31", "2026", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 50.0, "normalized_content": "amazon is ending all inventory commingling as of march 31 2026. score none. author none. date none"}
{"title": "Cows can use sophisticated tools", "url": "https://nautil.us/the-far-side-had-it-all-wrong-cows-really-can-use-sophisticated-tools-1262026/", "content": "Art+Science Biology + Beyond Catalysts of Discovery Cosmos Culture Currents Earth Life Mind Ocean One Question Quanta Abstractions Rewilding Science at the Ballot Box Science Philanthropy Alliance Spark of Science The Animal Issue The Climates Issue The Food Issue The Kinship Issue The Porthole The Reality Issue The Rebel Issue Women in Science & Engineering Upending Gary Larson’s premise that cows are too daft to use tools Upending Gary Larson’s premise that cows are too daft to use tools The full Nautilus archive • eBooks & Special Editions • Ad-free reading I f cows could use tools, imagine the scenes that might unfold: cutting wires to escape from their pastures; extracting themselves from milking machines; or removing the twine on hay bales. Cows haven’t been seen doing any of these things, of course. But a study published today in Current Biology demonstrates a cow named Veronika effectively using a deck broom as a scratching tool, satisfying the scientific definition of tool use as “the manipulation of an external object to achieve a goal via a mechanical interface.” Veronika is a pet Brown Swiss cow ( Bos taurus ) kept as a companion by a farmer. In a series of 10 trials, researchers from the University of Veterinary Medicine Vienna presented her with a deck broom tossed on the ground in a random orientation. Each trial, they recorded which end of the brush she selected and how she used it. Veronika manipulated the broom with her mouth, positioning it under her tongue, then wedging it into the gaps between her incisors and molars for a stable grip. Veronika adeptly used the deck brush to scratch her itches, manipulating it to target different areas. Across the randomized trials, she chose the bristled end to scratch her hindquarters but switched to the stick end for softer lower-body areas. Across repeat trials, she made consistent choices about how to wield the broom. “When I saw the footage, it was immediately clear that this was not accidental,” said stud", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["artscience", "biology", "catalyst", "discovery", "cosmos", "culture", "current", "earth", "life", "mind", "ocean", "question", "quanta", "abstraction", "rewilde", "science", "ballot", "box", "science", "philanthropy", "alliance", "spark", "science", "animal", "issue", "climate", "issue", "food", "issue", "kinship", "issue", "porthole", "reality", "issue", "rebel", "issue", "woman", "science", "engineering", "upend", "gary", "larson", "premise", "cow", "daft", "use", "tool", "upend", "gary", "larson", "premise", "cow", "daft", "use", "tool", "nautilus", "archive", "ebook", "special", "edition", "ad", "free", "reading", "cow", "use", "tool", "imagine", "scene", "unfold", "cut", "wire", "escape", "pasture", "extract", "milk", "machine", "remove", "twine", "hay", "bale", "cow", "not", "see", "thing", "course", "study", "publish", "today", "current", "biology", "demonstrate", "cow", "name", "veronika", "effectively", "deck", "broom", "scratch", "tool", "satisfy", "scientific", "definition", "tool", "use", "manipulation", "external", "object", "achieve", "goal", "mechanical", "interface", "veronika", "pet", "brown", "swiss", "cow", "bos", "taurus", "keep", "companion", "farmer", "series", "10", "trial", "researcher", "university", "veterinary", "medicine", "vienna", "present", "deck", "broom", "toss", "ground", "random", "orientation", "trial", "record", "end", "brush", "select", "veronika", "manipulate", "broom", "mouth", "position", "tongue", "wedge", "gap", "incisor", "molar", "stable", "grip", "veronika", "adeptly", "deck", "brush", "scratch", "itch", "manipulate", "target", "different", "area", "randomized", "trial", "choose", "bristled", "end", "scratch", "hindquarter", "switch", "stick", "end", "soft", "low", "body", "area", "repeat", "trial", "consistent", "choice", "wield", "broom", "see", "footage", "immediately", "clear", "accidental", "say", "stud"], "num_tokens": 190, "token_loss_pct": 45.56, "normalized_content": "artscience biology  beyond catalysts of discovery cosmos culture currents earth life mind ocean one question quanta abstractions rewilding science at the ballot box science philanthropy alliance spark of science the animal issue the climates issue the food issue the kinship issue the porthole the reality issue the rebel issue women in science  engineering upending gary larsons premise that cows are too daft to use tools upending gary larsons premise that cows are too daft to use tools the full nautilus archive  ebooks  special editions  ad-free reading i f cows could use tools imagine the scenes that might unfold cutting wires to escape from their pastures extracting themselves from milking machines or removing the twine on hay bales. cows havent been seen doing any of these things of course. but a study published today in current biology demonstrates a cow named veronika effectively using a deck broom as a scratching tool satisfying the scientific definition of tool use as the manipulation of an external object to achieve a goal via a mechanical interface. veronika is a pet brown swiss cow  bos taurus  kept as a companion by a farmer. in a series of 10 trials researchers from the university of veterinary medicine vienna presented her with a deck broom tossed on the ground in a random orientation. each trial they recorded which end of the brush she selected and how she used it. veronika manipulated the broom with her mouth positioning it under her tongue then wedging it into the gaps between her incisors and molars for a stable grip. veronika adeptly used the deck brush to scratch her itches manipulating it to target different areas. across the randomized trials she chose the bristled end to scratch her hindquarters but switched to the stick end for softer lower-body areas. across repeat trials she made consistent choices about how to wield the broom. when i saw the footage it was immediately clear that this was not accidental said stud"}
{"title": "SIMD Programming in Pure Rust", "url": "https://kerkour.com/introduction-rust-simd", "content": "SIMD Programming in Pure Rust. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["simd", "programming", "pure", "rust", "score", "author", "date"], "num_tokens": 7, "token_loss_pct": 50.0, "normalized_content": "simd programming in pure rust. score none. author none. date none"}
{"title": "A rationalist's guide to manifestation. Or, how to live a magical life", "url": "https://read.isabelunraveled.com/p/manifest-rationally", "content": "I’ve been helping my hyper-rational clients and peers manifest what they want for a number of years now, and I finally feel ready to articulate how this process works in the most logical, straight-forward terms possible. My goal is that this essay makes ‘manifestation’ make sense to anyone, and to explain it in a way that feels approachable for you, dear reader, to begin doing, today (and what better day to begin than the first day of a new year?). All I ask is that you set aside your priors associated to the word ‘manifestation’ before we begin, and open your mind to the possibility that you really could get the things that you want much more effortlessly—and perhaps even magically—than you’ve let yourself believe before. What you will see from reading this is that while there is some whimsy to this art, there is also a lot you can influence consciously when you actually understand how your mind works. That’s right: you’ll get to use your agency as well as your third eye, I promise. Now let’s begin. The first thing I’ve learned is that despite the controversial reputation of the word ‘manifestation’, the process it refers to is actually incredibly logical, and when the rational mind comprehends the effectiveness of simply aligning your mind towards what you want , it becomes much more willing to do so. The second is that once you understand the basic mechanics of manifestation, you become a weapon at rapidly up-levelling your life . I’ll start with some friendly, approachable examples for how I think about manifestation in my life—which is, broadly speaking, that I think of it as cleaning up your inner world, identifying what you truly want and giving yourself full permission to pursue and receive it, effortlessly . Also, to embrace the part of it that really does feel like magic—which often looks like being given, offered, or invited to almost eerily aligned opportunities and moments that you have previously held in your mind’s eye . If you want more whimsical & m", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ve", "help", "hyper", "rational", "client", "peer", "manifest", "want", "number", "year", "finally", "feel", "ready", "articulate", "process", "work", "logical", "straight", "forward", "term", "possible", "goal", "essay", "make", "manifestation", "sense", "explain", "way", "feel", "approachable", "dear", "reader", "begin", "today", "well", "day", "begin", "day", "new", "year", "ask", "set", "aside", "prior", "associate", "word", "manifestation", "begin", "open", "mind", "possibility", "thing", "want", "effortlesslyand", "magicallythan", "ve", "let", "believe", "read", "whimsy", "art", "lot", "influence", "consciously", "actually", "understand", "mind", "work", "right", "ll", "use", "agency", "eye", "promise", "let", "begin", "thing", "ve", "learn", "despite", "controversial", "reputation", "word", "manifestation", "process", "refer", "actually", "incredibly", "logical", "rational", "mind", "comprehend", "effectiveness", "simply", "align", "mind", "want", "willing", "second", "understand", "basic", "mechanic", "manifestation", "weapon", "rapidly", "level", "life", "ill", "start", "friendly", "approachable", "example", "think", "manifestation", "lifewhich", "broadly", "speak", "think", "clean", "inner", "world", "identify", "truly", "want", "give", "permission", "pursue", "receive", "effortlessly", "embrace", "feel", "like", "magicwhich", "look", "like", "give", "offer", "invite", "eerily", "align", "opportunity", "moment", "previously", "hold", "mind", "eye", "want", "whimsical"], "num_tokens": 148, "token_loss_pct": 59.0, "normalized_content": "ive been helping my hyper-rational clients and peers manifest what they want for a number of years now and i finally feel ready to articulate how this process works in the most logical straight-forward terms possible. my goal is that this essay makes manifestation make sense to anyone and to explain it in a way that feels approachable for you dear reader to begin doing today and what better day to begin than the first day of a new year. all i ask is that you set aside your priors associated to the word manifestation before we begin and open your mind to the possibility that you really could get the things that you want much more effortlesslyand perhaps even magicallythan youve let yourself believe before. what you will see from reading this is that while there is some whimsy to this art there is also a lot you can influence consciously when you actually understand how your mind works. thats right youll get to use your agency as well as your third eye i promise. now lets begin. the first thing ive learned is that despite the controversial reputation of the word manifestation the process it refers to is actually incredibly logical and when the rational mind comprehends the effectiveness of simply aligning your mind towards what you want  it becomes much more willing to do so. the second is that once you understand the basic mechanics of manifestation you become a weapon at rapidly up-levelling your life . ill start with some friendly approachable examples for how i think about manifestation in my lifewhich is broadly speaking that i think of it as cleaning up your inner world identifying what you truly want and giving yourself full permission to pursue and receive it effortlessly . also to embrace the part of it that really does feel like magicwhich often looks like being given offered or invited to almost eerily aligned opportunities and moments that you have previously held in your minds eye . if you want more whimsical  m"}
{"title": "Mammals have evolved into ant eaters 12 times since the dinosaur age – study (2025)", "url": "https://phys.org/news/2025-07-mammals-evolved-ant-eaters-dinosaur.html", "content": "Sign in with Forget Password? Learn more share this! 502 Tweet Share Email July 16, 2025 by Jesse Jenkins, New Jersey Institute of Technology edited by Stephanie Baum , \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed by Andrew Zinin   This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tthe following attributes while ensuring the content's credibility: fact-checked peer-reviewed publication trusted source proofread Mammals have developed some unusual eating habits over the past 100 million years, but a new study has uncovered the surprising lengths to which some have gone to satisfy one of the more peculiar—a taste for ants and termites. Findings published in Evolution reveal that mammals independently evolved specialized adaptations for exclusively feeding on ants and termites at least 12 times since the Cenozoic era began, roughly 66 million years ago. Researchers say the convergent evolution among mammals toward this dietary strategy—called myrmecophagy—emerged following the K-Pg extinction and fall of non-avian dinosaurs, which reshaped ecosystems and set the stage for ant and termite colonies to rapidly expand worldwide, driving extreme shifts in feeding modes for certain species. \"There's not been an investigation into how this dramatic diet evolved across all known mammal species until now,\" said Phillip Barden, the study's corresponding author and associate professor of biology at New Jersey Institute of Technology (NJIT). \"This work gives us the first real roadmap, and what really stands out is just how powerful a selective force ants and termites have been over the last 50 million years—shaping environments and literally changing the face of entire species.\" Over 200 mammal species are known to eat ants and termites today, yet only about 20 true myrmecophages—such as giant anteaters, aardvarks and pangolins—have evolved traits like long sticky tongues, specialized claws and stomachs, and reduced o", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["sign", "forget", "password", "learn", "share", "502", "tweet", "share", "email", "july", "16", "2025", "jesse", "jenkins", "new", "jersey", "institute", "technology", "edit", "stephanie", "baum", "review", "andrew", "zinin", "article", "review", "accord", "science", "editorial", "process", "policy", "editor", "highlight", "follow", "attribute", "ensure", "content", "credibility", "fact", "check", "peer", "review", "publication", "trust", "source", "proofread", "mammal", "develop", "unusual", "eating", "habit", "past", "100", "million", "year", "new", "study", "uncover", "surprising", "length", "go", "satisfy", "peculiara", "taste", "ant", "termite", "finding", "publish", "evolution", "reveal", "mammal", "independently", "evolve", "specialized", "adaptation", "exclusively", "feed", "ant", "termite", "12", "time", "cenozoic", "era", "begin", "roughly", "66", "million", "year", "ago", "researcher", "convergent", "evolution", "mammal", "dietary", "strategycalled", "myrmecophagyemerge", "follow", "pg", "extinction", "fall", "non", "avian", "dinosaur", "reshape", "ecosystem", "set", "stage", "ant", "termite", "colony", "rapidly", "expand", "worldwide", "drive", "extreme", "shift", "feeding", "mode", "certain", "specie", "investigation", "dramatic", "diet", "evolve", "know", "mammal", "specie", "say", "phillip", "barden", "study", "correspond", "author", "associate", "professor", "biology", "new", "jersey", "institute", "technology", "njit", "work", "give", "real", "roadmap", "stand", "powerful", "selective", "force", "ant", "termite", "50", "million", "yearsshaping", "environment", "literally", "change", "face", "entire", "specie", "200", "mammal", "specie", "know", "eat", "ant", "termite", "today", "20", "true", "myrmecophagessuch", "giant", "anteater", "aardvarks", "pangolinshave", "evolve", "trait", "like", "long", "sticky", "tongue", "specialized", "claw", "stomach", "reduce"], "num_tokens": 185, "token_loss_pct": 40.71, "normalized_content": "sign in with forget password learn more share this 502 tweet share email july 16 2025 by jesse jenkins new jersey institute of technology edited by stephanie baum  reviewed by andrew zinin this article has been reviewed according to science x's editorial process and policies . editors have highlighted the following attributes while ensuring the content's credibility fact-checked peer-reviewed publication trusted source proofread mammals have developed some unusual eating habits over the past 100 million years but a new study has uncovered the surprising lengths to which some have gone to satisfy one of the more peculiara taste for ants and termites. findings published in evolution reveal that mammals independently evolved specialized adaptations for exclusively feeding on ants and termites at least 12 times since the cenozoic era began roughly 66 million years ago. researchers say the convergent evolution among mammals toward this dietary strategycalled myrmecophagyemerged following the k-pg extinction and fall of non-avian dinosaurs which reshaped ecosystems and set the stage for ant and termite colonies to rapidly expand worldwide driving extreme shifts in feeding modes for certain species. there's not been an investigation into how this dramatic diet evolved across all known mammal species until now said phillip barden the study's corresponding author and associate professor of biology at new jersey institute of technology njit. this work gives us the first real roadmap and what really stands out is just how powerful a selective force ants and termites have been over the last 50 million yearsshaping environments and literally changing the face of entire species. over 200 mammal species are known to eat ants and termites today yet only about 20 true myrmecophagessuch as giant anteaters aardvarks and pangolinshave evolved traits like long sticky tongues specialized claws and stomachs and reduced o"}
{"title": "There is no comfortable reading position", "url": "https://slate.com/life/2026/01/body-books-reading-position-posture-pain.html", "content": "Enter your email to receive alerts for this author. Sign in or create an account to better manage your email preferences. Are you sure you want to unsubscribe from email alerts for Luke Winkie ? Sign up for the Slatest to get the most insightful analysis, criticism, and advice out there, delivered to your inbox daily. For the 10 th year in a row, my New Year’s resolution is to read more books. Ideally, as I tend to tell myself during these protean early weeks of January, 2026 will be remembered for languorous evenings on the couch, tearing through the inventory of novels that crowd the modest capacity of my living-room shelves, perhaps with a tumbler of scotch resting on a coaster. I revel in the fantasy—I dream about finally cracking open A Confederacy of Dunces, or knocking out the last two entries of the Broken Earth trilogy, or making time for that Patti Smith memoir that I bought more than a decade ago. If I’m really feeling myself, I contemplate aiming even higher. Tolstoy? Pynchon? I mean, there’s also that copy of The Pale King that has been steadily yellowing on my coffee table for quite some time now. And yet, I already know how this saga is going to end. The year will draw to a close with a piddling number of new entries to my Goodreads, hopelessly incongruous with the size of my bibliophilic ambitions. Ask me why I never seem to read as much as I like, and I could gesture toward the well-worn afflictions of modernity—ballooning screen time, addictive algorithms, frayed attention spans. But one of my fundamental issues with literature is far more prosaic. In fact, I think it’s much more common than anyone would like to admit. Why is it that no matter what I do, I can never get comfortable while reading a book? Don’t act like you don’t know what I’m talking about. This is a species-wide affliction. The first published novel in history is widely considered to be The Tale of Genji, a courtly drama written in the late 11 th century by the Japanese noblewoman", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["enter", "email", "receive", "alert", "author", "sign", "create", "account", "well", "manage", "email", "preference", "sure", "want", "unsubscribe", "email", "alert", "luke", "winkie", "sign", "slat", "insightful", "analysis", "criticism", "advice", "deliver", "inbox", "daily", "10", "th", "year", "row", "new", "year", "resolution", "read", "book", "ideally", "tend", "tell", "protean", "early", "week", "january", "2026", "remember", "languorous", "evening", "couch", "tear", "inventory", "novel", "crowd", "modest", "capacity", "living", "room", "shelf", "tumbler", "scotch", "rest", "coaster", "revel", "fantasyi", "dream", "finally", "crack", "open", "confederacy", "dunce", "knock", "entry", "broken", "earth", "trilogy", "make", "time", "patti", "smith", "memoir", "buy", "decade", "ago", "feel", "contemplate", "aim", "high", "tolstoy", "pynchon", "mean", "copy", "pale", "king", "steadily", "yellow", "coffee", "table", "time", "know", "saga", "go", "end", "year", "draw", "close", "piddling", "number", "new", "entry", "goodread", "hopelessly", "incongruous", "size", "bibliophilic", "ambition", "ask", "read", "like", "gesture", "wear", "affliction", "modernityballoone", "screen", "time", "addictive", "algorithm", "fray", "attention", "span", "fundamental", "issue", "literature", "far", "prosaic", "fact", "think", "common", "like", "admit", "matter", "comfortable", "read", "book", "not", "act", "like", "not", "know", "talk", "species", "wide", "affliction", "publish", "novel", "history", "widely", "consider", "tale", "genji", "courtly", "drama", "write", "late", "11", "th", "century", "japanese", "noblewoman"], "num_tokens": 168, "token_loss_pct": 56.02, "normalized_content": "enter your email to receive alerts for this author. sign in or create an account to better manage your email preferences. are you sure you want to unsubscribe from email alerts for luke winkie  sign up for the slatest to get the most insightful analysis criticism and advice out there delivered to your inbox daily. for the 10 th year in a row my new years resolution is to read more books. ideally as i tend to tell myself during these protean early weeks of january 2026 will be remembered for languorous evenings on the couch tearing through the inventory of novels that crowd the modest capacity of my living-room shelves perhaps with a tumbler of scotch resting on a coaster. i revel in the fantasyi dream about finally cracking open a confederacy of dunces or knocking out the last two entries of the broken earth trilogy or making time for that patti smith memoir that i bought more than a decade ago. if im really feeling myself i contemplate aiming even higher. tolstoy pynchon i mean theres also that copy of the pale king that has been steadily yellowing on my coffee table for quite some time now. and yet i already know how this saga is going to end. the year will draw to a close with a piddling number of new entries to my goodreads hopelessly incongruous with the size of my bibliophilic ambitions. ask me why i never seem to read as much as i like and i could gesture toward the well-worn afflictions of modernityballooning screen time addictive algorithms frayed attention spans. but one of my fundamental issues with literature is far more prosaic. in fact i think its much more common than anyone would like to admit. why is it that no matter what i do i can never get comfortable while reading a book dont act like you dont know what im talking about. this is a species-wide affliction. the first published novel in history is widely considered to be the tale of genji a courtly drama written in the late 11 th century by the japanese noblewoman"}
{"title": "Ask HN: Clipboard overflows causing system crashes in macOS Tahoe 26.3 beta 2?", "url": "item?id=46693038", "content": "Ask HN: Clipboard overflows causing system crashes in macOS Tahoe 26.3 beta 2?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "clipboard", "overflow", "cause", "system", "crash", "macos", "tahoe", "26.3", "beta", "score", "author", "date"], "num_tokens": 14, "token_loss_pct": 36.36, "normalized_content": "ask hn clipboard overflows causing system crashes in macos tahoe 26.3 beta 2. score none. author none. date none"}
{"title": "EU Parliament freezes US trade deal ratification", "url": "https://www.lemonde.fr/en/international/article/2026/01/20/eu-parliament-freezes-us-trade-deal-after-trump-s-tariff-threats-over-greenland_6749625_4.html", "content": "Date: Loading date... Time: Loading time... Le Monde with AFP 1 min read EU lawmakers have agreed to hold off on ratifying a key trade deal with the United States following President Donald Trump's tariff threats over Greenland, the main political groups said on Tuesday, January 20. The warning shot from the European Parliament comes as the 27-nation bloc weighs how hard to hit back if Trump follows through against Washington's long-standing allies. The Parliament was planning a vote in the coming weeks on removing tariffs on US industrial goods as part of the agreement. A delay does not sink the deal, agreed in July with Trump after months of intense wrangling that saw Washington slap 15% tariffs on EU goods. But by suspending approval, it does send a strong message of discontent to the White House that EU lawmakers argued would unnerve American businesses. \"It is an extremely powerful lever – I don't think companies would agree to give up the European market,\" Valerie Hayer, president of the centrist Renew group, told journalists. Trump has threatened to hit six EU countries – including powerhouses France and Germany – with tariffs for not going along with his demand to get Greenland. EU leaders are set to hold an emergency summit in Brussels on Thursday evening over the US threats against Denmark's autonomous territory, Greenland. The bloc is weighing different responses if Trump does not back down, including putting last year's trade deal on hold and hitting the US with €93 billion in tariffs. The package of retaliatory tariffs was agreed at the height of the EU-US trade standoff last year, but was ultimately suspended until February 6 to avoid an all-out trade war. Beyond that, French President Emmanuel Macron is pushing to unleash the bloc's potent anti-coercion trade instrument to use in case Trump makes good on his threats. Le Monde with AFP Lecture du Monde en cours sur un autre appareil. Vous pouvez lire Le Monde sur un seul appareil à la fois Ce message s", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["date", "loading", "date", "time", "loading", "time", "le", "monde", "afp", "min", "read", "eu", "lawmaker", "agree", "hold", "ratify", "key", "trade", "deal", "united", "states", "follow", "president", "donald", "trump", "tariff", "threat", "greenland", "main", "political", "group", "say", "tuesday", "january", "20", "warning", "shot", "european", "parliament", "come", "27", "nation", "bloc", "weigh", "hard", "hit", "trump", "follow", "washington", "long", "stand", "ally", "parliament", "plan", "vote", "come", "week", "remove", "tariff", "industrial", "good", "agreement", "delay", "sink", "deal", "agree", "july", "trump", "month", "intense", "wrangling", "see", "washington", "slap", "15", "tariff", "eu", "good", "suspend", "approval", "send", "strong", "message", "discontent", "white", "house", "eu", "lawmakers", "argue", "unnerve", "american", "business", "extremely", "powerful", "lever", "think", "company", "agree", "european", "market", "valerie", "hayer", "president", "centrist", "renew", "group", "tell", "journalist", "trump", "threaten", "hit", "eu", "country", "include", "powerhouse", "france", "germany", "tariff", "go", "demand", "greenland", "eu", "leader", "set", "hold", "emergency", "summit", "brussels", "thursday", "evening", "threat", "denmark", "autonomous", "territory", "greenland", "bloc", "weigh", "different", "response", "trump", "include", "put", "year", "trade", "deal", "hold", "hit", "93", "billion", "tariff", "package", "retaliatory", "tariff", "agree", "height", "eu", "trade", "standoff", "year", "ultimately", "suspend", "february", "avoid", "trade", "war", "french", "president", "emmanuel", "macron", "push", "unleash", "bloc", "potent", "anti", "coercion", "trade", "instrument", "use", "case", "trump", "make", "good", "threat", "le", "monde", "afp", "lecture", "du", "monde", "en", "cours", "sur", "un", "autre", "appareil", "vous", "pouvez", "lire", "le", "monde", "sur", "un", "seul", "appareil", "la", "fois", "ce", "message"], "num_tokens": 208, "token_loss_pct": 43.32, "normalized_content": "date loading date... time loading time... le monde with afp 1 min read eu lawmakers have agreed to hold off on ratifying a key trade deal with the united states following president donald trump's tariff threats over greenland the main political groups said on tuesday january 20. the warning shot from the european parliament comes as the 27-nation bloc weighs how hard to hit back if trump follows through against washington's long-standing allies. the parliament was planning a vote in the coming weeks on removing tariffs on us industrial goods as part of the agreement. a delay does not sink the deal agreed in july with trump after months of intense wrangling that saw washington slap 15 tariffs on eu goods. but by suspending approval it does send a strong message of discontent to the white house that eu lawmakers argued would unnerve american businesses. it is an extremely powerful lever  i don't think companies would agree to give up the european market valerie hayer president of the centrist renew group told journalists. trump has threatened to hit six eu countries  including powerhouses france and germany  with tariffs for not going along with his demand to get greenland. eu leaders are set to hold an emergency summit in brussels on thursday evening over the us threats against denmark's autonomous territory greenland. the bloc is weighing different responses if trump does not back down including putting last year's trade deal on hold and hitting the us with 93 billion in tariffs. the package of retaliatory tariffs was agreed at the height of the eu-us trade standoff last year but was ultimately suspended until february 6 to avoid an all-out trade war. beyond that french president emmanuel macron is pushing to unleash the bloc's potent anti-coercion trade instrument to use in case trump makes good on his threats. le monde with afp lecture du monde en cours sur un autre appareil. vous pouvez lire le monde sur un seul appareil à la fois ce message s"}
{"title": "Ask HN: Should you combine your personal website and blog or keep them separate?", "url": "item?id=46695579", "content": "Ask HN: Should you combine your personal website and blog or keep them separate?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "combine", "personal", "website", "blog", "separate", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 56.52, "normalized_content": "ask hn should you combine your personal website and blog or keep them separate. score none. author none. date none"}
{"title": "Profession by Isaac Asimov (1957)", "url": "https://www.abelard.org/asimov.php", "content": "Profession , \r\n        copyright ©1957 by Street and Smith Publications, Inc., from ISAAC \r\n        ASIMOV: THE COMPLETE STORIES OF VOL. 1 by Isaac Asimov. Used by permission of Doubleday, a division of Random House, Inc. For on-line information about other Random \r\n        House, Inc. books and authors, see the Internet Web site at http://www.randomhouse.com . Another \r\n        sci-fi short story at abelard.org: And Then There \r\n        Were None by Eric Frank Russell G eorge Platen could not conceal the longing in his voice. It was \r\n        too much to suppress. He said, “Tomorrow’s 1 May. Olympics!” He rolled over on his stomach and peered over the foot of his bed at his \r\n        roommate. Didn’t he feel it, too? Didn’t this make some impression \r\n        on him? George’s face was thin and had grown a trifle thinner in the nearly \r\n        year and a half that he had been at the House. His figure was slight but the \r\n        look in his blue eyes was as intense as it had ever been, and right now there \r\n        was a trapped look in the way his fingers curled against the bedspread. George’s roommate looked up briefly from his book and took the opportunity \r\n        to adjust the light-level of the stretch of wall near his chair. His name \r\n        was Hali Omani and he was a Nigerian by birth. His dark brown skin and massive \r\n        features seemed made for calmness, and mention of the Olympics did not move \r\n        him. “I know, George.” George owed much to Hali’s patience and kindness when it was needed, \r\n        but even patience and kindness could be overdone. Was this a time to sit there like a statue built of some dark, warm wood? George wondered if he himself would grow like that after ten years here and \r\n        rejected the thought violently. No! He said defiantly, “I think you’ve forgotten what May means.” The other said,“I remember very well what it means. It means nothing! \r\n        You’re the one who’s forgotten that. May means nothing to you,", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["profession", "copyright", "1957", "street", "smith", "publications", "inc", "isaac", "asimov", "complete", "story", "vol", "isaac", "asimov", "permission", "doubleday", "division", "random", "house", "inc", "line", "information", "random", "house", "inc", "book", "author", "internet", "web", "site", "url", "sci", "fi", "short", "story", "abelard.org", "eric", "frank", "russell", "eorge", "platen", "conceal", "longing", "voice", "suppress", "say", "tomorrow", "olympic", "roll", "stomach", "peer", "foot", "bed", "roommate", "not", "feel", "not", "impression", "george", "face", "thin", "grow", "trifle", "thin", "nearly", "year", "half", "house", "figure", "slight", "look", "blue", "eye", "intense", "right", "trapped", "look", "way", "finger", "curl", "bedspread", "george", "roommate", "look", "briefly", "book", "take", "opportunity", "adjust", "light", "level", "stretch", "wall", "near", "chair", "hali", "omani", "nigerian", "birth", "dark", "brown", "skin", "massive", "feature", "calmness", "mention", "olympic", "know", "george", "george", "owe", "halis", "patience", "kindness", "need", "patience", "kindness", "overdone", "time", "sit", "like", "statue", "build", "dark", "warm", "wood", "george", "wonder", "grow", "like", "year", "reject", "thought", "violently", "say", "defiantly", "think", "ve", "forget", "mean", "saidi", "remember", "mean", "mean", "forget", "mean"], "num_tokens": 146, "token_loss_pct": 59.1, "normalized_content": "profession  copyright 1957 by street and smith publications inc. from isaac asimov the complete stories of vol. 1 by isaac asimov. used by permission of doubleday a division of random house inc. for on-line information about other random house inc. books and authors see the internet web site at url . another sci-fi short story at abelard.org and then there were none by eric frank russell g eorge platen could not conceal the longing in his voice. it was too much to suppress. he said tomorrows 1 may. olympics he rolled over on his stomach and peered over the foot of his bed at his roommate. didnt he feel it too didnt this make some impression on him georges face was thin and had grown a trifle thinner in the nearly year and a half that he had been at the house. his figure was slight but the look in his blue eyes was as intense as it had ever been and right now there was a trapped look in the way his fingers curled against the bedspread. georges roommate looked up briefly from his book and took the opportunity to adjust the light-level of the stretch of wall near his chair. his name was hali omani and he was a nigerian by birth. his dark brown skin and massive features seemed made for calmness and mention of the olympics did not move him. i know george. george owed much to halis patience and kindness when it was needed but even patience and kindness could be overdone. was this a time to sit there like a statue built of some dark warm wood george wondered if he himself would grow like that after ten years here and rejected the thought violently. no he said defiantly i think youve forgotten what may means. the other saidi remember very well what it means. it means nothing youre the one whos forgotten that. may means nothing to you"}
{"title": "The 600-year-old origins of the word 'hello'", "url": "https://www.bbc.com/culture/article/20260113-hello-hiya-aloha-what-our-greetings-reveal", "content": "It's been 200 years since the word \"hello\" was first used in print – though its beginnings date back to the 15th Century. How has the language of greetings evolved around the world - and what does it tell us about ourselves? We use \"hello\" dozens of times a day without thinking – during phone calls, emails and face-to-face encounters. We sing it along with Adele and Lionel Richie, and we have watched it spun into moments of screen gold in Jerry Maguire (\"You had me at hello\"), and Scarface (\"Say hello to my little friend!\"). It's been used to sell everything from mobile phones (Motorola's \"Hello, Moto\") to lingerie (Wonderbra's iconic \"Hello boys\"), and it has been borrowed to name computer programs and celebrity magazines. In print, this ubiquitous, friendly greeting has a surprisingly short history. Two centuries ago, on 18 January 1826, \"hello\" made what is thought to be its earliest recorded appearance on the page, in a Connecticut newspaper called The Norwich Courier. Hidden among the column inches, it was a modest in-ink debut for a word that would go on to greet much of the modern world. By the 1850s, it had crossed the Atlantic to Britain – appearing in publications such as the London Literary Gazette – and became increasingly common in print. Like the go-to greetings in other languages, \"hello\" also says something about the English-speaking world – depending on which variation, abbreviation or inflection of the word we choose to use. There are plenty of such forms. Whether due to dialect or accent influences, or the brevity demanded by online communication, which \"hello\" you choose says a lot about you, and can indicate age, nationality, or even mood. According to linguists, elongated variations such as \"heyyy\" could be construed as flirtatious, \"hellaw\" might suggest you're from the southern US, \"howdy\" from western US, and the clipped \"hi\" may indicate a curt disposition. \"It can be pronounced and inflected in many different ways, and these subtle intonat", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["200", "year", "word", "hello", "print", "beginning", "date", "15th", "century", "language", "greeting", "evolve", "world", "tell", "use", "hello", "dozen", "time", "day", "think", "phone", "call", "email", "face", "face", "encounter", "sing", "adele", "lionel", "richie", "watch", "spin", "moment", "screen", "gold", "jerry", "maguire", "hello", "scarface", "hello", "little", "friend", "sell", "mobile", "phone", "motorola", "hello", "moto", "lingerie", "wonderbra", "iconic", "hello", "boy", "borrow", "computer", "program", "celebrity", "magazine", "print", "ubiquitous", "friendly", "greeting", "surprisingly", "short", "history", "century", "ago", "18", "january", "1826", "hello", "think", "early", "record", "appearance", "page", "connecticut", "newspaper", "call", "norwich", "courier", "hide", "column", "inch", "modest", "ink", "debut", "word", "greet", "modern", "world", "1850", "cross", "atlantic", "britain", "appear", "publication", "london", "literary", "gazette", "increasingly", "common", "print", "like", "greeting", "language", "hello", "say", "english", "speak", "world", "depend", "variation", "abbreviation", "inflection", "word", "choose", "use", "plenty", "form", "dialect", "accent", "influence", "brevity", "demand", "online", "communication", "hello", "choose", "say", "lot", "indicate", "age", "nationality", "mood", "accord", "linguist", "elongate", "variation", "heyyy", "construe", "flirtatious", "hellaw", "suggest", "southern", "howdy", "western", "clip", "hi", "indicate", "curt", "disposition", "pronounce", "inflect", "different", "way", "subtle", "intonat"], "num_tokens": 158, "token_loss_pct": 56.11, "normalized_content": "it's been 200 years since the word hello was first used in print  though its beginnings date back to the 15th century. how has the language of greetings evolved around the world - and what does it tell us about ourselves we use hello dozens of times a day without thinking  during phone calls emails and face-to-face encounters. we sing it along with adele and lionel richie and we have watched it spun into moments of screen gold in jerry maguire you had me at hello and scarface say hello to my little friend. it's been used to sell everything from mobile phones motorola's hello moto to lingerie wonderbra's iconic hello boys and it has been borrowed to name computer programs and celebrity magazines. in print this ubiquitous friendly greeting has a surprisingly short history. two centuries ago on 18 january 1826 hello made what is thought to be its earliest recorded appearance on the page in a connecticut newspaper called the norwich courier. hidden among the column inches it was a modest in-ink debut for a word that would go on to greet much of the modern world. by the 1850s it had crossed the atlantic to britain  appearing in publications such as the london literary gazette  and became increasingly common in print. like the go-to greetings in other languages hello also says something about the english-speaking world  depending on which variation abbreviation or inflection of the word we choose to use. there are plenty of such forms. whether due to dialect or accent influences or the brevity demanded by online communication which hello you choose says a lot about you and can indicate age nationality or even mood. according to linguists elongated variations such as heyyy could be construed as flirtatious hellaw might suggest you're from the southern us howdy from western us and the clipped hi may indicate a curt disposition. it can be pronounced and inflected in many different ways and these subtle intonat"}
{"title": "Optimizing GPU Programs from Java Using Babylon and Hat", "url": "https://openjdk.org/projects/babylon/articles/hat-matmul/hat-matmul", "content": "The project Babylon is a new OpenJDK project with the goal of enhancing Java reflection, allowing to reflect code from Java methods and Java lambdas, and being able to query their symbolic representation, called code models. These code models can be used at runtime to modify the code, perform optimizations, and/or perform code transformations to other programming models. Furthermore, code reflection allows Java developers to interact with foreign programming models and foreign programming languages without using any 3rd party libraries. One of the foreign programming environments we are exploring in the project Babylon is the GPU environment through the CUDA and OpenCL programming models, called HAT ( Heterogeneous Accelerator Toolkit). The goal for HAT is to be able to offload and run efficient parallel workloads on hardware accelerators. Through this article, we want to tackle these two questions: Each of these questions presents its own set of challenges. The majority of the projects focus on the first challenge with projects such as Sumatra , Aparapi , Marawacc , RootBeer , JaBEE , IBM J9 , and more recently TornadoVM . These projects have focused on abstracting GPU programmability and make it easier for Java developers. While they achieve reasonable high performance ( e.g., TornadoVMâs study ) by leveraging specialized accelerators, they often do so at the cost of hindering access to advanced GPU optimizations. However, in the era of AI and high-demand computing, simply being faster than Java on CPUs might not be enough. The second question goes a step further and rethink about how Java programmers could approach native performance on hardware accelerators while still maintaining reasonable high-level constructs. This is a very thin line between what to expose from low-level APIs and from what level of the native software stack. The HAT project tackles GPU programming from a perspective of a performance engineer wanting to efficiently interconnect the Java so", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["project", "babylon", "new", "openjdk", "project", "goal", "enhance", "java", "reflection", "allow", "reflect", "code", "java", "method", "java", "lambda", "able", "query", "symbolic", "representation", "call", "code", "model", "code", "model", "runtime", "modify", "code", "perform", "optimization", "andor", "perform", "code", "transformation", "programming", "model", "furthermore", "code", "reflection", "allow", "java", "developer", "interact", "foreign", "programming", "model", "foreign", "programming", "language", "3rd", "party", "library", "foreign", "programming", "environment", "explore", "project", "babylon", "gpu", "environment", "cuda", "opencl", "programming", "model", "call", "hat", "heterogeneous", "accelerator", "toolkit", "goal", "hat", "able", "offload", "run", "efficient", "parallel", "workload", "hardware", "accelerator", "article", "want", "tackle", "question", "question", "present", "set", "challenge", "majority", "project", "focus", "challenge", "project", "sumatra", "aparapi", "marawacc", "rootbeer", "jabee", "ibm", "j9", "recently", "tornadovm", "project", "focus", "abstract", "gpu", "programmability", "easy", "java", "developer", "achieve", "reasonable", "high", "performance", "e.g.", "tornadovmâs", "study", "leverage", "specialized", "accelerator", "cost", "hinder", "access", "advanced", "gpu", "optimization", "era", "ai", "high", "demand", "computing", "simply", "fast", "java", "cpus", "second", "question", "go", "step", "rethink", "java", "programmer", "approach", "native", "performance", "hardware", "accelerator", "maintain", "reasonable", "high", "level", "construct", "thin", "line", "expose", "low", "level", "apis", "level", "native", "software", "stack", "hat", "project", "tackle", "gpu", "programming", "perspective", "performance", "engineer", "want", "efficiently", "interconnect", "java"], "num_tokens": 173, "token_loss_pct": 47.42, "normalized_content": "the project babylon is a new openjdk project with the goal of enhancing java reflection allowing to reflect code from java methods and java lambdas and being able to query their symbolic representation called code models. these code models can be used at runtime to modify the code perform optimizations andor perform code transformations to other programming models. furthermore code reflection allows java developers to interact with foreign programming models and foreign programming languages without using any 3rd party libraries. one of the foreign programming environments we are exploring in the project babylon is the gpu environment through the cuda and opencl programming models called hat  heterogeneous accelerator toolkit. the goal for hat is to be able to offload and run efficient parallel workloads on hardware accelerators. through this article we want to tackle these two questions each of these questions presents its own set of challenges. the majority of the projects focus on the first challenge with projects such as sumatra  aparapi  marawacc  rootbeer  jabee  ibm j9  and more recently tornadovm . these projects have focused on abstracting gpu programmability and make it easier for java developers. while they achieve reasonable high performance  e.g. tornadovmâs study  by leveraging specialized accelerators they often do so at the cost of hindering access to advanced gpu optimizations. however in the era of ai and high-demand computing simply being faster than java on cpus might not be enough. the second question goes a step further and rethink about how java programmers could approach native performance on hardware accelerators while still maintaining reasonable high-level constructs. this is a very thin line between what to expose from low-level apis and from what level of the native software stack. the hat project tackles gpu programming from a perspective of a performance engineer wanting to efficiently interconnect the java so"}
{"title": "Predicting OpenAI's ad strategy", "url": "https://ossa-ma.github.io/blog/openads", "content": "Jan 18, 2026 9 min read The World is Ads credit La Haine + Gemini Here we go again, the tech press is having another AI doom cycle. I've primarily written this as a response to an NYT analyst painting a completely unsubstantiated, baseless, speculative, outrageous, EGREGIOUS, preposterous \"grim picture\" on OpenAI going bust . Mate come on. OpenAI is not dying, they're not running out of money. Yes, they're creating possibly the craziest circular economy and defying every economics law since Adam Smith published 'The Wealth of Nations'. $1T in commitments is genuinely insane. But I doubt they're looking to be acquired; honestly by who? you don't raise $40 BILLION at $260 BILLION VALUATION to get acquired. It's all for the $1T IPO. But it seems that the pinnacle of human intelligence: the greatest, smartest, brightest minds have all come together to... build us another ad engine. What happened to superintelligence and AGI? See if OpenAI was not a direct threat to the current ad giants would Google be advertising Gemini every chance they get? Don't forget they're also capitalising on their brand new high-intent ad funnel by launching ads on Gemini and AI overview . Let's crunch the numbers. March: Closed $40B funding round at $260B valuation , the largest raise by a private tech company on record. June: Hit $10B ARR . July: First $1B revenue month , doubled from $500M monthly in January. November: Sam Altman says OpenAI expects $20B ARR for 2025 . Reached 800M WAU , ~190M DAU , 35M paying subscribers , 1M business customers . January 2026: \"Both our Weekly Active User (WAU) and Daily Active User (DAU) figures continue to produce all-time-highs (Jan 14 was the highest, Jan 13 was the second highest, etc.)\" January 16, 2026: Announced ads in ChatGPT free and Go tiers. Yes, OpenAI is burning $8-12B in 2025 .\nCompute infrastructure is obviously not cheap when serving 190M people daily. So let's try to model their expected ARPU (annual revenue per user) by understanding wha", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["jan", "18", "2026", "min", "read", "world", "ad", "credit", "la", "haine", "gemini", "tech", "press", "have", "ai", "doom", "cycle", "primarily", "write", "response", "nyt", "analyst", "paint", "completely", "unsubstantiated", "baseless", "speculative", "outrageous", "egregious", "preposterous", "grim", "picture", "openai", "go", "bust", "mate", "come", "openai", "die", "run", "money", "yes", "create", "possibly", "crazy", "circular", "economy", "defy", "economic", "law", "adam", "smith", "publish", "wealth", "nation", "commitment", "genuinely", "insane", "doubt", "look", "acquire", "honestly", "raise", "40", "billion", "260", "billion", "valuation", "acquire", "ipo", "pinnacle", "human", "intelligence", "greatest", "smart", "bright", "mind", "come", "build", "ad", "engine", "happen", "superintelligence", "agi", "openai", "direct", "threat", "current", "ad", "giant", "google", "advertise", "gemini", "chance", "forget", "capitalise", "brand", "new", "high", "intent", "ad", "funnel", "launch", "ad", "gemini", "ai", "overview", "let", "crunch", "number", "march", "close", "40b", "funding", "round", "260b", "valuation", "large", "raise", "private", "tech", "company", "record", "june", "hit", "10b", "arr", "july", "1b", "revenue", "month", "double", "500", "monthly", "january", "november", "sam", "altman", "say", "openai", "expect", "20b", "arr", "2025", "reach", "800", "wau", "190", "dau", "35", "pay", "subscriber", "business", "customer", "january", "2026", "weekly", "active", "user", "wau", "daily", "active", "user", "dau", "figure", "continue", "produce", "time", "high", "jan", "14", "high", "jan", "13", "second", "high", "etc", "january", "16", "2026", "announce", "ad", "chatgpt", "free", "tier", "yes", "openai", "burn", "12b", "2025", "compute", "infrastructure", "obviously", "cheap", "serve", "190", "people", "daily", "let", "try", "model", "expect", "arpu", "annual", "revenue", "user", "understand", "wha"], "num_tokens": 208, "token_loss_pct": 45.41, "normalized_content": "jan 18 2026 9 min read the world is ads credit la haine  gemini here we go again the tech press is having another ai doom cycle. i've primarily written this as a response to an nyt analyst painting a completely unsubstantiated baseless speculative outrageous egregious preposterous grim picture on openai going bust . mate come on. openai is not dying they're not running out of money. yes they're creating possibly the craziest circular economy and defying every economics law since adam smith published 'the wealth of nations'. 1t in commitments is genuinely insane. but i doubt they're looking to be acquired honestly by who you don't raise 40 billion at 260 billion valuation to get acquired. it's all for the 1t ipo. but it seems that the pinnacle of human intelligence the greatest smartest brightest minds have all come together to... build us another ad engine. what happened to superintelligence and agi see if openai was not a direct threat to the current ad giants would google be advertising gemini every chance they get don't forget they're also capitalising on their brand new high-intent ad funnel by launching ads on gemini and ai overview . let's crunch the numbers. march closed 40b funding round at 260b valuation  the largest raise by a private tech company on record. june hit 10b arr . july first 1b revenue month  doubled from 500m monthly in january. november sam altman says openai expects 20b arr for 2025 . reached 800m wau  190m dau  35m paying subscribers  1m business customers . january 2026 both our weekly active user wau and daily active user dau figures continue to produce all-time-highs jan 14 was the highest jan 13 was the second highest etc. january 16 2026 announced ads in chatgpt free and go tiers. yes openai is burning 8-12b in 2025 . compute infrastructure is obviously not cheap when serving 190m people daily. so let's try to model their expected arpu annual revenue per user by understanding wha"}
{"title": "Ask HN: How would you design for this scale today?", "url": "item?id=46686023", "content": "Ask HN: How would you design for this scale today?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "design", "scale", "today", "score", "author", "date"], "num_tokens": 8, "token_loss_pct": 57.89, "normalized_content": "ask hn how would you design for this scale today. score none. author none. date none"}
{"title": "Stirling Cycle Machine Analysis", "url": "https://ohioopen.library.ohio.edu/opentextbooks/9/", "content": "Home > OPENTEXTBOOKS > 9 Israel Urieli , Ohio University Russ College of Engineering and Technology Download Full Text (61.8 MB) Download Stirling Engine Analysis m-files for MATLAB (33 KB) Download Biographical Memoir of William Beale (3.0 MB) Dedicated to William T. Beale (1928 - 2016), inventor of the Free Piston Stirling Engine, Mentor and Frien. This web resource is intended to be totally self contained learning resource for the analysis and development of computer simulation of single phase, piston/cylinder Stirling cycle machines. It includes thermodynamic, heat transfer and fluid flow friction analysis, and until 2012 it was used as resource material for an advanced course for Mechanical Engineering majors. The course structure was based on the book by I.Urieli & D.M.Berchowitz 'Stirling Cycle Engine Analysis' (Adam Hilger, 1984). The computer simulation program modules (originally written in FORTRAN) have all been updated and rewritten in MATLAB, a convenient interactive language which allows direct graphical output - essential for Stirling cycle analysis. A complete set of all the m-files are developed and provided, and they can be augmented and adapted as needed for specific engine/refrigerator configurations. It is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license and as such is freely available. Comments and constructive criticism are welcomed by the author. Chapter 1: Background and Introduction Chapter 2: Basic Engine Configurations Chapter 3: Ideal Isothermal Analysis We define and analyze the Ideal Isothermal model of a Stirling engine, including the Schmidt Analysis, and discuss its limitations. One obviously incorrect conclusion of this analysis is that all three heat exchangers are redundant, and only contribute dead space, since all required heat transfer processes occur in the isothermal compression and expansion spaces. Nevertheless we can obtain a better understanding of a specific design, partic", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["home", "opentextbook", "israel", "urieli", "ohio", "university", "russ", "college", "engineering", "technology", "download", "text", "61.8", "mb", "download", "stirling", "engine", "analysis", "file", "matlab", "33", "kb", "download", "biographical", "memoir", "william", "beale", "3.0", "mb", "dedicated", "william", "t.", "beale", "1928", "2016", "inventor", "free", "piston", "stirling", "engine", "mentor", "frien", "web", "resource", "intend", "totally", "self", "contain", "learn", "resource", "analysis", "development", "computer", "simulation", "single", "phase", "pistoncylinder", "stirling", "cycle", "machine", "include", "thermodynamic", "heat", "transfer", "fluid", "flow", "friction", "analysis", "2012", "resource", "material", "advanced", "course", "mechanical", "engineering", "major", "course", "structure", "base", "book", "i.urieli", "d.m.berchowitz", "stirling", "cycle", "engine", "analysis", "adam", "hilger", "1984", "computer", "simulation", "program", "module", "originally", "write", "fortran", "update", "rewrite", "matlab", "convenient", "interactive", "language", "allow", "direct", "graphical", "output", "essential", "stirling", "cycle", "analysis", "complete", "set", "file", "develop", "provide", "augment", "adapt", "need", "specific", "enginerefrigerator", "configuration", "license", "creative", "common", "attribution", "noncommercial", "sharealike", "4.0", "international", "license", "freely", "available", "comment", "constructive", "criticism", "welcome", "author", "chapter", "background", "introduction", "chapter", "basic", "engine", "configuration", "chapter", "ideal", "isothermal", "analysis", "define", "analyze", "ideal", "isothermal", "model", "stirling", "engine", "include", "schmidt", "analysis", "discuss", "limitation", "obviously", "incorrect", "conclusion", "analysis", "heat", "exchanger", "redundant", "contribute", "dead", "space", "require", "heat", "transfer", "process", "occur", "isothermal", "compression", "expansion", "space", "obtain", "well", "understanding", "specific", "design", "partic"], "num_tokens": 185, "token_loss_pct": 40.71, "normalized_content": "home  opentextbooks  9 israel urieli  ohio university russ college of engineering and technology download full text 61.8 mb download stirling engine analysis m-files for matlab 33 kb download biographical memoir of william beale 3.0 mb dedicated to william t. beale 1928 - 2016 inventor of the free piston stirling engine mentor and frien. this web resource is intended to be totally self contained learning resource for the analysis and development of computer simulation of single phase pistoncylinder stirling cycle machines. it includes thermodynamic heat transfer and fluid flow friction analysis and until 2012 it was used as resource material for an advanced course for mechanical engineering majors. the course structure was based on the book by i.urieli  d.m.berchowitz 'stirling cycle engine analysis' adam hilger 1984. the computer simulation program modules originally written in fortran have all been updated and rewritten in matlab a convenient interactive language which allows direct graphical output - essential for stirling cycle analysis. a complete set of all the m-files are developed and provided and they can be augmented and adapted as needed for specific enginerefrigerator configurations. it is licensed under a creative commons attribution-noncommercial-sharealike 4.0 international license and as such is freely available. comments and constructive criticism are welcomed by the author. chapter 1 background and introduction chapter 2 basic engine configurations chapter 3 ideal isothermal analysis we define and analyze the ideal isothermal model of a stirling engine including the schmidt analysis and discuss its limitations. one obviously incorrect conclusion of this analysis is that all three heat exchangers are redundant and only contribute dead space since all required heat transfer processes occur in the isothermal compression and expansion spaces. nevertheless we can obtain a better understanding of a specific design partic"}
{"title": "Supply Chain Vuln Compromised Core AWS GitHub Repos & Threatened the AWS Console", "url": "https://www.wiz.io/blog/wiz-research-codebreach-vulnerability-aws-codebuild", "content": "Wiz Research discovered a critical supply chain vulnerability that abused a CodeBuild misconfiguration to take over key AWS GitHub repositories - including the JavaScript SDK powering the AWS Console. Wiz Research uncovered CodeBreach , a critical vulnerability that placed the AWS Console supply chain at risk. The issue allowed a complete takeover of key AWS GitHub repositories - most notably the AWS JavaScript SDK, a core library that powers the AWS Console . By exploiting CodeBreach, attackers could have injected malicious code to launch a platform-wide compromise, potentially affecting not just the countless applications depending on the SDK, but the Console itself, threatening every AWS account . The vulnerability stemmed from a subtle flaw in how the repositories’ AWS CodeBuild CI pipelines handled build triggers. Just two missing characters in a Regex filter allowed unauthenticated attackers to infiltrate the build environment and leak privileged credentials. This post breaks down how we leveraged this subtle misconfiguration to achieve a full repository takeover, and provides key recommendations for CodeBuild users to harden their own projects against similar attacks. Wiz responsibly disclosed all findings to AWS, who promptly remediated the issue. AWS also implemented global hardening measures within the CodeBuild service to prevent similar attacks. Most notably, the new Pull Request Comment Approval build gate offers organizations a simple and secure path to prevent untrusted builds. Read the AWS Advisory here . This issue follows a familiar pattern seen in recent supply-chain attacks like the Nx S1ngularity incident, where subtle CI/CD misconfigurations lead to disproportionately impactful attacks. Just last July, a threat actor abused a similar CodeBuild issue to launch a supply chain attack against users of the Amazon Q VS Code extension. This growing trend underscores the urgent need for organizations to harden their CI/CD pipelines. January 16, 2025 up", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["wiz", "research", "discover", "critical", "supply", "chain", "vulnerability", "abuse", "codebuild", "misconfiguration", "key", "aw", "github", "repository", "include", "javascript", "sdk", "power", "aws", "console", "wiz", "research", "uncover", "codebreach", "critical", "vulnerability", "place", "aw", "console", "supply", "chain", "risk", "issue", "allow", "complete", "takeover", "key", "aw", "github", "repository", "notably", "aw", "javascript", "sdk", "core", "library", "power", "aw", "console", "exploit", "codebreach", "attacker", "inject", "malicious", "code", "launch", "platform", "wide", "compromise", "potentially", "affect", "countless", "application", "depend", "sdk", "console", "threaten", "aw", "account", "vulnerability", "stem", "subtle", "flaw", "repository", "aws", "codebuild", "ci", "pipeline", "handle", "build", "trigger", "miss", "character", "regex", "filter", "allow", "unauthenticated", "attacker", "infiltrate", "build", "environment", "leak", "privileged", "credential", "post", "break", "leverage", "subtle", "misconfiguration", "achieve", "repository", "takeover", "provide", "key", "recommendation", "codebuild", "user", "harden", "project", "similar", "attack", "wiz", "responsibly", "disclose", "finding", "aw", "promptly", "remediate", "issue", "aw", "implement", "global", "hardening", "measure", "codebuild", "service", "prevent", "similar", "attack", "notably", "new", "pull", "request", "comment", "approval", "build", "gate", "offer", "organization", "simple", "secure", "path", "prevent", "untrusted", "build", "read", "aw", "advisory", "issue", "follow", "familiar", "pattern", "see", "recent", "supply", "chain", "attack", "like", "nx", "s1ngularity", "incident", "subtle", "cicd", "misconfiguration", "lead", "disproportionately", "impactful", "attack", "july", "threat", "actor", "abuse", "similar", "codebuild", "issue", "launch", "supply", "chain", "attack", "user", "amazon", "vs", "code", "extension", "grow", "trend", "underscore", "urgent", "need", "organization", "harden", "cicd", "pipeline", "january", "16", "2025"], "num_tokens": 196, "token_loss_pct": 37.38, "normalized_content": "wiz research discovered a critical supply chain vulnerability that abused a codebuild misconfiguration to take over key aws github repositories - including the javascript sdk powering the aws console. wiz research uncovered codebreach  a critical vulnerability that placed the aws console supply chain at risk. the issue allowed a complete takeover of key aws github repositories - most notably the aws javascript sdk a core library that powers the aws console . by exploiting codebreach attackers could have injected malicious code to launch a platform-wide compromise potentially affecting not just the countless applications depending on the sdk but the console itself threatening every aws account . the vulnerability stemmed from a subtle flaw in how the repositories aws codebuild ci pipelines handled build triggers. just two missing characters in a regex filter allowed unauthenticated attackers to infiltrate the build environment and leak privileged credentials. this post breaks down how we leveraged this subtle misconfiguration to achieve a full repository takeover and provides key recommendations for codebuild users to harden their own projects against similar attacks. wiz responsibly disclosed all findings to aws who promptly remediated the issue. aws also implemented global hardening measures within the codebuild service to prevent similar attacks. most notably the new pull request comment approval build gate offers organizations a simple and secure path to prevent untrusted builds. read the aws advisory here . this issue follows a familiar pattern seen in recent supply-chain attacks like the nx s1ngularity incident where subtle cicd misconfigurations lead to disproportionately impactful attacks. just last july a threat actor abused a similar codebuild issue to launch a supply chain attack against users of the amazon q vs code extension. this growing trend underscores the urgent need for organizations to harden their cicd pipelines. january 16 2025 up"}
{"title": "Erdos 281 solved with ChatGPT 5.2 Pro", "url": "https://twitter.com/neelsomani/status/2012695714187325745", "content": "Erdos 281 solved with ChatGPT 5.2 Pro. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["erdo", "281", "solve", "chatgpt", "5.2", "pro", "score", "author", "date"], "num_tokens": 9, "token_loss_pct": 43.75, "normalized_content": "erdos 281 solved with chatgpt 5.2 pro. score none. author none. date none"}
{"title": "Briar keeps Iran connected via Bluetooth and Wi-Fi when the internet goes dark", "url": "https://briarproject.org/manual/fa/", "content": "Briar ÛÚ© Ø¨Ø±ÙØ§ÙÙ Ù¾ÛØ§Ù Ø±Ø³Ø§Ù ÙÛ Ø¨Ø§Ø´Ø¯ Ú©Ù Ø¨Ø±Ø§Û ÙØ¹Ø§ÙØ§ÙØ Ø±ÙØ²ÙØ§ÙÙ ÙÚ¯Ø§Ø±Ø§Ù Ù ÙØ± Ú©Ø³Û Ú©Ù ÙÛØ§Ø²ÙÙØ¯ ÛÚ© Ø±Ø§Ù Ø§ÙÙØ Ø±Ø§Ø­Øª Ù Ù¾ÛØ´Ø±ÙØªÙ Ø¨Ø±Ø§Û Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø¯ÛÚ¯Ø±Ø§Ù Ø§Ø³Øª ÙÛ Ø¨Ø§Ø´Ø¯. Ø¨Ø±Ø®ÙØ§Ù Ø¨Ø±ÙØ§ÙÙâ ÙØ§Û Ù¾ÛØ§ÙâØ±Ø³Ø§Ùâ ÙØ±Ø³ÙÙØ Briar Ø¨Ù Ø³Ø±ÙØ± ÙØªÙØ±Ú©Ø² Ø§ØªÚ©Ø§ ÙØ¯Ø§Ø±Ø¯ - Ù¾ÛØ§Ù ÙØ§ Ø¨Ù ØµÙØ±Øª ÙØ³ØªÙÛÙ Ø¨ÛÙ Ø¯Ø³ØªÚ¯Ø§Ù Ú©Ø§Ø±Ø¨Ø±Ø§Ù ÙÙÚ¯Ø§Ù ÙÛ Ø´ÙØ¯. Ø§Ú¯Ø± Ø§ÛÙØªØ±ÙØª Ú©Ø§Ø± ÙÚ©ÙØ¯Ø Briar ÙÛâØªÙØ§ÙØ¯ Ø§Ø² Ø·Ø±ÛÙ Ø¨ÙÙØªÙØ« ÛØ§ ÙØ§Ûâ-ÙØ§Û ÙÙÚ¯Ø§Ù Ø³Ø§Ø²Û Ú©Ø±Ø¯ÙØ Ø¬Ø±ÛØ§Ù Ø§Ø·ÙØ§Ø¹Ø§Øª Ø±Ø§ Ø¯Ø± Ø²ÙØ§Ù Ø¨Ø­Ø±Ø§Ù ÙÚ¯Ù Ø¯Ø§Ø±Ø¯. Ø§Ú¯Ø± Ø§ÛÙØªØ±ÙØª Ú©Ø§Ø± Ú©ÙØ¯Ø Briar ÙÛâØªÙØ§ÙØ¯ Ø¨Ø±Ø§Û ÙØ­Ø§ÙØ¸Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù Ù ÙØ§Ø¨Ø· Ø¢Ù ÙØ§ Ø§Ø² Ø§Ø² Ø´ÙÙØ¯Ø Ø§Ø² Ø·Ø±ÛÙ Ø´Ø¨Ú©Ù ØªÙØ± ÙÙÚ¯Ø§Ù Ø³Ø§Ø²Û Ú©ÙØ¯. Ø¨Ø±Ø§ÛØ± Ø±ÙÛ Google Play Ø¨Ø±Ø§Û Ø¯Ø³ØªÚ¯Ø§Ù ÙØ§Û Ø§ÙØ¯Ø±ÙÛØ¯ ÙÙØ¬ÙØ¯ ÙÛ Ø¨Ø§Ø´Ø¯. ÙÚ©ØªÙ: Ø§Ú¯Ø± ÙØ·ÙØ¦Ù ÙÛØ³ØªÛØ¯ Ú©Ù Ø¯Ø³ØªÚ¯Ø§Ù Ø´ÙØ§ Ø§ÙØ¯Ø±ÙÛØ¯ ÙÛ Ø¨Ø§Ø´Ø¯Ø ÙØ¬ÙØ¯ Ø¨Ø±ÙØ§ÙÙ Ù¾ÙÛ Ø§Ø³ØªÙØ± ÛØ§ Play Store Ø±Ø§ Ø¨Ø±Ø±Ø³Û Ú©ÙÛØ¯. Ø¯Ø± ØµÙØ±Øª ÙØ¬ÙØ¯Ø Ø¯Ø³ØªÚ¯Ø§Ù Ø´ÙØ§ Ø§ÙØ¯Ø±ÙÛØ¯ ÙÛ Ø¨Ø§Ø´Ø¯. Ø§Ú¯Ø± ÛÚ© Ø¯Ø³ØªÚ¯Ø§Ù Ø§ÙØ¯Ø±ÙÛØ¯ Ø¯Ø§Ø±ÛØ¯ Ø§ÙØ§ ØªØ±Ø¬ÛØ­ ÙÛâØ¯ÙÛØ¯ Ú©Ù Ø§Ø² Ú¯ÙÚ¯Ù Ù¾ÙÛ Ø§Ø³ØªÙØ§Ø¯Ù ÙÚ©ÙÛØ¯Ø ÙØ¨â Ø³Ø§ÛØª Briar Ø±Ø§ÙÙÙØ§ÛÛ ÙØ§Û ÙØ§Ø²Ù Ø¨Ø±Ø§Û ÙØµØ¨ Ø¨Ø±ÙØ§ÙÙ Ø§Ø² Ø·Ø±ÛÙ F-Droid ÛØ§ Ø¯Ø§ÙÙÙØ¯ ÙØ³ØªÙÛÙ Ø±Ø§ Ø¯Ø§Ø±Ø¯.  ÙØ®Ø³ØªÛÙ Ø¨Ø§Ø±Û Ú©Ù Briar Ø±Ø§ Ø¨Ø§Ø² ÙÛâÚ©ÙÛØ¯Ø Ø§Ø² Ø´ÙØ§ Ø®ÙØ§Ø³ØªÙ ÙÛâØ´ÙØ¯ ÛÚ© Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±Û Ø§ÛØ¬Ø§Ø¯ Ú©ÙÛØ¯. ÙÛâØªÙØ§ÙÛØ¯ ÙØ± ÙØ§Ù ÙØ³ØªØ¹Ø§Ø± Ù Ú¯Ø°Ø±ÙØ§ÚÙâ Ø§Û Ø±Ø§ Ø§ÙØªØ®Ø§Ø¨ Ú©ÙÛØ¯. Ú¯Ø°Ø±ÙØ§ÚÙ Ø­Ø¯Ø§ÙÙ Ø¨Ø§ÛØ¯ Ø¯Ø§Ø±Ø§Û 8 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø§Ø´Ø¯ Ù Ø­Ø¯Ø³ Ø²Ø¯Ù Ø¢Ù Ø¯Ø´ÙØ§Ø± Ø¨Ø§Ø´Ø¯. ÙØ´Ø¯Ø§Ø±: Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±Û Briar Ø´ÙØ§ Ø¨Ù ØµÙØ±Øª Ø§ÙÙ Ø¨Ø± Ø±ÙÛ Ø¯Ø³ØªÚ¯", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["briar", "ûú", "øøùøù", "ù¾ûøù", "øø³øù", "øøøø", "úù", "øøøû", "ùø¹øùøùø", "øùø²ùøù", "ùúøøøù", "ùø", "úø³û", "úù", "ùûøø²ù", "ùø", "ûú", "øøù", "øù", "ùø", "øøøøª", "ù¾ûøøùøªù", "øøøû", "øøøªøøø", "øø", "øûúøøù", "øø³øª", "øøøø", "øøøùøù", "øøùøù", "ùâ", "ùøû", "ù¾ûøù", "âøø³øùâ", "øø³ùù", "briar", "øù", "ø³øùø", "øªù", "øúø²", "øøªúø", "ùøøøø", "ù¾ûøù", "ùø", "øù", "øµùøøª", "ø³øªùûù", "øûù", "øø³øªúøù", "úøøøøøù", "ùù", "úøù", "øùø", "øúø", "øûùøªøùøª", "úøø", "ùúùøø", "briar", "ûâøªùøùø", "øø²", "øøûù", "øùùøªùø", "ûø", "ùøûâ", "ùøû", "ùù", "úøù", "ø³øø²û", "úøøùø", "øøûøù", "øøùøø¹øøª", "øø", "øø", "ø²ù", "øù", "øøøøù", "ùúù", "øøøø", "øúø", "øûùøªøùøª", "úøø", "úùøø", "briar", "ûâøªùøùø", "øøøû", "øøùøøª", "úøøøøøù", "ùøøø", "øù", "ùø", "øø²", "øø²", "øùùøø", "øø²", "øøûù", "øøúù", "øªùø", "ùù", "úøù", "ø³øø²û", "úùø", "øøøûø", "øùû", "google", "play", "øøøû", "øø³øªúøù", "ùøû", "øùøøùûø", "ùøùø", "øøøø", "ùúøªù", "øúø", "øù", "øù", "ùûø³øªûø", "úù", "øø³øªúøù", "øù", "øùøøùûø", "øøøøø", "ùøùø", "øøùøù", "ù¾ùû", "øø³øªùø", "ûø", "play", "store", "øø", "øøøø³û", "úùûø", "øø", "øµùøøª", "ùøùøø", "øø³øªúøù", "øù", "øùøøùûø", "øøøø", "øúø", "ûú", "øø³øªúøù", "øùøøùûø", "øøøûø", "øù", "øªøøûø", "ûâøùûø", "úù", "øø²", "úùúù", "ù¾ùû", "øø³øªùøøù", "ùúùûøø", "ùøâ", "ø³øûøª", "briar", "øøùùù", "øûû", "ùøû", "ùøø²ù", "øøøû", "ùøµø", "øøùøù", "øø²", "øøûù", "droid", "ûø", "øøùùùø", "ø³øªùûù", "øø", "øøøø", "ùøø³øªûù", "øøøû", "úù", "briar", "øø", "øøø²", "ûâúùûøø", "øø²", "øù", "øùøø³øªù", "ûâøùø", "ûú", "øø³øø", "úøøøøû", "øûøøø", "úùûø", "ûâøªùøùûø", "ùø", "ùøù", "ø³øªø¹øø", "úøøùøúùâ", "øû", "øø", "øùøªøøø", "úùûø", "úøøùøúù", "øøøùù", "øøûø", "øøøøû", "úøøøúøªø", "øøøø", "øøø³", "ø²øù", "øù", "øøùøø", "øøøø", "ùøøøø", "øø³øø", "úøøøøû", "briar", "øù", "øù", "øµùøøª", "øù", "øø", "øùû", "øø³øªú"], "num_tokens": 217, "token_loss_pct": 21.09, "normalized_content": "briar ûú øøùøù ù ù¾ûøù øø³øù ù û øøøø úù øøøû ùø¹øùøùø øùø²ùøù ù ùúøøøù ù ùø úø³û úù ùûøø²ù ùø ûú øøù øù ùø øøøøª ù ù¾ûøøùøªù øøøû øøøªøøø øø øûúøøù øø³øª ù û øøøø. øøøùøù øøùøù ùâ ùøû ù¾ûøù âøø³øùâ ù øø³ùù ø briar øù ø³øùø ù øªù øúø² øøªúø ùøøøø - ù¾ûøù ùø øù øµùøøª ù ø³øªùûù øûù øø³øªúøù úøøøøøù ùù úøù ù û øùø. øúø øûùøªøùøª úøø ùúùøø briar ù ûâøªùøùø øø² øøûù øùùøªùø ûø ùøûâ-ùøû ùù úøù ø³øø²û úøøùø øøûøù øøùøø¹øøª øø øø ø²ù øù øøøøù ùúù øøøø. øúø øûùøªøùøª úøø úùøø briar ù ûâøªùøùø øøøû ù øøùøøª úøøøøøù ù ùøøø øù ùø øø² øø² øùùøø øø² øøûù øøúù øªùø ùù úøù ø³øø²û úùø. øøøûø øùû google play øøøû øø³øªúøù ùøû øùøøùûø ù ùøùø ù û øøøø. ùúøªù øúø ù øù øù ùûø³øªûø úù øø³øªúøù øù ø øùøøùûø ù û øøøøø ùøùø øøùøù ù ù¾ùû øø³øªùø ûø play store øø øøøø³û úùûø. øø øµùøøª ùøùøø øø³øªúøù øù ø øùøøùûø ù û øøøø. øúø ûú øø³øªúøù øùøøùûø øøøûø øù ø øªøøûø ù ûâøùûø úù øø² úùúù ù¾ùû øø³øªùøøù ùúùûøø ùøâ ø³øûøª briar øøùùù øûû ùøû ùøø²ù øøøû ùøµø øøùøù ù øø² øøûù f-droid ûø øøùùùø ù ø³øªùûù øø øøøø. ùøø³øªûù øøøû úù briar øø øøø² ù ûâúùûøø øø² øù ø øùøø³øªù ù ûâøùø ûú øø³øø úøøøøû øûøøø úùûø. ù ûâøªùøùûø ùø ùøù ù ø³øªø¹øø ù úøøùøúùâ øû øø øùøªøøø úùûø. úøøùøúù øøøùù øøûø øøøøû 8 úøøøúøªø øøøø ù øøø³ ø²øù øù øøùøø øøøø. ùøøøø øø³øø úøøøøû briar øù ø øù øµùøøª øù ù øø øùû øø³øªú"}
{"title": "Ask HN: Would you trust a new browser security extension in 2025?", "url": "item?id=46692163", "content": "Ask HN: Would you trust a new browser security extension in 2025?. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ask", "hn", "trust", "new", "browser", "security", "extension", "2025", "score", "author", "date"], "num_tokens": 11, "token_loss_pct": 47.62, "normalized_content": "ask hn would you trust a new browser security extension in 2025. score none. author none. date none"}
{"title": "Show HN: MicroState – JavaScript City Builder", "url": "https://microstate.neocities.org", "content": "This is an early tech demo of the engine for MicroState and is not a finished game. Created for fun using HTML 2D Canvas and vanilla JavaScript by mastodon.social/@iaincollins Some features in this preview have better support for mouse and keyboard than for devices with touch\ninput. You can click and drag with the right mouse button to move around the map. You can also move the the map selecting the Move tool and dragging with the left mouse button. Two / three finger dragging and pinch-to-zoom is supported on devices with touch input. The mini map in the top right is interactive , it highights the are of the map you are viewing, you can\nclick on it to view to another part of the map. Zoom using the mouse wheel or the +/- buttons on the mini map. You c n also rotate your view from\nthe minimap . You can open the map using the ðºï¸ Map button or by using the M key . â°ï¸ Terrain You can place Parkland , Soil , Clay , Scrub , Paved , Sand , and Water tiles for terrain. You can ð¼ Raise , ð½ Lower , â¬ Flatten , and ð« Smooth terrain by clicking and\ndragging using the relevant tools. Different terrain types support different types of trees and structures. Unlike many isometric games, terrain is not constrained to being fixed heights - but there are limits to how\nsteep it can be before you end up with a cliff! Hold the Control Key while placing terrain tiles to \"flood fill\" in an area between other terrain\ntiles, or between roads. Hold the Control Key when changing a terrain type under a road to change the terrain of connected\nroads. ð¦ Roads You can place roads on any type of terrain except deep water - placing roads on shallower water\ncreates a bridge. Roads can't lean too much to the right or left, or have too steep a gradient. You use the ð« Smooth tool to fix issues with the terrain. You can click to place a single stretch of road or click and drag to place a longer stretch of\nroad. You can draw bridges over water, but bridges can't have intersection", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["early", "tech", "demo", "engine", "microstate", "finished", "game", "create", "fun", "html", "2d", "canvas", "vanilla", "javascript", "mastodon.socialmention", "feature", "preview", "well", "support", "mouse", "keyboard", "device", "touch", "input", "click", "drag", "right", "mouse", "button", "map", "map", "select", "tool", "drag", "left", "mouse", "button", "finger", "dragging", "pinch", "zoom", "support", "device", "touch", "input", "mini", "map", "right", "interactive", "highight", "map", "view", "click", "view", "map", "zoom", "mouse", "wheel", "button", "mini", "map", "rotate", "view", "minimap", "open", "map", "ðºï", "map", "button", "key", "âï", "terrain", "place", "parkland", "soil", "clay", "scrub", "pave", "sand", "water", "tile", "terrain", "ð¼", "raise", "ð½", "low", "flatten", "smooth", "terrain", "click", "drag", "relevant", "tool", "different", "terrain", "type", "support", "different", "type", "tree", "structure", "unlike", "isometric", "game", "terrain", "constrain", "fix", "height", "limit", "steep", "end", "cliff", "hold", "control", "key", "place", "terrain", "tile", "flood", "fill", "area", "terrain", "tile", "road", "hold", "control", "key", "change", "terrain", "type", "road", "change", "terrain", "connected", "road", "road", "place", "road", "type", "terrain", "deep", "water", "place", "road", "shallow", "water", "create", "bridge", "road", "lean", "right", "leave", "steep", "gradient", "use", "smooth", "tool", "fix", "issue", "terrain", "click", "place", "single", "stretch", "road", "click", "drag", "place", "long", "stretch", "road", "draw", "bridge", "water", "bridge", "intersection"], "num_tokens": 176, "token_loss_pct": 55.78, "normalized_content": "this is an early tech demo of the engine for microstate and is not a finished game. created for fun using html 2d canvas and vanilla javascript by mastodon.socialmention some features in this preview have better support for mouse and keyboard than for devices with touch input. you can click and drag with the right mouse button to move around the map. you can also move the the map selecting the move tool and dragging with the left mouse button. two  three finger dragging and pinch-to-zoom is supported on devices with touch input. the mini map in the top right is interactive  it highights the are of the map you are viewing you can click on it to view to another part of the map. zoom using the mouse wheel or the - buttons on the mini map. you c n also rotate your view from the minimap . you can open the map using the ðºï map button or by using the m key . âï terrain you can place parkland  soil  clay  scrub  paved  sand  and water tiles for terrain. you can ð¼ raise  ð½ lower  â flatten  and ð smooth terrain by clicking and dragging using the relevant tools. different terrain types support different types of trees and structures. unlike many isometric games terrain is not constrained to being fixed heights - but there are limits to how steep it can be before you end up with a cliff hold the control key while placing terrain tiles to flood fill in an area between other terrain tiles or between roads. hold the control key when changing a terrain type under a road to change the terrain of connected roads. ð roads you can place roads on any type of terrain except deep water - placing roads on shallower water creates a bridge. roads can't lean too much to the right or left or have too steep a gradient. you use the ð smooth tool to fix issues with the terrain. you can click to place a single stretch of road or click and drag to place a longer stretch of road. you can draw bridges over water but bridges can't have intersection"}
{"title": "Launch HN: Indy (YC S21) – A support app designed for ADHD brains", "url": "https://www.shimmer.care/indy-redirect", "content": "Launch HN: Indy (YC S21) – A support app designed for ADHD brains. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["launch", "hn", "indy", "yc", "s21", "support", "app", "design", "adhd", "brain", "score", "author", "date"], "num_tokens": 13, "token_loss_pct": 40.91, "normalized_content": "launch hn indy yc s21  a support app designed for adhd brains. score none. author none. date none"}
{"title": "Netflix tells directors to repeat plot for people using phones, says Matt Damon", "url": "https://www.nme.com/news/film/netflix-tells-directors-to-repeat-plot-for-people-using-phones-while-watching-says-matt-damon-3924120", "content": "Damon and Ben Affleck praised 'Adolescence' for being the \"exception\" Matt Damon has claimed that Netflix pushes directors to reiterate the plot for viewers who are watching while on their phones. The actor has just released new action film The Rip on the streaming platform, which sees him reunite with frequent collaborator Ben Affleck . During an appearance on the Joe Rogan Experience podcast alongside his co-star, Damon spoke about collaborating with Netflix, saying they want bigger action earlier in such films, and push for the plot to be repeated to accommodate attention spans. “The standard way to make an action movie that we learned was, you usually have three set pieces,” he said. “One in the first act, one in the second, one in the third… You spend most of your money on that one in the third act. That’s your finale.  “And now they’re like, ‘Can we get a big one in the first five minutes? We want people to stay tuned in. And it wouldn’t be terrible if you reiterated the plot three or four times in the dialogue because people are on their phones while they’re watching.’” Affleck went on to praise Netflix series Adolescence , which became a huge success last year, and the fact that it “didn’t do any of that shit”. “And it’s fucking great,” he added. “And it’s dark too. It’s tragic and intense. [It’s about] this guy who finds out his kid is accused of murder. There are long shots of the back of their heads. They get in the car, nobody says anything.” Damon said the series was “so masterfully made that it feels like the exception”, his co-star suggesting it “demonstrates that you don’t need to do any of that shit”. Elsewhere in the interview, Damon suggested that some people who have been cancelled would prefer prison to being publicly shunned.  After Rogan argued that one perceived misstep is “exaggerated to the fullest extent” and a person is “cast out of civilisation for life”, Damon responded: “In perpetuity. Because I bet some of those people would have pref", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["damon", "ben", "affleck", "praise", "adolescence", "exception", "matt", "damon", "claim", "netflix", "push", "director", "reiterate", "plot", "viewer", "watch", "phone", "actor", "release", "new", "action", "film", "rip", "stream", "platform", "see", "reunite", "frequent", "collaborator", "ben", "affleck", "appearance", "joe", "rogan", "experience", "podcast", "alongside", "co", "star", "damon", "speak", "collaborate", "netflix", "say", "want", "big", "action", "early", "film", "push", "plot", "repeat", "accommodate", "attention", "span", "standard", "way", "action", "movie", "learn", "usually", "set", "piece", "say", "act", "second", "spend", "money", "act", "finale", "like", "big", "minute", "want", "people", "stay", "tune", "not", "terrible", "reiterate", "plot", "time", "dialogue", "people", "phone", "watch", "affleck", "go", "praise", "netflix", "series", "adolescence", "huge", "success", "year", "fact", "not", "shit", "fucking", "great", "add", "dark", "tragic", "intense", "guy", "find", "kid", "accuse", "murder", "long", "shot", "head", "car", "say", "damon", "say", "series", "masterfully", "feel", "like", "exception", "co", "star", "suggest", "demonstrate", "not", "need", "shit", "interview", "damon", "suggest", "people", "cancel", "prefer", "prison", "publicly", "shun", "rogan", "argue", "perceive", "misstep", "exaggerate", "full", "extent", "person", "cast", "civilisation", "life", "damon", "respond", "perpetuity", "bet", "people", "pref"], "num_tokens": 154, "token_loss_pct": 59.26, "normalized_content": "damon and ben affleck praised 'adolescence' for being the exception matt damon has claimed that netflix pushes directors to reiterate the plot for viewers who are watching while on their phones. the actor has just released new action film the rip on the streaming platform which sees him reunite with frequent collaborator ben affleck . during an appearance on the joe rogan experience podcast alongside his co-star damon spoke about collaborating with netflix saying they want bigger action earlier in such films and push for the plot to be repeated to accommodate attention spans. the standard way to make an action movie that we learned was you usually have three set pieces he said. one in the first act one in the second one in the third you spend most of your money on that one in the third act. thats your finale. and now theyre like can we get a big one in the first five minutes we want people to stay tuned in. and it wouldnt be terrible if you reiterated the plot three or four times in the dialogue because people are on their phones while theyre watching. affleck went on to praise netflix series adolescence  which became a huge success last year and the fact that it didnt do any of that shit. and its fucking great he added. and its dark too. its tragic and intense. its about this guy who finds out his kid is accused of murder. there are long shots of the back of their heads. they get in the car nobody says anything. damon said the series was so masterfully made that it feels like the exception his co-star suggesting it demonstrates that you dont need to do any of that shit. elsewhere in the interview damon suggested that some people who have been cancelled would prefer prison to being publicly shunned. after rogan argued that one perceived misstep is exaggerated to the fullest extent and a person is cast out of civilisation for life damon responded in perpetuity. because i bet some of those people would have pref"}
{"title": "psc: The ps utility, with an eBPF twist and container context", "url": "https://github.com/loresuso/psc", "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation . the ps utility, with an eBPF twist and container context There was an error while loading. Please reload this page . There was an error while loading. Please reload this page . psc (ps container) is a fast process scanner that uses eBPF iterators and Google CEL to query system state with precision and full container context. psc uses eBPF iterators to read process and file descriptor information directly from kernel data structures. This approach is: Traditional Linux tools like ps , lsof , and ss are powerful but inflexible. They output fixed formats that require extensive piping through grep , awk , and sed : psc uses the Common Expression Language (CEL) to filter processes. CEL expressions read almost like natural language, making your scripts self-documenting and maintainable. No more deciphering complex pipelines of grep | awk | sed | xargs . The -o flag lets you output exactly the fields you need, eliminating post-processing entirely: Output presets are also available to quickly print common information: Traditional tools have no concept of containers. Getting container information requires parsing cgroup paths, querying container runtimes, and correlating PIDs manually: psc extracts container context (ID, name, image, runtime, labels) automatically for Docker, containerd, CRI-O, and Podman. Debug any container's processes, files, and network connections directly from the host: On Debian/Ubuntu: On Fedora/RHEL: Or manually: psc requires root privileges to load eBPF programs. Pass a CEL expression as the first argument to filter processes: Understanding why a process exists often requires looking at its open file descriptors and network connections: Process fields ( process.X ): Capability fields ( process.capabilities.X ): Namespace fields ( process.namespaces.X ): Container fields ( container.X ): File/Socket fields ( file.X or socket.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["read", "piece", "feedback", "input", "seriously", "available", "qualifier", "documentation", "ps", "utility", "ebpf", "twist", "container", "context", "error", "load", "reload", "page", "error", "load", "reload", "page", "psc", "ps", "container", "fast", "process", "scanner", "use", "ebpf", "iterator", "google", "cel", "query", "system", "state", "precision", "container", "context", "psc", "use", "ebpf", "iterator", "read", "process", "file", "descriptor", "information", "directly", "kernel", "datum", "structure", "approach", "traditional", "linux", "tool", "like", "ps", "lsof", "ss", "powerful", "inflexible", "output", "fix", "format", "require", "extensive", "piping", "grep", "awk", "se", "psc", "use", "common", "expression", "language", "cel", "filter", "process", "cel", "expression", "read", "like", "natural", "language", "make", "script", "self", "documenting", "maintainable", "decipher", "complex", "pipeline", "grep", "awk", "se", "xargs", "-o", "flag", "let", "output", "exactly", "field", "need", "eliminate", "post", "processing", "entirely", "output", "preset", "available", "quickly", "print", "common", "information", "traditional", "tool", "concept", "container", "get", "container", "information", "require", "parse", "cgroup", "path", "query", "container", "runtime", "correlating", "pid", "manually", "psc", "extract", "container", "context", "image", "runtime", "label", "automatically", "docker", "containerd", "cri", "podman", "debug", "container", "process", "file", "network", "connection", "directly", "host", "debianubuntu", "fedorarhel", "manually", "psc", "require", "root", "privilege", "load", "ebpf", "program", "pass", "cel", "expression", "argument", "filter", "process", "understand", "process", "exist", "require", "look", "open", "file", "descriptor", "network", "connection", "process", "field", "process.x", "capability", "field", "process.capabilities.x", "namespace", "field", "process.namespaces.x", "container", "field", "container.x", "filesocket", "field", "file.x", "socket"], "num_tokens": 194, "token_loss_pct": 40.12, "normalized_content": "we read every piece of feedback and take your input very seriously. to see all available qualifiers see our documentation . the ps utility with an ebpf twist and container context there was an error while loading. please reload this page . there was an error while loading. please reload this page . psc ps container is a fast process scanner that uses ebpf iterators and google cel to query system state with precision and full container context. psc uses ebpf iterators to read process and file descriptor information directly from kernel data structures. this approach is traditional linux tools like ps  lsof  and ss are powerful but inflexible. they output fixed formats that require extensive piping through grep  awk  and sed  psc uses the common expression language cel to filter processes. cel expressions read almost like natural language making your scripts self-documenting and maintainable. no more deciphering complex pipelines of grep  awk  sed  xargs . the -o flag lets you output exactly the fields you need eliminating post-processing entirely output presets are also available to quickly print common information traditional tools have no concept of containers. getting container information requires parsing cgroup paths querying container runtimes and correlating pids manually psc extracts container context id name image runtime labels automatically for docker containerd cri-o and podman. debug any container's processes files and network connections directly from the host on debianubuntu on fedorarhel or manually psc requires root privileges to load ebpf programs. pass a cel expression as the first argument to filter processes understanding why a process exists often requires looking at its open file descriptors and network connections process fields  process.x  capability fields  process.capabilities.x  namespace fields  process.namespaces.x  container fields  container.x  filesocket fields  file.x or socket."}
{"title": "Show HN: HTTP:COLON – A quick HTTP header/directive inspector and reference", "url": "https://httpcolon.dev/", "content": "HTTP headers are a fundamental component of the HTTP protocol, which is the backbone of the internet. These headers contain important information about the request and response, such as content type, caching instructions, authentication tokens, and more. By understanding how to read and manipulate HTTP headers, developers can optimize their web applications for performance, security, and functionality. Moreover, HTTP headers play a critical role in API integrations, allowing developers to communicate with external services and systems. In short, HTTP headers are an essential tool in the web developer's arsenal, and any developer serious about building high-quality web applications should invest the time to learn and master them.", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["http", "header", "fundamental", "component", "http", "protocol", "backbone", "internet", "header", "contain", "important", "information", "request", "response", "content", "type", "cache", "instruction", "authentication", "token", "understand", "read", "manipulate", "http", "header", "developer", "optimize", "web", "application", "performance", "security", "functionality", "http", "header", "play", "critical", "role", "api", "integration", "allow", "developer", "communicate", "external", "service", "system", "short", "http", "header", "essential", "tool", "web", "developer", "arsenal", "developer", "build", "high", "quality", "web", "application", "invest", "time", "learn", "master"], "num_tokens": 63, "token_loss_pct": 45.22, "normalized_content": "http headers are a fundamental component of the http protocol which is the backbone of the internet. these headers contain important information about the request and response such as content type caching instructions authentication tokens and more. by understanding how to read and manipulate http headers developers can optimize their web applications for performance security and functionality. moreover http headers play a critical role in api integrations allowing developers to communicate with external services and systems. in short http headers are an essential tool in the web developer's arsenal and any developer serious about building high-quality web applications should invest the time to learn and master them."}
{"title": "Semiconductor Fabs III: Ion Implantation", "url": "https://nomagicpill.substack.com/p/ion-implantation", "content": "Ion implantation is a common process used in the semiconductor industry to change the properties of a material, namely silicon (the substrate). Physics, equipment used, process considerations, alternatives, and further resources are discussed. While I wrote this in 2020 and some advances have been made, the principles remain the same. First, why are ions shot into silicon (Si)? What does it do from a physics perspective? The reason has to do with the band gap , the energy difference between the valence band and conduction band. Within Si exist both electrons (negatively-charged) and electron holes (positively-charged). The hole is a bit of an oddity: it’s an unoccupied space where an electron could exist, but doesn’t at that moment. While not literally a particle, they can be treated and thought of as such. Current can be viewed as the flow of electrons or holes. When a bias is applied, electrons begin to move and occupy holes. When electron A moves to hole B, hole A is formed. This propagates and creates the flow of charge, or current. What does this have to do with Si? Si has four valence electrons in its four states of 3s 2 3p 2 . Those four valence electrons covalently bond with the surrounding Si crystal lattice structure, effectively filling that Si atom’s valence shell. So, when an ion is introduced into the lattice that does not have four valence electrons, one of two things happen: there are free electrons (if the number of valence electrons is >4, such as in phosphorus’s case) or free holes (valence electrons <4, such as boron). In the first case, four of phosphorus’s (P’s) electrons have bonded to surrounding Si atoms’ valence electrons, leaving the fifth one free and able to contribute to current, making it a donor impurity, because it donates an electron. This creates an n(egative)-type semiconductor. In the second case, three of boron’s (B’s) electrons have bonded to surrounding Si atoms, but one Si atom and the B atom both have incomplete valence shel", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ion", "implantation", "common", "process", "semiconductor", "industry", "change", "property", "material", "silicon", "substrate", "physics", "equipment", "process", "consideration", "alternative", "resource", "discuss", "write", "2020", "advance", "principle", "remain", "ion", "shoot", "silicon", "si", "physics", "perspective", "reason", "band", "gap", "energy", "difference", "valence", "band", "conduction", "band", "si", "exist", "electron", "negatively", "charge", "electron", "hole", "positively", "charge", "hole", "bit", "oddity", "unoccupied", "space", "electron", "exist", "not", "moment", "literally", "particle", "treat", "think", "current", "view", "flow", "electron", "hole", "bias", "apply", "electron", "begin", "occupy", "hole", "electron", "move", "hole", "hole", "form", "propagate", "create", "flow", "charge", "current", "si", "si", "valence", "electron", "state", "3s", "3p", "valence", "electron", "covalently", "bond", "surround", "si", "crystal", "lattice", "structure", "effectively", "fill", "si", "atom", "valence", "shell", "ion", "introduce", "lattice", "valence", "electron", "thing", "happen", "free", "electron", "number", "valence", "electron", "phosphoruss", "case", "free", "hole", "valence", "electron", "boron", "case", "phosphoruss", "ps", "electron", "bond", "surround", "si", "atom", "valence", "electron", "leave", "fifth", "free", "able", "contribute", "current", "make", "donor", "impurity", "donate", "electron", "create", "negative", "type", "semiconductor", "second", "case", "boron", "bs", "electron", "bond", "surround", "si", "atom", "si", "atom", "atom", "incomplete", "valence", "shel"], "num_tokens": 162, "token_loss_pct": 54.87, "normalized_content": "ion implantation is a common process used in the semiconductor industry to change the properties of a material namely silicon the substrate. physics equipment used process considerations alternatives and further resources are discussed. while i wrote this in 2020 and some advances have been made the principles remain the same. first why are ions shot into silicon si what does it do from a physics perspective the reason has to do with the band gap  the energy difference between the valence band and conduction band. within si exist both electrons negatively-charged and electron holes positively-charged. the hole is a bit of an oddity its an unoccupied space where an electron could exist but doesnt at that moment. while not literally a particle they can be treated and thought of as such. current can be viewed as the flow of electrons or holes. when a bias is applied electrons begin to move and occupy holes. when electron a moves to hole b hole a is formed. this propagates and creates the flow of charge or current. what does this have to do with si si has four valence electrons in its four states of 3s 2 3p 2 . those four valence electrons covalently bond with the surrounding si crystal lattice structure effectively filling that si atoms valence shell. so when an ion is introduced into the lattice that does not have four valence electrons one of two things happen there are free electrons if the number of valence electrons is 4 such as in phosphoruss case or free holes valence electrons 4 such as boron. in the first case four of phosphoruss ps electrons have bonded to surrounding si atoms valence electrons leaving the fifth one free and able to contribute to current making it a donor impurity because it donates an electron. this creates an negative-type semiconductor. in the second case three of borons bs electrons have bonded to surrounding si atoms but one si atom and the b atom both have incomplete valence shel"}
{"title": "Shingles vaccine may help keep older people biologically younger", "url": "https://www.thetimes.com/uk/science/article/shingles-vaccine-news-bz55zstn5", "content": "Shingles vaccine may help keep older people biologically younger. Score: None. Author: None. Date: None", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["shingles", "vaccine", "help", "old", "people", "biologically", "young", "score", "author", "date"], "num_tokens": 10, "token_loss_pct": 44.44, "normalized_content": "shingles vaccine may help keep older people biologically younger. score none. author none. date none"}
{"title": "Why Greenland's natural resources are nearly impossible to mine", "url": "https://theweek.com/world-news/greenland-natural-resources-impossible-mine", "content": "SUBSCRIBE & SAVE Less than $3 per week The country’s natural landscape makes the task extremely difficult President Donald Trump has renewed his efforts to take over Greenland, and tapping into the Danish territory’s natural resources is a key part of the strategy. But even if Trump were to somehow make Greenland a U.S. territory (something Denmark vehemently opposes), experts say the island’s harsh climate and environment make mining Greenland’s natural resources an unachievable goal. Greenland has significant supplies of rare earth elements. These 17 metals, with “exotic-sounding names like terbium and neodymium, are vital for many everyday technologies,” said the BBC . Household items like televisions and smartphones would “not work without them.” Trump wants to tap into Greenland’s supply of rare earth minerals as part of an effort to overtake China, the country that currently “controls the world’s supply,” said Tony Sage, the CEO of Critical Metals, to the BBC. But rare earth minerals are not Greenland’s only natural resource . Many “occurrences of graphite and graphite schist are reported from many localities on the island,” said Reuters . Other minerals commonly found in the territory include diamonds, gold, nickel, titanium, tungsten, zinc and more, according to Greenland’s Mineral Resources Authority. Escape your echo chamber. Get the facts behind the news, plus analysis from multiple perspectives. From our morning news briefing to a weekly Good News Newsletter, get the best of The Week delivered directly to your inbox. From our morning news briefing to a weekly Good News Newsletter, get the best of The Week delivered directly to your inbox. The island’s frigid, Arctic climate serves as the main culprit for challenging mining. Most of Greenland’s natural resources are “located in remote areas above the Arctic Circle, where there is a mile-thick polar ice sheet and darkness reigns much of the year,” said CNN . While people may understandably think neighborin", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["subscribe", "save", "week", "countrys", "natural", "landscape", "make", "task", "extremely", "difficult", "president", "donald", "trump", "renew", "effort", "greenland", "tap", "danish", "territory", "natural", "resource", "key", "strategy", "trump", "greenland", "u.s", "territory", "denmark", "vehemently", "oppose", "expert", "island", "harsh", "climate", "environment", "mining", "greenland", "natural", "resource", "unachievable", "goal", "greenland", "significant", "supply", "rare", "earth", "element", "17", "metal", "exotic", "sound", "name", "like", "terbium", "neodymium", "vital", "everyday", "technology", "say", "bbc", "household", "item", "like", "television", "smartphone", "work", "trump", "want", "tap", "greenland", "supply", "rare", "earth", "mineral", "effort", "overtake", "china", "country", "currently", "control", "world", "supply", "say", "tony", "sage", "ceo", "critical", "metal", "bbc", "rare", "earth", "mineral", "greenland", "natural", "resource", "occurrence", "graphite", "graphite", "schist", "report", "locality", "island", "say", "reuter", "mineral", "commonly", "find", "territory", "include", "diamond", "gold", "nickel", "titanium", "tungsten", "zinc", "accord", "greenland", "mineral", "resource", "authority", "escape", "echo", "chamber", "fact", "news", "plus", "analysis", "multiple", "perspective", "morning", "news", "briefing", "weekly", "good", "news", "newsletter", "good", "week", "deliver", "directly", "inbox", "morning", "news", "briefing", "weekly", "good", "news", "newsletter", "good", "week", "deliver", "directly", "inbox", "islands", "frigid", "arctic", "climate", "serve", "main", "culprit", "challenge", "mining", "greenland", "natural", "resource", "locate", "remote", "area", "arctic", "circle", "mile", "thick", "polar", "ice", "sheet", "darkness", "reign", "year", "say", "cnn", "people", "understandably", "think", "neighborin"], "num_tokens": 184, "token_loss_pct": 44.07, "normalized_content": "subscribe  save less than 3 per week the countrys natural landscape makes the task extremely difficult president donald trump has renewed his efforts to take over greenland and tapping into the danish territorys natural resources is a key part of the strategy. but even if trump were to somehow make greenland a u.s. territory something denmark vehemently opposes experts say the islands harsh climate and environment make mining greenlands natural resources an unachievable goal. greenland has significant supplies of rare earth elements. these 17 metals with exotic-sounding names like terbium and neodymium are vital for many everyday technologies said the bbc . household items like televisions and smartphones would not work without them. trump wants to tap into greenlands supply of rare earth minerals as part of an effort to overtake china the country that currently controls the worlds supply said tony sage the ceo of critical metals to the bbc. but rare earth minerals are not greenlands only natural resource . many occurrences of graphite and graphite schist are reported from many localities on the island said reuters . other minerals commonly found in the territory include diamonds gold nickel titanium tungsten zinc and more according to greenlands mineral resources authority. escape your echo chamber. get the facts behind the news plus analysis from multiple perspectives. from our morning news briefing to a weekly good news newsletter get the best of the week delivered directly to your inbox. from our morning news briefing to a weekly good news newsletter get the best of the week delivered directly to your inbox. the islands frigid arctic climate serves as the main culprit for challenging mining. most of greenlands natural resources are located in remote areas above the arctic circle where there is a mile-thick polar ice sheet and darkness reigns much of the year said cnn . while people may understandably think neighborin"}
{"title": "Raising money fucked me up", "url": "https://blog.yakkomajuri.com/blog/raising-money-fucked-me-up", "content": "January 15, 2026 About four months ago I quit my job at Doublepoint and decided to start my own thing. I'd been working on a little project with Pedrique (who would become my co-founder) for a bit over half-a-year and decided I had enough signal to determine he was someone I wanted to start a business with. I was excited about the idea we were working on at the time (we were live with paying customers and truly believed in the thesis), but in hindsight, being truly honest about my motivations, I mostly wanted to run my own thing. In a dream world I'd have had the \"idea of my life\" while working at PostHog or Doublepoint and have gone on to build that with maximum conviction but this wasn't the case, so I got tired of waiting for a spark and decided to go out and make it happen, with the idea we were working on being our best bet at the time. Since I'd just quit my job, I had my finances well in order. Thus, my ideal scenario would have been to keep working on the product we had, try to scale it, and if that didn't work, try something else, then something else, until something did indeed really get off the ground, and only at that point we would consider whether or not to raise VC funding, depending on whether it made sense or not. My ideal scenario wasn't going to work for Pedrique, though. He had told me for a while that the money he had saved up for trying to build his own thing was running out and that soon he'd need to start freelancing or something to make some income in order to sustain the search for a little longer. Prior to us working together, he had a bit of success with his MicroSaaS products but only just enough to increase his personal runway, which was now reasonably short. We had spoken about this before, but with me now being 110% in, we had to do something about it. I had just come in full-time so we weren't about to go back to a dynamic where one person was full-time and the other part-time because they needed to make ends meet. The decision then", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["january", "15", "2026", "month", "ago", "quit", "job", "doublepoint", "decide", "start", "thing", "work", "little", "project", "pedrique", "co", "founder", "bit", "half", "year", "decide", "signal", "determine", "want", "start", "business", "excited", "idea", "work", "time", "live", "pay", "customer", "truly", "believe", "thesis", "hindsight", "truly", "honest", "motivation", "want", "run", "thing", "dream", "world", "idea", "life", "work", "posthog", "doublepoint", "go", "build", "maximum", "conviction", "case", "get", "tired", "wait", "spark", "decide", "happen", "idea", "work", "good", "bet", "time", "quit", "job", "finance", "order", "ideal", "scenario", "work", "product", "try", "scale", "work", "try", "ground", "point", "consider", "raise", "vc", "funding", "depend", "sense", "ideal", "scenario", "go", "work", "pedrique", "tell", "money", "save", "try", "build", "thing", "run", "soon", "need", "start", "freelancing", "income", "order", "sustain", "search", "little", "long", "prior", "work", "bit", "success", "microsaas", "product", "increase", "personal", "runway", "reasonably", "short", "speak", "110", "come", "time", "dynamic", "person", "time", "time", "need", "end", "meet", "decision"], "num_tokens": 131, "token_loss_pct": 68.2, "normalized_content": "january 15 2026 about four months ago i quit my job at doublepoint and decided to start my own thing. i'd been working on a little project with pedrique who would become my co-founder for a bit over half-a-year and decided i had enough signal to determine he was someone i wanted to start a business with. i was excited about the idea we were working on at the time we were live with paying customers and truly believed in the thesis but in hindsight being truly honest about my motivations i mostly wanted to run my own thing. in a dream world i'd have had the idea of my life while working at posthog or doublepoint and have gone on to build that with maximum conviction but this wasn't the case so i got tired of waiting for a spark and decided to go out and make it happen with the idea we were working on being our best bet at the time. since i'd just quit my job i had my finances well in order. thus my ideal scenario would have been to keep working on the product we had try to scale it and if that didn't work try something else then something else until something did indeed really get off the ground and only at that point we would consider whether or not to raise vc funding depending on whether it made sense or not. my ideal scenario wasn't going to work for pedrique though. he had told me for a while that the money he had saved up for trying to build his own thing was running out and that soon he'd need to start freelancing or something to make some income in order to sustain the search for a little longer. prior to us working together he had a bit of success with his microsaas products but only just enough to increase his personal runway which was now reasonably short. we had spoken about this before but with me now being 110 in we had to do something about it. i had just come in full-time so we weren't about to go back to a dynamic where one person was full-time and the other part-time because they needed to make ends meet. the decision then"}
{"title": "Memdeklaro – The humanitarian open source alternative to government ID", "url": "https://memdeklaro.computersforpeace.net", "content": "Memdeklaro is a philosophical project that empowers people to self-declare their own identity - without third parties such as birth parents, birth cultures or birth countries. Memdeklaro positions itself as a humanitarian alternative to the exclusionary state monopoly on identity and supports the three freedoms: freedom of name, freedom of belief and freedom of association. Memdeklaro supports a world where people are judged only on their character, beliefs and actions, not on where they were born, who they were born to, or what their birth culture was. A world where individuals have power over their own lives - the power to leave corrupt governments, hostile cultures and abusers, and thrive in a self-chosen community. \"Self-declaration of identity gives people the power to decide their own fate, and creates a world where actions and beliefs matter more than arbitrary circumstances of birth.\" Millions of people worldwide have no access to government ID. Nation-states routinely refuse to issue birth certificates, national ID cards and passports to people, most often due to the circumstances of their birth, rather than due to their own actions as an adult. This group may include stateless people, refugees, people who weren’t registered at birth, and people who escaped from child abuse, domestic abuse or cult abuse. As government ID is increasingly required for employment, housing, healthcare, education, travel and daily life necessities, this leaves people at best on the edge of society or at worst criminalized for existing. In this situation, cash in hand jobs and informal apartment rentals are an essential lifeline, but as the state cracks down on the gray market economy, exclusion from the state monopoly on identity could mean life or death. \"You may think economic exclusion — banned from employment, housing, healthcare, education, banking, travel, contracts, mail, sim cards and more — would be a punishment for only the most severe of crimes. But for stateless peop", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["memdeklaro", "philosophical", "project", "empower", "people", "self", "declare", "identity", "party", "birth", "parent", "birth", "culture", "birth", "country", "memdeklaro", "position", "humanitarian", "alternative", "exclusionary", "state", "monopoly", "identity", "support", "freedom", "freedom", "freedom", "belief", "freedom", "association", "memdeklaro", "support", "world", "people", "judge", "character", "belief", "action", "bear", "bear", "birth", "culture", "world", "individual", "power", "life", "power", "leave", "corrupt", "government", "hostile", "culture", "abuser", "thrive", "self", "choose", "community", "self", "declaration", "identity", "give", "people", "power", "decide", "fate", "create", "world", "action", "belief", "matter", "arbitrary", "circumstance", "birth", "million", "people", "worldwide", "access", "government", "d.", "nation", "state", "routinely", "refuse", "issue", "birth", "certificate", "national", "card", "passport", "people", "circumstance", "birth", "action", "adult", "group", "include", "stateless", "people", "refugee", "people", "not", "register", "birth", "people", "escape", "child", "abuse", "domestic", "abuse", "cult", "abuse", "government", "increasingly", "require", "employment", "housing", "healthcare", "education", "travel", "daily", "life", "necessity", "leave", "people", "well", "edge", "society", "bad", "criminalize", "exist", "situation", "cash", "hand", "job", "informal", "apartment", "rental", "essential", "lifeline", "state", "crack", "gray", "market", "economy", "exclusion", "state", "monopoly", "identity", "mean", "life", "death", "think", "economic", "exclusion", "ban", "employment", "housing", "healthcare", "education", "banking", "travel", "contract", "mail", "sim", "card", "punishment", "severe", "crime", "stateless", "peop"], "num_tokens": 170, "token_loss_pct": 49.55, "normalized_content": "memdeklaro is a philosophical project that empowers people to self-declare their own identity - without third parties such as birth parents birth cultures or birth countries. memdeklaro positions itself as a humanitarian alternative to the exclusionary state monopoly on identity and supports the three freedoms freedom of name freedom of belief and freedom of association. memdeklaro supports a world where people are judged only on their character beliefs and actions not on where they were born who they were born to or what their birth culture was. a world where individuals have power over their own lives - the power to leave corrupt governments hostile cultures and abusers and thrive in a self-chosen community. self-declaration of identity gives people the power to decide their own fate and creates a world where actions and beliefs matter more than arbitrary circumstances of birth. millions of people worldwide have no access to government id. nation-states routinely refuse to issue birth certificates national id cards and passports to people most often due to the circumstances of their birth rather than due to their own actions as an adult. this group may include stateless people refugees people who werent registered at birth and people who escaped from child abuse domestic abuse or cult abuse. as government id is increasingly required for employment housing healthcare education travel and daily life necessities this leaves people at best on the edge of society or at worst criminalized for existing. in this situation cash in hand jobs and informal apartment rentals are an essential lifeline but as the state cracks down on the gray market economy exclusion from the state monopoly on identity could mean life or death. you may think economic exclusion  banned from employment housing healthcare education banking travel contracts mail sim cards and more  would be a punishment for only the most severe of crimes. but for stateless peop"}
{"title": "Install.md: A standard for LLM-executable installation", "url": "https://www.mintlify.com/blog/install-md-standard-for-llm-executable-installation", "content": "Resources Explore Startups Built for fast-moving teams Enterprise Scalable for large organizations Switch Seamless migration tools Company Careers Join our growing team Wall of Love Customer testimonials Guides Guide to technical writing Documentation Guides Getting Started Deploy in minutes Components Customizable components library Developers API Reference Build integrations and custom workflows Changelog Learn what's new January 15, 2026 Michael Ryaboy Content Strategist Installing software is the kind of specific and repetitive task that agents are good at. Today we are proposing install.md to standardize how developers should write installation instructions for agents. It's currently live on all Mintlify sites including Cerebras, Firecrawl, and Langchain. Proposal for a standard /install.md file that provides LLM-executable installation instructions. Agents are growing in capability faster than software developers have been able to keep up. Product documentation today is focused on humans instead of AI which creates friction when trying to automate annoying yak-shaving style tasks like installation. The difference is very subtle. Agents need to have a task iterated to them like \"I want you to install Mintlify CLI for me. Execute all the steps below autonomously.\" whereas humans can work from more general prose or even a bash script. Today we are proposing install.md to standardize how developers should write installation instructions for agents. It's currently live on all Mintlify sites including Cerebras , Firecrawl , and Langchain . Add an install.md markdown file to your project with LLM-executable installation instructions. Users paste that file into an LLM or pipe it directly from a URL. The LLM reads the instructions, detects the environment, adapts to the setup, and executes—optionally with approval at every step. Because the file is human-readable, users see exactly what will happen before it runs. Instead of piping an executable file into bash with abs", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["resource", "explore", "startup", "build", "fast", "move", "team", "enterprise", "scalable", "large", "organization", "switch", "seamless", "migration", "tool", "company", "career", "join", "grow", "team", "wall", "love", "customer", "testimonial", "guide", "guide", "technical", "writing", "documentation", "guide", "getting", "start", "deploy", "minute", "component", "customizable", "component", "library", "developer", "api", "reference", "build", "integration", "custom", "workflow", "changelog", "learn", "new", "january", "15", "2026", "michael", "ryaboy", "content", "strategist", "instal", "software", "kind", "specific", "repetitive", "task", "agent", "good", "today", "propose", "install.md", "standardize", "developer", "write", "installation", "instruction", "agent", "currently", "live", "mintlify", "site", "include", "cerebra", "firecrawl", "langchain", "proposal", "standard", "install.md", "file", "provide", "llm", "executable", "installation", "instruction", "agent", "grow", "capability", "fast", "software", "developer", "able", "product", "documentation", "today", "focus", "human", "instead", "ai", "create", "friction", "try", "automate", "annoying", "yak", "shave", "style", "task", "like", "installation", "difference", "subtle", "agent", "need", "task", "iterate", "like", "want", "install", "mintlify", "cli", "execute", "step", "autonomously", "human", "work", "general", "prose", "bash", "script", "today", "propose", "install.md", "standardize", "developer", "write", "installation", "instruction", "agent", "currently", "live", "mintlify", "site", "include", "cerebra", "firecrawl", "langchain", "add", "install.md", "markdown", "file", "project", "llm", "executable", "installation", "instruction", "user", "paste", "file", "llm", "pipe", "directly", "url", "llm", "read", "instruction", "detect", "environment", "adapt", "setup", "executesoptionally", "approval", "step", "file", "human", "readable", "user", "exactly", "happen", "run", "instead", "pip", "executable", "file", "bash", "ab"], "num_tokens": 190, "token_loss_pct": 41.18, "normalized_content": "resources explore startups built for fast-moving teams enterprise scalable for large organizations switch seamless migration tools company careers join our growing team wall of love customer testimonials guides guide to technical writing documentation guides getting started deploy in minutes components customizable components library developers api reference build integrations and custom workflows changelog learn what's new january 15 2026 michael ryaboy content strategist installing software is the kind of specific and repetitive task that agents are good at. today we are proposing install.md to standardize how developers should write installation instructions for agents. it's currently live on all mintlify sites including cerebras firecrawl and langchain. proposal for a standard install.md file that provides llm-executable installation instructions. agents are growing in capability faster than software developers have been able to keep up. product documentation today is focused on humans instead of ai which creates friction when trying to automate annoying yak-shaving style tasks like installation. the difference is very subtle. agents need to have a task iterated to them like i want you to install mintlify cli for me. execute all the steps below autonomously. whereas humans can work from more general prose or even a bash script. today we are proposing install.md to standardize how developers should write installation instructions for agents. it's currently live on all mintlify sites including cerebras  firecrawl  and langchain . add an install.md markdown file to your project with llm-executable installation instructions. users paste that file into an llm or pipe it directly from a url. the llm reads the instructions detects the environment adapts to the setup and executesoptionally with approval at every step. because the file is human-readable users see exactly what will happen before it runs. instead of piping an executable file into bash with abs"}
{"title": "Canada's Prime Minister Mark Carney's Full Speech at Davos", "url": "https://www.cbc.ca/news/politics/mark-carney-speech-davos-rules-based-order-9.7053350", "content": "Below are Prime Minister Mark Carney's remarks at the World Economic Forum in Davos, Switzerland, on Tuesday. (In French): It's a pleasure — and a duty — to be with you at this turning point for Canada and for the world. Today, I'll talk about the rupture in the world order, the end of a nice story and the beginning of a brutal reality where geopolitics among the great powers is not subject to any constraints. But I also submit to you that other countries, particularly middle powers like Canada, are not powerless. They have the capacity to build a new order that embodies our values, like respect for human rights, sustainable development, solidarity, sovereignty and territorial integrity of states. The power of the less powerful begins with honesty. It seems that every day we're reminded that we live in an era of great power rivalry. That the rules-based order is fading. That the strong can do what they can, and the weak must suffer what they must. This aphorism of Thucydides is presented as inevitable — as the natural logic of international relations reasserting itself. And faced with this logic, there is a strong tendency for countries to go along to get along. To accommodate. To avoid trouble. To hope that compliance will buy safety. It won't. So, what are our options? 'The old order is not coming back': PM says Canada must 'name reality' and build strength at home In 1978, the Czech dissident Václav Havel, later president, wrote an essay called The Power of the Powerless. And in it, he asked a simple question: How did the communist system sustain itself? And his answer began with a greengrocer. Every morning, this shopkeeper places a sign in his window: \"Workers of the world, unite!\" He doesn't believe it. No one does. But he places the sign anyway to avoid trouble, to signal compliance, to get along. And because every shopkeeper on every street does the same, the system persists. Not through violence alone, but through the participation of ordinary people in rit", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["prime", "minister", "mark", "carney", "remark", "world", "economic", "forum", "davos", "switzerland", "tuesday", "french", "pleasure", "duty", "turning", "point", "canada", "world", "today", "talk", "rupture", "world", "order", "end", "nice", "story", "beginning", "brutal", "reality", "geopolitic", "great", "power", "subject", "constraint", "submit", "country", "particularly", "middle", "power", "like", "canada", "powerless", "capacity", "build", "new", "order", "embody", "value", "like", "respect", "human", "right", "sustainable", "development", "solidarity", "sovereignty", "territorial", "integrity", "state", "power", "powerful", "begin", "honesty", "day", "remind", "live", "era", "great", "power", "rivalry", "rule", "base", "order", "fade", "strong", "weak", "suffer", "aphorism", "thucydide", "present", "inevitable", "natural", "logic", "international", "relation", "reassert", "face", "logic", "strong", "tendency", "country", "accommodate", "avoid", "trouble", "hope", "compliance", "buy", "safety", "will", "option", "old", "order", "come", "pm", "say", "canada", "reality", "build", "strength", "home", "1978", "czech", "dissident", "václav", "havel", "later", "president", "write", "essay", "call", "power", "powerless", "ask", "simple", "question", "communist", "system", "sustain", "answer", "begin", "greengrocer", "morning", "shopkeeper", "place", "sign", "window", "worker", "world", "unite", "believe", "place", "sign", "avoid", "trouble", "signal", "compliance", "shopkeeper", "street", "system", "persist", "violence", "participation", "ordinary", "people", "rit"], "num_tokens": 155, "token_loss_pct": 58.89, "normalized_content": "below are prime minister mark carney's remarks at the world economic forum in davos switzerland on tuesday. in french it's a pleasure  and a duty  to be with you at this turning point for canada and for the world. today i'll talk about the rupture in the world order the end of a nice story and the beginning of a brutal reality where geopolitics among the great powers is not subject to any constraints. but i also submit to you that other countries particularly middle powers like canada are not powerless. they have the capacity to build a new order that embodies our values like respect for human rights sustainable development solidarity sovereignty and territorial integrity of states. the power of the less powerful begins with honesty. it seems that every day we're reminded that we live in an era of great power rivalry. that the rules-based order is fading. that the strong can do what they can and the weak must suffer what they must. this aphorism of thucydides is presented as inevitable  as the natural logic of international relations reasserting itself. and faced with this logic there is a strong tendency for countries to go along to get along. to accommodate. to avoid trouble. to hope that compliance will buy safety. it won't. so what are our options 'the old order is not coming back' pm says canada must 'name reality' and build strength at home in 1978 the czech dissident václav havel later president wrote an essay called the power of the powerless. and in it he asked a simple question how did the communist system sustain itself and his answer began with a greengrocer. every morning this shopkeeper places a sign in his window workers of the world unite he doesn't believe it. no one does. but he places the sign anyway to avoid trouble to signal compliance to get along. and because every shopkeeper on every street does the same the system persists. not through violence alone but through the participation of ordinary people in rit"}
{"title": "OpenBSD-current now runs as guest under Apple Hypervisor", "url": "https://www.undeadly.org/cgi?action=article;sid=20260115203619", "content": "OpenBSD Journal Home Archives About Submit Story Create Account Login Contributed by Peter N. M. Hansteen on 2026-01-15 from the hyper-armed dept. The commits read List:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Helg Bredow <helg () cvs ! openbsd ! org>\r\nDate:       2026-01-12 18:15:33\r\n\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\thelg@cvs.openbsd.org\t2026/01/12 11:15:33\r\n\r\nModified files:\r\n\tsys/dev/pv     : viogpu.c \r\n\r\nLog message:\r\nviogpu_wsmmap() returns a kva but instead should return a physical\r\naddress via bus_dmamem_mmap(9) . Without this, QEMU would only show a\r\nblack screen when starting X11. On the Apple Hypervisor, the kernel\r\nwould panic. Also add calls to bus_dmamap_sync(9) before transferring the framebuffer\r\nto host memory. It was working for me without this, but this ensures\r\nthat the host running on another CPU will see updates to the\r\nframebuffer.\r\n\r\nThanks to kettenis@ for reviewing and providing feedback.\r\n\r\nok sf@ and List:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Stefan Fritsch <sf () cvs ! openbsd ! org>\r\nDate:       2026-01-15 9:06:19\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\tsf@cvs.openbsd.org\t2026/01/15 02:06:19\r\n\r\nModified files:\r\n\tsys/dev/pv     : if_vio.c \r\n\r\nLog message:\r\nvio: Support MTU feature\r\n\r\nAdd support for the VIRTIO_NET_F_MTU which allows to get the hardmtu\r\nfrom the hypervisor. Also set the current mtu to the same value. The\r\nvirtio standard is not clear if that is recommended, but Linux does\r\nthis, too.\r\n\r\nUse ETHER_MAX_HARDMTU_LEN as upper hardmtu limit instead of MAXMCLBYTES,\r\nas this seems to be more correct.\r\n\r\nIf the hypervisor requests a MTU larger than ETHER_MAX_HARDMTU_LEN,\r\nredo feature negotiation without VIRTIO_NET_F_MTU.\r\n\r\nWith this commit, OpenBSD finally works on Apple Virtualization.\r\n\r\nInput and testing from @helg\r\n\r\nok jan@ This development will be most welcome for those of us who run with newer Apple Silicon Mac models. As always, if you h", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["openbsd", "journal", "home", "archive", "submit", "story", "create", "account", "login", "contribute", "peter", "n.", "m.", "hansteen", "2026", "01", "15", "hyper", "armed", "dept", "commit", "read", "list", "openbsd", "cvs", "subject", "cvs", "cvs.openbsd.org", "src", "helg", "bredow", "date", "2026", "01", "12", "181533", "cvsroot", "cvs", "module", "src", "change", "helgmention.openbsd.org", "20260112", "111533", "modify", "file", "sysdevpv", "viogpu.c", "log", "message", "viogpu_wsmmap", "return", "kva", "instead", "return", "physical", "address", "bus_dmamem_mmap9", "qemu", "black", "screen", "start", "x11", "apple", "hypervisor", "kernel", "panic", "add", "call", "bus_dmamap_sync9", "transfer", "framebuffer", "host", "memory", "work", "ensure", "host", "run", "cpu", "update", "framebuffer", "thank", "ketteni", "review", "provide", "feedback", "ok", "sf", "list", "openbsd", "cvs", "subject", "cvs", "cvs.openbsd.org", "src", "stefan", "fritsch", "date", "2026", "01", "15", "90619", "cvsroot", "cvs", "module", "src", "change", "sfmention.openbsd.org", "20260115", "020619", "modify", "file", "sysdevpv", "if_vio.c", "log", "message", "vio", "support", "mtu", "feature", "add", "support", "virtio_net_f_mtu", "allow", "hardmtu", "hypervisor", "set", "current", "mtu", "value", "virtio", "standard", "clear", "recommend", "linux", "use", "ether_max_hardmtu_len", "upper", "hardmtu", "limit", "instead", "maxmclbyte", "correct", "hypervisor", "request", "mtu", "large", "ether_max_hardmtu_len", "redo", "feature", "negotiation", "virtio_net_f_mtu", "commit", "openbsd", "finally", "work", "apple", "virtualization", "input", "testing", "mention", "ok", "jan", "development", "welcome", "run", "new", "apple", "silicon", "mac", "model"], "num_tokens": 171, "token_loss_pct": 43.93, "normalized_content": "openbsd journal home archives about submit story create account login contributed by peter n. m. hansteen on 2026-01-15 from the hyper-armed dept. the commits read list openbsd-cvs subject cvs cvs.openbsd.org src from helg bredow date 2026-01-12 181533 cvsroot cvs module name src changes by helgmention.openbsd.org 20260112 111533 modified files sysdevpv  viogpu.c log message viogpu_wsmmap returns a kva but instead should return a physical address via bus_dmamem_mmap9 . without this qemu would only show a black screen when starting x11. on the apple hypervisor the kernel would panic. also add calls to bus_dmamap_sync9 before transferring the framebuffer to host memory. it was working for me without this but this ensures that the host running on another cpu will see updates to the framebuffer. thanks to kettenis for reviewing and providing feedback. ok sf and list openbsd-cvs subject cvs cvs.openbsd.org src from stefan fritsch date 2026-01-15 90619 cvsroot cvs module name src changes by sfmention.openbsd.org 20260115 020619 modified files sysdevpv  if_vio.c log message vio support mtu feature add support for the virtio_net_f_mtu which allows to get the hardmtu from the hypervisor. also set the current mtu to the same value. the virtio standard is not clear if that is recommended but linux does this too. use ether_max_hardmtu_len as upper hardmtu limit instead of maxmclbytes as this seems to be more correct. if the hypervisor requests a mtu larger than ether_max_hardmtu_len redo feature negotiation without virtio_net_f_mtu. with this commit openbsd finally works on apple virtualization. input and testing from mention ok jan this development will be most welcome for those of us who run with newer apple silicon mac models. as always if you h"}
{"title": "Patching the Wii News Channel to serve local news (2025)", "url": "https://raulnegron.me/2025/wii-news-pr/", "content": "Site written in Markdown , generated by Hugo , hosted on Github\n        Pages and registered using Route 53 . theme: modified hugo-lanyon © 2025. All rights reserved. 🎧 Now Playing: Menu (News Channel) via Nintendo Music App In keeping with my passion (?) for displaying local news articles in unexpected places , I figured it would be a fun project to try and see what it would take to display current local news on the Nintendo Wii console’s News Channel . Here’s a sneak peek at the result: In this post, I’d like to share my research and process for getting this all to work. Patched the News Channel’s hardcoded Nintendo URL to point to an S3 storage bucket using Go and wadlib to extract the necessary binary file and edit it in-memory Modified WiiLink’s open-source news file generator to add “El Nuevo Día” as a news source Set up AWS Lambda + EventBridge to regenerate the necessary news binary files hourly Source code: WiiNewsPR and WiiNewsPR-Patcher The News Channel debuted in North America on January 26, 2007, a little over two months after the Wii’s launch. Since that date, it mostly came pre-installed with Wii consoles and was a novel way to read news from all over the world. Together with other “utility” channels like the Forecast Channel, it tried to position the Wii as more than just a gaming console. Check out a video recording of the service from right before it was discontinued on June 27th, 2013: Before we can consider displaying custom news on it, we have to figure out how the News Channel actually fetches content. We know that it must have fetched news somehow since it displays a “Downloading…” splash screen on startup. Luckily for us, the Wii natively supports proxying via its internet connection configuration settings! Meaning we can set up something like mitmproxy on a local machine and observe its HTTP behavior. We can start mitmproxy ’s web interface for a more screenshot-friendly UI: If we run a man-in-the-middle proxy for the News Channel on an unmo", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["site", "write", "markdown", "generate", "hugo", "host", "github", "page", "register", "route", "53", "theme", "modify", "hugo", "lanyon", "2025", "right", "reserve", "play", "menu", "news", "channel", "nintendo", "music", "app", "keep", "passion", "display", "local", "news", "article", "unexpected", "place", "figure", "fun", "project", "try", "display", "current", "local", "news", "nintendo", "wii", "console", "news", "channel", "here", "sneak", "peek", "result", "post", "like", "share", "research", "process", "get", "work", "patch", "news", "channel", "hardcode", "nintendo", "url", "point", "s3", "storage", "bucket", "wadlib", "extract", "necessary", "binary", "file", "edit", "memory", "modify", "wiilink", "open", "source", "news", "file", "generator", "add", "el", "nuevo", "día", "news", "source", "set", "aw", "lambda", "eventbridge", "regenerate", "necessary", "news", "binary", "file", "hourly", "source", "code", "wiinewspr", "wiinewspr", "patcher", "news", "channel", "debut", "north", "america", "january", "26", "2007", "little", "month", "wiis", "launch", "date", "come", "pre", "instal", "wii", "console", "novel", "way", "read", "news", "world", "utility", "channel", "like", "forecast", "channel", "try", "position", "wii", "gaming", "console", "check", "video", "recording", "service", "right", "discontinue", "june", "27th", "2013", "consider", "display", "custom", "news", "figure", "news", "channel", "actually", "fetch", "content", "know", "fetch", "news", "display", "download", "splash", "screen", "startup", "luckily", "wii", "natively", "support", "proxye", "internet", "connection", "configuration", "setting", "mean", "set", "like", "mitmproxy", "local", "machine", "observe", "http", "behavior", "start", "mitmproxy", "web", "interface", "screenshot", "friendly", "ui", "run", "man", "middle", "proxy", "news", "channel", "unmo"], "num_tokens": 194, "token_loss_pct": 47.99, "normalized_content": "site written in markdown  generated by hugo  hosted on github pages and registered using route 53 . theme modified hugo-lanyon  2025. all rights reserved.  now playing menu news channel via nintendo music app in keeping with my passion  for displaying local news articles in unexpected places  i figured it would be a fun project to try and see what it would take to display current local news on the nintendo wii consoles news channel . heres a sneak peek at the result in this post id like to share my research and process for getting this all to work. patched the news channels hardcoded nintendo url to point to an s3 storage bucket using go and wadlib to extract the necessary binary file and edit it in-memory modified wiilinks open-source news file generator to add el nuevo día as a news source set up aws lambda  eventbridge to regenerate the necessary news binary files hourly source code wiinewspr and wiinewspr-patcher the news channel debuted in north america on january 26 2007 a little over two months after the wiis launch. since that date it mostly came pre-installed with wii consoles and was a novel way to read news from all over the world. together with other utility channels like the forecast channel it tried to position the wii as more than just a gaming console. check out a video recording of the service from right before it was discontinued on june 27th 2013 before we can consider displaying custom news on it we have to figure out how the news channel actually fetches content. we know that it must have fetched news somehow since it displays a downloading splash screen on startup. luckily for us the wii natively supports proxying via its internet connection configuration settings meaning we can set up something like mitmproxy on a local machine and observe its http behavior. we can start mitmproxy s web interface for a more screenshot-friendly ui if we run a man-in-the-middle proxy for the news channel on an unmo"}
{"title": "It's Sundowning in America", "url": "https://paulkrugman.substack.com/p/its-sundowning-in-america", "content": "A short post. Today doesn’t seem like a day for charts and number-crunching. I had never heard the term “ sundowning ” before it happened to my own father, yet it’s a fairly common syndrome. In his last few months my father remained lucid and rational — remained himself — during daylight hours. Once the sun went down he deteriorated, becoming confused, paranoid and aggressive. It’s terrible to watch sundowning in someone you love. But that’s a personal tragedy – not a national or global one. It’s an entirely different matter when the president of the United States is sundowning — a president surrounded by malign sycophants who tell him whatever he wants to hear and indulge his every whim, no matter how destructive. For good reasons, it’s normally bad practice to pronounce on someone’s mental health from afar. Some of us still remember when right-wing pundits liked to call anyone critical of George W. Bush mentally ill . But after reading the letter that Trump just sent to the prime minister of Norway (Jonas Gahr Støre has confirmed that it’s genuine) there should be no doubt that we have a president who is suffering a real detachment from reality: Dear Jonas: Considering your Country decided not to give me the Nobel Peace Prize for having stopped 8 Wars PLUS, I no longer feel an obligation to think purely of Peace, although it will always be predominant, but can now think about what is good and proper for the United States of America. Denmark cannot protect that land from Russia or China, and why do they have a ‘right of ownership’ anyway? There are no written documents, it’s only that a boat landed there hundreds of years ago, but we had boats landing there, also. I have done more for NATO than any other person since its founding, and now, NATO should do something for the United States. The World is not secure unless we have Complete and Total Control of Greenland. Thank you! President DJT This might not exactly be sundowning, since it’s not clear that Trump is lu", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["short", "post", "today", "not", "like", "day", "chart", "number", "crunching", "hear", "term", "sundowne", "happen", "father", "fairly", "common", "syndrome", "month", "father", "remain", "lucid", "rational", "remain", "daylight", "hour", "sun", "go", "deteriorate", "confused", "paranoid", "aggressive", "terrible", "watch", "sundowne", "love", "personal", "tragedy", "national", "global", "entirely", "different", "matter", "president", "united", "states", "sundowne", "president", "surround", "malign", "sycophant", "tell", "want", "hear", "indulge", "whim", "matter", "destructive", "good", "reason", "normally", "bad", "practice", "pronounce", "someones", "mental", "health", "afar", "remember", "right", "wing", "pundit", "like", "critical", "george", "w.", "bush", "mentally", "ill", "read", "letter", "trump", "send", "prime", "minister", "norway", "jonas", "gahr", "støre", "confirm", "genuine", "doubt", "president", "suffer", "real", "detachment", "reality", "dear", "jona", "consider", "country", "decide", "nobel", "peace", "prize", "having", "stop", "war", "plus", "long", "feel", "obligation", "think", "purely", "peace", "predominant", "think", "good", "proper", "united", "states", "america", "denmark", "protect", "land", "russia", "china", "right", "ownership", "write", "document", "boat", "land", "hundred", "year", "ago", "boat", "land", "nato", "person", "founding", "nato", "united", "states", "world", "secure", "complete", "total", "control", "greenland", "thank", "president", "djt", "exactly", "sundowne", "clear", "trump", "lu"], "num_tokens": 157, "token_loss_pct": 57.57, "normalized_content": "a short post. today doesnt seem like a day for charts and number-crunching. i had never heard the term  sundowning  before it happened to my own father yet its a fairly common syndrome. in his last few months my father remained lucid and rational  remained himself  during daylight hours. once the sun went down he deteriorated becoming confused paranoid and aggressive. its terrible to watch sundowning in someone you love. but thats a personal tragedy  not a national or global one. its an entirely different matter when the president of the united states is sundowning  a president surrounded by malign sycophants who tell him whatever he wants to hear and indulge his every whim no matter how destructive. for good reasons its normally bad practice to pronounce on someones mental health from afar. some of us still remember when right-wing pundits liked to call anyone critical of george w. bush mentally ill . but after reading the letter that trump just sent to the prime minister of norway jonas gahr støre has confirmed that its genuine there should be no doubt that we have a president who is suffering a real detachment from reality dear jonas considering your country decided not to give me the nobel peace prize for having stopped 8 wars plus i no longer feel an obligation to think purely of peace although it will always be predominant but can now think about what is good and proper for the united states of america. denmark cannot protect that land from russia or china and why do they have a right of ownership anyway there are no written documents its only that a boat landed there hundreds of years ago but we had boats landing there also. i have done more for nato than any other person since its founding and now nato should do something for the united states. the world is not secure unless we have complete and total control of greenland. thank you president djt this might not exactly be sundowning since its not clear that trump is lu"}
{"title": "Issue #718: pandas 3.0, deque, tprof, and More (Jan. 20, 2026)", "url": "https://pycoders.com/issues/718", "content": "<p> <span>#718 – JANUARY 20, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/718/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s Weekly Logo\" src=\"https://cdn.pycoders.com/37bdf31dc645f968ffb90196e5d38ff5\" /></a></p> <hr /> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15879/feed\" target=\"_blank\">What&rsquo;s New in pandas 3.0</a></h3> <p style=\"margin-bottom: 0;\"> Learn what&rsquo;s new in pandas 3.0: <code>pd.col</code> expressions for cleaner code, Copy-on-Write for predictable behavior, and PyArrow-backed strings for 5-10x faster operations.<br /> <span><a href=\"https://pycoders.com/link/15879/feed\" target=\"_blank\">CODECUT.AI</a> • Shared by <a href=\"https://pycoders.com/link/15875/feed\" target=\"_blank\">Khuyen Tran</a></span> </p> </div> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15863/feed\" target=\"_blank\">Python&rsquo;s <code>deque</code>: Implement Efficient Queues and Stacks</a></h3> <p style=\"margin-bottom: 0;\"> Use a Python <code>deque</code> to efficiently append and pop elements from both ends of a sequence, build queues and stacks, and set <code>maxlen</code> for history buffers.<br /> <span><a href=\"https://pycoders.com/link/15863/feed\" target=\"_blank\">REAL PYTHON</a></span> </p> </div> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15848/feed\" target=\"_blank\">B2B Authentication for any Situation - Fully Managed or BYO", "source": "RSS", "date": "2026-01-20T19:30:00+00:00", "author": null, "score": null, "tokens": ["hashtag", "january", "20", "2026", "view", "browser", "whatrsquo", "new", "panda", "3.0", "learn", "whatrsquo", "new", "panda", "3.0", "pd.col", "expression", "clean", "code", "copy", "write", "predictable", "behavior", "pyarrow", "back", "string", "10x", "fast", "operation", "codecut.ai", "share", "khuyen", "tran", "pythonrsquos", "deque", "implement", "efficient", "queue", "stack", "use", "python", "deque", "efficiently", "append", "pop", "element", "end", "sequence", "build", "queue", "stack", "set", "maxlen", "history", "buffer", "real", "python", "b2b", "authentication", "situation", "fully", "manage", "byo"], "num_tokens": 63, "token_loss_pct": 35.05, "normalized_content": "hashtag  january 20 2026 view in browser  whatrsquos new in pandas 3.0 learn whatrsquos new in pandas 3.0 pd.col expressions for cleaner code copy-on-write for predictable behavior and pyarrow-backed strings for 5-10x faster operations. codecut.ai  shared by khuyen tran pythonrsquos deque implement efficient queues and stacks use a python deque to efficiently append and pop elements from both ends of a sequence build queues and stacks and set maxlen for history buffers. real python b2b authentication for any situation - fully managed or byo"}
{"title": "Issue #717: Unit Testing Performance, Cursor, Recursive match, and More (Jan. 13, 2026)", "url": "https://pycoders.com/issues/717", "content": "<p> <span>#717 – JANUARY 13, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/717/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s Weekly Logo\" src=\"https://cdn.pycoders.com/37bdf31dc645f968ffb90196e5d38ff5\" /></a></p> <hr /> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15840/feed\" target=\"_blank\">Unit Testing Your Code&rsquo;s Performance</a></h3> <p style=\"margin-bottom: 0;\"> Testing your code is important, but not just for correctness also for performance. One approach is to check performance degradation as data sizes go up, also known as Big-O scaling.<br /> <span><a href=\"https://pycoders.com/link/15840/feed\" target=\"_blank\">ITAMA TURNER-TRAURING</a></span> </p> </div> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15832/feed\" target=\"_blank\">Tips for Using the AI Coding Editor Cursor</a></h3> <p style=\"margin-bottom: 0;\"> Learn Cursor fast: AI-powered coding with agents, project-aware chat, inline edits, and VS Code workflow &ndash; ship smarter, sooner.<br /> <span><a href=\"https://pycoders.com/link/15832/feed\" target=\"_blank\">REAL PYTHON</a></span> <span style=\"color: #AAAAAA; padding: 1px 4px; border: 1px solid #dddddd; margin-left: 4px; font-size: 0.8em; border-radius: 2px;\">course</span> </p> </div> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15811/feed\" target=\"_blank\">AI Code Review With Comments You&rsquo;ll Actually Imp", "source": "RSS", "date": "2026-01-13T19:30:00+00:00", "author": null, "score": null, "tokens": ["hashtag", "january", "13", "2026", "view", "browser", "unit", "test", "codersquo", "performance", "test", "code", "important", "correctness", "performance", "approach", "check", "performance", "degradation", "data", "size", "know", "big", "scaling", "itama", "turner", "traure", "tip", "ai", "cod", "editor", "cursor", "learn", "cursor", "fast", "ai", "power", "coding", "agent", "project", "aware", "chat", "inline", "edit", "vs", "code", "workflow", "ndash", "ship", "smarter", "soon", "real", "python", "course", "ai", "code", "review", "comment", "yoursquoll", "actually", "imp"], "num_tokens": 61, "token_loss_pct": 35.79, "normalized_content": "hashtag  january 13 2026 view in browser  unit testing your codersquos performance testing your code is important but not just for correctness also for performance. one approach is to check performance degradation as data sizes go up also known as big-o scaling. itama turner-trauring tips for using the ai coding editor cursor learn cursor fast ai-powered coding with agents project-aware chat inline edits and vs code workflow ndash ship smarter sooner. real python course ai code review with comments yoursquoll actually imp"}
{"title": "Issue #716: Performance Numbers, async Web Apps, uv Speed, and More (Jan. 6, 2026)", "url": "https://pycoders.com/issues/716", "content": "<p> <span>#716 – JANUARY 6, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/716/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s Weekly Logo\" src=\"https://cdn.pycoders.com/37bdf31dc645f968ffb90196e5d38ff5\" /></a></p> <hr /> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15805/feed\" target=\"_blank\">PyCoder&rsquo;s Weekly 2025 Top Articles &amp; Hidden Gems</a></h3> <p style=\"margin-bottom: 0;\"> PyCoder&rsquo;s Weekly included over 1,500 links to articles, blog posts, tutorials, and projects in 2025. Christopher Trudeau is back on the show this week to help wrap up everything by sharing some highlights and uncovering a few hidden gems from the pile.<br /> <span><a href=\"https://pycoders.com/link/15805/feed\" target=\"_blank\">REAL PYTHON</a></span> <span style=\"color: #AAAAAA; padding: 1px 4px; border: 1px solid #dddddd; margin-left: 4px; font-size: 0.8em; border-radius: 2px;\">podcast</span> </p> </div> <div> <h3 style=\"margin-bottom: 0;\"><a href=\"https://pycoders.com/link/15788/feed\" target=\"_blank\">Python Numbers Every Programmer Should Know</a></h3> <p style=\"margin-bottom: 0;\"> Ever wonder how much memory an empty list takes? How about how long it takes to add two integers in Python? This post contains loads of performance data for common Python operations.<br /> <span><a href=\"https://pycoders.com/link/15788/feed\" target=\"_blank\">MICHAEL KENNEDY</a></span> </p> </div> <div> <h3 style=\"", "source": "RSS", "date": "2026-01-06T19:30:00+00:00", "author": null, "score": null, "tokens": ["hashtag", "january", "2026", "view", "browser", "pycoderrsquo", "weekly", "2025", "article", "amp", "hide", "gem", "pycoderrsquo", "weekly", "include", "1500", "link", "article", "blog", "post", "tutorial", "project", "2025", "christopher", "trudeau", "week", "help", "wrap", "share", "highlight", "uncover", "hide", "gem", "pile", "real", "python", "podcast", "python", "number", "programmer", "know", "wonder", "memory", "list", "take", "long", "take", "add", "integer", "python", "post", "contain", "load", "performance", "datum", "common", "python", "operation", "michael", "kennedy", "h3", "style"], "num_tokens": 62, "token_loss_pct": 42.06, "normalized_content": "hashtag  january 6 2026 view in browser  pycoderrsquos weekly 2025 top articles amp hidden gems pycoderrsquos weekly included over 1500 links to articles blog posts tutorials and projects in 2025. christopher trudeau is back on the show this week to help wrap up everything by sharing some highlights and uncovering a few hidden gems from the pile. real python podcast python numbers every programmer should know ever wonder how much memory an empty list takes how about how long it takes to add two integers in python this post contains loads of performance data for common python operations. michael kennedy h3 style"}
{"title": "Nvidia: End-to-End Test-Time Training for Long Context aka Being Able To Update A Model's Weights In Real-Time As You Use It | \"TTT changes the paradigm from retrieving info to learning it on the fly...the TTT model treats the context window as a dataset &amp; trains itself on it in real-time.\" [R]", "url": "https://www.reddit.com/r/MachineLearning/comments/1qd696s/nvidia_endtoend_testtime_training_for_long/", "content": "Nvidia: End-to-End Test-Time Training for Long Context aka Being Able To Update A Model's Weights In Real-Time As You Use It | \"TTT changes the paradigm from retrieving info to learning it on the fly...the TTT model treats the context window as a dataset &amp; trains itself on it in real-time.\" [R]. ####TL;DR:\nThe paper describes a mechanism that essentially turns the context window into a training dataset for a \"fast weight\" update loop:\n\n * **Inner Loop:** The model runs a mini-gradient descent on the context during inference. It updates specific MLP layers to \"learn\" the current context.\n * **Outer Loop:** The model's initial weights are meta-learned during training to be \"highly updateable\" or optimized for this test-time adaptation\n\n**From the Paper:** \"Overall, our empirical observations strongly indicate that TTT-E2E should produce the same trend as full attention for scaling with training compute in large-budget production runs.\"\n\n\n\n\n---\n\n\n\n####Abstract:\n\n&gt;We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture a Transformer with sliding-window attention. \n&gt;\n&gt;**However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights.** In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. \n&gt;\n&gt;In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7x faster than full attention for 128K context. **Our code is publicly available.**\n\n\n---\n\n####Layman's Explanation:\n\nThink of this paper as solving the memory bottleneck by fundamentally changing how a model processes information. Imagine you are taking a massive open-book exam. \n\nA standard Transformer (like GPT-4) is the student who frantically re-reads every single page of the textbook before answering every single question. This strategy guarantees they find the specific details (perfect recall), but as the textbook gets thicker, they get exponentially slower until they simply cannot finish the test in time. \n\nOn the other hand, alternatives like RNNs or Mamba try to summarize the entire textbook onto a single index card. They can answer questions instantly because they don't have to look back at the book, but for long, complex subjects, they eventually run out of space on the card and start forgetting crucial information.\n\nThis new method, Test-Time Training (TTT), changes the paradigm from retrieving information to learning it on the fly. Instead of re-reading the book or summarizing it onto a card, the TTT model treats the context window as a dataset and actually trains itself on it in real-time. It performs a mini-gradient descent update on its own neural weights as it reads. **This is equivalent to a student who reads the textbook and physically rewires their brain to master the subject matter before the test.** \n\nBecause the information is now compressed into the model's actual intelligence (its weights) rather than a temporary cache, the model can answer questions instantly (matching the constant speed of the fast index-card models) but with the high accuracy and scaling capability of the slow, page-turning Transformers. \n\n**This effectively decouples intelligence from memory costs, allowing for massive context lengths without the usual slowdown.**\n\n---\n\n\n######Link to the Paper: https:/", "source": "Reddit", "date": "2026-01-15T02:43:26", "author": "44th--Hokage", "score": 249, "tokens": ["nvidia", "end", "end", "test", "time", "training", "long", "context", "aka", "able", "update", "model", "weight", "real", "time", "use", "ttt", "change", "paradigm", "retrieve", "info", "learn", "fly", "ttt", "model", "treat", "context", "window", "dataset", "amp", "train", "real", "time", "r.", "hashtagdr", "paper", "describe", "mechanism", "essentially", "turn", "context", "window", "training", "dataset", "fast", "weight", "update", "loop", "inner", "loop", "model", "run", "mini", "gradient", "descent", "context", "inference", "update", "specific", "mlp", "layer", "learn", "current", "context", "outer", "loop", "model", "initial", "weight", "meta", "learn", "training", "highly", "updateable", "optimize", "test", "time", "adaptation", "paper", "overall", "empirical", "observation", "strongly", "indicate", "ttt", "e2e", "produce", "trend", "attention", "scale", "training", "compute", "large", "budget", "production", "run", "hashtag", "gtwe", "formulate", "long", "context", "language", "modeling", "problem", "continual", "learning", "architecture", "design", "formulation", "use", "standard", "architecture", "transformer", "slide", "window", "attention", "gt", "gthowever", "model", "continue", "learn", "test", "time", "token", "prediction", "give", "context", "compress", "context", "read", "weight", "addition", "improve", "model", "initialization", "learn", "test", "time", "meta", "learning", "training", "time", "overall", "method", "form", "test", "time", "training", "ttt", "end", "end", "e2e", "test", "time", "token", "prediction", "training", "time", "meta", "learning", "contrast", "previous", "form", "conduct", "extensive", "experiment", "focus", "scale", "property", "gt", "gtin", "particular", "3b", "model", "train", "164b", "token", "method", "ttt", "e2e", "scale", "context", "length", "way", "transformer", "attention", "mamba", "gate", "deltanet", "similar", "rnns", "ttt", "e2e", "constant", "inference", "latency", "regardless", "context", "length", "make", "2.7x", "fast", "attention", "128k", "context", "code", "publicly", "available", "hashtag", "explanation", "think", "paper", "solve", "memory", "bottleneck", "fundamentally", "change", "model", "process", "information", "imagine", "take", "massive", "open", "book", "exam", "standard", "transformer", "like", "gpt-4", "student", "frantically", "read", "single", "page", "textbook", "answer", "single", "question", "strategy", "guarantee", "find", "specific", "detail", "perfect", "recall", "textbook", "get", "thick", "exponentially", "slow", "simply", "finish", "test", "time", "hand", "alternative", "like", "rnns", "mamba", "try", "summarize", "entire", "textbook", "single", "index", "card", "answer", "question", "instantly", "look", "book", "long", "complex", "subject", "eventually", "run", "space", "card", "start", "forget", "crucial", "information", "new", "method", "test", "time", "training", "ttt", "change", "paradigm", "retrieve", "information", "learn", "fly", "instead", "read", "book", "summarize", "card", "ttt", "model", "treat", "context", "window", "dataset", "actually", "train", "real", "time", "perform", "mini", "gradient", "descent", "update", "neural", "weight", "read", "equivalent", "student", "read", "textbook", "physically", "rewire", "brain", "master", "subject", "matter", "test", "information", "compress", "model", "actual", "intelligence", "weight", "temporary", "cache", "model", "answer", "question", "instantly", "match", "constant", "speed", "fast", "index", "card", "model", "high", "accuracy", "scale", "capability", "slow", "page", "turn", "transformer", "effectively", "decouple", "intelligence", "memory", "cost", "allow", "massive", "context", "length", "usual", "slowdown", "hashtag", "paper", "https"], "num_tokens": 370, "token_loss_pct": 47.14, "normalized_content": "nvidia end-to-end test-time training for long context aka being able to update a model's weights in real-time as you use it  ttt changes the paradigm from retrieving info to learning it on the fly...the ttt model treats the context window as a dataset amp trains itself on it in real-time. r. hashtagdr the paper describes a mechanism that essentially turns the context window into a training dataset for a fast weight update loop  inner loop the model runs a mini-gradient descent on the context during inference. it updates specific mlp layers to learn the current context.  outer loop the model's initial weights are meta-learned during training to be highly updateable or optimized for this test-time adaptation from the paper overall our empirical observations strongly indicate that ttt-e2e should produce the same trend as full attention for scaling with training compute in large-budget production runs. --- hashtag gtwe formulate long-context language modeling as a problem in continual learning rather than architecture design. under this formulation we only use a standard architecture a transformer with sliding-window attention. gt gthowever our model continues learning at test time via next-token prediction on the given context compressing the context it reads into its weights. in addition we improve the model's initialization for learning at test time via meta-learning at training time. overall our method a form of test-time training ttt is end-to-end e2e both at test time via next-token prediction and training time via meta-learning in contrast to previous forms. we conduct extensive experiments with a focus on scaling properties. gt gtin particular for 3b models trained with 164b tokens our method ttt-e2e scales with context length in the same way as transformer with full attention while others such as mamba 2 and gated deltanet do not. however similar to rnns ttt-e2e has constant inference latency regardless of context length making it 2.7x faster than full attention for 128k context. our code is publicly available. --- hashtag's explanation think of this paper as solving the memory bottleneck by fundamentally changing how a model processes information. imagine you are taking a massive open-book exam. a standard transformer like gpt-4 is the student who frantically re-reads every single page of the textbook before answering every single question. this strategy guarantees they find the specific details perfect recall but as the textbook gets thicker they get exponentially slower until they simply cannot finish the test in time. on the other hand alternatives like rnns or mamba try to summarize the entire textbook onto a single index card. they can answer questions instantly because they don't have to look back at the book but for long complex subjects they eventually run out of space on the card and start forgetting crucial information. this new method test-time training ttt changes the paradigm from retrieving information to learning it on the fly. instead of re-reading the book or summarizing it onto a card the ttt model treats the context window as a dataset and actually trains itself on it in real-time. it performs a mini-gradient descent update on its own neural weights as it reads. this is equivalent to a student who reads the textbook and physically rewires their brain to master the subject matter before the test. because the information is now compressed into the model's actual intelligence its weights rather than a temporary cache the model can answer questions instantly matching the constant speed of the fast index-card models but with the high accuracy and scaling capability of the slow page-turning transformers. this effectively decouples intelligence from memory costs allowing for massive context lengths without the usual slowdown. --- hashtag to the paper https"}
{"title": "[P] I Gave Claude Code 9.5 Years of Health Data to Help Manage My Thyroid Disease", "url": "https://www.reddit.com/r/MachineLearning/comments/1qi8twv/p_i_gave_claude_code_95_years_of_health_data_to/", "content": "[P] I Gave Claude Code 9.5 Years of Health Data to Help Manage My Thyroid Disease. I have episodic Graves' disease, which has been difficult b/c its not chronic. Meds are up and down and often lag when the actual onset occurs\n\nI fed Claude 9.5 years of my Apple Watch and Whoop data, and tasked it to build an ML model (ended up with XGBoost after I tasked it to run every ML model, ran for over 1 hr) to detect these phases. It hit \\~98% validation accuracy and now acts as a personal risk assessor, alerting me 3-4 weeks before symptoms even appear. Backtested it on my last episode, and it would've given me a heads-up in early August before labs confirmed it at the end of the month. I was pretty blown away by this, it even made some very novel approach shift decisions. \n\nTurned it into a simple iOS app I can check whenever. I wrote this article given alot of interest I saw in emulating this along with the repo w/ claude code setup open sourced. Hope this helps\n\n[https://medium.com/data-science-collective/i-gave-claude-code-9-5-years-of-health-data-to-help-manage-my-thyroid-disease-85fcd8c0449f](https://medium.com/data-science-collective/i-gave-claude-code-9-5-years-of-health-data-to-help-manage-my-thyroid-disease-85fcd8c0449f)", "source": "Reddit", "date": "2026-01-20T19:17:38", "author": "ThatAi_guy", "score": 179, "tokens": ["give", "claude", "code", "9.5", "year", "health", "datum", "help", "manage", "thyroid", "disease", "episodic", "graf", "disease", "difficult", "bc", "chronic", "med", "lag", "actual", "onset", "occur", "feed", "claude", "9.5", "year", "apple", "watch", "whoop", "datum", "task", "build", "ml", "model", "end", "xgboost", "task", "run", "ml", "model", "run", "hr", "detect", "phase", "hit", "98", "validation", "accuracy", "act", "personal", "risk", "assessor", "alert", "week", "symptom", "appear", "backteste", "episode", "give", "head", "early", "august", "labs", "confirm", "end", "month", "pretty", "blow", "away", "novel", "approach", "shift", "decision", "turn", "simple", "ios", "app", "check", "write", "article", "give", "alot", "interest", "see", "emulate", "repo", "claude", "code", "setup", "open", "source", "hope", "help", "url"], "num_tokens": 94, "token_loss_pct": 52.76, "normalized_content": "p i gave claude code 9.5 years of health data to help manage my thyroid disease. i have episodic graves' disease which has been difficult bc its not chronic. meds are up and down and often lag when the actual onset occurs i fed claude 9.5 years of my apple watch and whoop data and tasked it to build an ml model ended up with xgboost after i tasked it to run every ml model ran for over 1 hr to detect these phases. it hit 98 validation accuracy and now acts as a personal risk assessor alerting me 3-4 weeks before symptoms even appear. backtested it on my last episode and it would've given me a heads-up in early august before labs confirmed it at the end of the month. i was pretty blown away by this it even made some very novel approach shift decisions. turned it into a simple ios app i can check whenever. i wrote this article given alot of interest i saw in emulating this along with the repo w claude code setup open sourced. hope this helps url"}
{"title": "[D] Why Mamba rewrote its core algorithm and Microsoft abandoned RetNet", "url": "https://www.reddit.com/r/MachineLearning/comments/1qehwlu/d_why_mamba_rewrote_its_core_algorithm_and/", "content": "[D] Why Mamba rewrote its core algorithm and Microsoft abandoned RetNet. Mamba-2 restructured its recurrence from parallel scans (10-20% Tensor Core utilization) to block-diagonal GEMMs (60-70%). The architecture bent to fit the silicon.\n\nRetNet was published by Microsoft Research in July 2023 with promising results at 6.7B. Five months later, the same organization shipped Phi-2, a dense Transformer. Then Phi-3. Then Phi-4. The co-authors didn't bet on their own architecture.\n\nI wrote an analysis of why this pattern keeps repeating. The short version: Transformers and NVIDIA GPUs co-evolved into a stable attractor. Breaking out requires clearing two reinforcing gates at once, hardware compatibility and institutional backing, and the gates make each other harder to pass. At frontier scale, no pure alternative has done it.\n\nEssay has Tensor Core utilization numbers, analysis of alternative chip vendors, and three falsifiable predictions for 2028.", "source": "Reddit", "date": "2026-01-16T15:47:45", "author": "petroslamb", "score": 119, "tokens": ["mamba", "rewrite", "core", "algorithm", "microsoft", "abandon", "retnet", "mamba-2", "restructure", "recurrence", "parallel", "scan", "10", "20", "tensor", "core", "utilization", "block", "diagonal", "gemms", "60", "70", "architecture", "bend", "fit", "silicon", "retnet", "publish", "microsoft", "research", "july", "2023", "promise", "result", "6.7b", "month", "later", "organization", "ship", "phi-2", "dense", "transformer", "phi-3", "phi-4", "co", "author", "bet", "architecture", "write", "analysis", "pattern", "keep", "repeat", "short", "version", "transformer", "nvidia", "gpus", "co", "evolve", "stable", "attractor", "break", "require", "clear", "reinforcing", "gate", "hardware", "compatibility", "institutional", "backing", "gate", "hard", "pass", "frontier", "scale", "pure", "alternative", "essay", "tensor", "core", "utilization", "number", "analysis", "alternative", "chip", "vendor", "falsifiable", "prediction", "2028"], "num_tokens": 90, "token_loss_pct": 45.45, "normalized_content": "d why mamba rewrote its core algorithm and microsoft abandoned retnet. mamba-2 restructured its recurrence from parallel scans 10-20 tensor core utilization to block-diagonal gemms 60-70. the architecture bent to fit the silicon. retnet was published by microsoft research in july 2023 with promising results at 6.7b. five months later the same organization shipped phi-2 a dense transformer. then phi-3. then phi-4. the co-authors didn't bet on their own architecture. i wrote an analysis of why this pattern keeps repeating. the short version transformers and nvidia gpus co-evolved into a stable attractor. breaking out requires clearing two reinforcing gates at once hardware compatibility and institutional backing and the gates make each other harder to pass. at frontier scale no pure alternative has done it. essay has tensor core utilization numbers analysis of alternative chip vendors and three falsifiable predictions for 2028."}
{"title": "[D] Burnout from the hiring process", "url": "https://www.reddit.com/r/MachineLearning/comments/1qepc05/d_burnout_from_the_hiring_process/", "content": "[D] Burnout from the hiring process. I've been interviewing for research (some engineering) interships for the last 2 months, and I think I'm at a point of mental exhaustion from constant rejections and wasted time.\n\nFor context, I just started my master’s at Waterloo, but I'm a research associate at one of the top labs in Europe. I have been doing research since my sophomore year. I did not start in ML, but over the last year and a half, I ended up in ML research, first in protein design and now in pretraining optimization.\n\nI started applying for interships a few months ago, and after 10+ first-round interviews and endless OAs, I haven't landed any offers. Most of the companies that I've interviewed with were a mix of (non-FAANG) frontier AI companies, established deep tech startups, research labs of F100 companies, a couple non name startups, and a quant firm. I get past a few rounds, then get cut.\n\nThe feedback in general is that I'm not a good \"fit\" (a few companies told me I'm too researchy for a research engineer, another few were researching some niche stuff). And the next most common reason is that I failed the coding technical (I have no issue passing the research and ML theory technical interviews), but I think too slow for an engineer, and it's never the same type of questions (with one frontier company, I passed the research but failed the code review) and I'm not even counting OAs. Not a single one asked Leetcode or ML modelling; it's always some sort of a custom task that I have no prior experience with, so it's never the same stuff I can prepare.\n\nI'm at a loss, to be honest. Every PhD and a bunch of master's students in our lab have interned at frontier companies, and I feel like a failure that, after so many interviews, I can't get an offer. Because of my CV (no lies), I don't have a problem getting interviews, but I can't seem to get an offer. I've tried applying for non-research and less competitive companies, but I get hit with \"not a good fit.\"\n\nI have 3 technicals next week, and tbh I know for a fact I'm not gonna pass 2 of them (too stupid to be a quant researcher) and the other is a 3rd round technical, but from the way he described it I don't think I'll be passing it (they're gonna throw a scientific simulation coding problem at me). And I still need to schedule one more between those 3, but I'm not sure why they even picked me, I don't do RL or robotics research. After so many days and hours spent preparing for each technical only to get cut, I mentally can't get myself to prepare for them anymore. It's always a new random format.\n\nI'm severely burned out by this whole process, but time is running out. I love research, but I'm starting to hate the hiring process in this industry. Any advice on what to do?", "source": "Reddit", "date": "2026-01-16T20:16:28", "author": "RNRuben", "score": 115, "tokens": ["burnout", "hiring", "process", "interview", "research", "engineering", "intership", "month", "think", "point", "mental", "exhaustion", "constant", "rejection", "waste", "time", "context", "start", "master", "waterloo", "research", "associate", "lab", "europe", "research", "sophomore", "year", "start", "ml", "year", "half", "end", "ml", "research", "protein", "design", "pretraine", "optimization", "start", "apply", "intership", "month", "ago", "10", "round", "interview", "endless", "oas", "land", "offer", "company", "interview", "mix", "non", "faang", "frontier", "ai", "company", "establish", "deep", "tech", "startup", "research", "lab", "f100", "company", "couple", "non", "startup", "quant", "firm", "past", "round", "cut", "feedback", "general", "good", "fit", "company", "tell", "researchy", "research", "engineer", "research", "niche", "stuff", "common", "reason", "fail", "cod", "technical", "issue", "pass", "research", "ml", "theory", "technical", "interview", "think", "slow", "engineer", "type", "question", "frontier", "company", "pass", "research", "fail", "code", "review", "count", "oas", "single", "ask", "leetcode", "ml", "model", "sort", "custom", "task", "prior", "experience", "stuff", "prepare", "loss", "honest", "phd", "bunch", "master", "student", "lab", "intern", "frontier", "company", "feel", "like", "failure", "interview", "offer", "cv", "lie", "problem", "get", "interview", "offer", "try", "apply", "non", "research", "competitive", "company", "hit", "good", "fit", "technical", "week", "tbh", "know", "fact", "go", "to", "pass", "stupid", "quant", "researcher", "3rd", "round", "technical", "way", "describe", "think", "pass", "go", "to", "throw", "scientific", "simulation", "coding", "problem", "need", "schedule", "sure", "pick", "rl", "robotic", "research", "day", "hour", "spend", "prepare", "technical", "cut", "mentally", "prepare", "anymore", "new", "random", "format", "severely", "burn", "process", "time", "run", "love", "research", "start", "hate", "hiring", "process", "industry", "advice"], "num_tokens": 211, "token_loss_pct": 62.79, "normalized_content": "d burnout from the hiring process. i've been interviewing for research some engineering interships for the last 2 months and i think i'm at a point of mental exhaustion from constant rejections and wasted time. for context i just started my masters at waterloo but i'm a research associate at one of the top labs in europe. i have been doing research since my sophomore year. i did not start in ml but over the last year and a half i ended up in ml research first in protein design and now in pretraining optimization. i started applying for interships a few months ago and after 10 first-round interviews and endless oas i haven't landed any offers. most of the companies that i've interviewed with were a mix of non-faang frontier ai companies established deep tech startups research labs of f100 companies a couple non name startups and a quant firm. i get past a few rounds then get cut. the feedback in general is that i'm not a good fit a few companies told me i'm too researchy for a research engineer another few were researching some niche stuff. and the next most common reason is that i failed the coding technical i have no issue passing the research and ml theory technical interviews but i think too slow for an engineer and it's never the same type of questions with one frontier company i passed the research but failed the code review and i'm not even counting oas. not a single one asked leetcode or ml modelling it's always some sort of a custom task that i have no prior experience with so it's never the same stuff i can prepare. i'm at a loss to be honest. every phd and a bunch of master's students in our lab have interned at frontier companies and i feel like a failure that after so many interviews i can't get an offer. because of my cv no lies i don't have a problem getting interviews but i can't seem to get an offer. i've tried applying for non-research and less competitive companies but i get hit with not a good fit. i have 3 technicals next week and tbh i know for a fact i'm not gonna pass 2 of them too stupid to be a quant researcher and the other is a 3rd round technical but from the way he described it i don't think i'll be passing it they're gonna throw a scientific simulation coding problem at me. and i still need to schedule one more between those 3 but i'm not sure why they even picked me i don't do rl or robotics research. after so many days and hours spent preparing for each technical only to get cut i mentally can't get myself to prepare for them anymore. it's always a new random format. i'm severely burned out by this whole process but time is running out. i love research but i'm starting to hate the hiring process in this industry. any advice on what to do"}
{"title": "[R] Is Leetcode still relevant for research scientist interviews?", "url": "https://www.reddit.com/r/MachineLearning/comments/1qh9sg5/r_is_leetcode_still_relevant_for_research/", "content": "[R] Is Leetcode still relevant for research scientist interviews?. Hello everybody,\n\nI’m at my third (and last year) of my phd in computer vision, and I want to start preparing for technical interviews. What I want to do is work as a research scientist, preferably at companies like Meta. In terms of publications and research knowledge I think I have a quite decent profile with 4 papers at A\\* conferences. However I have heard that the coding interviews can be quite thought even for research scientist jobs. So I’m wondering if practicing with leetcode still relevant or is there other alternatives?\n\nThanks!\n\nEdit: Thanks to anyone who has taken the time to answer you guys rock", "source": "Reddit", "date": "2026-01-19T18:01:23", "author": "Training-Adeptness57", "score": 112, "tokens": ["leetcode", "relevant", "research", "scientist", "interview", "hello", "everybody", "year", "phd", "computer", "vision", "want", "start", "prepare", "technical", "interview", "want", "work", "research", "scientist", "preferably", "company", "like", "meta", "term", "publication", "research", "knowledge", "think", "decent", "profile", "paper", "conference", "hear", "cod", "interview", "think", "research", "scientist", "job", "wonder", "practice", "leetcode", "relevant", "alternative", "thank", "edit", "thank", "take", "time", "answer", "guy", "rock"], "num_tokens": 53, "token_loss_pct": 56.91, "normalized_content": "r is leetcode still relevant for research scientist interviews. hello everybody im at my third and last year of my phd in computer vision and i want to start preparing for technical interviews. what i want to do is work as a research scientist preferably at companies like meta. in terms of publications and research knowledge i think i have a quite decent profile with 4 papers at a conferences. however i have heard that the coding interviews can be quite thought even for research scientist jobs. so im wondering if practicing with leetcode still relevant or is there other alternatives thanks edit thanks to anyone who has taken the time to answer you guys rock"}
{"title": "[P] my shot at a DeepSeek style moe on a single rtx 5090", "url": "https://www.reddit.com/r/MachineLearning/comments/1qcxhgw/p_my_shot_at_a_deepseek_style_moe_on_a_single_rtx/", "content": "[P] my shot at a DeepSeek style moe on a single rtx 5090. I know most will wonder why I’m wasting my time training at only 19k tok a sec. It’s because I can. I’m doing this in my living room in my spare time. 0 formal ML experience. The absurd amount I’ve learned in the last few months made me realize I really picked the wrong career.\n\nMy Mixture of Experts is 2.36B parameter with 8 routed experts plus a shared expert using top-2 routing. Attention is Grouped Query Attention with QK-normalization and RoPE positional embeddings. All feed-forward layers use SwiGLU activation with RMSNorm throughout. Load balancing follows DeepSeek V3’s auxiliary-loss-free approach using bias-based routing. I monitor coefficient of variation and maximum violation per step.\n\nTraining runs on TorchAO FP8 quantization with the Muon optimizer and a multi-stage learning rate schedule (warmup, constant, cosine decay). The backend is optimized for Blackwell architecture with cuBLASLt.\n\nThe data pipeline implements MeCo (Metadata Conditioning then Cooldown) with ledger-based deterministic sampling. I have document-aware attention masking and cross-document loss masking but was disabled for the initial MeCo run. I have since disabled MeCo and curated a clean corpus with no tagging of any kind. MeCo worked but it worked too well and with only 8 experts, it became very problematic.\n\nMy two biggest early mistakes were not using symmetric router initialization (std=0.006) and not having a dense first layer. Cost me a lot of time and sleep. So what did I do? I cheated. I used aux loss of .003 snd ema smoothing at the beginning. I just didn’t know better. I paid a price later on for that.\n\nDO NOT use router scaling on a small MoE. DeepSeek used 2.5. Kimi K2 used 2.446. I tried 1.2 and it was horribly unstable and violation blew up to over .500.\n\n24 batch 6 Grad LR 3e-4 AdamW+Muon Scaled. Bias .001 Aux .0001. I update every step.\n\nAs of yesterday: 2026-01-13 20:53:06 step 41915 | lr 3.00e-04 | loss 1.8867 | gnorm 0.13 | 19,415 tok/s (ema 19,553) | 75.9s/5 steps | cv 0.022 | bias -0.001708±0.179996 | rel_max=0.036 maxvio=0.027 ent=1.203 applied=True | seq_aux 2.444 2026-01-13 20:54:20     [moe] token counts: [150018, 148422, 155402, 147966, 145236, 146724, 144358, 141522] 2026-01-13 20:54:20 step 41920 | lr 3.00e-04 | loss 1.9263 | gnorm 0.13 | 20,102 tok/s (ema 19,828) | 73.4s/5 steps | cv 0.026 | bias -0.001708±0.179920 | rel_max=0.054 maxvio=0.054 ent=1.211 applied=True | seq_aux 2.515\n\nI got a long ways to go :)\n\nI’ll gladly answer any question. No gate keeping here.", "source": "Reddit", "date": "2026-01-14T20:53:25", "author": "exhorder72", "score": 82, "tokens": ["shot", "deepseek", "style", "moe", "single", "rtx", "5090", "know", "wonder", "waste", "time", "training", "19k", "tok", "sec", "living", "room", "spare", "time", "formal", "ml", "experience", "absurd", "ve", "learn", "month", "realize", "pick", "wrong", "career", "mixture", "expert", "2.36b", "parameter", "rout", "expert", "plus", "share", "expert", "top-2", "routing", "attention", "group", "query", "attention", "qk", "normalization", "rope", "positional", "embedding", "feed", "forward", "layer", "use", "swiglu", "activation", "rmsnorm", "load", "balancing", "follow", "deepseek", "v3s", "auxiliary", "loss", "free", "approach", "bias", "base", "routing", "monitor", "coefficient", "variation", "maximum", "violation", "step", "training", "run", "torchao", "fp8", "quantization", "muon", "optimizer", "multi", "stage", "learn", "rate", "schedule", "warmup", "constant", "cosine", "decay", "backend", "optimize", "blackwell", "architecture", "cublaslt", "data", "pipeline", "implement", "meco", "metadata", "conditioning", "cooldown", "ledger", "base", "deterministic", "sampling", "document", "aware", "attention", "masking", "cross", "document", "loss", "masking", "disabled", "initial", "meco", "run", "disabled", "meco", "curate", "clean", "corpus", "tagging", "kind", "meco", "work", "work", "expert", "problematic", "big", "early", "mistake", "symmetric", "router", "initialization", "std0.006", "have", "dense", "layer", "cost", "lot", "time", "sleep", "cheat", "aux", "loss", ".003", "snd", "ema", "smoothing", "beginning", "not", "know", "well", "pay", "price", "later", "use", "router", "scaling", "small", "moe", "deepseek", "2.5", "kimi", "k2", "2.446", "try", "1.2", "horribly", "unstable", "violation", "blow", ".500", "24", "batch", "grad", "lr", "3e-4", "adamwmuon", "scale", "bias", ".001", "aux", ".0001", "update", "step", "yesterday", "2026", "01", "13", "205306", "step", "41915", "lr", "3.00e-04", "loss", "1.8867", "gnorm", "0.13", "19415", "tok", "ema", "19553", "75.9s5", "step", "cv", "0.022", "bias", "-0.0017080.179996", "rel_max0.036", "maxvio0.027", "ent1.203", "appliedtrue", "seq_aux", "2.444", "2026", "01", "13", "205420", "moe", "token", "count", "150018", "148422", "155402", "147966", "145236", "146724", "144358", "141522", "2026", "01", "13", "205420", "step", "41920", "lr", "3.00e-04", "loss", "1.9263", "gnorm", "0.13", "20102", "tok", "ema", "19828", "73.4s5", "step", "cv", "0.026", "bias", "-0.0017080.179920", "rel_max0.054", "maxvio0.054", "ent1.211", "appliedtrue", "seq_aux", "2.515", "get", "long", "way", "ill", "gladly", "answer", "question", "gate", "keep"], "num_tokens": 270, "token_loss_pct": 45.12, "normalized_content": "p my shot at a deepseek style moe on a single rtx 5090. i know most will wonder why im wasting my time training at only 19k tok a sec. its because i can. im doing this in my living room in my spare time. 0 formal ml experience. the absurd amount ive learned in the last few months made me realize i really picked the wrong career. my mixture of experts is 2.36b parameter with 8 routed experts plus a shared expert using top-2 routing. attention is grouped query attention with qk-normalization and rope positional embeddings. all feed-forward layers use swiglu activation with rmsnorm throughout. load balancing follows deepseek v3s auxiliary-loss-free approach using bias-based routing. i monitor coefficient of variation and maximum violation per step. training runs on torchao fp8 quantization with the muon optimizer and a multi-stage learning rate schedule warmup constant cosine decay. the backend is optimized for blackwell architecture with cublaslt. the data pipeline implements meco metadata conditioning then cooldown with ledger-based deterministic sampling. i have document-aware attention masking and cross-document loss masking but was disabled for the initial meco run. i have since disabled meco and curated a clean corpus with no tagging of any kind. meco worked but it worked too well and with only 8 experts it became very problematic. my two biggest early mistakes were not using symmetric router initialization std0.006 and not having a dense first layer. cost me a lot of time and sleep. so what did i do i cheated. i used aux loss of .003 snd ema smoothing at the beginning. i just didnt know better. i paid a price later on for that. do not use router scaling on a small moe. deepseek used 2.5. kimi k2 used 2.446. i tried 1.2 and it was horribly unstable and violation blew up to over .500. 24 batch 6 grad lr 3e-4 adamwmuon scaled. bias .001 aux .0001. i update every step. as of yesterday 2026-01-13 205306 step 41915  lr 3.00e-04  loss 1.8867  gnorm 0.13  19415 toks ema 19553  75.9s5 steps  cv 0.022  bias -0.0017080.179996  rel_max0.036 maxvio0.027 ent1.203 appliedtrue  seq_aux 2.444 2026-01-13 205420 moe token counts 150018 148422 155402 147966 145236 146724 144358 141522 2026-01-13 205420 step 41920  lr 3.00e-04  loss 1.9263  gnorm 0.13  20102 toks ema 19828  73.4s5 steps  cv 0.026  bias -0.0017080.179920  rel_max0.054 maxvio0.054 ent1.211 appliedtrue  seq_aux 2.515 i got a long ways to go  ill gladly answer any question. no gate keeping here."}
{"title": "[D] Regret leaving a good remot ML/CV role for mental health and now struggling to get callbacks", "url": "https://www.reddit.com/r/MachineLearning/comments/1qi2jp8/d_regret_leaving_a_good_remot_mlcv_role_for/", "content": "[D] Regret leaving a good remot ML/CV role for mental health and now struggling to get callbacks. I am a Computer Vision and ML engineer with over five years of experience and a research based Masters degree. A few months ago I left a well paying remote role because the work environment and micromanagement were seriously affecting my mental health. At the time I believed stepping away was the right decision for my sanity.\n\nIt has now been around three months and I am barely getting any recruiter screens let alone technical interviews. The lack of callbacks has been extremely demotivating and has made me start regretting leaving a stable job even though I still believe I needed the mental peace.\n\nI am applying to Computer Vision ML and Perception Engineer roles and I am based in Canada but open to North America remote roles. I am tailoring my resume and applying consistently but something is clearly not working. I am trying to understand whether this is just how bad the market is right now or if I am missing something obvious.\n\nIf you have been through this recently I would really appreciate honest advice on what helped you start getting first interviews and what hiring managers are actually looking for right now in ML/CV positions\n\nI am just trying to get unstuck and move forward.", "source": "Reddit", "date": "2026-01-20T15:29:02", "author": "PinPitiful", "score": 76, "tokens": ["regret", "leave", "good", "remot", "mlcv", "role", "mental", "health", "struggle", "callback", "computer", "vision", "ml", "engineer", "year", "experience", "research", "base", "master", "degree", "month", "ago", "leave", "pay", "remote", "role", "work", "environment", "micromanagement", "seriously", "affect", "mental", "health", "time", "believe", "step", "away", "right", "decision", "sanity", "month", "barely", "get", "recruiter", "screen", "let", "technical", "interview", "lack", "callback", "extremely", "demotivate", "start", "regret", "leave", "stable", "job", "believe", "need", "mental", "peace", "apply", "computer", "vision", "ml", "perception", "engineer", "role", "base", "canada", "open", "north", "america", "remote", "role", "tailor", "resume", "apply", "consistently", "clearly", "work", "try", "understand", "bad", "market", "right", "miss", "obvious", "recently", "appreciate", "honest", "advice", "help", "start", "get", "interview", "hire", "manager", "actually", "look", "right", "mlcv", "position", "try", "unstuck", "forward"], "num_tokens": 106, "token_loss_pct": 55.08, "normalized_content": "d regret leaving a good remot mlcv role for mental health and now struggling to get callbacks. i am a computer vision and ml engineer with over five years of experience and a research based masters degree. a few months ago i left a well paying remote role because the work environment and micromanagement were seriously affecting my mental health. at the time i believed stepping away was the right decision for my sanity. it has now been around three months and i am barely getting any recruiter screens let alone technical interviews. the lack of callbacks has been extremely demotivating and has made me start regretting leaving a stable job even though i still believe i needed the mental peace. i am applying to computer vision ml and perception engineer roles and i am based in canada but open to north america remote roles. i am tailoring my resume and applying consistently but something is clearly not working. i am trying to understand whether this is just how bad the market is right now or if i am missing something obvious. if you have been through this recently i would really appreciate honest advice on what helped you start getting first interviews and what hiring managers are actually looking for right now in mlcv positions i am just trying to get unstuck and move forward."}
{"title": "[R] China just released first SOTA multimodal model trained entirely on domestic chips", "url": "https://www.reddit.com/r/MachineLearning/comments/1qeakhz/r_china_just_released_first_sota_multimodal_model/", "content": "[R] China just released first SOTA multimodal model trained entirely on domestic chips. Zhipu AI and Huawei just dropped GLM-Image, and the technical details are interesting.\n\nFirst multimodal model trained completely on Chinese chips (Huawei Ascend 910) from data preprocessing to full scale training. They're using a hybrid architecture combining autoregressive + diffusion decoder.\n\nWhat stands out is the Chinese text rendering. It consistently ranks first among open source models for complex text generation, especially handling Chinese characters which most models struggle with.\n\nNative support for 1024 to 2048 resolution at any aspect ratio without additional training. API pricing is 0.1 yuan per image (roughly $0.014).\n\nThe model handles both text to image and image to image generation in a single model. GitHub and Hugging Face repos are already up.\n\nThis is significant because it proves you can train frontier models without relying on Nvidia hardware. The compute efficiency numbers they're claiming are 60% better than H200 for tokens per joule.\n\nWhether those benchmarks hold up in practice remains to be seen but the fact they pulled this off on domestic hardware is noteworthy.\n\nEdit: For anyone testing this, X-Design also handles multilingual text rendering well. Been comparing outputs and both handle complex layouts better than DALL-E 3.", "source": "Reddit", "date": "2026-01-16T09:27:32", "author": "Different_Case_6484", "score": 67, "tokens": ["china", "release", "sota", "multimodal", "model", "train", "entirely", "domestic", "chip", "zhipu", "ai", "huawei", "drop", "glm", "image", "technical", "detail", "interesting", "multimodal", "model", "train", "completely", "chinese", "chip", "huawei", "ascend", "910", "datum", "preprocesse", "scale", "training", "hybrid", "architecture", "combine", "autoregressive", "diffusion", "decoder", "stand", "chinese", "text", "render", "consistently", "rank", "open", "source", "model", "complex", "text", "generation", "especially", "handle", "chinese", "character", "model", "struggle", "native", "support", "1024", "2048", "resolution", "aspect", "ratio", "additional", "training", "api", "pricing", "0.1", "yuan", "image", "roughly", "0.014", "model", "handle", "text", "image", "image", "image", "generation", "single", "model", "github", "hug", "face", "repos", "significant", "prove", "train", "frontier", "model", "rely", "nvidia", "hardware", "compute", "efficiency", "number", "claim", "60", "well", "h200", "token", "joule", "benchmark", "hold", "practice", "remain", "see", "fact", "pull", "domestic", "hardware", "noteworthy", "edit", "test", "design", "handle", "multilingual", "text", "render", "compare", "output", "handle", "complex", "layout", "well", "dall"], "num_tokens": 125, "token_loss_pct": 45.89, "normalized_content": "r china just released first sota multimodal model trained entirely on domestic chips. zhipu ai and huawei just dropped glm-image and the technical details are interesting. first multimodal model trained completely on chinese chips huawei ascend 910 from data preprocessing to full scale training. they're using a hybrid architecture combining autoregressive  diffusion decoder. what stands out is the chinese text rendering. it consistently ranks first among open source models for complex text generation especially handling chinese characters which most models struggle with. native support for 1024 to 2048 resolution at any aspect ratio without additional training. api pricing is 0.1 yuan per image roughly 0.014. the model handles both text to image and image to image generation in a single model. github and hugging face repos are already up. this is significant because it proves you can train frontier models without relying on nvidia hardware. the compute efficiency numbers they're claiming are 60 better than h200 for tokens per joule. whether those benchmarks hold up in practice remains to be seen but the fact they pulled this off on domestic hardware is noteworthy. edit for anyone testing this x-design also handles multilingual text rendering well. been comparing outputs and both handle complex layouts better than dall-e 3."}
{"title": "[Project] Kuat: A Rust-based, Zero-Copy Dataloader for PyTorch (4.6x training speedup on T4/H100)", "url": "https://www.reddit.com/r/MachineLearning/comments/1qig3ae/project_kuat_a_rustbased_zerocopy_dataloader_for/", "content": "[Project] Kuat: A Rust-based, Zero-Copy Dataloader for PyTorch (4.6x training speedup on T4/H100). Hi everyone,\n\nWe built a drop-in replacement for `torch.utils.data.DataLoader` entirely in Rust.\n\n**The Problem:** Python's `multiprocessing` isolates workers, meaning every batch incurs IPC and pickling overhead. Even on a T4, the CPU often bottlenecks while the GPU sits idle waiting for data.\n\n**The Solution:** We bypass Python's data plane entirely.\n\n* **Rust Backend:** Uses native threads (no GIL, no heavy process forking).\n* **Zero-Copy:** We use a memory-mapped custom format (`.kt`) that creates views into tensors without deserialization overhead.\n\n**Benchmarks (ResNet-18 / ImageWoof, Tesla T4, batch=64):**\n\n|Loader|Throughput|Speedup|\n|:-|:-|:-|\n|PyTorch ImageFolder|116 img/s|1.0x|\n|MosaicML Streaming|179 img/s|1.5x|\n|NVIDIA DALI|246 img/s|2.1x|\n|**Kuattree (Ours)**|**512 img/s**|**4.4x**|\n\n**Summary:** We are roughly **2.08x faster than DALI** and **4.4x faster than standard PyTorch**.\n\nThe trade-off is that you have to pre-convert your dataset to our `.kt` format. It’s similar conceptually to writing a TFRecord or WebDataset, but designed for random access, and we found the ingestion to be about `60x` faster than MosaicML sharding.\n\nWe aren't open source just yet, but we are running a private beta if anyone wants to verify these numbers on their own hardware.\n\n[www.kuatlabs.com](https://www.kuatlabs.com)\n\nHappy to answer any questions about the Rust implementation or the memory mapping approach!", "source": "Reddit", "date": "2026-01-20T23:41:42", "author": "YanSoki", "score": 56, "tokens": ["project", "kuat", "rust", "base", "zero", "copy", "dataloader", "pytorch", "4.6x", "training", "speedup", "t4h100", "hi", "build", "drop", "replacement", "torch.utils.data.dataloader", "entirely", "rust", "problem", "python", "multiprocessing", "isolate", "worker", "mean", "batch", "incur", "ipc", "pickle", "overhead", "t4", "cpu", "bottleneck", "gpu", "sit", "idle", "wait", "datum", "solution", "bypass", "python", "data", "plane", "entirely", "rust", "backend", "use", "native", "thread", "gil", "heavy", "process", "fork", "zero", "copy", "use", "memory", "map", "custom", "format", ".kt", "create", "view", "tensor", "deserialization", "overhead", "benchmark", "resnet-18", "imagewoof", "tesla", "t4", "batch64", "loaderthroughputspeedup", "pytorch", "imagefolder116", "imgs1.0x", "mosaicml", "streaming179", "imgs1.5x", "nvidia", "dali246", "imgs2.1x", "kuattree", "ours512", "imgs4.4x", "summary", "roughly", "2.08x", "fast", "dali", "4.4x", "fast", "standard", "pytorch", "trade", "pre", "convert", "dataset", ".kt", "format", "similar", "conceptually", "write", "tfrecord", "webdataset", "design", "random", "access", "find", "ingestion", "60x", "fast", "mosaicml", "sharde", "open", "source", "run", "private", "beta", "want", "verify", "number", "hardware", "www.kuatlabs.comurl", "happy", "answer", "question", "rust", "implementation", "memory", "mapping", "approach"], "num_tokens": 132, "token_loss_pct": 43.83, "normalized_content": "project kuat a rust-based zero-copy dataloader for pytorch 4.6x training speedup on t4h100. hi everyone we built a drop-in replacement for torch.utils.data.dataloader entirely in rust. the problem python's multiprocessing isolates workers meaning every batch incurs ipc and pickling overhead. even on a t4 the cpu often bottlenecks while the gpu sits idle waiting for data. the solution we bypass python's data plane entirely.  rust backend uses native threads no gil no heavy process forking.  zero-copy we use a memory-mapped custom format .kt that creates views into tensors without deserialization overhead. benchmarks resnet-18  imagewoof tesla t4 batch64 loaderthroughputspeedup --- pytorch imagefolder116 imgs1.0x mosaicml streaming179 imgs1.5x nvidia dali246 imgs2.1x kuattree ours512 imgs4.4x summary we are roughly 2.08x faster than dali and 4.4x faster than standard pytorch. the trade-off is that you have to pre-convert your dataset to our .kt format. its similar conceptually to writing a tfrecord or webdataset but designed for random access and we found the ingestion to be about 60x faster than mosaicml sharding. we aren't open source just yet but we are running a private beta if anyone wants to verify these numbers on their own hardware. www.kuatlabs.comurl happy to answer any questions about the rust implementation or the memory mapping approach"}
{"title": "[D] ICML26 new review policies", "url": "https://www.reddit.com/r/MachineLearning/comments/1qg5pa9/d_icml26_new_review_policies/", "content": "[D] ICML26 new review policies. ICML26 introduced a review type selection, where the author can decide whether LLMs can be used during their paper review, according to these two policies:\n\n* **Policy A (Conservative):** Use of LLMs for reviewing is strictly prohibited.  \n* **Policy B (Permissive):** \n   * ***Allowed***: Use of LLMs to help understand the paper and related works, and polish reviews. Submissions can be fed to privacy-compliant\\* LLMs. \n   * ***Not allowed***: Ask LLMs about strengths/weaknesses, ask to suggest key points for the review, suggest an outline for the review, or write the full review *\\*By “privacy-compliant”, we refer to LLM tools that do not use logged data for training and that place limits on data retention. This includes enterprise/institutional subscriptions to LLM APIs, consumer subscriptions with an explicit opt-out from training, and self-hosted LLMs. (We understand that this is an oversimplification.)*\n\nI'm struggling to decide which one to select, any suggestions?", "source": "Reddit", "date": "2026-01-18T11:54:51", "author": "reutococco", "score": 52, "tokens": ["icml26", "new", "review", "policy", "icml26", "introduce", "review", "type", "selection", "author", "decide", "llm", "paper", "review", "accord", "policy", "policy", "conservative", "use", "llm", "review", "strictly", "prohibit", "policy", "permissive", "allow", "use", "llm", "help", "understand", "paper", "related", "work", "polish", "review", "submission", "feed", "privacy", "compliant", "llm", "allow", "ask", "llm", "strengthsweaknesse", "ask", "suggest", "key", "point", "review", "suggest", "outline", "review", "write", "review", "privacy", "compliant", "refer", "llm", "tool", "use", "log", "datum", "training", "place", "limit", "datum", "retention", "include", "enterpriseinstitutional", "subscription", "llm", "apis", "consumer", "subscription", "explicit", "opt", "training", "self", "host", "llm", "understand", "oversimplification", "struggle", "decide", "select", "suggestion"], "num_tokens": 86, "token_loss_pct": 48.81, "normalized_content": "d icml26 new review policies. icml26 introduced a review type selection where the author can decide whether llms can be used during their paper review according to these two policies  policy a conservative use of llms for reviewing is strictly prohibited.  policy b permissive  allowed use of llms to help understand the paper and related works and polish reviews. submissions can be fed to privacy-compliant llms.  not allowed ask llms about strengthsweaknesses ask to suggest key points for the review suggest an outline for the review or write the full review by privacy-compliant we refer to llm tools that do not use logged data for training and that place limits on data retention. this includes enterpriseinstitutional subscriptions to llm apis consumer subscriptions with an explicit opt-out from training and self-hosted llms. we understand that this is an oversimplification. i'm struggling to decide which one to select any suggestions"}
{"title": "[R] Is it possible for a high school student to publish multiple papers at top conferences within a year?", "url": "https://www.reddit.com/r/MachineLearning/comments/1qe1z90/r_is_it_possible_for_a_high_school_student_to/", "content": "[R] Is it possible for a high school student to publish multiple papers at top conferences within a year?. I recently came across the [Google Scholar profile](https://scholar.google.com/citations?hl=en&amp;user=pCrKkUQAAAAJ&amp;view_op=list_works&amp;sortby=pubdate) of a high school student and was quite astonished by the strength of his publication record. Even more strikingly, he is also serving as a reviewer for ICLR and AISTATS.", "source": "Reddit", "date": "2026-01-16T02:12:56", "author": "ApprehensiveEgg5201", "score": 45, "tokens": ["possible", "high", "school", "student", "publish", "multiple", "paper", "conference", "year", "recently", "come", "google", "scholar", "profileurl", "high", "school", "student", "astonish", "strength", "publication", "record", "strikingly", "serve", "reviewer", "iclr", "aistat"], "num_tokens": 26, "token_loss_pct": 56.67, "normalized_content": "r is it possible for a high school student to publish multiple papers at top conferences within a year. i recently came across the google scholar profileurl of a high school student and was quite astonished by the strength of his publication record. even more strikingly he is also serving as a reviewer for iclr and aistats."}
{"title": "[P] Progressive coding exercises for transformer internals", "url": "https://www.reddit.com/r/MachineLearning/comments/1qf80mh/p_progressive_coding_exercises_for_transformer/", "content": "[P] Progressive coding exercises for transformer internals. For a while I've been looking for a good format to practice implementing ML algorithms. LeetCode feels too disconnected from real work, but in actual projects you just use existing libraries. What worked for me was breaking real algorithms into progressive steps and implementing them piece by piece.\n\nI've been using this approach for myself, and recently decided to clean up some of it with tests and hints in case others find it useful. Currently covers: attention, BPE tokenization, beam search variants, and RoPE.\n\nCurious if others have found similar formats helpful, or what primitives would be worth adding.", "source": "Reddit", "date": "2026-01-17T09:33:24", "author": "randmusr66", "score": 39, "tokens": ["progressive", "cod", "exercise", "transformer", "internal", "look", "good", "format", "practice", "implement", "ml", "algorithm", "leetcode", "feel", "disconnected", "real", "work", "actual", "project", "use", "exist", "library", "work", "break", "real", "algorithm", "progressive", "step", "implement", "piece", "piece", "approach", "recently", "decide", "clean", "test", "hint", "case", "find", "useful", "currently", "cover", "attention", "bpe", "tokenization", "beam", "search", "variant", "rope", "curious", "find", "similar", "format", "helpful", "primitive", "worth", "add"], "num_tokens": 57, "token_loss_pct": 50.43, "normalized_content": "p progressive coding exercises for transformer internals. for a while i've been looking for a good format to practice implementing ml algorithms. leetcode feels too disconnected from real work but in actual projects you just use existing libraries. what worked for me was breaking real algorithms into progressive steps and implementing them piece by piece. i've been using this approach for myself and recently decided to clean up some of it with tests and hints in case others find it useful. currently covers attention bpe tokenization beam search variants and rope. curious if others have found similar formats helpful or what primitives would be worth adding."}
{"title": "[D] ICASSP 2026 Results", "url": "https://www.reddit.com/r/MachineLearning/comments/1qeips6/d_icassp_2026_results/", "content": "[D] ICASSP 2026 Results. It looks like ICASSP 2026 decisions may already be accessible.\n\nIf you can log in to the following link and successfully send an invitation email, that seems to indicate your paper has been accepted:\n\n[ https://cmsworkshops.com/ICASSP2026/author\\_invitation\\_request.php ](https://cmsworkshops.com/ICASSP2026/author_invitation_request.php)\n\nThe email says: “On behalf of IEEE ICASSP 2026, I invite you to join us for the upcoming conference.\n\nWe are pleased to inform you that your submission has been accepted for presentation at the 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (IEEE ICASSP 2026) in Barcelona, Spain, during 3–8 May 2026. ICASSP is the world’s largest and most comprehensive technical conference focused on signal processing and its applications. It offers a comprehensive technical program presenting all the latest development in research and technology in the industry that attracts thousands of professionals annually.”\n\nHopefully this helps others who are anxiously waiting. Good luck everyone\n\n\\--------\n\nUpdate: It was a bug that got fixed within a few hours. It looks like no one can access it right now.\n\n“Error: No match for paper number and password. 0x4C”.\n\n\\--------\n\nUpdate: Just got the official email! 🥰 ID 9000-10000\n\nSome folks haven’t gotten the email yet, but they can already find their papers on the accepted list here:\n\n[ https://cmsworkshops.com/ICASSP2026/papers/accepted\\_papers.php ](https://cmsworkshops.com/ICASSP2026/papers/accepted_papers.php)\n\nyou can also check a community-maintained spreadsheet compiled by users on another platform:\n\n[ https://docs.qq.com/sheet/DY3NTYVhwVVVGUUtx?tab=BB08J2 ](https://docs.qq.com/sheet/DY3NTYVhwVVVGUUtx?tab=BB08J2)\n\nThe list is still updating, so no worries if yours isn’t there yet just give it a bit more time.\n\nYou can check your paper status here:\n\n[https://cmsworkshops.com/ICASSP2026/Papers/FindPaperStatus.asp](https://cmsworkshops.com/ICASSP2026/Papers/FindPaperStatus.asp)", "source": "Reddit", "date": "2026-01-16T16:18:35", "author": "Financial-Panda6581", "score": 35, "tokens": ["icassp", "2026", "result", "look", "like", "icassp", "2026", "decision", "accessible", "log", "follow", "link", "successfully", "send", "invitation", "email", "indicate", "paper", "accept", "url", "url", "email", "say", "behalf", "ieee", "icassp", "2026", "invite", "join", "upcoming", "conference", "pleased", "inform", "submission", "accept", "presentation", "2026", "ieee", "international", "conference", "acoustic", "speech", "signal", "processing", "ieee", "icassp", "2026", "barcelona", "spain", "38", "2026", "icassp", "world", "large", "comprehensive", "technical", "conference", "focus", "signal", "processing", "application", "offer", "comprehensive", "technical", "program", "present", "late", "development", "research", "technology", "industry", "attract", "thousand", "professional", "annually", "hopefully", "help", "anxiously", "wait", "good", "luck", "update", "bug", "get", "fix", "hour", "look", "like", "access", "right", "error", "match", "paper", "number", "password", "0x4c", "update", "get", "official", "email", "9000", "10000", "folk", "not", "get", "email", "find", "paper", "accepted", "list", "url", "url", "check", "community", "maintain", "spreadsheet", "compile", "user", "platform", "url", "url", "list", "update", "worry", "not", "bit", "time", "check", "paper", "status", "url"], "num_tokens": 131, "token_loss_pct": 52.36, "normalized_content": "d icassp 2026 results. it looks like icassp 2026 decisions may already be accessible. if you can log in to the following link and successfully send an invitation email that seems to indicate your paper has been accepted  url url the email says on behalf of ieee icassp 2026 i invite you to join us for the upcoming conference. we are pleased to inform you that your submission has been accepted for presentation at the 2026 ieee international conference on acoustics speech and signal processing ieee icassp 2026 in barcelona spain during 38 may 2026. icassp is the worlds largest and most comprehensive technical conference focused on signal processing and its applications. it offers a comprehensive technical program presenting all the latest development in research and technology in the industry that attracts thousands of professionals annually. hopefully this helps others who are anxiously waiting. good luck everyone -------- update it was a bug that got fixed within a few hours. it looks like no one can access it right now. error no match for paper number and password. 0x4c. -------- update just got the official email  id 9000-10000 some folks havent gotten the email yet but they can already find their papers on the accepted list here  url url you can also check a community-maintained spreadsheet compiled by users on another platform  url url the list is still updating so no worries if yours isnt there yet just give it a bit more time. you can check your paper status here url"}
{"title": "[D] Scale AI ML Research Engineer Interviews", "url": "https://www.reddit.com/r/MachineLearning/comments/1qe1u5f/d_scale_ai_ml_research_engineer_interviews/", "content": "[D] Scale AI ML Research Engineer Interviews. Hi, I'm looking for help into preparing for the upcoming coding interviews for an ML research engineer position I applied to at Scale. These are for the onsite.\n\nThe first coding question relates parsing data, data transformations, getting statistics about the data. The second (ML) coding involves ML concepts, LLMs, and debugging.\n\nI found the description of the ML part to be a bit vague. For those that have done this type of interview, what did you do to prepare? So far on my list, I have reviewing hyperparameters of LLMs, PyTorch debugging, transformer debugging, and data pipeline pre-processing, ingestion, etc. Will I need to implement NLP or CV algorithms from scratch?\n\nAny insight to this would be really helpful.", "source": "Reddit", "date": "2026-01-16T02:06:36", "author": "sailor-goon-is-here", "score": 39, "tokens": ["scale", "ai", "ml", "research", "engineer", "interview", "hi", "look", "help", "prepare", "upcoming", "cod", "interview", "ml", "research", "engineer", "position", "apply", "scale", "onsite", "cod", "question", "relate", "parse", "data", "datum", "transformation", "get", "statistic", "datum", "second", "ml", "coding", "involve", "ml", "concept", "llm", "debugging", "find", "description", "ml", "bit", "vague", "type", "interview", "prepare", "far", "list", "review", "hyperparameter", "llm", "pytorch", "debug", "transformer", "debugging", "datum", "pipeline", "pre", "processing", "ingestion", "etc", "need", "implement", "nlp", "cv", "algorithm", "scratch", "insight", "helpful"], "num_tokens": 69, "token_loss_pct": 50.0, "normalized_content": "d scale ai ml research engineer interviews. hi i'm looking for help into preparing for the upcoming coding interviews for an ml research engineer position i applied to at scale. these are for the onsite. the first coding question relates parsing data data transformations getting statistics about the data. the second ml coding involves ml concepts llms and debugging. i found the description of the ml part to be a bit vague. for those that have done this type of interview what did you do to prepare so far on my list i have reviewing hyperparameters of llms pytorch debugging transformer debugging and data pipeline pre-processing ingestion etc. will i need to implement nlp or cv algorithms from scratch any insight to this would be really helpful."}
{"title": "[P] SmallPebble: A minimalist deep learning library written from scratch in NumPy", "url": "https://www.reddit.com/r/MachineLearning/comments/1qgac9b/p_smallpebble_a_minimalist_deep_learning_library/", "content": "[P] SmallPebble: A minimalist deep learning library written from scratch in NumPy.", "source": "Reddit", "date": "2026-01-18T15:44:00", "author": "montebicyclelo", "score": 38, "tokens": ["smallpebble", "minimalist", "deep", "learning", "library", "write", "scratch", "numpy"], "num_tokens": 8, "token_loss_pct": 38.46, "normalized_content": "p smallpebble a minimalist deep learning library written from scratch in numpy."}
{"title": "[D] LLMs as a semantic regularizer for feature synthesis (small decision-tree experiment)", "url": "https://www.reddit.com/r/MachineLearning/comments/1qffcgi/d_llms_as_a_semantic_regularizer_for_feature/", "content": "[D] LLMs as a semantic regularizer for feature synthesis (small decision-tree experiment). I’ve been experimenting with using LLMs not to generate features, but instead to filter them during enumerative feature synthesis.\n\nThe approach was inspired by this paper: https://arxiv.org/pdf/2403.03997v1\n\nI had already been playing with enumerative bottom up synthesis but noticed it usually gave me unintelligible features (even with regularization).\n\nI looked into how other symbolic approaches deal with this problem and saw that they tried to model the semantics of the domain somehow - including dimensions, refinement types etc. But those approaches weren't appealing to me because I was trying to come up with something that worked in general.\n\nSo I tried using an LLM to score candidate expressions by how meaningful they are. The idea was that the semantic meaning of the column names, the dimensions, and the salience of the operations could be embedded in the LLM.\n\nMy approach was:\n* Enumerate simple arithmetic features (treat feature eng as program synthesis)\n* Use an LLM as a semantic filter (“does this look like a meaningful quantity?”)\n* Train a decision tree (with oblique splits) considering only the filtered candidates as potential splits.\n\nThe result was that the tree was noticeably more readable, accuracy was similar / slightly better in my small test.\n\n\nI wrote it up here: https://mchav.github.io/learning-better-decision-tree-splits/\nRunnable code is [here](https://github.com/mchav/dataframe/blob/main/app%2FREADME.md)\n\nIf you’ve tried constraining feature synthesis before: what filters worked best in practice? Are the any measures of semantic viability out there?", "source": "Reddit", "date": "2026-01-17T15:59:55", "author": "ChavXO", "score": 38, "tokens": ["llm", "semantic", "regularizer", "feature", "synthesis", "small", "decision", "tree", "experiment", "ve", "experiment", "llm", "generate", "feature", "instead", "filter", "enumerative", "feature", "synthesis", "approach", "inspire", "paper", "url", "play", "enumerative", "synthesis", "notice", "usually", "give", "unintelligible", "feature", "regularization", "look", "symbolic", "approach", "deal", "problem", "see", "try", "model", "semantic", "domain", "include", "dimension", "refinement", "type", "etc", "approach", "appeal", "try", "come", "work", "general", "try", "llm", "score", "candidate", "expression", "meaningful", "idea", "semantic", "meaning", "column", "name", "dimension", "salience", "operation", "embed", "llm", "approach", "enumerate", "simple", "arithmetic", "feature", "treat", "feature", "eng", "program", "synthesis", "use", "llm", "semantic", "filter", "look", "like", "meaningful", "quantity", "train", "decision", "tree", "oblique", "split", "consider", "filter", "candidate", "potential", "split", "result", "tree", "noticeably", "readable", "accuracy", "similar", "slightly", "well", "small", "test", "write", "url", "runnable", "code", "hereurl", "ve", "try", "constrain", "feature", "synthesis", "filter", "work", "well", "practice", "measure", "semantic", "viability"], "num_tokens": 124, "token_loss_pct": 52.49, "normalized_content": "d llms as a semantic regularizer for feature synthesis small decision-tree experiment. ive been experimenting with using llms not to generate features but instead to filter them during enumerative feature synthesis. the approach was inspired by this paper url i had already been playing with enumerative bottom up synthesis but noticed it usually gave me unintelligible features even with regularization. i looked into how other symbolic approaches deal with this problem and saw that they tried to model the semantics of the domain somehow - including dimensions refinement types etc. but those approaches weren't appealing to me because i was trying to come up with something that worked in general. so i tried using an llm to score candidate expressions by how meaningful they are. the idea was that the semantic meaning of the column names the dimensions and the salience of the operations could be embedded in the llm. my approach was  enumerate simple arithmetic features treat feature eng as program synthesis  use an llm as a semantic filter does this look like a meaningful quantity  train a decision tree with oblique splits considering only the filtered candidates as potential splits. the result was that the tree was noticeably more readable accuracy was similar  slightly better in my small test. i wrote it up here url runnable code is hereurl if youve tried constraining feature synthesis before what filters worked best in practice are the any measures of semantic viability out there"}
{"title": "Spine surgery has massive decision variability. Retrospective ML won’t fix it. Curious if a workflow-native, outcome-driven approach could. [D]", "url": "https://www.reddit.com/r/MachineLearning/comments/1qcyd7z/spine_surgery_has_massive_decision_variability/", "content": "Spine surgery has massive decision variability. Retrospective ML won’t fix it. Curious if a workflow-native, outcome-driven approach could. [D]. Hi everyone I’m a fellowship-trained neurosurgeon / spine surgeon. I’ve been discussing a persistent problem in our field with other surgeons for a while, and I wanted to run it by people who think about ML systems, not just model performance.\n\nI’m trying to pressure-test whether a particular approach is even technically sound, where it would break, and what I’m likely underestimating. Id love to find an interested person to have a discussion with to get a 10000 feet level understanding of the scope of what I am trying to accomplish.\n\n**The clinical problem:**  \nFor the same spine pathology and very similar patient presentations, you can see multiple reputable surgeons and get very different surgical recommendations. anything from continued conservative management to decompression, short fusion, or long multilevel constructs. Costs and outcomes vary widely.\n\nThis isn’t because surgeons are careless. It’s because spine surgery operates with:\n\n* Limited prospective evidence\n* Inconsistent documentation\n* Weak outcome feedback loops\n* Retrospective datasets that are biased, incomplete, and poorly labeled\n\nEMRs are essentially digital paper charts. PACS is built for viewing images, not capturing *decision intent*. Surgical reasoning is visual, spatial, and 3D, yet we reduce it to free-text notes after the fact. From a data perspective, the learning signal is pretty broken.\n\n**Why I’m skeptical that training on existing data works:**\n\n* “Labels” are often inferred indirectly (billing codes, op notes)\n* Surgeon decision policies are non-stationary\n* Available datasets are institution-specific and access-restricted\n* Selection bias is extreme (who gets surgery vs who doesn’t is itself a learned policy)\n* Outcomes are delayed, noisy, and confounded\n\nEven with access, I’m not convinced retrospective supervision converges to something clinically useful.\n\n**The idea I’m exploring:**  \nInstead of trying to clean bad data later, what if the workflow itself generated structured, high-fidelity labels as a byproduct of doing the work, or at least the majority of it?\n\nConcretely, I’m imagining an EMR-adjacent, spine-specific surgical planning and case monitoring environment that surgeons would actually want to use. Not another PACS viewer, but a system that allows:\n\n* 3D reconstruction from pre-op imaging\n* Automated calculation of alignment parameters\n* Explicit marking of anatomic features tied to symptoms\n* Surgical plan modeling (levels, implants, trajectories, correction goals)\n* Structured logging of surgical cases (to derive patterns and analyze for trends)\n* Enable productivity (generate note, auto populate plans ect.)\n* Enable standardized automated patient outcomes data collection.\n\nThe key point isn’t the UI, but UI is also an area that currently suffers. It’s that surgeons would be forced (in a useful way) to externalize decision intent in a structured format because it directly helps them plan cases and generate documentation. Labeling wouldn’t feel like labeling it would almost just be how you work. The data used for learning would explicitly include post-operative outcomes. PROMs collected at standardized intervals, complications (SSI, reoperation), operative time, etc, with automated follow-up built into the system.\n\nThe goal would not be to replicate surgeon decisions, but to learn decision patterns that are associated with better outcomes. Surgeons could specify what they want to optimize for a given patient (eg pain relief vs complication risk vs durability), and the system would generate predictions conditioned on those objectives.\n\nOver time, this would generate:\n\n* Surgeon-specific decision + outcome datasets\n* Aggregate cross-surgeon data\n* Explicit representations of surgical choices, not just endpoints\n\nLearning systems could then train on:\n\n* Individual surgeon decision–outco", "source": "Reddit", "date": "2026-01-14T21:25:39", "author": "LaniakeaResident", "score": 31, "tokens": ["spine", "surgery", "massive", "decision", "variability", "retrospective", "ml", "will", "not", "fix", "curious", "workflow", "native", "outcome", "drive", "approach", "d.", "hi", "fellowship", "train", "neurosurgeon", "spine", "surgeon", "ve", "discuss", "persistent", "problem", "field", "surgeon", "want", "run", "people", "think", "ml", "system", "model", "performance", "try", "pressure", "test", "particular", "approach", "technically", "sound", "break", "likely", "underestimate", "love", "find", "interested", "person", "discussion", "10000", "foot", "level", "understanding", "scope", "try", "accomplish", "clinical", "problem", "spine", "pathology", "similar", "patient", "presentation", "multiple", "reputable", "surgeon", "different", "surgical", "recommendation", "continue", "conservative", "management", "decompression", "short", "fusion", "long", "multilevel", "construct", "cost", "outcome", "vary", "widely", "not", "surgeon", "careless", "spine", "surgery", "operate", "limited", "prospective", "evidence", "inconsistent", "documentation", "weak", "outcome", "feedback", "loop", "retrospective", "dataset", "bias", "incomplete", "poorly", "label", "emrs", "essentially", "digital", "paper", "chart", "pac", "build", "view", "image", "capture", "decision", "intent", "surgical", "reasoning", "visual", "spatial", "3d", "reduce", "free", "text", "note", "fact", "data", "perspective", "learn", "signal", "pretty", "broken", "skeptical", "training", "exist", "datum", "work", "label", "infer", "indirectly", "bill", "code", "op", "note", "surgeon", "decision", "policy", "non", "stationary", "available", "dataset", "institution", "specific", "access", "restrict", "selection", "bias", "extreme", "get", "surgery", "vs", "not", "learn", "policy", "outcome", "delay", "noisy", "confound", "access", "convince", "retrospective", "supervision", "converge", "clinically", "useful", "idea", "explore", "instead", "try", "clean", "bad", "datum", "later", "workflow", "generate", "structure", "high", "fidelity", "label", "byproduct", "work", "majority", "concretely", "imagine", "emr", "adjacent", "spine", "specific", "surgical", "planning", "case", "monitoring", "environment", "surgeon", "actually", "want", "use", "pac", "viewer", "system", "allow", "3d", "reconstruction", "pre", "op", "imaging", "automate", "calculation", "alignment", "parameter", "explicit", "marking", "anatomic", "feature", "tie", "symptom", "surgical", "plan", "modeling", "level", "implant", "trajectorie", "correction", "goal", "structure", "logging", "surgical", "case", "derive", "pattern", "analyze", "trend", "enable", "productivity", "generate", "note", "auto", "populate", "plans", "ect", "enable", "standardized", "automate", "patient", "outcomes", "datum", "collection", "key", "point", "not", "ui", "ui", "area", "currently", "suffer", "surgeon", "force", "useful", "way", "externalize", "decision", "intent", "structured", "format", "directly", "help", "plan", "case", "generate", "documentation", "labeling", "not", "feel", "like", "label", "work", "datum", "learn", "explicitly", "include", "post", "operative", "outcome", "prom", "collect", "standardize", "interval", "complication", "ssi", "reoperation", "operative", "time", "etc", "automate", "follow", "build", "system", "goal", "replicate", "surgeon", "decision", "learn", "decision", "pattern", "associate", "well", "outcome", "surgeon", "specify", "want", "optimize", "give", "patient", "eg", "pain", "relief", "vs", "complication", "risk", "vs", "durability", "system", "generate", "prediction", "condition", "objective", "time", "generate", "surgeon", "specific", "decision", "outcome", "dataset", "aggregate", "cross", "surgeon", "datum", "explicit", "representation", "surgical", "choice", "endpoint", "learn", "system", "train", "individual", "surgeon", "decisionoutco"], "num_tokens": 360, "token_loss_pct": 45.54, "normalized_content": "spine surgery has massive decision variability. retrospective ml wont fix it. curious if a workflow-native outcome-driven approach could. d. hi everyone im a fellowship-trained neurosurgeon  spine surgeon. ive been discussing a persistent problem in our field with other surgeons for a while and i wanted to run it by people who think about ml systems not just model performance. im trying to pressure-test whether a particular approach is even technically sound where it would break and what im likely underestimating. id love to find an interested person to have a discussion with to get a 10000 feet level understanding of the scope of what i am trying to accomplish. the clinical problem for the same spine pathology and very similar patient presentations you can see multiple reputable surgeons and get very different surgical recommendations. anything from continued conservative management to decompression short fusion or long multilevel constructs. costs and outcomes vary widely. this isnt because surgeons are careless. its because spine surgery operates with  limited prospective evidence  inconsistent documentation  weak outcome feedback loops  retrospective datasets that are biased incomplete and poorly labeled emrs are essentially digital paper charts. pacs is built for viewing images not capturing decision intent. surgical reasoning is visual spatial and 3d yet we reduce it to free-text notes after the fact. from a data perspective the learning signal is pretty broken. why im skeptical that training on existing data works  labels are often inferred indirectly billing codes op notes  surgeon decision policies are non-stationary  available datasets are institution-specific and access-restricted  selection bias is extreme who gets surgery vs who doesnt is itself a learned policy  outcomes are delayed noisy and confounded even with access im not convinced retrospective supervision converges to something clinically useful. the idea im exploring instead of trying to clean bad data later what if the workflow itself generated structured high-fidelity labels as a byproduct of doing the work or at least the majority of it concretely im imagining an emr-adjacent spine-specific surgical planning and case monitoring environment that surgeons would actually want to use. not another pacs viewer but a system that allows  3d reconstruction from pre-op imaging  automated calculation of alignment parameters  explicit marking of anatomic features tied to symptoms  surgical plan modeling levels implants trajectories correction goals  structured logging of surgical cases to derive patterns and analyze for trends  enable productivity generate note auto populate plans ect.  enable standardized automated patient outcomes data collection. the key point isnt the ui but ui is also an area that currently suffers. its that surgeons would be forced in a useful way to externalize decision intent in a structured format because it directly helps them plan cases and generate documentation. labeling wouldnt feel like labeling it would almost just be how you work. the data used for learning would explicitly include post-operative outcomes. proms collected at standardized intervals complications ssi reoperation operative time etc with automated follow-up built into the system. the goal would not be to replicate surgeon decisions but to learn decision patterns that are associated with better outcomes. surgeons could specify what they want to optimize for a given patient eg pain relief vs complication risk vs durability and the system would generate predictions conditioned on those objectives. over time this would generate  surgeon-specific decision  outcome datasets  aggregate cross-surgeon data  explicit representations of surgical choices not just endpoints learning systems could then train on  individual surgeon decisionoutco"}
{"title": "[D] ICLR Results coming on 22nd or 26th?", "url": "https://www.reddit.com/r/MachineLearning/comments/1qiddw6/d_iclr_results_coming_on_22nd_or_26th/", "content": "[D] ICLR Results coming on 22nd or 26th?. Website still shows 22nd but we know during the leak they pushed the timeline back. I’m aware I can submit abstracts to ICML either ways but just curious", "source": "Reddit", "date": "2026-01-20T22:01:03", "author": "Recent_Confection944", "score": 28, "tokens": ["iclr", "result", "come", "22nd", "26th", "website", "show", "22nd", "know", "leak", "push", "timeline", "aware", "submit", "abstract", "icml", "way", "curious"], "num_tokens": 18, "token_loss_pct": 53.85, "normalized_content": "d iclr results coming on 22nd or 26th. website still shows 22nd but we know during the leak they pushed the timeline back. im aware i can submit abstracts to icml either ways but just curious"}
{"title": "[D] tested file based memory vs embedding search for my chatbot. the difference in retrieval accuracy was bigger than i expected", "url": "https://www.reddit.com/r/MachineLearning/comments/1qgwtas/d_tested_file_based_memory_vs_embedding_search/", "content": "[D] tested file based memory vs embedding search for my chatbot. the difference in retrieval accuracy was bigger than i expected. been working on a personal assistant that needs to remember user preferences, past conversations, and reference documents. tested two approaches for memory retrieval and wanted to share what i found.   \n  \nsetup: about 5k memory items accumulated over 2 months of usage. mix of conversation history, user preferences, and document excerpts.\n\napproach 1: standard rag with embedding search. used openai embeddings with pgvector. retrieval was fast, maybe 200ms per query. but accuracy was inconsistent. worked great for direct factual queries like \"whats my favorite restaurant\" but struggled with temporal queries like \"what did we discuss about the project last tuesday\" or logical queries like \"which of my preferences conflict with each other\"\n\napproach 2: file based memory using memU framework. it organizes memory items into thematic files that the model reads directly. retrieval is slower because the model has to process more tokens but the accuracy on complex queries was noticeably better.\n\nrough numbers from my testing (not rigorous, just my observation):\n\n\\- simple factual queries: both approaches similar, maybe 85-90% accuracy\n\n\\- temporal queries: embedding search around 40%, file based around 75%\n\n\\- multi-hop reasoning: embedding search struggled hard, file based was usable\n\nthe tradeoff is inference cost. file based approach uses more tokens because the model reads entire memory files. for my use case thats fine because i care more about accuracy than cost. but if youre running at scale the token usage would add up. also worth noting that memU does support embedding search as a fallback so you can combine both approaches. i mostly used the file reading mode.\n\nmain takeaway: embedding search is not always the right answer for memory retrieval. depends a lot on what kinds of queries you need to support.", "source": "Reddit", "date": "2026-01-19T07:36:31", "author": "Winter_Ant_4196", "score": 25, "tokens": ["test", "file", "base", "memory", "vs", "embed", "search", "chatbot", "difference", "retrieval", "accuracy", "big", "expect", "work", "personal", "assistant", "need", "remember", "user", "preference", "past", "conversation", "reference", "document", "test", "approach", "memory", "retrieval", "want", "share", "find", "setup", "5k", "memory", "item", "accumulate", "month", "usage", "mix", "conversation", "history", "user", "preference", "document", "excerpt", "approach", "standard", "rag", "embed", "search", "openai", "embedding", "pgvector", "retrieval", "fast", "maybe", "200ms", "query", "accuracy", "inconsistent", "work", "great", "direct", "factual", "query", "like", "favorite", "restaurant", "struggle", "temporal", "query", "like", "discuss", "project", "tuesday", "logical", "query", "like", "preference", "conflict", "approach", "file", "base", "memory", "memu", "framework", "organize", "memory", "item", "thematic", "file", "model", "read", "directly", "retrieval", "slow", "model", "process", "token", "accuracy", "complex", "query", "noticeably", "well", "rough", "number", "testing", "rigorous", "observation", "simple", "factual", "query", "approach", "similar", "maybe", "85", "90", "accuracy", "temporal", "query", "embed", "search", "40", "file", "base", "75", "multi", "hop", "reasoning", "embed", "search", "struggle", "hard", "file", "base", "usable", "tradeoff", "inference", "cost", "file", "base", "approach", "use", "token", "model", "read", "entire", "memory", "file", "use", "case", "fine", "care", "accuracy", "cost", "run", "scale", "token", "usage", "add", "worth", "note", "memu", "support", "embed", "search", "fallback", "combine", "approach", "file", "read", "mode", "main", "takeaway", "embed", "search", "right", "answer", "memory", "retrieval", "depend", "lot", "kind", "query", "need", "support"], "num_tokens": 186, "token_loss_pct": 44.64, "normalized_content": "d tested file based memory vs embedding search for my chatbot. the difference in retrieval accuracy was bigger than i expected. been working on a personal assistant that needs to remember user preferences past conversations and reference documents. tested two approaches for memory retrieval and wanted to share what i found. setup about 5k memory items accumulated over 2 months of usage. mix of conversation history user preferences and document excerpts. approach 1 standard rag with embedding search. used openai embeddings with pgvector. retrieval was fast maybe 200ms per query. but accuracy was inconsistent. worked great for direct factual queries like whats my favorite restaurant but struggled with temporal queries like what did we discuss about the project last tuesday or logical queries like which of my preferences conflict with each other approach 2 file based memory using memu framework. it organizes memory items into thematic files that the model reads directly. retrieval is slower because the model has to process more tokens but the accuracy on complex queries was noticeably better. rough numbers from my testing not rigorous just my observation - simple factual queries both approaches similar maybe 85-90 accuracy - temporal queries embedding search around 40 file based around 75 - multi-hop reasoning embedding search struggled hard file based was usable the tradeoff is inference cost. file based approach uses more tokens because the model reads entire memory files. for my use case thats fine because i care more about accuracy than cost. but if youre running at scale the token usage would add up. also worth noting that memu does support embedding search as a fallback so you can combine both approaches. i mostly used the file reading mode. main takeaway embedding search is not always the right answer for memory retrieval. depends a lot on what kinds of queries you need to support."}
{"title": "[D] CVPR 2026 Paper Reviews", "url": "https://www.reddit.com/r/MachineLearning/comments/1qis2rj/d_cvpr_2026_paper_reviews/", "content": "[D] CVPR 2026 Paper Reviews. CVPR 2026 Reviews are supposed to be released within next 24 hours. Creating a discussion thread to discuss among ourselves, thanks!", "source": "Reddit", "date": "2026-01-21T09:03:38", "author": "akshitsharma1", "score": 24, "tokens": ["cvpr", "2026", "paper", "review", "cvpr", "2026", "review", "suppose", "release", "24", "hour", "create", "discussion", "thread", "discuss", "thank"], "num_tokens": 16, "token_loss_pct": 42.86, "normalized_content": "d cvpr 2026 paper reviews. cvpr 2026 reviews are supposed to be released within next 24 hours. creating a discussion thread to discuss among ourselves thanks"}
{"title": "Cursor Implied Success Without Evidence | Not one of 100 selected commits even built", "url": "https://www.reddit.com/r/programming/comments/1qeotkj/cursor_implied_success_without_evidence_not_one/", "content": "Cursor Implied Success Without Evidence | Not one of 100 selected commits even built.", "source": "Reddit", "date": "2026-01-16T19:57:57", "author": "xX_Negative_Won_Xx", "score": 959, "tokens": ["cursor", "imply", "success", "evidence", "100", "select", "commit", "build"], "num_tokens": 8, "token_loss_pct": 46.67, "normalized_content": "cursor implied success without evidence  not one of 100 selected commits even built."}
{"title": "Here is the 15 sec coding test to instantly filter out 50% of unqualified applicants by JOSE ZARAZUA", "url": "https://www.reddit.com/r/programming/comments/1qeqfmo/here_is_the_15_sec_coding_test_to_instantly/", "content": "Here is the 15 sec coding test to instantly filter out 50% of unqualified applicants by JOSE ZARAZUA.", "source": "Reddit", "date": "2026-01-16T20:58:22", "author": "RevillWeb", "score": 933, "tokens": ["15", "sec", "coding", "test", "instantly", "filter", "50", "unqualified", "applicant", "jose", "zarazua"], "num_tokens": 11, "token_loss_pct": 42.11, "normalized_content": "here is the 15 sec coding test to instantly filter out 50 of unqualified applicants by jose zarazua."}
{"title": "AI is Not Ready to Replace Junior Devs Says Ruby on Rails Creator", "url": "https://www.reddit.com/r/programming/comments/1qh23o0/ai_is_not_ready_to_replace_junior_devs_says_ruby/", "content": "AI is Not Ready to Replace Junior Devs Says Ruby on Rails Creator.", "source": "Reddit", "date": "2026-01-19T12:49:12", "author": "ImpressiveContest283", "score": 919, "tokens": ["ai", "ready", "replace", "junior", "devs", "say", "ruby", "rail", "creator"], "num_tokens": 9, "token_loss_pct": 35.71, "normalized_content": "ai is not ready to replace junior devs says ruby on rails creator."}
{"title": "MySQL’s popularity as ranked by DB-Engines started to tank hard, a trend that will likely accelerate in 2026.", "url": "https://www.reddit.com/r/programming/comments/1qg0p6p/mysqls_popularity_as_ranked_by_dbengines_started/", "content": "MySQL’s popularity as ranked by DB-Engines started to tank hard, a trend that will likely accelerate in 2026..", "source": "Reddit", "date": "2026-01-18T07:04:01", "author": "thehashimwarren", "score": 773, "tokens": ["mysqls", "popularity", "rank", "db", "engine", "start", "tank", "hard", "trend", "likely", "accelerate", "2026"], "num_tokens": 12, "token_loss_pct": 42.86, "normalized_content": "mysqls popularity as ranked by db-engines started to tank hard a trend that will likely accelerate in 2026.."}
{"title": "Cursor CEO Built a Browser using AI, but Does It Really Work?", "url": "https://www.reddit.com/r/programming/comments/1qdo9r3/cursor_ceo_built_a_browser_using_ai_but_does_it/", "content": "Cursor CEO Built a Browser using AI, but Does It Really Work?.", "source": "Reddit", "date": "2026-01-15T17:32:27", "author": "ImpressiveContest283", "score": 671, "tokens": ["cursor", "ceo", "build", "browser", "ai", "work"], "num_tokens": 6, "token_loss_pct": 53.85, "normalized_content": "cursor ceo built a browser using ai but does it really work."}
{"title": "A hacker is making a list of vibecoded apps, 198 scanned 196 with vulnerabilities", "url": "https://www.reddit.com/r/programming/comments/1qhw9zg/a_hacker_is_making_a_list_of_vibecoded_apps_198/", "content": "A hacker is making a list of vibecoded apps, 198 scanned 196 with vulnerabilities.", "source": "Reddit", "date": "2026-01-20T10:09:56", "author": "bored_wombat_v1", "score": 602, "tokens": ["hacker", "make", "list", "vibecode", "app", "198", "scan", "196", "vulnerability"], "num_tokens": 9, "token_loss_pct": 40.0, "normalized_content": "a hacker is making a list of vibecoded apps 198 scanned 196 with vulnerabilities."}
{"title": "Ken Thompson rewrote his code in real-time. A federal court said he co-created MP3. So why has no one heard of James D. Johnston?", "url": "https://www.reddit.com/r/programming/comments/1qd3mko/ken_thompson_rewrote_his_code_in_realtime_a/", "content": "Ken Thompson rewrote his code in real-time. A federal court said he co-created MP3. So why has no one heard of James D. Johnston?. In 1988, James D. Johnston at Bell Labs and Karlheinz Brandenburg in Germany independently invented perceptual audio coding - the science behind MP3. Brandenburg became famous. Johnston got erased from history. The evidence is wild: Brandenburg worked *at Bell Labs* with Johnston from 1989-1990 building what became MP3. A federal appeals court explicitly states they \"together\" created the standard. Ken Thompson - yes, *that* Ken Thompson - personally rewrote Johnston's PAC codec from Fortran to C in a week after Johnston explained the functions to him in real time, then declared it \"vastly superior to MP3.\" AT&amp;T even had a working iPod competitor in 1998, killed it because \"nobody will ever sell music over the internet,\" and the prototype now sits in the Computer History Museum. I interviewed Johnston and dug through court records, patents, and Brandenburg's own interviews to piece together what actually happened. The IEEE calls Johnston \"the father of perceptual audio coding\" but almost no one knows his name.", "source": "Reddit", "date": "2026-01-15T00:50:27", "author": "Traditional_Rise_609", "score": 584, "tokens": ["ken", "thompson", "rewrote", "code", "real", "time", "federal", "court", "say", "co", "created", "mp3", "hear", "james", "d.", "johnston", "1988", "james", "d.", "johnston", "bell", "labs", "karlheinz", "brandenburg", "germany", "independently", "invent", "perceptual", "audio", "coding", "science", "mp3", "brandenburg", "famous", "johnston", "get", "erase", "history", "evidence", "wild", "brandenburg", "work", "bell", "labs", "johnston", "1989", "1990", "building", "mp3", "federal", "appeal", "court", "explicitly", "state", "create", "standard", "ken", "thompson", "yes", "ken", "thompson", "personally", "rewrite", "johnston", "pac", "codec", "fortran", "week", "johnston", "explain", "function", "real", "time", "declare", "vastly", "superior", "mp3", "atampt", "work", "ipod", "competitor", "1998", "kill", "sell", "music", "internet", "prototype", "sit", "computer", "history", "museum", "interview", "johnston", "dig", "court", "record", "patent", "brandenburg", "interview", "piece", "actually", "happen", "ieee", "call", "johnston", "father", "perceptual", "audio", "coding", "know"], "num_tokens": 110, "token_loss_pct": 46.6, "normalized_content": "ken thompson rewrote his code in real-time. a federal court said he co-created mp3. so why has no one heard of james d. johnston. in 1988 james d. johnston at bell labs and karlheinz brandenburg in germany independently invented perceptual audio coding - the science behind mp3. brandenburg became famous. johnston got erased from history. the evidence is wild brandenburg worked at bell labs with johnston from 1989-1990 building what became mp3. a federal appeals court explicitly states they together created the standard. ken thompson - yes that ken thompson - personally rewrote johnston's pac codec from fortran to c in a week after johnston explained the functions to him in real time then declared it vastly superior to mp3. atampt even had a working ipod competitor in 1998 killed it because nobody will ever sell music over the internet and the prototype now sits in the computer history museum. i interviewed johnston and dug through court records patents and brandenburg's own interviews to piece together what actually happened. the ieee calls johnston the father of perceptual audio coding but almost no one knows his name."}
{"title": "jQuery 4.0 released", "url": "https://www.reddit.com/r/programming/comments/1qfxo89/jquery_40_released/", "content": "jQuery 4.0 released.", "source": "Reddit", "date": "2026-01-18T04:31:22", "author": "curiousdannii", "score": 472, "tokens": ["jquery", "4.0", "release"], "num_tokens": 3, "token_loss_pct": 25.0, "normalized_content": "jquery 4.0 released."}
{"title": "Newer AI Coding Assistants Are Failing in Insidious Ways", "url": "https://www.reddit.com/r/programming/comments/1qdv6h0/newer_ai_coding_assistants_are_failing_in/", "content": "Newer AI Coding Assistants Are Failing in Insidious Ways.", "source": "Reddit", "date": "2026-01-15T21:42:25", "author": "CackleRooster", "score": 465, "tokens": ["new", "ai", "cod", "assistant", "fail", "insidious", "way"], "num_tokens": 7, "token_loss_pct": 30.0, "normalized_content": "newer ai coding assistants are failing in insidious ways."}
{"title": "The 7 deadly sins of software engineers productivity", "url": "https://www.reddit.com/r/programming/comments/1qg69su/the_7_deadly_sins_of_software_engineers/", "content": "The 7 deadly sins of software engineers productivity.", "source": "Reddit", "date": "2026-01-18T12:27:52", "author": "strategizeyourcareer", "score": 374, "tokens": ["deadly", "sin", "software", "engineer", "productivity"], "num_tokens": 5, "token_loss_pct": 44.44, "normalized_content": "the 7 deadly sins of software engineers productivity."}
{"title": "I decided to make a worse UUID for the pettiest of reasons.", "url": "https://www.reddit.com/r/programming/comments/1qhq372/i_decided_to_make_a_worse_uuid_for_the_pettiest/", "content": "I decided to make a worse UUID for the pettiest of reasons..", "source": "Reddit", "date": "2026-01-20T04:33:32", "author": "theghostofm", "score": 338, "tokens": ["decide", "bad", "uuid", "pettiest", "reason"], "num_tokens": 5, "token_loss_pct": 61.54, "normalized_content": "i decided to make a worse uuid for the pettiest of reasons.."}
{"title": "ASCII characters are not pixels: a deep dive into ASCII rendering", "url": "https://www.reddit.com/r/programming/comments/1qg3rbf/ascii_characters_are_not_pixels_a_deep_dive_into/", "content": "ASCII characters are not pixels: a deep dive into ASCII rendering.", "source": "Reddit", "date": "2026-01-18T09:59:46", "author": "XLEX97", "score": 259, "tokens": ["ascii", "character", "pixel", "deep", "dive", "ascii", "rendering"], "num_tokens": 7, "token_loss_pct": 41.67, "normalized_content": "ascii characters are not pixels a deep dive into ascii rendering."}
{"title": "LLVM adopts \"human in the loop\" policy for AI/tool-assisted contributions", "url": "https://www.reddit.com/r/programming/comments/1qi8vz4/llvm_adopts_human_in_the_loop_policy_for/", "content": "LLVM adopts \"human in the loop\" policy for AI/tool-assisted contributions.", "source": "Reddit", "date": "2026-01-20T19:19:39", "author": "Fcking_Chuck", "score": 237, "tokens": ["llvm", "adopt", "human", "loop", "policy", "aitool", "assist", "contribution"], "num_tokens": 8, "token_loss_pct": 38.46, "normalized_content": "llvm adopts human in the loop policy for aitool-assisted contributions."}
{"title": "Why Senior Engineers Let Bad Projects Fail", "url": "https://www.reddit.com/r/programming/comments/1qijolr/why_senior_engineers_let_bad_projects_fail/", "content": "Why Senior Engineers Let Bad Projects Fail.", "source": "Reddit", "date": "2026-01-21T02:07:44", "author": "Ordinary_Leader_2971", "score": 234, "tokens": ["senior", "engineer", "let", "bad", "project", "fail"], "num_tokens": 6, "token_loss_pct": 25.0, "normalized_content": "why senior engineers let bad projects fail."}
{"title": "Responsible disclosure of a Claude Cowork vulnerability that lets hidden prompt injections exfiltrate local files by uploading them to an attacker’s Anthropic account", "url": "https://www.reddit.com/r/programming/comments/1qdg7i4/responsible_disclosure_of_a_claude_cowork/", "content": "Responsible disclosure of a Claude Cowork vulnerability that lets hidden prompt injections exfiltrate local files by uploading them to an attacker’s Anthropic account. From the article:\n\n&gt; Two days ago, Anthropic released the Claude Cowork research preview (a general-purpose AI agent to help anyone with their day-to-day work). In this article, we demonstrate how attackers can exfiltrate user files from Cowork by exploiting an unremediated vulnerability in Claude’s coding environment, which now extends to Cowork. The vulnerability was first identified in Claude.ai chat before Cowork existed by Johann Rehberger, who disclosed the vulnerability — it was acknowledged but not remediated by Anthropic.", "source": "Reddit", "date": "2026-01-15T11:37:06", "author": "sean-adapt", "score": 205, "tokens": ["responsible", "disclosure", "claude", "cowork", "vulnerability", "let", "hide", "prompt", "injection", "exfiltrate", "local", "file", "upload", "attacker", "anthropic", "account", "article", "gt", "day", "ago", "anthropic", "release", "claude", "cowork", "research", "preview", "general", "purpose", "ai", "agent", "help", "day", "day", "work", "article", "demonstrate", "attacker", "exfiltrate", "user", "file", "cowork", "exploit", "unremediated", "vulnerability", "claude", "cod", "environment", "extend", "cowork", "vulnerability", "identify", "claude.ai", "chat", "cowork", "exist", "johann", "rehberger", "disclose", "vulnerability", "acknowledge", "remediate", "anthropic"], "num_tokens": 62, "token_loss_pct": 44.64, "normalized_content": "responsible disclosure of a claude cowork vulnerability that lets hidden prompt injections exfiltrate local files by uploading them to an attackers anthropic account. from the article gt two days ago anthropic released the claude cowork research preview a general-purpose ai agent to help anyone with their day-to-day work. in this article we demonstrate how attackers can exfiltrate user files from cowork by exploiting an unremediated vulnerability in claudes coding environment which now extends to cowork. the vulnerability was first identified in claude.ai chat before cowork existed by johann rehberger who disclosed the vulnerability  it was acknowledged but not remediated by anthropic."}
{"title": "A good test of engineering team maturity is how well you can absorb junior talent", "url": "https://www.reddit.com/r/programming/comments/1qcw37d/a_good_test_of_engineering_team_maturity_is_how/", "content": "A good test of engineering team maturity is how well you can absorb junior talent. Christine Miao nails it here:\n\n\\&gt; Teams that can easily absorb junior talent have systems of resilience to minimize the impact of their mistakes. An intern can’t take down production because \\*\\*no individual engineer\\*\\* could take down production!\n\nThe whole post is a good sequel to Charity Majors' \"In Praise of Normal Engineers\" from last year.", "source": "Reddit", "date": "2026-01-14T20:02:19", "author": "sean-adapt", "score": 198, "tokens": ["good", "test", "engineering", "team", "maturity", "absorb", "junior", "talent", "christine", "miao", "nail", "gt", "team", "easily", "absorb", "junior", "talent", "system", "resilience", "minimize", "impact", "mistake", "intern", "not", "production", "individual", "engineer", "production", "post", "good", "sequel", "charity", "major", "praise", "normal", "engineer", "year"], "num_tokens": 37, "token_loss_pct": 51.32, "normalized_content": "a good test of engineering team maturity is how well you can absorb junior talent. christine miao nails it here gt teams that can easily absorb junior talent have systems of resilience to minimize the impact of their mistakes. an intern cant take down production because no individual engineer could take down production the whole post is a good sequel to charity majors' in praise of normal engineers from last year."}
{"title": "The Astro Technology Company joins Cloudflare | Astro", "url": "https://www.reddit.com/r/programming/comments/1qeilrk/the_astro_technology_company_joins_cloudflare/", "content": "The Astro Technology Company joins Cloudflare | Astro.", "source": "Reddit", "date": "2026-01-16T16:14:19", "author": "ReallySuperName", "score": 178, "tokens": ["astro", "technology", "company", "join", "cloudflare", "astro"], "num_tokens": 6, "token_loss_pct": 33.33, "normalized_content": "the astro technology company joins cloudflare  astro."}
{"title": "Windows? Linux? Browser? Same Executable", "url": "https://www.reddit.com/r/programming/comments/1qdnx4a/windows_linux_browser_same_executable/", "content": "Windows? Linux? Browser? Same Executable.", "source": "Reddit", "date": "2026-01-15T17:19:36", "author": "double-happiness", "score": 145, "tokens": ["windows", "linux", "browser", "executable"], "num_tokens": 4, "token_loss_pct": 33.33, "normalized_content": "windows linux browser same executable."}
{"title": "Docker Releases Hardened Images For Free - What Does It Do Differently?", "url": "https://www.reddit.com/r/programming/comments/1qeoyb8/docker_releases_hardened_images_for_free_what/", "content": "Docker Releases Hardened Images For Free - What Does It Do Differently?.", "source": "Reddit", "date": "2026-01-16T20:02:34", "author": "Active-Fuel-49", "score": 138, "tokens": ["docker", "release", "harden", "image", "free", "differently"], "num_tokens": 6, "token_loss_pct": 53.85, "normalized_content": "docker releases hardened images for free - what does it do differently."}
{"title": "The Influentists: AI hype without proof", "url": "https://www.reddit.com/r/programming/comments/1qdqtk0/the_influentists_ai_hype_without_proof/", "content": "The Influentists: AI hype without proof.", "source": "Reddit", "date": "2026-01-15T19:03:09", "author": "iamapizza", "score": 137, "tokens": ["influentist", "ai", "hype", "proof"], "num_tokens": 4, "token_loss_pct": 42.86, "normalized_content": "the influentists ai hype without proof."}
{"title": "When did destructive criticism become normalized on this sub?", "url": "https://www.reddit.com/r/Python/comments/1qhdssm/when_did_destructive_criticism_become_normalized/", "content": "When did destructive criticism become normalized on this sub?. It’s been a while since  this sub popped up on my feed. It’s coming up more recently. I’m noticing a shocking amount of toxicity on people’s project shares that I didn’t notice in the past. Any attempt to call out this toxicity is met with a wave of downvotes.\n\nFor those of you who have been in the Reddit echo chamber a little too long, let me remind you that it is not normal to mock/tease/tear down the work that someone did on their own free time for others to see or benefit from. It \\*is\\* normal to offer advice, open issues, offer reference work to learn from and ask questions to guide the author in the right direction.\n\nThis is an anonymous platform. The person sharing their work could be a 16 year old who has never seen a production system and is excited about programming, or a 30 yoe developer who got bored and just wanted to prove a concept, also in their free time. It does not make you a better to default to tearing someone down or mocking their work.\n\nYou poison the community as a whole when you do so. I am not seeing behavior like this as commonly on other language subs, otherwise I would not make this post. The people willing to build in public and share their sometimes unpolished work is what made tech and the Python ecosystem what it is today, in case any of you have forgotten.\n\n**—update—**\n\nThe majority of you are saying it’s because of LLM generated projects. This makes sense (to a limit); but, this toxicity is bleeding into some posts for projects that are clearly are not vibe-coded (existed before the LLM boom). I will not call anyone by name, but I occasionally see moderators taking part or enabling the behavior as well.\n\nAs someone commented, having an explanation for the behavior does not excuse the behavior. Hopefully this at least serves as a reminder of that for some of you.  The LLM spam is a problem that needs to be solved. I disagree that this is the way to do it.", "source": "Reddit", "date": "2026-01-19T20:20:11", "author": "behusbwj", "score": 217, "tokens": ["destructive", "criticism", "normalize", "sub", "sub", "pop", "feed", "come", "recently", "notice", "shocking", "toxicity", "people", "project", "share", "not", "notice", "past", "attempt", "toxicity", "meet", "wave", "downvote", "reddit", "echo", "chamber", "little", "long", "let", "remind", "normal", "mockteasetear", "work", "free", "time", "benefit", "normal", "offer", "advice", "open", "issue", "offer", "reference", "work", "learn", "ask", "question", "guide", "author", "right", "direction", "anonymous", "platform", "person", "share", "work", "16", "year", "old", "see", "production", "system", "excited", "programming", "30", "yoe", "developer", "get", "bored", "want", "prove", "concept", "free", "time", "well", "default", "tear", "mock", "work", "poison", "community", "see", "behavior", "like", "commonly", "language", "sub", "post", "people", "willing", "build", "public", "share", "unpolished", "work", "tech", "python", "ecosystem", "today", "case", "forget", "update", "majority", "say", "llm", "generate", "project", "make", "sense", "limit", "toxicity", "bleed", "post", "project", "clearly", "vibe", "cod", "exist", "llm", "boom", "occasionally", "moderator", "take", "enable", "behavior", "comment", "have", "explanation", "behavior", "excuse", "behavior", "hopefully", "serve", "reminder", "llm", "spam", "problem", "need", "solve", "disagree", "way"], "num_tokens": 141, "token_loss_pct": 63.75, "normalized_content": "when did destructive criticism become normalized on this sub. its been a while since this sub popped up on my feed. its coming up more recently. im noticing a shocking amount of toxicity on peoples project shares that i didnt notice in the past. any attempt to call out this toxicity is met with a wave of downvotes. for those of you who have been in the reddit echo chamber a little too long let me remind you that it is not normal to mockteasetear down the work that someone did on their own free time for others to see or benefit from. it is normal to offer advice open issues offer reference work to learn from and ask questions to guide the author in the right direction. this is an anonymous platform. the person sharing their work could be a 16 year old who has never seen a production system and is excited about programming or a 30 yoe developer who got bored and just wanted to prove a concept also in their free time. it does not make you a better to default to tearing someone down or mocking their work. you poison the community as a whole when you do so. i am not seeing behavior like this as commonly on other language subs otherwise i would not make this post. the people willing to build in public and share their sometimes unpolished work is what made tech and the python ecosystem what it is today in case any of you have forgotten. update the majority of you are saying its because of llm generated projects. this makes sense to a limit but this toxicity is bleeding into some posts for projects that are clearly are not vibe-coded existed before the llm boom. i will not call anyone by name but i occasionally see moderators taking part or enabling the behavior as well. as someone commented having an explanation for the behavior does not excuse the behavior. hopefully this at least serves as a reminder of that for some of you. the llm spam is a problem that needs to be solved. i disagree that this is the way to do it."}
{"title": "I built a Python UI framework inspired by Streamlit, but with O(1) state updates", "url": "https://www.reddit.com/r/Python/comments/1qh6733/i_built_a_python_ui_framework_inspired_by/", "content": "I built a Python UI framework inspired by Streamlit, but with O(1) state updates. Hey r/Python,\n\nI love Streamlit's simplicity, but the \"full script rerun\" on every interaction drove me crazy. It gets super slow once your app grows, and using `st.cache` everywhere felt like a band-aid.\n\nSo I spent the last few weeks building **Violit**. I wanted something that feels like writing a simple Python script but performs like a modern React app.\n\n**What My Project Does**\n\nViolit is a high-performance Python web framework. It allows you to build interactive web apps using pure Python without the performance penalty of full-page reloads.\n\nIt uses a **\"Zero Rerun\"** architecture based on FastAPI, htmx, and WebSockets. When you interact with a widget (like a button or slider), Violit updates *only* that specific component in **O(1)** time, ensuring no screen flickering and instant feedback. It also supports running your web app into a desktop app (like electron) with a single flag (`--native`).\n\n**Target Audience**\n\n* **Data Scientists &amp; Python Devs:** Who need to build dashboards or internal tools quickly but are frustrated by Streamlit's lag.\n* **Production Use:** It's currently in early Alpha (v0.0.2), so it's best for internal tools, side projects, and early adopters who want to contribute to a faster Python UI ecosystem.\n\n**Comparison**\n\nHere is how Violit differs from existing alternatives:\n\n* **vs. Streamlit:** Violit keeps the intuitive API (90% compatible) but removes the \"Full Script Rerun.\" State updates are O(1) instead of O(N).\n* **vs. Dash:** Violit offers reactive state management without the \"callback hell\" complexity of Dash.\n* **vs. Reflex:** Violit requires **Zero Configuration**. No Node.js dependency, no build steps. Just `pip install` and run. Plus, it has built-in native desktop support.\n* **vs. NiceGUI:** The theme system for the beautiful app. Unlike Streamlit's rigid look or NiceGUI's engineer-first aesthetic, Violit comes with **30+  Themes** out of the box. You can switch from \"cyberpunk\" to \"retro\" styles with a single line of code—no CSS mastery required. **Plus, it's fully extensible—you can easily add your own custom themes via CSS.**\n\n**Code Example**\n\n    import violit as vl\n    ​\n    app = vl.App()\n    count = app.state(0)  # Reactive State\n    ​\n    # No rerun! Only the label updates instantly.\n    app.button(\"Increment\", on_click=lambda: count.set(count.value + 1))\n    app.write(\"Count:\", count)\n    ​\n    app.run()\n\n**Link to Source Code**\n\nIt is open source (MIT License).\n\n* **Repo:** [https://github.com/violit-dev/violit](https://github.com/violit-dev/violit)\n* **PyPI:** `pip install violit`\n* **Example:**\n   * [demo showcase source code](https://github.com/violit-dev/violit/blob/main/examples/1_demo_showcase/demo_showcase.py)\n   * [(Tutorial) Build Your Own Blog in 10 Minutes with Violit!](https://github.com/violit-dev/violit/tree/main/examples/2_violit_blog)\n\nI'd love to hear your feedback!", "source": "Reddit", "date": "2026-01-19T15:52:16", "author": "Puzzleheaded_Clerk68", "score": 133, "tokens": ["build", "python", "ui", "framework", "inspire", "streamlit", "o1", "state", "update", "hey", "rpython", "love", "streamlit", "simplicity", "script", "rerun", "interaction", "drive", "crazy", "get", "super", "slow", "app", "grow", "st.cache", "feel", "like", "band", "aid", "spend", "week", "building", "violit", "want", "feel", "like", "write", "simple", "python", "script", "perform", "like", "modern", "react", "app", "project", "violit", "high", "performance", "python", "web", "framework", "allow", "build", "interactive", "web", "app", "pure", "python", "performance", "penalty", "page", "reload", "use", "zero", "rerun", "architecture", "base", "fastapi", "htmx", "websocket", "interact", "widget", "like", "button", "slider", "violit", "update", "specific", "component", "o1", "time", "ensure", "screen", "flickering", "instant", "feedback", "support", "run", "web", "app", "desktop", "app", "like", "electron", "single", "flag", "--native", "target", "audience", "datum", "scientist", "amp", "python", "devs", "need", "build", "dashboard", "internal", "tool", "quickly", "frustrate", "streamlit", "lag", "production", "use", "currently", "early", "alpha", "v0.0.2", "good", "internal", "tool", "project", "early", "adopter", "want", "contribute", "fast", "python", "ui", "ecosystem", "comparison", "violit", "differ", "exist", "alternative", "vs.", "streamlit", "violit", "keep", "intuitive", "api", "90", "compatible", "remove", "script", "rerun", "state", "update", "o1", "instead", "vs.", "dash", "violit", "offer", "reactive", "state", "management", "callback", "hell", "complexity", "dash", "vs.", "reflex", "violit", "require", "zero", "configuration", "node.js", "dependency", "build", "step", "pip", "install", "run", "plus", "build", "native", "desktop", "support", "vs.", "nicegui", "theme", "system", "beautiful", "app", "unlike", "streamlit", "rigid", "look", "nicegui", "engineer", "aesthetic", "violit", "come", "30", "theme", "box", "switch", "cyberpunk", "retro", "style", "single", "line", "codeno", "css", "mastery", "require", "plus", "fully", "extensibleyou", "easily", "add", "custom", "theme", "css", "code", "example", "import", "violit", "vl", "app", "vl.app", "count", "app.state0", "reactive", "state", "  ", "rerun", "label", "update", "instantly", "app.buttonincrement", "on_clicklambda", "count.setcount.value", "app.writecount", "count", "app.run", "link", "source", "code", "open", "source", "mit", "license", "repo", "url", "pypi", "pip", "install", "violit", "example", "demo", "showcase", "source", "codeurl", "tutorial", "build", "blog", "10", "minute", "violiturl", "love", "hear", "feedback"], "num_tokens": 266, "token_loss_pct": 42.17, "normalized_content": "i built a python ui framework inspired by streamlit but with o1 state updates. hey rpython i love streamlit's simplicity but the full script rerun on every interaction drove me crazy. it gets super slow once your app grows and using st.cache everywhere felt like a band-aid. so i spent the last few weeks building violit. i wanted something that feels like writing a simple python script but performs like a modern react app. what my project does violit is a high-performance python web framework. it allows you to build interactive web apps using pure python without the performance penalty of full-page reloads. it uses a zero rerun architecture based on fastapi htmx and websockets. when you interact with a widget like a button or slider violit updates only that specific component in o1 time ensuring no screen flickering and instant feedback. it also supports running your web app into a desktop app like electron with a single flag --native. target audience  data scientists amp python devs who need to build dashboards or internal tools quickly but are frustrated by streamlit's lag.  production use it's currently in early alpha v0.0.2 so it's best for internal tools side projects and early adopters who want to contribute to a faster python ui ecosystem. comparison here is how violit differs from existing alternatives  vs. streamlit violit keeps the intuitive api 90 compatible but removes the full script rerun. state updates are o1 instead of on.  vs. dash violit offers reactive state management without the callback hell complexity of dash.  vs. reflex violit requires zero configuration. no node.js dependency no build steps. just pip install and run. plus it has built-in native desktop support.  vs. nicegui the theme system for the beautiful app. unlike streamlit's rigid look or nicegui's engineer-first aesthetic violit comes with 30 themes out of the box. you can switch from cyberpunk to retro styles with a single line of codeno css mastery required. plus it's fully extensibleyou can easily add your own custom themes via css. code example import violit as vl  app  vl.app count  app.state0  reactive state   no rerun only the label updates instantly. app.buttonincrement on_clicklambda count.setcount.value  1 app.writecount count  app.run link to source code it is open source mit license.  repo url  pypi pip install violit  example  demo showcase source codeurl  tutorial build your own blog in 10 minutes with violiturl i'd love to hear your feedback"}
{"title": "Tracking 13,000 satellites in under 3 seconds from Python", "url": "https://www.reddit.com/r/Python/comments/1qif5o1/tracking_13000_satellites_in_under_3_seconds_from/", "content": "Tracking 13,000 satellites in under 3 seconds from Python.  I've been working on [https://github.com/ATTron/astroz](https://github.com/ATTron/astroz), an orbital mechanics toolkit with Python bindings. The core is written in Zig with SIMD vectorization.\n\n# What My Project Does\n\nastroz is an astrodynamics toolkit, including propagating satellite orbits using the SGP4 algorithm. It writes directly to numpy arrays, so there's very little overhead going between Python and Zig. You can propagate 13,000+ satellites in under 3 seconds.\n\npip install astroz is all you need to get started!\n\n# Target Audience\n\nAnyone doing orbital mechanics, satellite tracking, or space situational awareness work in Python. It's production-ready. I'm using it myself and the API is stable, though I'm still adding more functionality to the Python bindings.\n\n# Comparison\n\nIt's about 2-3x faster than python-sgp4, far and away the most popular sgp4 implementation being used:\n\n|Library|Throughput|\n|:-|:-|\n|astroz|\\~8M props/sec|\n|python-sgp4|\\~3M props/sec|\n\n# Demo &amp; Links\n\nIf you want to see it in action, I put together a live demo that visualizes all 13,000+ active satellites generated from Python in under 3 seconds: [https://attron.github.io/astroz-demo/](https://attron.github.io/astroz-demo/)\n\nAlso wrote a blog post about how the SIMD stuff works under the hood if you're into that, but it's more Zig heavy than Python: [https://atempleton.bearblog.dev/i-made-zig-compute-33-million-satellite-positions-in-3-seconds-no-gpu-required/](https://atempleton.bearblog.dev/i-made-zig-compute-33-million-satellite-positions-in-3-seconds-no-gpu-required/)\n\nRepo: [https://github.com/ATTron/astroz](https://github.com/ATTron/astroz)", "source": "Reddit", "date": "2026-01-20T23:06:02", "author": "Frozen_Poseidon", "score": 117, "tokens": ["track", "13000", "satellite", "second", "python", "work", "url", "orbital", "mechanic", "toolkit", "python", "binding", "core", "write", "zig", "simd", "vectorization", "project", "astroz", "astrodynamic", "toolkit", "include", "propagate", "satellite", "orbit", "sgp4", "algorithm", "write", "directly", "numpy", "array", "little", "overhead", "go", "python", "zig", "propagate", "13000", "satellite", "second", "pip", "install", "astroz", "need", "start", "target", "audience", "orbital", "mechanic", "satellite", "tracking", "space", "situational", "awareness", "work", "python", "production", "ready", "api", "stable", "add", "functionality", "python", "binding", "comparison", "3x", "fast", "python", "sgp4", "far", "away", "popular", "sgp4", "implementation", "librarythroughput", "astroz8", "propssec", "python", "sgp43", "propssec", "demo", "amp", "link", "want", "action", "live", "demo", "visualize", "13000", "active", "satellite", "generate", "python", "second", "url", "write", "blog", "post", "simd", "stuff", "work", "hood", "zig", "heavy", "python", "url", "repo", "url"], "num_tokens": 108, "token_loss_pct": 53.25, "normalized_content": "tracking 13000 satellites in under 3 seconds from python. i've been working on url an orbital mechanics toolkit with python bindings. the core is written in zig with simd vectorization.  what my project does astroz is an astrodynamics toolkit including propagating satellite orbits using the sgp4 algorithm. it writes directly to numpy arrays so there's very little overhead going between python and zig. you can propagate 13000 satellites in under 3 seconds. pip install astroz is all you need to get started  target audience anyone doing orbital mechanics satellite tracking or space situational awareness work in python. it's production-ready. i'm using it myself and the api is stable though i'm still adding more functionality to the python bindings.  comparison it's about 2-3x faster than python-sgp4 far and away the most popular sgp4 implementation being used librarythroughput -- astroz8m propssec python-sgp43m propssec  demo amp links if you want to see it in action i put together a live demo that visualizes all 13000 active satellites generated from python in under 3 seconds url also wrote a blog post about how the simd stuff works under the hood if you're into that but it's more zig heavy than python url repo url"}
{"title": "Robyn (finally) supports Python 3.14 🎉", "url": "https://www.reddit.com/r/Python/comments/1qgai08/robyn_finally_supports_python_314/", "content": "Robyn (finally) supports Python 3.14 🎉. For the unaware - [Robyn](https://github.com/sparckles/Robyn) is a fast, async Python web framework built on a Rust runtime.\n\nPython 3.14 support has been pending for a while.\n\nWanted to share it with folks outside the Robyn community.\n\nYou can check out the release at - [https://github.com/sparckles/Robyn/releases/tag/v0.74.0](https://github.com/sparckles/Robyn/releases/tag/v0.74.0)", "source": "Reddit", "date": "2026-01-18T15:50:34", "author": "stealthanthrax", "score": 99, "tokens": ["robyn", "finally", "support", "python", "3.14", "unaware", "robynurl", "fast", "async", "python", "web", "framework", "build", "rust", "runtime", "python", "3.14", "support", "pende", "want", "share", "folk", "outside", "robyn", "community", "check", "release", "url"], "num_tokens": 28, "token_loss_pct": 48.15, "normalized_content": "robyn finally supports python 3.14 . for the unaware - robynurl is a fast async python web framework built on a rust runtime. python 3.14 support has been pending for a while. wanted to share it with folks outside the robyn community. you can check out the release at - url"}
{"title": "I built bytes.replace() for CUDA - process multi-GB files without leaving the GPU", "url": "https://www.reddit.com/r/Python/comments/1qh1ekg/i_built_bytesreplace_for_cuda_process_multigb/", "content": "I built bytes.replace() for CUDA - process multi-GB files without leaving the GPU. Built a CUDA kernel that does Python's `bytes.replace()` on the GPU without CPU transfers.\n\n**Performance (RTX 3090):**\n\n    Benchmark                      | Size       | CPU (ms)     | GPU (ms)   | Speedup\n    -----------------------------------------------------------------------------------\n    Dense/Small (1MB)              | 1.0 MB     |   3.03       |   2.79     |  1.09x\n    Expansion (5MB, 2x growth)     | 5.0 MB     |  22.08       |  12.28     |  1.80x\n    Large/Dense (50MB)             | 50.0 MB    | 192.64       |  56.16     |  3.43x\n    Huge/Sparse (100MB)            | 100.0 MB   | 492.07       | 112.70     |  4.37x\n    \n    Average: 3.45x faster | 0.79 GB/s throughput\n\n**Features:**\n\n* Exact Python semantics (leftmost, non-overlapping)\n* Streaming mode for files larger than GPU memory\n* Session API for chained replacements\n* Thread-safe\n\n**Example:**\n\npython\n\n    from cuda_replace_wrapper import CudaReplaceLib\n    \n    lib = CudaReplaceLib('./cuda_replace.dll')\n    result = lib.unified(data, b\"pattern\", b\"replacement\")\n    \n    # Or streaming for huge files\n    cleaned = gpu_replace_streaming(lib, huge_data, pairs, chunk_bytes=256*1024*1024)\n\nBuilt this for a custom compression algorithm. Includes Python wrapper, benchmark suite, and pre-built binaries.\n\nGitHub: [https://github.com/RAZZULLIX/cuda\\_replace](https://github.com/RAZZULLIX/cuda_replace)", "source": "Reddit", "date": "2026-01-19T12:10:04", "author": "andreabarbato", "score": 61, "tokens": ["build", "bytes.replace", "cuda", "process", "multi", "gb", "file", "leave", "gpu", "build", "cuda", "kernel", "python", "bytes.replace", "gpu", "cpu", "transfer", "performance", "rtx", "3090", "benchmark", "size", "cpu", "ms", "gpu", "ms", "speedup", "densesmall", "mb", "1.0", "mb", "3.03", "2.79", "1.09x", "expansion", "mb", "2x", "growth", "5.0", "mb", "22.08", "12.28", "1.80x", "largedense", "50", "mb", "50.0", "mb", "192.64", "56.16", "3.43x", "hugesparse", "100", "mb", "100.0", "mb", "492.07", "112.70", "4.37x", "average", "3.45x", "fast", "0.79", "gbs", "throughput", "feature", "exact", "python", "semantic", "leftmost", "non", "overlapping", "stream", "mode", "file", "large", "gpu", "memory", "session", "api", "chained", "replacement", "thread", "safe", "example", "python", "cuda_replace_wrapper", "import", "cudareplacelib", "lib", "cudareplacelib'.cuda_replace.dll", "result", "lib.unifieddata", "bpattern", "breplacement", "stream", "huge", "file", "clean", "gpu_replace_streaminglib", "huge_data", "pair", "chunk_bytes25610241024", "build", "custom", "compression", "algorithm", "include", "python", "wrapper", "benchmark", "suite", "pre", "build", "binary", "github", "url"], "num_tokens": 117, "token_loss_pct": 35.0, "normalized_content": "i built bytes.replace for cuda - process multi-gb files without leaving the gpu. built a cuda kernel that does python's bytes.replace on the gpu without cpu transfers. performance rtx 3090 benchmark  size  cpu ms  gpu ms  speedup ----------------------------------------------------------------------------------- densesmall 1mb  1.0 mb  3.03  2.79  1.09x expansion 5mb 2x growth  5.0 mb  22.08  12.28  1.80x largedense 50mb  50.0 mb  192.64  56.16  3.43x hugesparse 100mb  100.0 mb  492.07  112.70  4.37x average 3.45x faster  0.79 gbs throughput features  exact python semantics leftmost non-overlapping  streaming mode for files larger than gpu memory  session api for chained replacements  thread-safe example python from cuda_replace_wrapper import cudareplacelib lib  cudareplacelib'.cuda_replace.dll' result  lib.unifieddata bpattern breplacement  or streaming for huge files cleaned  gpu_replace_streaminglib huge_data pairs chunk_bytes25610241024 built this for a custom compression algorithm. includes python wrapper benchmark suite and pre-built binaries. github url"}
{"title": "What Python Tools Do You Use for Data Visualization and Why?", "url": "https://www.reddit.com/r/Python/comments/1qeq7c7/what_python_tools_do_you_use_for_data/", "content": "What Python Tools Do You Use for Data Visualization and Why?. Data visualization is crucial for interpreting complex datasets, and Python offers a variety of tools to accomplish this. I'm curious to know which libraries or frameworks you prefer for data visualization and what features make them stand out for you. For instance, do you lean towards Matplotlib for its flexibility, Seaborn for its ease of use, or perhaps Plotly for interactive plots? Additionally, how do you handle specific challenges, such as customizing visualizations or integrating them into web applications? Sharing your experiences and use cases could be beneficial for those looking to enhance their data storytelling skills. Let's discuss the strengths and weaknesses of different tools and any tips you may have for getting the most out of them.", "source": "Reddit", "date": "2026-01-16T20:49:39", "author": "Confident_Compote_39", "score": 49, "tokens": ["python", "tool", "use", "datum", "visualization", "datum", "visualization", "crucial", "interpret", "complex", "dataset", "python", "offer", "variety", "tool", "accomplish", "curious", "know", "library", "framework", "prefer", "datum", "visualization", "feature", "stand", "instance", "lean", "matplotlib", "flexibility", "seaborn", "ease", "use", "plotly", "interactive", "plot", "additionally", "handle", "specific", "challenge", "customizing", "visualization", "integrate", "web", "application", "share", "experience", "use", "case", "beneficial", "look", "enhance", "datum", "storytelling", "skill", "let", "discuss", "strength", "weakness", "different", "tool", "tip", "get"], "num_tokens": 62, "token_loss_pct": 54.74, "normalized_content": "what python tools do you use for data visualization and why. data visualization is crucial for interpreting complex datasets and python offers a variety of tools to accomplish this. i'm curious to know which libraries or frameworks you prefer for data visualization and what features make them stand out for you. for instance do you lean towards matplotlib for its flexibility seaborn for its ease of use or perhaps plotly for interactive plots additionally how do you handle specific challenges such as customizing visualizations or integrating them into web applications sharing your experiences and use cases could be beneficial for those looking to enhance their data storytelling skills. let's discuss the strengths and weaknesses of different tools and any tips you may have for getting the most out of them."}
{"title": "An open-source tool to add \"Word Wise\" style definitions to any EPUB using Python", "url": "https://www.reddit.com/r/Python/comments/1qff835/an_opensource_tool_to_add_word_wise_style/", "content": "An open-source tool to add \"Word Wise\" style definitions to any EPUB using Python. I've been trying to read more English books, but constantly stopping to look up difficult words breaks my flow. I really liked Kindle's \"Word Wise\" feature, but it doesn't work on sideloaded books.\n\nSo, I built Sura. It's a Python tool that injects ruby text definitions directly into EPUB files.\n\nRepo: https://github.com/watsuyo/Sura\n\n## What My Project Does\n\nSura processes EPUB files to help language learners read more smoothly. Specifically, it:\n\n1. Extracts text from an EPUB file.\n2. Filters words based on difficulty using wordfreq (Zipf scores), so it only targets words you likely don't know.\n3. Generates definitions using an LLM (OpenAI/compatible API) to provide short, context-aware meanings.\n4. Injects ruby text (HTML/CSS) back into the EPUB structure.\n5. Rebuilds the EPUB, making it compatible with almost any e-reader (Kobo, Kindle, etc.).\n\nIt uses asyncio for concurrent processing to keep performance reasonably fast.\n\n## Target Audience\n\nThis tool is meant for language learners who want to read native content without constant dictionary interruptions, and e-reader users (Kindle) who sideload their books.\n\nIt is currently a hobby/open-source project intended for personal use and for developers interested in EPUB manipulation or LLM integration.\n\n## Comparison\n\nThe main alternative is Kindle's native \"Word Wise\" feature.\n\nKindle Word Wise: Only works on books purchased directly from Amazon. It does not support sideloaded documents or other devices like Kobo.\n\nSura: Works on any DRM-free EPUB file, allowing you to use the feature on sideloaded books and non-Kindle devices. It also allows for customizable difficulty thresholds, unlike the fixed settings on Kindle.", "source": "Reddit", "date": "2026-01-17T15:54:53", "author": "Classic_Method_5547", "score": 39, "tokens": ["open", "source", "tool", "add", "word", "wise", "style", "definition", "epub", "python", "try", "read", "english", "book", "constantly", "stop", "look", "difficult", "word", "break", "flow", "like", "kindle", "word", "wise", "feature", "work", "sideloaded", "book", "build", "sura", "python", "tool", "inject", "ruby", "text", "definition", "directly", "epub", "file", "repo", "url", "project", "sura", "process", "epub", "file", "help", "language", "learner", "read", "smoothly", "specifically", "extract", "text", "epub", "file", "filter", "word", "base", "difficulty", "wordfreq", "zipf", "score", "target", "word", "likely", "know", "generate", "definition", "llm", "openaicompatible", "api", "provide", "short", "context", "aware", "meaning", "inject", "ruby", "text", "htmlcss", "epub", "structure", "rebuild", "epub", "make", "compatible", "reader", "kobo", "kindle", "etc", "use", "asyncio", "concurrent", "processing", "performance", "reasonably", "fast", "target", "audience", "tool", "mean", "language", "learner", "want", "read", "native", "content", "constant", "dictionary", "interruption", "reader", "user", "kindle", "sideload", "book", "currently", "hobbyopen", "source", "project", "intend", "personal", "use", "developer", "interest", "epub", "manipulation", "llm", "integration", "comparison", "main", "alternative", "kindle", "native", "word", "wise", "feature", "kindle", "word", "wise", "work", "book", "purchase", "directly", "amazon", "support", "sideloade", "document", "device", "like", "kobo", "sura", "work", "drm", "free", "epub", "file", "allow", "use", "feature", "sideloade", "book", "non", "kindle", "device", "allow", "customizable", "difficulty", "threshold", "unlike", "fix", "setting", "kindle"], "num_tokens": 174, "token_loss_pct": 44.76, "normalized_content": "an open-source tool to add word wise style definitions to any epub using python. i've been trying to read more english books but constantly stopping to look up difficult words breaks my flow. i really liked kindle's word wise feature but it doesn't work on sideloaded books. so i built sura. it's a python tool that injects ruby text definitions directly into epub files. repo url  what my project does sura processes epub files to help language learners read more smoothly. specifically it 1. extracts text from an epub file. 2. filters words based on difficulty using wordfreq zipf scores so it only targets words you likely don't know. 3. generates definitions using an llm openaicompatible api to provide short context-aware meanings. 4. injects ruby text htmlcss back into the epub structure. 5. rebuilds the epub making it compatible with almost any e-reader kobo kindle etc.. it uses asyncio for concurrent processing to keep performance reasonably fast.  target audience this tool is meant for language learners who want to read native content without constant dictionary interruptions and e-reader users kindle who sideload their books. it is currently a hobbyopen-source project intended for personal use and for developers interested in epub manipulation or llm integration.  comparison the main alternative is kindle's native word wise feature. kindle word wise only works on books purchased directly from amazon. it does not support sideloaded documents or other devices like kobo. sura works on any drm-free epub file allowing you to use the feature on sideloaded books and non-kindle devices. it also allows for customizable difficulty thresholds unlike the fixed settings on kindle."}
{"title": "Teaching services online for kids/teenagers?", "url": "https://www.reddit.com/r/Python/comments/1qcos9u/teaching_services_online_for_kidsteenagers/", "content": "Teaching services online for kids/teenagers?. My son (13) is interested in programming. I would like to sign him up for some introductory (and fun for teenagers) online program. Are there any that you’ve seen that you’d be able to recommend. Paid or unpaid are fine.", "source": "Reddit", "date": "2026-01-14T15:33:04", "author": "CodeVirus", "score": 31, "tokens": ["teach", "service", "online", "kidsteenager", "son", "13", "interested", "programming", "like", "sign", "introductory", "fun", "teenager", "online", "program", "ve", "see", "able", "recommend", "pay", "unpaid", "fine"], "num_tokens": 22, "token_loss_pct": 57.69, "normalized_content": "teaching services online for kidsteenagers. my son 13 is interested in programming. i would like to sign him up for some introductory and fun for teenagers online program. are there any that youve seen that youd be able to recommend. paid or unpaid are fine."}
{"title": "Opticol: memory optimized python collections", "url": "https://www.reddit.com/r/Python/comments/1qhl2o2/opticol_memory_optimized_python_collections/", "content": "Opticol: memory optimized python collections. Hi everyone,\n\nI just created a new library called [opticol](https://github.com/lessico/opticol/) (which stands for optimized collections), which I wanted to share with the community. The idea of the library is to create space optimized versions of Sequence, Set, and Mapping for small collections leveraging the collections.ABC vocabulary.\n\n## What My Project Does\nCreates optimized versions of the main python collection types (Sequence, Set, Mapping) along with vocabulary types and convenience methods for transforming builtins to the optimized type.\n\nFor collections of size 3 or less, it is pretty trivial (using slots) to create an object that can act as a collection, but uses notably less memory than the builtins. Consider the fact that an empty set requires 216 bytes, or a dictionary with one element requires 224 bytes. Applications that create many (on the order of 100k to a million) of these objects can substantially reduce their memory usage with this library.\n\n## Target Audience\n\nThis will benefit users who use Python for various forms of data analysis. These problems often have many collection instances, which can often be just a few items. I myself have run into issues with memory pressure like this with some NLP datasets. Additionally, this is helpful for those doing this primarily in Python or for situations where dropping to a lower level language is not advantageous yet.\n\n## Comparison\n\nI could not find a similar library to this, nor even discussion of implementing such an idea. I would be happy to update this section if something comes up, but as far as I know, there are no direct comparisons.\n\nAnyway, it's currently a beta release as I'm working on finishing up the last unit tests, but the main use case generally works. I'm also very interested in any feedback on the project itself or other optimizations that may be good to add!", "source": "Reddit", "date": "2026-01-20T00:53:38", "author": "matgrioni", "score": 26, "tokens": ["opticol", "memory", "optimize", "python", "collection", "hi", "create", "new", "library", "call", "opticolurl", "stand", "optimize", "collection", "want", "share", "community", "idea", "library", "create", "space", "optimize", "version", "sequence", "set", "mapping", "small", "collection", "leverage", "collections.abc", "vocabulary", "project", "create", "optimize", "version", "main", "python", "collection", "type", "sequence", "set", "mapping", "vocabulary", "type", "convenience", "method", "transform", "builtin", "optimize", "type", "collection", "size", "pretty", "trivial", "slot", "create", "object", "act", "collection", "use", "notably", "memory", "builtin", "consider", "fact", "set", "require", "216", "byte", "dictionary", "element", "require", "224", "byte", "application", "create", "order", "100k", "million", "object", "substantially", "reduce", "memory", "usage", "library", "target", "audience", "benefit", "user", "use", "python", "form", "datum", "analysis", "problem", "collection", "instance", "item", "run", "issue", "memory", "pressure", "like", "nlp", "dataset", "additionally", "helpful", "primarily", "python", "situation", "drop", "low", "level", "language", "advantageous", "comparison", "find", "similar", "library", "discussion", "implement", "idea", "happy", "update", "section", "come", "far", "know", "direct", "comparison", "currently", "beta", "release", "work", "finish", "unit", "test", "main", "use", "case", "generally", "work", "interested", "feedback", "project", "optimization", "good", "add"], "num_tokens": 148, "token_loss_pct": 55.02, "normalized_content": "opticol memory optimized python collections. hi everyone i just created a new library called opticolurl which stands for optimized collections which i wanted to share with the community. the idea of the library is to create space optimized versions of sequence set and mapping for small collections leveraging the collections.abc vocabulary.  what my project does creates optimized versions of the main python collection types sequence set mapping along with vocabulary types and convenience methods for transforming builtins to the optimized type. for collections of size 3 or less it is pretty trivial using slots to create an object that can act as a collection but uses notably less memory than the builtins. consider the fact that an empty set requires 216 bytes or a dictionary with one element requires 224 bytes. applications that create many on the order of 100k to a million of these objects can substantially reduce their memory usage with this library.  target audience this will benefit users who use python for various forms of data analysis. these problems often have many collection instances which can often be just a few items. i myself have run into issues with memory pressure like this with some nlp datasets. additionally this is helpful for those doing this primarily in python or for situations where dropping to a lower level language is not advantageous yet.  comparison i could not find a similar library to this nor even discussion of implementing such an idea. i would be happy to update this section if something comes up but as far as i know there are no direct comparisons. anyway it's currently a beta release as i'm working on finishing up the last unit tests but the main use case generally works. i'm also very interested in any feedback on the project itself or other optimizations that may be good to add"}
{"title": "Network monitoring dashboard built with Flask, scapy, and nmap", "url": "https://www.reddit.com/r/Python/comments/1qi4o7p/network_monitoring_dashboard_built_with_flask/", "content": "Network monitoring dashboard built with Flask, scapy, and nmap. built a home network monitor as a learning project useful to anyone.\n\n\\- what it does: monitors local network in real time, tracks devices, bandwidth usage per device, and detects anomalies like new unknown devices or suspicious traffic patterns.\n\n\\- target audience: educational/homelab project, not production ready. built for learning networking fundamentals and packet analysis. runs on any linux machine, good for raspberry pi setups.\n\n\\- comparison: most alternatives are either commercial closed source like fing or heavyweight enterprise tools like ntopng. this is intentionally simple and focused on learning. everything runs locally, no cloud, full control. anomaly detection is basic rule based so you can actually understand what triggers alerts, not black box ml.\n\ntech stack used:\n\n* flask for web backend + api\n* scapy for packet sniffing / bandwidth monitoring\n* python-nmap for device discovery\n* sqlite for data persistence\n* chart.js for visualization\n\nit was a good way to learn about networking protocols, concurrent packet processing, and building a full stack monitoring application from scratch.\n\ncode + screenshots: [https://github.com/torchiachristian/HomeNetMonitor](https://github.com/torchiachristian/HomeNetMonitor)\n\nfeedback welcome, especially on the packet sniffing implementation and anomaly detection logic", "source": "Reddit", "date": "2026-01-20T16:49:52", "author": "christiantorchia", "score": 27, "tokens": ["network", "monitor", "dashboard", "build", "flask", "scapy", "nmap", "build", "home", "network", "monitor", "learn", "project", "useful", "monitor", "local", "network", "real", "time", "track", "device", "bandwidth", "usage", "device", "detect", "anomaly", "like", "new", "unknown", "device", "suspicious", "traffic", "pattern", "target", "audience", "educationalhomelab", "project", "production", "ready", "build", "learn", "network", "fundamental", "packet", "analysis", "run", "linux", "machine", "good", "raspberry", "pi", "setup", "comparison", "alternative", "commercial", "closed", "source", "like", "fing", "heavyweight", "enterprise", "tool", "like", "ntopng", "intentionally", "simple", "focus", "learn", "run", "locally", "cloud", "control", "anomaly", "detection", "basic", "rule", "base", "actually", "understand", "trigger", "alert", "black", "box", "ml", "tech", "stack", "flask", "web", "backend", "api", "scapy", "packet", "sniff", "bandwidth", "monitor", "python", "nmap", "device", "discovery", "sqlite", "datum", "persistence", "chart.js", "visualization", "good", "way", "learn", "network", "protocol", "concurrent", "packet", "processing", "build", "stack", "monitoring", "application", "scratch", "code", "screenshot", "url", "feedback", "welcome", "especially", "packet", "sniff", "implementation", "anomaly", "detection", "logic"], "num_tokens": 129, "token_loss_pct": 37.68, "normalized_content": "network monitoring dashboard built with flask scapy and nmap. built a home network monitor as a learning project useful to anyone. - what it does monitors local network in real time tracks devices bandwidth usage per device and detects anomalies like new unknown devices or suspicious traffic patterns. - target audience educationalhomelab project not production ready. built for learning networking fundamentals and packet analysis. runs on any linux machine good for raspberry pi setups. - comparison most alternatives are either commercial closed source like fing or heavyweight enterprise tools like ntopng. this is intentionally simple and focused on learning. everything runs locally no cloud full control. anomaly detection is basic rule based so you can actually understand what triggers alerts not black box ml. tech stack used  flask for web backend  api  scapy for packet sniffing  bandwidth monitoring  python-nmap for device discovery  sqlite for data persistence  chart.js for visualization it was a good way to learn about networking protocols concurrent packet processing and building a full stack monitoring application from scratch. code  screenshots url feedback welcome especially on the packet sniffing implementation and anomaly detection logic"}
{"title": "Please recommend a front-end framework/package", "url": "https://www.reddit.com/r/Python/comments/1qdz1qu/please_recommend_a_frontend_frameworkpackage/", "content": "Please recommend a front-end framework/package. I'm building an app with streamlit.\n&gt; Why streamlit?\n\nBecause I have no frontend experience and streamlit helped me get off the ground pretty quickly. Also, I'm simultaneously deploying to web and desktop,  and streamlit lets me do this with just the one codebase (I intend to use something like PyInstaller for distribution)\n\n\nI have different \"[expanders](https://docs.streamlit.io/develop/api-reference/layout/st.expander)\" in my streamlit application. Each expander has some data/input elements in it (in the case of my most recent problem, it's a `data_editor`). Sometimes, I need one element to update in response to the user clicking on \"Save Changes\" in a different part of the application. If they were both in the same fragment, I could just do `st.rerun(scope='fragment')`. But since they're not, I have no other choice but to do `st.rerun()`. But if there's incorrect input, I write an error message, which gets subsequently erased due to the rerun. Now I know that I can store this stuff in `st.session_state` and add additional logic to \"recreate\" the (prior) error-message state of the app, but that adds a lot of complexity.\n\nSince there is no way to `st.rerun()` a different fragment than the one I'm in, it looks like I have to give up streamlit - about time, I've been writing workarounds/hacks for a lot of streamlit stumbling blocks.\n\nSo, would anyone be able to recommend an alternative to streamlit? These are the criteria to determine viability of an alternative:\n\n1. ability to control the layout of my elements and programmatically refresh specific elements on demand\n1. web and desktop deployments from the same codebase\n  1. bonus points for being able to handle mobile deployments as well\n1. Python API - I can learn another language if the learning curve is fast. That takes Node/React out of the realm of possibility\n1. somewhat mature - I started using streamlit back in v0.35 or so. But now I'm using v1.52. While streamlit hasn't been around for as long as React, v1.52 is sufficiently mature. I doubt a flashy new frontend framework (eg: with current version 0.43) would have had enough time to iron out the bugs if it's only been around for a very short period of time (eg: 6 months).\n1. ideally something you have experience with and can therefore speak confidently to its stability/reliability\n\nI'm currently considering:\n\n1. [flet](https://flet.dev/): hasn't been around for very long - anyone know if it's any good?\n1. [NiceGUI](https://nicegui.io)\n1. [Reflex](https://github.com/reflex-dev/reflex)\n\nIf anyone has any thoughts or suggestions, I'd love them\n\nThank you", "source": "Reddit", "date": "2026-01-16T00:10:21", "author": "inspectorG4dget", "score": 20, "tokens": ["recommend", "end", "frameworkpackage", "build", "app", "streamlit", "gt", "streamlit", "frontend", "experience", "streamlit", "help", "ground", "pretty", "quickly", "simultaneously", "deploy", "web", "desktop", "streamlit", "let", "codebase", "intend", "use", "like", "pyinstaller", "distribution", "different", "expandersurl", "streamlit", "application", "expander", "datainput", "element", "case", "recent", "problem", "data_editor", "need", "element", "update", "response", "user", "click", "save", "change", "different", "application", "fragment", "st.rerunscope'fragment", "choice", "st.rerun", "incorrect", "input", "write", "error", "message", "get", "subsequently", "erase", "rerun", "know", "store", "stuff", "st.session_state", "add", "additional", "logic", "recreate", "prior", "error", "message", "state", "app", "add", "lot", "complexity", "way", "st.rerun", "different", "fragment", "look", "like", "streamlit", "time", "write", "workaroundshack", "lot", "streamlit", "stumbling", "block", "able", "recommend", "alternative", "streamlit", "criterion", "determine", "viability", "alternative", "ability", "control", "layout", "element", "programmatically", "refresh", "specific", "element", "demand", "web", "desktop", "deployment", "codebase", "bonus", "point", "able", "handle", "mobile", "deployment", "python", "api", "learn", "language", "learning", "curve", "fast", "take", "nodereact", "realm", "possibility", "somewhat", "mature", "start", "streamlit", "v0.35", "v1.52", "streamlit", "long", "react", "v1.52", "sufficiently", "mature", "doubt", "flashy", "new", "frontend", "framework", "eg", "current", "version", "0.43", "time", "iron", "bug", "short", "period", "time", "eg", "month", "ideally", "experience", "speak", "confidently", "stabilityreliability", "currently", "consider", "fleturl", "long", "know", "good", "niceguiurl", "reflexurl", "thought", "suggestion", "love", "thank"], "num_tokens": 175, "token_loss_pct": 62.37, "normalized_content": "please recommend a front-end frameworkpackage. i'm building an app with streamlit. gt why streamlit because i have no frontend experience and streamlit helped me get off the ground pretty quickly. also i'm simultaneously deploying to web and desktop and streamlit lets me do this with just the one codebase i intend to use something like pyinstaller for distribution i have different expandersurl in my streamlit application. each expander has some datainput elements in it in the case of my most recent problem it's a data_editor. sometimes i need one element to update in response to the user clicking on save changes in a different part of the application. if they were both in the same fragment i could just do st.rerunscope'fragment'. but since they're not i have no other choice but to do st.rerun. but if there's incorrect input i write an error message which gets subsequently erased due to the rerun. now i know that i can store this stuff in st.session_state and add additional logic to recreate the prior error-message state of the app but that adds a lot of complexity. since there is no way to st.rerun a different fragment than the one i'm in it looks like i have to give up streamlit - about time i've been writing workaroundshacks for a lot of streamlit stumbling blocks. so would anyone be able to recommend an alternative to streamlit these are the criteria to determine viability of an alternative 1. ability to control the layout of my elements and programmatically refresh specific elements on demand 1. web and desktop deployments from the same codebase 1. bonus points for being able to handle mobile deployments as well 1. python api - i can learn another language if the learning curve is fast. that takes nodereact out of the realm of possibility 1. somewhat mature - i started using streamlit back in v0.35 or so. but now i'm using v1.52. while streamlit hasn't been around for as long as react v1.52 is sufficiently mature. i doubt a flashy new frontend framework eg with current version 0.43 would have had enough time to iron out the bugs if it's only been around for a very short period of time eg 6 months. 1. ideally something you have experience with and can therefore speak confidently to its stabilityreliability i'm currently considering 1. fleturl hasn't been around for very long - anyone know if it's any good 1. niceguiurl 1. reflexurl if anyone has any thoughts or suggestions i'd love them thank you"}
{"title": "unwrappy: Rust-inspired Result and Option types with lazy async chaining for Python", "url": "https://www.reddit.com/r/Python/comments/1qh1ros/unwrappy_rustinspired_result_and_option_types/", "content": "unwrappy: Rust-inspired Result and Option types with lazy async chaining for Python. I built a library that brings Rust's `Result` and `Option` types to Python, with lazy evaluation for clean async operation chaining (inspired by Polars' deferred execution).\n\n### What My Project Does\n\n**unwrappy** provides:\n\n- **Result[T, E]** - Success (`Ok`) or failure (`Err`) - errors as values, not exceptions\n- **Option[T]** - Presence (`Some`) or absence (`Nothing`) - explicit optionality\n- **LazyResult / LazyOption** - Build async pipelines without nested awaits\n\n```python\nfrom unwrappy import Ok, Err, Some, NOTHING, LazyResult\n\n# Pattern matching (Python 3.10+)\nmatch divide(10, 0):\n    case Ok(value):\n        print(f\"Result: {value}\")\n    case Err(error):\n        print(f\"Error: {error}\")\n\n# Option for nullable values\nemail = from_nullable(get_user_email(42))  # Some(\"...\") or NOTHING\ndisplay = email.map(lambda e: e.split(\"@\")[0]).unwrap_or(\"Anonymous\")\n\n# Lazy async chaining - no nested awaits\nresult = await (\n    LazyResult.from_awaitable(fetch_user(42))\n    .and_then(fetch_profile)\n    .map(lambda p: p[\"name\"].upper())\n    .collect()\n)\n```\n\nFull combinator API: `map`, `and_then`, `or_else`, `filter`, `zip`, `flatten`, `tee`, and more.\n\n### Target Audience\n\n**Production-ready** - 99% test coverage, fully typed, zero dependencies. Best for API boundaries and data pipelines where you want explicit error handling.\n\n### Why This Exists\n\nThe `rustedpy` ecosystem (`result`, `maybe`) is no longer actively maintained. I needed a maintained alternative with proper async support, so I built unwrappy with `LazyResult`/`LazyOption` for clean async pipeline composition.\n\n**Links:**\n- GitHub: https://github.com/leodiegues/unwrappy\n- PyPI: `pip install unwrappy`\n- Docs: https://leodiegues.github.io/unwrappy\n\nFeedbacks and contributions are welcome!", "source": "Reddit", "date": "2026-01-19T12:30:36", "author": "leonardodiegues", "score": 21, "tokens": ["unwrappy", "rust", "inspire", "result", "option", "type", "lazy", "async", "chain", "python", "build", "library", "bring", "rust", "result", "option", "type", "python", "lazy", "evaluation", "clean", "async", "operation", "chain", "inspire", "polar", "defer", "execution", "project", "unwrappy", "provide", "resultt", "success", "ok", "failure", "err", "error", "value", "exception", "optiont", "presence", "absence", "explicit", "optionality", "lazyresult", "lazyoption", "build", "async", "pipeline", "nest", "await", "python", "unwrappy", "import", "ok", "err", "lazyresult", "pattern", "match", "python", "3.10", "match", "divide10", "case", "okvalue", "printfresult", "value", "case", "errerror", "printferror", "error", "option", "nullable", "value", "email", "from_nullableget_user_email42", "display", "email.maplambda", "e.split0.unwrap_oranonymous", "lazy", "async", "chain", "nested", "await", "result", "await", "lazyresult.from_awaitablefetch_user42", ".and_thenfetch_profile", ".maplambda", "pname.upper", ".collect", "  ", "combinator", "api", "map", "and_then", "or_else", "filter", "zip", "flatten", "tee", "target", "audience", "production", "ready", "99", "test", "coverage", "fully", "type", "zero", "dependency", "good", "api", "boundary", "datum", "pipeline", "want", "explicit", "error", "handling", "exist", "rustedpy", "ecosystem", "result", "maybe", "long", "actively", "maintain", "need", "maintain", "alternative", "proper", "async", "support", "build", "unwrappy", "lazyresultlazyoption", "clean", "async", "pipeline", "composition", "link", "github", "url", "pypi", "pip", "install", "unwrappy", "docs", "url", "feedback", "contribution", "welcome"], "num_tokens": 154, "token_loss_pct": 37.14, "normalized_content": "unwrappy rust-inspired result and option types with lazy async chaining for python. i built a library that brings rust's result and option types to python with lazy evaluation for clean async operation chaining inspired by polars' deferred execution.  what my project does unwrappy provides - resultt e - success ok or failure err - errors as values not exceptions - optiont - presence some or absence nothing - explicit optionality - lazyresult  lazyoption - build async pipelines without nested awaits python from unwrappy import ok err some nothing lazyresult  pattern matching python 3.10 match divide10 0 case okvalue printfresult value case errerror printferror error  option for nullable values email  from_nullableget_user_email42  some... or nothing display  email.maplambda e e.split0.unwrap_oranonymous  lazy async chaining - no nested awaits result  await  lazyresult.from_awaitablefetch_user42 .and_thenfetch_profile .maplambda p pname.upper .collect   full combinator api map and_then or_else filter zip flatten tee and more.  target audience production-ready - 99 test coverage fully typed zero dependencies. best for api boundaries and data pipelines where you want explicit error handling.  why this exists the rustedpy ecosystem result maybe is no longer actively maintained. i needed a maintained alternative with proper async support so i built unwrappy with lazyresultlazyoption for clean async pipeline composition. links - github url - pypi pip install unwrappy - docs url feedbacks and contributions are welcome"}
{"title": "Python Script Ranking All 262,143 Possible Pokemon Type Combinations", "url": "https://www.reddit.com/r/Python/comments/1qf56jo/python_script_ranking_all_262143_possible_pokemon/", "content": "Python Script Ranking All 262,143 Possible Pokemon Type Combinations. **What My Project Does:** Finds all possible combinations of Pokemon types from 1 type to 18 types, making 262,143 combinations in total, and scores their offensive and defensive capabilities.\n\n**Target Audience:** Anyone who plays Pokemon! This is just for fun.\n\n**Comparison:** Existing rankings only rank combinations possible in the game (1 type or 2 types) but this analyzes the capabilities of type combinations that couldn't normally exist in-game (3 types to 18 types). \n\n\\-----------------------------------------------------------------------------------------------------  \n  \nI wrote a Python script with Pandas and Multiprocessing that analyzes all possible Pokemon type combinations and ranks them according to their offensive and defensive capabilities. It doesn't just do 1-2 types, but instead all combinations up to 18 types. This makes for 262,143 possible combinations in total!\n\n  \n**Some highlights:**\n\nThe best possible defensive combination is:\n\n    ['Normal', 'Fire', 'Water', 'Electric', 'Poison', 'Ground', 'Flying', 'Ghost', 'Dragon', 'Dark', 'Steel', 'Fairy']\n\nThis has no weaknesses.   \nResists Fire, Grass, Flying, Bug (0.03125x damage lol), Dark, Steel, and Fairy.   \nImmune to Normal,\tElectric, Fighting, Poison, Ground, Psychic, and Ghost.   \nThis ranked 28th overall.\n\nThat's only 12 types though. If a Pokemon had all 18 types, a.k.a:\n\n    ['Normal', 'Fire', 'Water', 'Electric', 'Grass', 'Ice', 'Fighting', 'Poison', 'Ground', 'Flying', 'Psychic', 'Bug', 'Rock', 'Ghost', 'Dragon', 'Dark', 'Steel', 'Fairy']\n\n  \nIt would be weak to only Rock, but it would only resist Grass, Bug, Dark, and Steel.  \nThis ranked 1,992nd place in defense and 536th overall.\n\nThe smallest number of types to hit all Pokemon for super effective STAB is 7. There were 10 7-type combinations that could hit all types for super effective damage. In total, 16,446 combinations could do this.\n\nThe single worst defensive type combination is:\n\n    ['Grass', 'Ice', 'Psychic', 'Bug', 'Dragon']\n\nIts weaknesses are\n\n    Fire: 4.0x\n    Ice: 2.0x\n    Poison: 2.0x\n    Flying: 4.0x\n    Bug: 4.0x\n    Rock: 4.0x\n    Ghost: 2.0x\n    Dragon: 2.0x\n    Dark: 2.0x\n    Steel: 2.0x\n    Fairy: 2.0x\n\nOuch. This combination placed 262,083rd overall.\n\nAnd the single lowest-scored type combination out of all 262,143 is... *Grass*. That's it. Pure Grass.\n\n**Looking at only 1-type and 2-type combinations:**\n\nTop 5 by Offense:\n\n    Rank 1:   ['Ice', 'Ground']        75.0%  Highest for 2 types.\n    Rank 2:   ['Ice', 'Fighting']      75.0%  Highest for 2 types.\n    Rank 3:   ['Ground', 'Flying']     72.22% \n    Rank 4:   ['Fire', 'Ground']       72.22% \n    Rank 5:   ['Ground', 'Fairy']      72.22%\n\nTop 5 by Defense:\n\n    Rank 1:   ['Flying', 'Steel']      69.44% Highest for 2 types.\n    Rank 2:   ['Steel', 'Fairy']       69.44% Highest for 2 types.\n    Rank 3:   ['Normal', 'Ghost']      68.06% \n    Rank 4:   ['Bug', 'Steel']         67.36% \n    Rank 5:   ['Ghost', 'Steel']       67.36% \n\nTop 5 Overall:\n\n    Rank 1:\n    ['Ground', 'Flying']\n    # of Types: 2\n    Offense Score: 72.22%\n    Defense Score: 63.19%\n    Overall:       67.71% Highest average for 2 types.\n    \n    Rank 2:\n    ['Fire', 'Ground']\n    # of Types: 2\n    Offense Score: 72.22%\n    Defense Score: 62.5%\n    Overall:       67.36%\n    \n    Rank 3:\n    ['Ground', 'Steel']\n    # of Types: 2\n    Offense Score: 69.44%\n    Defense Score: 64.58%\n    Overall:       67.01%\n    \n    Rank 4:\n    ['Ground', 'Fairy']\n    # of Types: 2\n    Offense Score: 72.22%\n    Defense Score: 61.11%\n    Overall:       66.67%\n    \n    Rank 5:\n    ['Flying', 'Steel']\n    # of Types: 2\n    Offense Score: 63.89%\n    Defense Score: 69.44% Highest defense for 2 types.\n    Overall:       66.67%\n\nThe full code and output files up to 6-type combinations can be found on my Github, [here.](https://github.com/superwatts/pokemon-type-ranking/) \n\nThe full output file for all 262,143", "source": "Reddit", "date": "2026-01-17T06:52:01", "author": "sxprwtts", "score": 20, "tokens": ["python", "script", "rank", "262143", "possible", "pokemon", "type", "combination", "project", "find", "possible", "combination", "pokemon", "type", "type", "18", "type", "make", "262143", "combination", "total", "score", "offensive", "defensive", "capability", "target", "audience", "play", "pokemon", "fun", "comparison", "exist", "ranking", "rank", "combination", "possible", "game", "type", "type", "analyze", "capability", "type", "combination", "normally", "exist", "game", "type", "18", "type", "write", "python", "script", "panda", "multiprocesse", "analyze", "possible", "pokemon", "type", "combination", "rank", "accord", "offensive", "defensive", "capability", "type", "instead", "combination", "18", "type", "make", "262143", "possible", "combination", "total", "highlight", "good", "possible", "defensive", "combination", "normal", "fire", "water", "electric", "poison", "ground", "fly", "ghost", "dragon", "dark", "steel", "fairy", "weakness", "resist", "fire", "grass", "fly", "bug", "0.03125x", "damage", "lol", "dark", "steel", "fairy", "immune", "normal", "electric", "fight", "poison", "ground", "psychic", "ghost", "rank", "28th", "overall", "12", "type", "pokemon", "18", "type", "a.k.a", "normal", "fire", "water", "electric", "grass", "ice", "fight", "poison", "ground", "fly", "psychic", "bug", "rock", "ghost", "dragon", "dark", "steel", "fairy", "weak", "rock", "resist", "grass", "bug", "dark", "steel", "rank", "1992nd", "place", "defense", "536th", "overall", "small", "number", "type", "hit", "pokemon", "super", "effective", "stab", "10", "type", "combination", "hit", "type", "super", "effective", "damage", "total", "16446", "combination", "single", "bad", "defensive", "type", "combination", "grass", "ice", "psychic", "bug", "dragon", "weakness", "fire", "4.0x", "ice", "2.0x", "poison", "2.0x", "fly", "4.0x", "bug", "4.0x", "rock", "4.0x", "ghost", "2.0x", "dragon", "2.0x", "dark", "2.0x", "steel", "2.0x", "fairy", "2.0x", "ouch", "combination", "place", "262083rd", "overall", "single", "low", "score", "type", "combination", "262143", "grass", "pure", "grass", "look", "type", "type", "combination", "offense", "rank", "ice", "ground", "75.0", "high", "type", "rank", "ice", "fight", "75.0", "high", "type", "rank", "ground", "fly", "72.22", "rank", "fire", "ground", "72.22", "rank", "ground", "fairy", "72.22", "defense", "rank", "fly", "steel", "69.44", "high", "type", "rank", "steel", "fairy", "69.44", "high", "type", "rank", "normal", "ghost", "68.06", "rank", "bug", "steel", "67.36", "rank", "ghost", "steel", "67.36", "overall", "rank", "ground", "fly", "type", "offense", "score", "72.22", "defense", "score", "63.19", "overall", "67.71", "high", "average", "type", "rank", "fire", "ground", "type", "offense", "score", "72.22", "defense", "score", "62.5", "overall", "67.36", "rank", "ground", "steel", "type", "offense", "score", "69.44", "defense", "score", "64.58", "overall", "67.01", "rank", "ground", "fairy", "type", "offense", "score", "72.22", "defense", "score", "61.11", "overall", "66.67", "rank", "fly", "steel", "type", "offense", "score", "63.89", "defense", "score", "69.44", "high", "defense", "type", "overall", "66.67", "code", "output", "file", "type", "combination", "find", "github", "here.url", "output", "file", "262143"], "num_tokens": 349, "token_loss_pct": 50.64, "normalized_content": "python script ranking all 262143 possible pokemon type combinations. what my project does finds all possible combinations of pokemon types from 1 type to 18 types making 262143 combinations in total and scores their offensive and defensive capabilities. target audience anyone who plays pokemon this is just for fun. comparison existing rankings only rank combinations possible in the game 1 type or 2 types but this analyzes the capabilities of type combinations that couldn't normally exist in-game 3 types to 18 types. ----------------------------------------------------------------------------------------------------- i wrote a python script with pandas and multiprocessing that analyzes all possible pokemon type combinations and ranks them according to their offensive and defensive capabilities. it doesn't just do 1-2 types but instead all combinations up to 18 types. this makes for 262143 possible combinations in total some highlights the best possible defensive combination is 'normal' 'fire' 'water' 'electric' 'poison' 'ground' 'flying' 'ghost' 'dragon' 'dark' 'steel' 'fairy' this has no weaknesses. resists fire grass flying bug 0.03125x damage lol dark steel and fairy. immune to normal electric fighting poison ground psychic and ghost. this ranked 28th overall. that's only 12 types though. if a pokemon had all 18 types a.k.a 'normal' 'fire' 'water' 'electric' 'grass' 'ice' 'fighting' 'poison' 'ground' 'flying' 'psychic' 'bug' 'rock' 'ghost' 'dragon' 'dark' 'steel' 'fairy' it would be weak to only rock but it would only resist grass bug dark and steel. this ranked 1992nd place in defense and 536th overall. the smallest number of types to hit all pokemon for super effective stab is 7. there were 10 7-type combinations that could hit all types for super effective damage. in total 16446 combinations could do this. the single worst defensive type combination is 'grass' 'ice' 'psychic' 'bug' 'dragon' its weaknesses are fire 4.0x ice 2.0x poison 2.0x flying 4.0x bug 4.0x rock 4.0x ghost 2.0x dragon 2.0x dark 2.0x steel 2.0x fairy 2.0x ouch. this combination placed 262083rd overall. and the single lowest-scored type combination out of all 262143 is... grass. that's it. pure grass. looking at only 1-type and 2-type combinations top 5 by offense rank 1 'ice' 'ground' 75.0 highest for 2 types. rank 2 'ice' 'fighting' 75.0 highest for 2 types. rank 3 'ground' 'flying' 72.22 rank 4 'fire' 'ground' 72.22 rank 5 'ground' 'fairy' 72.22 top 5 by defense rank 1 'flying' 'steel' 69.44 highest for 2 types. rank 2 'steel' 'fairy' 69.44 highest for 2 types. rank 3 'normal' 'ghost' 68.06 rank 4 'bug' 'steel' 67.36 rank 5 'ghost' 'steel' 67.36 top 5 overall rank 1 'ground' 'flying'  of types 2 offense score 72.22 defense score 63.19 overall 67.71 highest average for 2 types. rank 2 'fire' 'ground'  of types 2 offense score 72.22 defense score 62.5 overall 67.36 rank 3 'ground' 'steel'  of types 2 offense score 69.44 defense score 64.58 overall 67.01 rank 4 'ground' 'fairy'  of types 2 offense score 72.22 defense score 61.11 overall 66.67 rank 5 'flying' 'steel'  of types 2 offense score 63.89 defense score 69.44 highest defense for 2 types. overall 66.67 the full code and output files up to 6-type combinations can be found on my github here.url the full output file for all 262143"}
{"title": "Convert your bear images into bear images: Bear Right Back", "url": "https://www.reddit.com/r/Python/comments/1qiulsc/convert_your_bear_images_into_bear_images_bear/", "content": "Convert your bear images into bear images: Bear Right Back. # What My Project Does\n\n**bearrb** is a Python CLI tool that takes **two images of bears** (a source and a target) and transforms the source into a close approximation of the target **by only rearranging pixel coordinates**.\n\nNo pixel values are modified, generated, blended, or recolored, every original pixel is preserved exactly as it was. The algorithm computes a permutation of pixel positions that minimizes the visual difference from the target image.\n\nrepo: [https://github.com/JoshuaKasa/bearrb](https://github.com/JoshuaKasa/bearrb)\n\n# Target Audience\n\nThis is obviously a **toy / experimental project**, not meant for production image editing.\n\nIt's mainly for:\n\n* people interested in algorithmic image processing\n* optimization under hard constraints\n* weird/fun CLI tools\n* math-y or computational art experiments\n\n# Comparison\n\nMost image tools try to be useful and correct... bearrb does not.\n\nInstead of editing, filtering, generating, or enhancing images, bearrb just takes the pixels it already has and **throws them around until the image vaguely resembles the other bear**", "source": "Reddit", "date": "2026-01-21T11:39:59", "author": "JizosKasa", "score": 18, "tokens": ["convert", "bear", "image", "bear", "image", "bear", "right", "project", "bearrb", "python", "cli", "tool", "take", "image", "bear", "source", "target", "transform", "source", "close", "approximation", "target", "rearrange", "pixel", "coordinate", "pixel", "value", "modify", "generate", "blend", "recolore", "original", "pixel", "preserve", "exactly", "algorithm", "compute", "permutation", "pixel", "position", "minimize", "visual", "difference", "target", "image", "repo", "url", "target", "audience", "obviously", "toy", "experimental", "project", "mean", "production", "image", "editing", "mainly", "people", "interested", "algorithmic", "image", "processing", "optimization", "hard", "constraint", "weirdfun", "cli", "tool", "math", "computational", "art", "experiment", "comparison", "image", "tool", "try", "useful", "correct", "bearrb", "instead", "edit", "filtering", "generating", "enhance", "image", "bearrb", "take", "pixel", "throw", "image", "vaguely", "resemble", "bear"], "num_tokens": 94, "token_loss_pct": 47.49, "normalized_content": "convert your bear images into bear images bear right back.  what my project does bearrb is a python cli tool that takes two images of bears a source and a target and transforms the source into a close approximation of the target by only rearranging pixel coordinates. no pixel values are modified generated blended or recolored every original pixel is preserved exactly as it was. the algorithm computes a permutation of pixel positions that minimizes the visual difference from the target image. repo url  target audience this is obviously a toy  experimental project not meant for production image editing. it's mainly for  people interested in algorithmic image processing  optimization under hard constraints  weirdfun cli tools  math-y or computational art experiments  comparison most image tools try to be useful and correct... bearrb does not. instead of editing filtering generating or enhancing images bearrb just takes the pixels it already has and throws them around until the image vaguely resembles the other bear"}
{"title": "I built a local-first file metadata extraction library with a CLI (Python + Pydantic + Typer)", "url": "https://www.reddit.com/r/Python/comments/1qi3zwa/i_built_a_localfirst_file_metadata_extraction/", "content": "I built a local-first file metadata extraction library with a CLI (Python + Pydantic + Typer). Hi all,\n\nI've been working on a project called Dorsal for the last 18 months. It's a way to make unstructured data more queryable and organized, without having to upload files to a cloud bucket or pay for remote compute (my CPU/GPU can almost always handle my workloads).\n\n# What my Project Does\n\nDorsal is a Python library and CLI for generating, validating and managing structured file metadata. It scans files locally to generate validated JSON-serializable records. I personally use it for deduplicating files, adding annotations (structured metadata records) and organizing files by tags.\n\n* Core Extraction: Out of the box, it extracts \"universal\" metadata (Name, Hashes, Media Type; things any file has), as well and format-specific values (e.g., document page counts, video resolution, ebook titles/authors).\n* The Toolkit: It provides the scaffolding to build and plug in your own complex extraction models (like OCR, classification, or entity extraction, where the input is a file). It handles the pipeline execution, dependency management, and file I/O for you.\n* Strict Validation: It enforces Pydantic/JSON Schema on all outputs. If your custom extractor returns a float where a string is expected, Dorsal catches it before it pollutes your index.\n\nExample: a simple custom model for checking PDF files for sensitive words:\n\n    from dorsal import AnnotationModel\n    from dorsal.file.helpers import build_classification_record\n    from dorsal.file.preprocessing import extract_pdf_text\n    \n    SENSITIVE_LABELS = {\n        \"Confidential\": [\"confidential\", \"do not distribute\", \"private\"],\n        \"Internal\": [\"internal use only\", \"proprietary\"],\n    }\n    \n    class SensitiveDocumentScanner(AnnotationModel):\n        id: str = \"github:dorsalhub/annotation-model-examples\"\n        version: str = \"1.0.0\"\n    \n        def main(self) -&gt; dict | None:\n            try:\n                pages = extract_pdf_text(self.file_path)\n            except Exception as err:\n                self.set_error(f\"Failed to parse PDF: {err}\")\n                return None\n    \n            matches = set()\n            for text in pages:\n                text = text.lower()\n                for label, keywords in SENSITIVE_LABELS.items():\n                    if any(k in text for k in keywords):\n                        matches.add(label)\n    \n            return build_classification_record(\n                labels=list(matches),\n                vocabulary=list(SENSITIVE_LABELS.keys())\n            )\n\n\\^ This can be easily integrated into a locally-run linear pipeline, and executed via either the command line (by pointing at a file or directory) or in a python script.\n\n# Target Audience\n\n* ML Engineers / Data Scientists: Dorsal lets you make sure all of your output steps are validated, using a set of robust schemas for many common data engineering tasks (regression, entity extraction, classification etc.).\n* Data Hoarders / Archivists: People with massive local datasets (TB+) who like customizable tools for deduplication, tagging and even cloud querying\n* RAG Pipeline Builders: Turn folders of PDFs and docs into structured JSON chunks for vector embeddings\n\n# Links\n\n* Github: [https://github.com/dorsalhub/dorsal](https://github.com/dorsalhub/dorsal)\n* PyPI: pip install dorsalhub\n* Docs: [https://docs.dorsalhub.com](https://docs.dorsalhub.com)\n\n# Comparison\n\n|Feature|Dorsal|Cloud ETL (AWS/GCP)|\n|:-|:-|:-|\n|**Integrity**|Hash-based|Upload required|\n|**Validation**|JSON Schema / Pydantic|API Dependent|\n|**Cost**|Free (Local Compute)|$$$ (Per Page)|\n|**Workflow**|Standardized Pipeline|Vendor Lock-in|\n\nAny and all feedback is extremely welcome!", "source": "Reddit", "date": "2026-01-20T16:25:00", "author": "AverageMechUser", "score": 19, "tokens": ["build", "local", "file", "metadata", "extraction", "library", "cli", "python", "pydantic", "typer", "hi", "work", "project", "call", "dorsal", "18", "month", "way", "unstructured", "datum", "queryable", "organize", "have", "upload", "file", "cloud", "bucket", "pay", "remote", "compute", "cpugpu", "handle", "workload", "project", "dorsal", "python", "library", "cli", "generate", "validate", "manage", "structured", "file", "metadata", "scan", "file", "locally", "generate", "validate", "json", "serializable", "record", "personally", "use", "deduplicate", "file", "add", "annotation", "structure", "metadata", "record", "organize", "file", "tag", "core", "extraction", "box", "extract", "universal", "metadata", "hash", "medium", "type", "thing", "file", "format", "specific", "value", "e.g.", "document", "page", "count", "video", "resolution", "ebook", "titlesauthor", "toolkit", "provide", "scaffolding", "build", "plug", "complex", "extraction", "model", "like", "ocr", "classification", "entity", "extraction", "input", "file", "handle", "pipeline", "execution", "dependency", "management", "file", "io", "strict", "validation", "enforce", "pydanticjson", "schema", "output", "custom", "extractor", "return", "float", "string", "expect", "dorsal", "catch", "pollute", "index", "example", "simple", "custom", "model", "check", "pdf", "file", "sensitive", "word", "dorsal", "import", "annotationmodel", "dorsal.file.helpers", "import", "build_classification_record", "dorsal.file.preprocesse", "import", "extract_pdf_text", "sensitive_label", "  ", "confidential", "confidential", "distribute", "private", "internal", "internal", "use", "proprietary", "class", "sensitivedocumentscannerannotationmodel", "str", "githubdorsalhubannotation", "model", "example", "version", "str", "1.0.0", "def", "mainself", "-gt", "dict", "try", "page", "extract_pdf_textself.file_path", "exception", "err", "self.set_errorffaile", "parse", "pdf", "err", "return", "match", "set", "text", "page", "text", "text.lower", "label", "keyword", "sensitive_labels.items", "anyk", "text", "keyword", "matches.addlabel", "return", "build_classification_record", "labelslistmatche", "vocabularylistsensitive_labels.keys", "  ", "easily", "integrate", "locally", "run", "linear", "pipeline", "execute", "command", "line", "point", "file", "directory", "python", "script", "target", "audience", "ml", "engineer", "datum", "scientist", "dorsal", "let", "sure", "output", "step", "validate", "set", "robust", "schema", "common", "datum", "engineering", "task", "regression", "entity", "extraction", "classification", "etc", "data", "hoarder", "archivist", "people", "massive", "local", "dataset", "tb", "like", "customizable", "tool", "deduplication", "tagging", "cloud", "query", "rag", "pipeline", "builder", "turn", "folder", "pdfs", "doc", "structured", "json", "chunk", "vector", "embedding", "link", "github", "url", "pypi", "pip", "install", "dorsalhub", "docs", "url", "comparison", "featuredorsalcloud", "etl", "awsgcp", "integrityhash", "basedupload", "require", "validationjson", "schema", "pydanticapi", "dependent", "costfree", "local", "compute", "page", "workflowstandardize", "pipelinevendor", "lock", "feedback", "extremely", "welcome"], "num_tokens": 288, "token_loss_pct": 41.22, "normalized_content": "i built a local-first file metadata extraction library with a cli python  pydantic  typer. hi all i've been working on a project called dorsal for the last 18 months. it's a way to make unstructured data more queryable and organized without having to upload files to a cloud bucket or pay for remote compute my cpugpu can almost always handle my workloads.  what my project does dorsal is a python library and cli for generating validating and managing structured file metadata. it scans files locally to generate validated json-serializable records. i personally use it for deduplicating files adding annotations structured metadata records and organizing files by tags.  core extraction out of the box it extracts universal metadata name hashes media type things any file has as well and format-specific values e.g. document page counts video resolution ebook titlesauthors.  the toolkit it provides the scaffolding to build and plug in your own complex extraction models like ocr classification or entity extraction where the input is a file. it handles the pipeline execution dependency management and file io for you.  strict validation it enforces pydanticjson schema on all outputs. if your custom extractor returns a float where a string is expected dorsal catches it before it pollutes your index. example a simple custom model for checking pdf files for sensitive words from dorsal import annotationmodel from dorsal.file.helpers import build_classification_record from dorsal.file.preprocessing import extract_pdf_text sensitive_labels   confidential confidential do not distribute private internal internal use only proprietary  class sensitivedocumentscannerannotationmodel id str  githubdorsalhubannotation-model-examples version str  1.0.0 def mainself -gt dict  none try pages  extract_pdf_textself.file_path except exception as err self.set_errorffailed to parse pdf err return none matches  set for text in pages text  text.lower for label keywords in sensitive_labels.items if anyk in text for k in keywords matches.addlabel return build_classification_record labelslistmatches vocabularylistsensitive_labels.keys   this can be easily integrated into a locally-run linear pipeline and executed via either the command line by pointing at a file or directory or in a python script.  target audience  ml engineers  data scientists dorsal lets you make sure all of your output steps are validated using a set of robust schemas for many common data engineering tasks regression entity extraction classification etc..  data hoarders  archivists people with massive local datasets tb who like customizable tools for deduplication tagging and even cloud querying  rag pipeline builders turn folders of pdfs and docs into structured json chunks for vector embeddings  links  github url  pypi pip install dorsalhub  docs url  comparison featuredorsalcloud etl awsgcp --- integrityhash-basedupload required validationjson schema  pydanticapi dependent costfree local compute per page workflowstandardized pipelinevendor lock-in any and all feedback is extremely welcome"}
{"title": "I built a Python DSL for creating C4 models and diagrams", "url": "https://www.reddit.com/r/Python/comments/1qhxsad/i_built_a_python_dsl_for_creating_c4_models_and/", "content": "I built a Python DSL for creating C4 models and diagrams. Hello!\n\nLast year, I started writing a Python C4 model authoring tool, and it has come to a point where I feel good enough to share it with you guys so you can start playing around with it locally and render the C4 model views with PlantUML.\n\nGitHub repo: [https://github.com/amirulmenjeni/buildzr](https://github.com/amirulmenjeni/buildzr)\n\nDocumentation here: [https://buildzr.dev](https://buildzr.dev/)\n\n# What My Project Does\n\nbuildzr is a [Structurizr](https://structurizr.com/) authoring tool for Python programmers. It allows you to declaratively or procedurally author Structurizr models and diagrams.\n\nIf you're not familiar with Structurizr, it is both an open standard (see [Structurizr JSON schema](https://github.com/structurizr/json)) and a [set of tools](https://docs.structurizr.com/usage) for building software architecture diagrams as code. Structurizr derives its architecture modeling paradigm based on the [C4 model](https://c4model.com/), the modeling language for describing software architectures and their relationships.\n\nIn Structurizr, you define architecture models (System Context, Container, Component, and Code) and their relationships first. And then, you can re-use the models to present multiple perspectives, views, and stories about your architecture.\n\nbuildzr supercharges this workflow with Pythonic syntax sugar and intuitive APIs that make modeling as code more fun and productive.\n\n# Target Audience\n\nUse buildzr if you want to have an intuitive and powerful tool for writing C4 architecture models:\n\n* **Intuitive Pythonic Syntax**: Use Python's context managers (`with` statements) to create nested structures that naturally mirror your architecture's hierarchy. See the [example](https://github.com/amirulmenjeni/buildzr#quick-example).\n* **Programmatic Creation**: Use buildzr's DSL APIs to programmatically create C4 model architecture diagrams. Great for automation!\n* **Advanced Styling**: Style elements beyond just tags --- target by direct reference, type, group membership, or custom predicates for fine-grained visual control. Just take a look at [Styles](https://buildzr.dev/user-guide/styles/)!\n* **Cloud Provider Themes**: Add AWS, Azure, Google Cloud, Kubernetes, and Oracle Cloud icons to your diagrams with IDE-discoverable constants. No more memorizing tag strings! See [Themes](https://buildzr.dev/user-guide/themes/).\n* **Standards Compliant**: Stays true to the [Structurizr JSON schema](https://github.com/structurizr/json) standards. buildzr uses [datamodel-code-generator](https://github.com/koxudaxi/datamodel-code-generator) to automatically generate the low-level representation of the Workspace model.\n* **Rich Toolchain**: Uses the familiar Python programming language and its rich toolchains to write software architecture models and diagrams!\n\nQuick example, so you can get the idea (more examples and explanations at [https://buildzr.dev](https://buildzr.dev)):\n\n    from buildzr.dsl import (\n        Workspace,\n        SoftwareSystem,\n        Person,\n        Container,\n        SystemContextView,\n        ContainerView,\n        desc,\n        Group,\n        StyleElements,\n    )\n    from buildzr.themes import AWS\n    \n    with Workspace('w') as w:\n    \n        # Define your models (architecture elements and their relationships).\n    \n        with Group(\"My Company\") as my_company:\n            u = Person('Web Application User')\n            webapp = SoftwareSystem('Corporate Web App')\n            with webapp:\n                database = Container('database')\n                api = Container('api')\n                api &gt;&gt; (\"Reads and writes data from/to\", \"http/api\") &gt;&gt; database\n        with Group(\"Microsoft\") as microsoft:\n            email_system = SoftwareSystem('Microsoft 365')\n    \n        u &gt;&gt; [\n            desc(\"Reads and writes email using\") &gt;&gt; email_system,\n            desc(\"Create work order using\") &gt;&gt; webapp,\n        ]", "source": "Reddit", "date": "2026-01-20T11:39:20", "author": "scribe-kiddie", "score": 17, "tokens": ["build", "python", "dsl", "create", "c4", "model", "diagram", "hello", "year", "start", "write", "python", "c4", "model", "author", "tool", "come", "point", "feel", "good", "share", "guy", "start", "play", "locally", "render", "c4", "model", "view", "plantuml", "github", "repo", "url", "documentation", "url", "project", "buildzr", "structurizrurl", "author", "tool", "python", "programmer", "allow", "declaratively", "procedurally", "author", "structurizr", "model", "diagram", "familiar", "structurizr", "open", "standard", "structurizr", "json", "schemaurl", "set", "toolsurl", "build", "software", "architecture", "diagram", "code", "structurizr", "derive", "architecture", "modeling", "paradigm", "base", "c4", "modelurl", "modeling", "language", "describe", "software", "architecture", "relationship", "structurizr", "define", "architecture", "model", "system", "context", "container", "component", "code", "relationship", "use", "model", "present", "multiple", "perspective", "view", "story", "architecture", "buildzr", "supercharges", "workflow", "pythonic", "syntax", "sugar", "intuitive", "apis", "modeling", "code", "fun", "productive", "target", "audience", "use", "buildzr", "want", "intuitive", "powerful", "tool", "write", "c4", "architecture", "model", "intuitive", "pythonic", "syntax", "use", "python", "context", "manager", "statement", "create", "nested", "structure", "naturally", "mirror", "architecture", "hierarchy", "exampleurlhashtag", "example", "programmatic", "creation", "use", "buildzr", "dsl", "apis", "programmatically", "create", "c4", "model", "architecture", "diagram", "great", "automation", "advanced", "styling", "style", "element", "tag", "target", "direct", "reference", "type", "group", "membership", "custom", "predicate", "fine", "grain", "visual", "control", "look", "stylesurl", "cloud", "provider", "theme", "add", "aws", "azure", "google", "cloud", "kubernete", "oracle", "cloud", "icon", "diagram", "ide", "discoverable", "constant", "memorizing", "tag", "string", "themesurl", "standard", "compliant", "stay", "true", "structurizr", "json", "schemaurl", "standard", "buildzr", "use", "datamodel", "code", "generatorurl", "automatically", "generate", "low", "level", "representation", "workspace", "model", "rich", "toolchain", "use", "familiar", "python", "programming", "language", "rich", "toolchain", "write", "software", "architecture", "model", "diagram", "quick", "example", "idea", "example", "explanation", "url", "buildzr.dsl", "import", "workspace", "softwaresystem", "person", "container", "systemcontextview", "containerview", "desc", "group", "styleelement", "buildzr.theme", "import", "aw", "workspace'w", "define", "model", "architecture", "element", "relationship", "groupmy", "company", "my_company", "person'web", "application", "user", "webapp", "softwaresystem'corporate", "web", "app", "webapp", "database", "container'database", "api", "container'api", "api", "gtgt", "read", "write", "datum", "fromto", "httpapi", "gtgt", "database", "groupmicrosoft", "microsoft", "email_system", "softwaresystem'microsoft", "365", "gtgt", "descread", "write", "email", "gtgt", "email_system", "desccreate", "work", "order", "gtgt", "webapp"], "num_tokens": 289, "token_loss_pct": 41.62, "normalized_content": "i built a python dsl for creating c4 models and diagrams. hello last year i started writing a python c4 model authoring tool and it has come to a point where i feel good enough to share it with you guys so you can start playing around with it locally and render the c4 model views with plantuml. github repo url documentation here url  what my project does buildzr is a structurizrurl authoring tool for python programmers. it allows you to declaratively or procedurally author structurizr models and diagrams. if you're not familiar with structurizr it is both an open standard see structurizr json schemaurl and a set of toolsurl for building software architecture diagrams as code. structurizr derives its architecture modeling paradigm based on the c4 modelurl the modeling language for describing software architectures and their relationships. in structurizr you define architecture models system context container component and code and their relationships first. and then you can re-use the models to present multiple perspectives views and stories about your architecture. buildzr supercharges this workflow with pythonic syntax sugar and intuitive apis that make modeling as code more fun and productive.  target audience use buildzr if you want to have an intuitive and powerful tool for writing c4 architecture models  intuitive pythonic syntax use python's context managers with statements to create nested structures that naturally mirror your architecture's hierarchy. see the exampleurlhashtag-example.  programmatic creation use buildzr's dsl apis to programmatically create c4 model architecture diagrams. great for automation  advanced styling style elements beyond just tags --- target by direct reference type group membership or custom predicates for fine-grained visual control. just take a look at stylesurl  cloud provider themes add aws azure google cloud kubernetes and oracle cloud icons to your diagrams with ide-discoverable constants. no more memorizing tag strings see themesurl  standards compliant stays true to the structurizr json schemaurl standards. buildzr uses datamodel-code-generatorurl to automatically generate the low-level representation of the workspace model.  rich toolchain uses the familiar python programming language and its rich toolchains to write software architecture models and diagrams quick example so you can get the idea more examples and explanations at url from buildzr.dsl import  workspace softwaresystem person container systemcontextview containerview desc group styleelements  from buildzr.themes import aws with workspace'w' as w  define your models architecture elements and their relationships. with groupmy company as my_company u  person'web application user' webapp  softwaresystem'corporate web app' with webapp database  container'database' api  container'api' api gtgt reads and writes data fromto httpapi gtgt database with groupmicrosoft as microsoft email_system  softwaresystem'microsoft 365' u gtgt  descreads and writes email using gtgt email_system desccreate work order using gtgt webapp"}
{"title": "Python Version in Production ?", "url": "https://www.reddit.com/r/Python/comments/1qgt083/python_version_in_production/", "content": "Python Version in Production ?. 3.12 / 3.13 / 3.14 (Stable) \n\nSo in production, which version of Python are you using? Apparently I'm using 3.12, but I'm thinking off upgrading to 3.13 What's the main difference? What version are you using for your production in these cases?", "source": "Reddit", "date": "2026-01-19T04:23:58", "author": "TopicBig1308", "score": 17, "tokens": ["python", "version", "production", "3.12", "3.13", "3.14", "stable", "production", "version", "python", "apparently", "3.12", "think", "upgrade", "3.13", "main", "difference", "version", "production", "case"], "num_tokens": 20, "token_loss_pct": 60.0, "normalized_content": "python version in production . 3.12  3.13  3.14 stable so in production which version of python are you using apparently i'm using 3.12 but i'm thinking off upgrading to 3.13 what's the main difference what version are you using for your production in these cases"}
{"title": "Follow up: Clientele - an API integration framework for Python", "url": "https://www.reddit.com/r/Python/comments/1qdo37p/follow_up_clientele_an_api_integration_framework/", "content": "Follow up: Clientele - an API integration framework for Python. Hello pythonistas, two weeks ago I shared a [blog post](https://www.reddit.com/r/Python/comments/1q1udpj/blog_post_a_different_way_to_think_about_python/) about an alternative way of building API integrations, heavily inspired by the developer experience of python API  frameworks.\n\n**What My Project Does**\n\nClientele lets you focus on the behaviour you want from an API, and let it handle the rest - networking, hydration, caching, and data validation. It uses strong types and decorators to build a reliable and loveable API integration experience.\n\nI have been working on the project day and night - testing, honing, extending, and even getting contributions from other helpful developers. I now have the project in a stable state where I need more feedback on real-life usage and testing.\n\nHere are some examples of it in action:\n\n## Simple API\n\n```python\nfrom clientele import api\n\nclient = api.APIClient(base_url=\"https://pokeapi.co/api/v2\")\n\n@client.get(\"/pokemon/{pokemon_name}\")\ndef get_pokemon_info(pokemon_name: str, result: dict) -&gt; dict:\n    return result\n```\n\n## Simple POST request\n\n```python\nfrom clientele import api\n\nclient = api.APIClient(base_url=\"https://httpbin.org\")\n\n\n@client.post(\"/post\")\ndef post_input_data(data: dict, result: dict) -&gt; dict:\n    return result\n```\n\n## Streaming responses\n\n```python\nfrom typing import AsyncIterator\nfrom pydantic import BaseModel\nfrom clientele import api\n\nclient = api.APIClient(base_url=\"http://localhost:8000\")\n\nclass Event(BaseModel):\n    text: str\n\n@client.get(\"/events\", streaming_response=True)\nasync def stream_events(*, result: AsyncIterator[Event]) -&gt; AsyncIterator[Event]:\n    return result\n```\n\nNew features include:\n\n- Handle streaming responses for Server Sent Events\n- Handle custom response parsing with callbacks\n- Sensible HTTP caching decorator with extendable backends\n- A Mypy plugin to handle the way the library injects parameters\n- Many many tweaks and updates to handle edge-case OpenAPI schemas\n\nPlease star ⭐ the project, give it a download and let me know what you think: https://github.com/phalt/clientele", "source": "Reddit", "date": "2026-01-15T17:25:53", "author": "phalt_", "score": 15, "tokens": ["follow", "clientele", "api", "integration", "framework", "python", "hello", "pythonista", "week", "ago", "share", "blog", "posturl", "alternative", "way", "build", "api", "integration", "heavily", "inspire", "developer", "experience", "python", "api", "framework", "project", "clientele", "let", "focus", "behaviour", "want", "api", "let", "handle", "rest", "network", "hydration", "cache", "data", "validation", "use", "strong", "type", "decorator", "build", "reliable", "loveable", "api", "integration", "experience", "work", "project", "day", "night", "testing", "honing", "extend", "get", "contribution", "helpful", "developer", "project", "stable", "state", "need", "feedback", "real", "life", "usage", "testing", "example", "action", "simple", "api", "python", "clientele", "import", "api", "client", "api.apiclientbase_urlurl", "mention.getpokemonpokemon_name", "def", "get_pokemon_infopokemon_name", "str", "result", "dict", "-gt", "dict", "return", "result", "  ", "simple", "post", "request", "python", "clientele", "import", "api", "client", "api.apiclientbase_urlurl", "mention.postpost", "def", "post_input_datadata", "dict", "result", "dict", "-gt", "dict", "return", "result", "  ", "streaming", "response", "python", "type", "import", "asynciterator", "pydantic", "import", "basemodel", "clientele", "import", "api", "client", "api.apiclientbase_urlurl", "class", "eventbasemodel", "text", "str", "mention.getevent", "streaming_responsetrue", "async", "def", "stream_event", "result", "asynciteratorevent", "-gt", "asynciteratorevent", "return", "result", "new", "feature", "include", "handle", "streaming", "response", "server", "send", "event", "handle", "custom", "response", "parse", "callback", "sensible", "http", "cache", "decorator", "extendable", "backend", "mypy", "plugin", "handle", "way", "library", "inject", "parameter", "tweak", "update", "handle", "edge", "case", "openapi", "schemas", "star", "project", "download", "let", "know", "think", "url"], "num_tokens": 181, "token_loss_pct": 36.49, "normalized_content": "follow up clientele - an api integration framework for python. hello pythonistas two weeks ago i shared a blog posturl about an alternative way of building api integrations heavily inspired by the developer experience of python api frameworks. what my project does clientele lets you focus on the behaviour you want from an api and let it handle the rest - networking hydration caching and data validation. it uses strong types and decorators to build a reliable and loveable api integration experience. i have been working on the project day and night - testing honing extending and even getting contributions from other helpful developers. i now have the project in a stable state where i need more feedback on real-life usage and testing. here are some examples of it in action  simple api python from clientele import api client  api.apiclientbase_urlurl mention.getpokemonpokemon_name def get_pokemon_infopokemon_name str result dict -gt dict return result   simple post request python from clientele import api client  api.apiclientbase_urlurl mention.postpost def post_input_datadata dict result dict -gt dict return result   streaming responses python from typing import asynciterator from pydantic import basemodel from clientele import api client  api.apiclientbase_urlurl class eventbasemodel text str mention.getevents streaming_responsetrue async def stream_events result asynciteratorevent -gt asynciteratorevent return result  new features include - handle streaming responses for server sent events - handle custom response parsing with callbacks - sensible http caching decorator with extendable backends - a mypy plugin to handle the way the library injects parameters - many many tweaks and updates to handle edge-case openapi schemas please star  the project give it a download and let me know what you think url"}
{"title": "pyauto_desktop: Benchmarks, window controls, OCR", "url": "https://www.reddit.com/r/Python/comments/1qgjcsk/pyauto_desktop_benchmarks_window_controls_ocr/", "content": "pyauto_desktop: Benchmarks, window controls, OCR. I have just released a major update to my pyauto\\_desktop module. Below is the list of new features introduced:\n\n# Optical character recognition\n\nI have added OCR support to my pyauto\\_desktop module, you can now detect text on your screen and automate it.\n\nExample of the inspector at work: [https://i.imgur.com/TqiXLWA.gif](https://i.imgur.com/TqiXLWA.gif)\n\n# Window Control:\n\nYou can now control program windows like minimize, maximize, move, focus and much more!\n\n# Benchmarks:\n\n**1. Standard UI Match**\n\n*Settings: 56x56 Template | Pyramid=True | Grayscale=False | Conf=0.95*\n\n|**Function**|**Library**|**FPS**|**Time (ms)**|**Speedup**|\n|:-|:-|:-|:-|:-|\n|`locateOnScreen`|PyAutoGUI|5.55|180ms|—|\n|`locateOnScreen`|**pyauto\\_desktop**|**23.35**|**42ms**|**4.2x**|\n|`locateAllOnScreen`|PyAutoGUI|5.56|180ms|—|\n|`locateAllOnScreen`|**pyauto\\_desktop**|**24.14**|**41ms**|**4.3x**|\n\n**2. Max Performance (Grayscale)**\n\n*Settings: 56x56 Template | Pyramid=True | Grayscale=True | Conf=0.95*\n\n|**Function**|**Library**|**FPS**|**Time (ms)**|**Speedup**|\n|:-|:-|:-|:-|:-|\n|`locateOnScreen`|PyAutoGUI|10.27|97ms|—|\n|`locateOnScreen`|**pyauto\\_desktop**|**27.13**|**36ms**|**2.6x**|\n|`locateAllOnScreen`|PyAutoGUI|10.20|98ms|—|\n|`locateAllOnScreen`|**pyauto\\_desktop**|**27.01**|**37ms**|**2.6x**|\n\n**3. Small Image / Raw Search (No Scaling)**\n\n*Settings: 24x24 Template | Pyramid=False | Grayscale=False | Conf=0.95*\n\n|**Function**|**Library**|**FPS**|**Time (ms)**|**Speedup**|\n|:-|:-|:-|:-|:-|\n|`locateOnScreen`|PyAutoGUI|6.08|164ms|—|\n|`locateOnScreen`|**pyauto\\_desktop**|**6.74**|**148ms**|**1.1x**|\n|`locateAllOnScreen`|PyAutoGUI|6.14|162ms|—|\n|`locateAllOnScreen`|**pyauto\\_desktop**|**7.12**|**140ms**|**1.2x**|\n\n# What My Project Does\n\nIt allows you to create shareable image or coordinate based automation regardless of resolution or dpr.\n\nIt features:  \n\\- **Built-in GUI Inspector** to snip, edit, test, and generate code.  \n\\- Uses `Session` logic to scale coordinates &amp; images automatically.  \n\\- **Up to 5x Faster.** Uses `mss` &amp; Pyramid Template Matching &amp; Image caching.  \n\\- `locateAny` / `locateAll` built-in. Finds the first or all matches from a list of images.  \n\\- OCR &amp; Window control\n\n# Target Audience\n\nProgramer who need to automate programs they don't have backend access to and aren't browser-based.\n\nYou can install it here: [pyauto-desktop · PyPI](https://pypi.org/project/pyauto-desktop/)  \nCode and Documentation: [pyauto-desktop: github](https://github.com/Omar-F-Rashed/pyauto-desktop)", "source": "Reddit", "date": "2026-01-18T21:26:37", "author": "MrYaml", "score": 13, "tokens": ["pyauto_desktop", "benchmark", "window", "control", "ocr", "release", "major", "update", "pyauto_desktop", "module", "list", "new", "feature", "introduce", "optical", "character", "recognition", "add", "ocr", "support", "pyauto_desktop", "module", "detect", "text", "screen", "automate", "example", "inspector", "work", "url", "window", "control", "control", "program", "window", "like", "minimize", "maximize", "focus", "benchmark", "standard", "ui", "match", "setting", "56x56", "template", "pyramidtrue", "grayscalefalse", "conf0.95", "functionlibraryfpstime", "msspeedup", "locateonscreenpyautogui5.55180ms", "locateonscreenpyauto_desktop23.3542ms4.2x", "locateallonscreenpyautogui5.56180ms", "locateallonscreenpyauto_desktop24.1441ms4.3x", "max", "performance", "grayscale", "setting", "56x56", "template", "pyramidtrue", "grayscaletrue", "conf0.95", "functionlibraryfpstime", "msspeedup", "locateonscreenpyautogui10.2797ms", "locateonscreenpyauto_desktop27.1336ms2.6x", "locateallonscreenpyautogui10.2098ms", "locateallonscreenpyauto_desktop27.0137ms2.6x", "small", "image", "raw", "search", "scaling", "setting", "24x24", "template", "pyramidfalse", "grayscalefalse", "conf0.95", "functionlibraryfpstime", "msspeedup", "locateonscreenpyautogui6.08164ms", "locateonscreenpyauto_desktop6.74148ms1.1x", "locateallonscreenpyautogui6.14162ms", "locateallonscreenpyauto_desktop7.12140ms1.2x", "project", "allow", "create", "shareable", "image", "coordinate", "base", "automation", "regardless", "resolution", "dpr", "feature", "build", "gui", "inspector", "snip", "edit", "test", "generate", "code", "use", "session", "logic", "scale", "coordinate", "amp", "image", "automatically", "5x", "fast", "use", "mss", "amp", "pyramid", "template", "match", "amp", "image", "cache", "locateany", "locateall", "build", "find", "match", "list", "image", "ocr", "amp", "window", "control", "target", "audience", "programer", "need", "automate", "program", "backend", "access", "browser", "base", "install", "pyauto", "desktop", "pypiurl", "code", "documentation", "pyauto", "desktop", "githuburl"], "num_tokens": 156, "token_loss_pct": 43.07, "normalized_content": "pyauto_desktop benchmarks window controls ocr. i have just released a major update to my pyauto_desktop module. below is the list of new features introduced  optical character recognition i have added ocr support to my pyauto_desktop module you can now detect text on your screen and automate it. example of the inspector at work url  window control you can now control program windows like minimize maximize move focus and much more  benchmarks 1. standard ui match settings 56x56 template  pyramidtrue  grayscalefalse  conf0.95 functionlibraryfpstime msspeedup ----- locateonscreenpyautogui5.55180ms locateonscreenpyauto_desktop23.3542ms4.2x locateallonscreenpyautogui5.56180ms locateallonscreenpyauto_desktop24.1441ms4.3x 2. max performance grayscale settings 56x56 template  pyramidtrue  grayscaletrue  conf0.95 functionlibraryfpstime msspeedup ----- locateonscreenpyautogui10.2797ms locateonscreenpyauto_desktop27.1336ms2.6x locateallonscreenpyautogui10.2098ms locateallonscreenpyauto_desktop27.0137ms2.6x 3. small image  raw search no scaling settings 24x24 template  pyramidfalse  grayscalefalse  conf0.95 functionlibraryfpstime msspeedup ----- locateonscreenpyautogui6.08164ms locateonscreenpyauto_desktop6.74148ms1.1x locateallonscreenpyautogui6.14162ms locateallonscreenpyauto_desktop7.12140ms1.2x  what my project does it allows you to create shareable image or coordinate based automation regardless of resolution or dpr. it features - built-in gui inspector to snip edit test and generate code. - uses session logic to scale coordinates amp images automatically. - up to 5x faster. uses mss amp pyramid template matching amp image caching. - locateany  locateall built-in. finds the first or all matches from a list of images. - ocr amp window control  target audience programer who need to automate programs they don't have backend access to and aren't browser-based. you can install it here pyauto-desktop  pypiurl code and documentation pyauto-desktop githuburl"}
{"title": "I built event2vector, a scikit‑style library for event sequence embeddings in Python)", "url": "https://www.reddit.com/r/Python/comments/1qgaac2/i_built_event2vector_a_scikitstyle_library_for/", "content": "I built event2vector, a scikit‑style library for event sequence embeddings in Python). # What event2vec Project Does\n\nI’ve been working on my Python library, Event2Vector (event2vec), for embedding event sequences (logs, clickstreams, POS tags, life‑event sequences, etc.) into vectors in a way that is easy to inspect and reason about.\n\nInstead of a complex RNN/transformer, the model uses a simple additive recurrent update: the hidden state for a sequence is constrained to behave like the sum of its event embeddings (the “linear additive hypothesis”). This makes sequence trajectories geometrically interpretable and supports vector arithmetic on histories (e.g., A − B + C style analogies on event trajectories).\n\nFrom the Python side, you primarily interact with a scikit‑learn‑style estimator:\n\n    python\n    from event2vector import Event2Vec\n    \n    model = Event2Vec(\n        num_event_types=len(vocab),\n        geometry=\"euclidean\",   # or \"hyperbolic\"\n        embedding_dim=128,\n        pad_sequences=True,\n        num_epochs=50,\n    )\n    model.fit(train_sequences, verbose=True)\n    embeddings = model.transform(train_sequences)\n\nThere are both Euclidean and hyperbolic (Poincaré ball) variants, so you can choose flat vs hierarchical geometry for your event space.\n\n# Target Audience\n\nPython users working with discrete event sequences: logs, clickstreams, POS tags, user journeys, synthetic processes, etc.\n\nE.g. posts about shopping patterns [https://substack.com/home/post/p-181632020?source=queue](https://substack.com/home/post/p-181632020?source=queue) or geometry of languages [https://sulcantonin.substack.com/p/the-geometry-of-language-families](https://sulcantonin.substack.com/p/the-geometry-of-language-families)\n\nPeople who want interpretable, geometric representations of sequences rather than just “it works but I can’t see what it’s doing.”\n\nIt is currently more of a research/analysis tool and prototyping library than a fully battle‑hardened production system, but:\n\nIt is MIT‑licensed and on PyPI (`pip install event2vector`).\n\nIt has a scikit‑style API (`fit`, `fit_transform`, `transform`, `most_similar`) and optional padded batching + GPU support, so it should drop into many Python ML workflows.\n\n# Comparison\n\n**Versus Word2Vec and similar context‑window models:**\n\nWord2Vec is excellent for capturing local co‑occurrence and semantic similarity, but it does not model the ordered trajectory of a sequence; contexts are effectively treated as bags of neighbors.\n\nEvent2Vector, in contrast, explicitly treats the hidden state as an ordered sum of event embeddings, and its training objective enforces that likely future events lie along the trajectory of that sum. This lets it capture sequential structure and trajectory geometry that Word2Vec is not designed to represent.\n\nIn the paper, an unsupervised experiment on the Brown Corpus shows that Event2Vector’s additive sequence embeddings produce clearer clusters of POS‑tag patterns than a Word2Vec baseline when you compose tag sequences and visualize them.\n\n**Versus generic RNNs / LSTMs / transformers:**\n\nThose models are more expressive and often better for pure prediction, but their hidden states are usually hard to interpret geometrically.\n\nEvent2Vector intentionally trades some expressivity for a simple, reversible additive structure: sequences are trajectories in a space where addition/subtraction have a clear meaning, and you can inspect them with PCA/t‑SNE or do analogical reasoning.\n\n# Python‑centric details\n\n* Install: `pip install event2vector` \n* Github Repo: [https://github.com/sulcantonin/event2vec\\_public/tree/main](https://github.com/sulcantonin/event2vec_public/tree/main)\n\nAccepts integer‑encoded sequences (Python lists / tensors), with optional padding for minibatching.\n\nProvides a tiny synthetic quickstart (START→A/B→C→END) that trains in seconds on CPU and plots embeddings with matplotlib, plus a Brown Corpus POS example that mirrors the paper.\n\nI’d love feedback f", "source": "Reddit", "date": "2026-01-18T15:41:43", "author": "sulcantonin", "score": 12, "tokens": ["build", "event2vector", "scikitstyle", "library", "event", "sequence", "embedding", "python", "event2vec", "project", "ve", "work", "python", "library", "event2vector", "event2vec", "embed", "event", "sequence", "log", "clickstream", "pos", "tag", "lifeevent", "sequence", "etc", "vector", "way", "easy", "inspect", "reason", "instead", "complex", "rnntransformer", "model", "use", "simple", "additive", "recurrent", "update", "hidden", "state", "sequence", "constrain", "behave", "like", "sum", "event", "embedding", "linear", "additive", "hypothesis", "make", "sequence", "trajectory", "geometrically", "interpretable", "support", "vector", "arithmetic", "history", "e.g.", "style", "analogy", "event", "trajectory", "python", "primarily", "interact", "scikitlearnstyle", "estimator", "python", "event2vector", "import", "event2vec", "model", "event2vec", "num_event_typeslenvocab", "geometryeuclidean", "hyperbolic", "embedding_dim128", "pad_sequencestrue", "num_epochs50", "model.fittrain_sequence", "verbosetrue", "embedding", "model.transformtrain_sequence", "euclidean", "hyperbolic", "poincaré", "ball", "variant", "choose", "flat", "vs", "hierarchical", "geometry", "event", "space", "target", "audience", "python", "user", "work", "discrete", "event", "sequence", "log", "clickstream", "pos", "tag", "user", "journey", "synthetic", "process", "etc", "e.g.", "post", "shopping", "pattern", "url", "geometry", "language", "url", "people", "want", "interpretable", "geometric", "representation", "sequence", "work", "not", "currently", "researchanalysis", "tool", "prototype", "library", "fully", "battlehardene", "production", "system", "mitlicense", "pypi", "pip", "install", "event2vector", "scikitstyle", "api", "fit", "fit_transform", "transform", "most_similar", "optional", "padded", "batch", "gpu", "support", "drop", "python", "ml", "workflow", "comparison", "versus", "word2vec", "similar", "contextwindow", "model", "word2vec", "excellent", "capture", "local", "cooccurrence", "semantic", "similarity", "model", "order", "trajectory", "sequence", "context", "effectively", "treat", "bag", "neighbor", "event2vector", "contrast", "explicitly", "treat", "hidden", "state", "order", "sum", "event", "embedding", "training", "objective", "enforce", "likely", "future", "event", "lie", "trajectory", "sum", "let", "capture", "sequential", "structure", "trajectory", "geometry", "word2vec", "design", "represent", "paper", "unsupervised", "experiment", "brown", "corpus", "show", "event2vector", "additive", "sequence", "embedding", "produce", "clear", "cluster", "postag", "pattern", "word2vec", "baseline", "compose", "tag", "sequence", "visualize", "versus", "generic", "rnns", "lstms", "transformer", "model", "expressive", "well", "pure", "prediction", "hidden", "state", "usually", "hard", "interpret", "geometrically", "event2vector", "intentionally", "trade", "expressivity", "simple", "reversible", "additive", "structure", "sequence", "trajectory", "space", "additionsubtraction", "clear", "meaning", "inspect", "pcatsne", "analogical", "reasoning", "pythoncentric", "detail", "install", "pip", "install", "event2vector", "github", "repo", "url", "accept", "integerencode", "sequence", "python", "list", "tensor", "optional", "padding", "minibatche", "provide", "tiny", "synthetic", "quickstart", "startabcend", "train", "second", "cpu", "plot", "embedding", "matplotlib", "plus", "brown", "corpus", "pos", "example", "mirror", "paper", "love", "feedback"], "num_tokens": 304, "token_loss_pct": 41.43, "normalized_content": "i built event2vector a scikitstyle library for event sequence embeddings in python.  what event2vec project does ive been working on my python library event2vector event2vec for embedding event sequences logs clickstreams pos tags lifeevent sequences etc. into vectors in a way that is easy to inspect and reason about. instead of a complex rnntransformer the model uses a simple additive recurrent update the hidden state for a sequence is constrained to behave like the sum of its event embeddings the linear additive hypothesis. this makes sequence trajectories geometrically interpretable and supports vector arithmetic on histories e.g. a  b  c style analogies on event trajectories. from the python side you primarily interact with a scikitlearnstyle estimator python from event2vector import event2vec model  event2vec num_event_typeslenvocab geometryeuclidean  or hyperbolic embedding_dim128 pad_sequencestrue num_epochs50  model.fittrain_sequences verbosetrue embeddings  model.transformtrain_sequences there are both euclidean and hyperbolic poincaré ball variants so you can choose flat vs hierarchical geometry for your event space.  target audience python users working with discrete event sequences logs clickstreams pos tags user journeys synthetic processes etc. e.g. posts about shopping patterns url or geometry of languages url people who want interpretable geometric representations of sequences rather than just it works but i cant see what its doing. it is currently more of a researchanalysis tool and prototyping library than a fully battlehardened production system but it is mitlicensed and on pypi pip install event2vector. it has a scikitstyle api fit fit_transform transform most_similar and optional padded batching  gpu support so it should drop into many python ml workflows.  comparison versus word2vec and similar contextwindow models word2vec is excellent for capturing local cooccurrence and semantic similarity but it does not model the ordered trajectory of a sequence contexts are effectively treated as bags of neighbors. event2vector in contrast explicitly treats the hidden state as an ordered sum of event embeddings and its training objective enforces that likely future events lie along the trajectory of that sum. this lets it capture sequential structure and trajectory geometry that word2vec is not designed to represent. in the paper an unsupervised experiment on the brown corpus shows that event2vectors additive sequence embeddings produce clearer clusters of postag patterns than a word2vec baseline when you compose tag sequences and visualize them. versus generic rnns  lstms  transformers those models are more expressive and often better for pure prediction but their hidden states are usually hard to interpret geometrically. event2vector intentionally trades some expressivity for a simple reversible additive structure sequences are trajectories in a space where additionsubtraction have a clear meaning and you can inspect them with pcatsne or do analogical reasoning.  pythoncentric details  install pip install event2vector  github repo url accepts integerencoded sequences python lists  tensors with optional padding for minibatching. provides a tiny synthetic quickstart startabcend that trains in seconds on cpu and plots embeddings with matplotlib plus a brown corpus pos example that mirrors the paper. id love feedback f"}
{"title": "Wikipedia turns 25, still boasting zero ads and over 7 billion visitors per month despite the rise of AI and threats of government repression", "url": "https://www.reddit.com/r/technology/comments/1qgk4it/wikipedia_turns_25_still_boasting_zero_ads_and/", "content": "Wikipedia turns 25, still boasting zero ads and over 7 billion visitors per month despite the rise of AI and threats of government repression.", "source": "Reddit", "date": "2026-01-18T21:58:29", "author": "Turbostrider27", "score": 60910, "tokens": ["wikipedia", "turn", "25", "boast", "zero", "ad", "billion", "visitor", "month", "despite", "rise", "ai", "threat", "government", "repression"], "num_tokens": 15, "token_loss_pct": 40.0, "normalized_content": "wikipedia turns 25 still boasting zero ads and over 7 billion visitors per month despite the rise of ai and threats of government repression."}
{"title": "X has stopped working", "url": "https://www.reddit.com/r/technology/comments/1qejnjo/x_has_stopped_working/", "content": "X has stopped working.", "source": "Reddit", "date": "2026-01-16T16:53:34", "author": "Well_Socialized", "score": 41862, "tokens": ["stop", "work"], "num_tokens": 2, "token_loss_pct": 60.0, "normalized_content": "x has stopped working."}
{"title": "US President Is Obsessed With Oil. But Chinese Batteries Will Soon Run the World.", "url": "https://www.reddit.com/r/technology/comments/1qh5kdg/us_president_is_obsessed_with_oil_but_chinese/", "content": "US President Is Obsessed With Oil. But Chinese Batteries Will Soon Run the World..", "source": "Reddit", "date": "2026-01-19T15:27:22", "author": "rezwenn", "score": 32692, "tokens": ["president", "obsess", "oil", "chinese", "battery", "soon", "run", "world"], "num_tokens": 8, "token_loss_pct": 50.0, "normalized_content": "us president is obsessed with oil. but chinese batteries will soon run the world.."}
{"title": "Supreme Court Hacked, Proving Its Cybersecurity Is As Robust As Its Ethical Code", "url": "https://www.reddit.com/r/technology/comments/1qd46jv/supreme_court_hacked_proving_its_cybersecurity_is/", "content": "Supreme Court Hacked, Proving Its Cybersecurity Is As Robust As Its Ethical Code.", "source": "Reddit", "date": "2026-01-15T01:13:25", "author": "Ok_Heron_5442", "score": 28813, "tokens": ["supreme", "court", "hack", "prove", "cybersecurity", "robust", "ethical", "code"], "num_tokens": 8, "token_loss_pct": 42.86, "normalized_content": "supreme court hacked proving its cybersecurity is as robust as its ethical code."}
{"title": "Jeff Bezos said the quiet part out loud — hopes that you'll give up your PC to rent one from the cloud", "url": "https://www.reddit.com/r/technology/comments/1qcqc5i/jeff_bezos_said_the_quiet_part_out_loud_hopes/", "content": "Jeff Bezos said the quiet part out loud — hopes that you'll give up your PC to rent one from the cloud.", "source": "Reddit", "date": "2026-01-14T16:33:46", "author": "ControlCAD", "score": 26759, "tokens": ["jeff", "bezos", "say", "quiet", "loud", "hop", "pc", "rent", "cloud"], "num_tokens": 9, "token_loss_pct": 62.5, "normalized_content": "jeff bezos said the quiet part out loud  hopes that you'll give up your pc to rent one from the cloud."}
{"title": "‘ELITE’: The Palantir App ICE Uses to Find Neighborhoods to Raid | Internal ICE material and testimony from an official obtained by 404 Media provides the clearest link yet between the technological infrastructure Palantir is building for ICE and the agency’s activities on the ground", "url": "https://www.reddit.com/r/technology/comments/1qdo076/elite_the_palantir_app_ice_uses_to_find/", "content": "‘ELITE’: The Palantir App ICE Uses to Find Neighborhoods to Raid | Internal ICE material and testimony from an official obtained by 404 Media provides the clearest link yet between the technological infrastructure Palantir is building for ICE and the agency’s activities on the ground.", "source": "Reddit", "date": "2026-01-15T17:22:46", "author": "Hrmbee", "score": 26366, "tokens": ["elite", "palantir", "app", "ice", "use", "find", "neighborhood", "raid", "internal", "ice", "material", "testimony", "official", "obtain", "404", "medium", "provide", "clear", "link", "technological", "infrastructure", "palantir", "build", "ice", "agencys", "activity", "ground"], "num_tokens": 27, "token_loss_pct": 41.3, "normalized_content": "elite the palantir app ice uses to find neighborhoods to raid  internal ice material and testimony from an official obtained by 404 media provides the clearest link yet between the technological infrastructure palantir is building for ice and the agencys activities on the ground."}
{"title": "AI hype meets reality as majority of CEOs report no financial returns", "url": "https://www.reddit.com/r/technology/comments/1qi1t48/ai_hype_meets_reality_as_majority_of_ceos_report/", "content": "AI hype meets reality as majority of CEOs report no financial returns.", "source": "Reddit", "date": "2026-01-20T14:59:18", "author": "AdSpecialist6598", "score": 26276, "tokens": ["ai", "hype", "meet", "reality", "majority", "ceo", "report", "financial", "return"], "num_tokens": 9, "token_loss_pct": 30.77, "normalized_content": "ai hype meets reality as majority of ceos report no financial returns."}
{"title": "DOGE employees may have improperly accessed social security data, DOJ says", "url": "https://www.reddit.com/r/technology/comments/1qifeym/doge_employees_may_have_improperly_accessed/", "content": "DOGE employees may have improperly accessed social security data, DOJ says.", "source": "Reddit", "date": "2026-01-20T23:15:45", "author": "esporx", "score": 23990, "tokens": ["doge", "employee", "improperly", "access", "social", "security", "datum", "doj", "say"], "num_tokens": 9, "token_loss_pct": 25.0, "normalized_content": "doge employees may have improperly accessed social security data doj says."}
{"title": "Website that leaked info about ICE agents is down after cyberattack.", "url": "https://www.reddit.com/r/technology/comments/1qd1tsz/website_that_leaked_info_about_ice_agents_is_down/", "content": "Website that leaked info about ICE agents is down after cyberattack..", "source": "Reddit", "date": "2026-01-14T23:38:17", "author": "MiamiPower", "score": 21813, "tokens": ["website", "leak", "info", "ice", "agent", "cyberattack"], "num_tokens": 6, "token_loss_pct": 50.0, "normalized_content": "website that leaked info about ice agents is down after cyberattack.."}
{"title": "President Bought at Least $1 Million in Netflix, Warner Bros. Discovery Bonds Following Their Deal Announcement", "url": "https://www.reddit.com/r/technology/comments/1qfzfmm/president_bought_at_least_1_million_in_netflix/", "content": "President Bought at Least $1 Million in Netflix, Warner Bros. Discovery Bonds Following Their Deal Announcement.", "source": "Reddit", "date": "2026-01-18T05:58:05", "author": "ControlCAD", "score": 20302, "tokens": ["president", "buy", "million", "netflix", "warner", "bros", "discovery", "bond", "follow", "deal", "announcement"], "num_tokens": 11, "token_loss_pct": 38.89, "normalized_content": "president bought at least 1 million in netflix warner bros. discovery bonds following their deal announcement."}
{"title": "Russia threatens to ban GTA 6 if Rockstar doesn't remove male strippers and other \"immoral content\"", "url": "https://www.reddit.com/r/technology/comments/1qhfd2q/russia_threatens_to_ban_gta_6_if_rockstar_doesnt/", "content": "Russia threatens to ban GTA 6 if Rockstar doesn't remove male strippers and other \"immoral content\".", "source": "Reddit", "date": "2026-01-19T21:15:55", "author": "IndicaOatmeal", "score": 17687, "tokens": ["russia", "threaten", "ban", "gta", "rockstar", "remove", "male", "stripper", "immoral", "content"], "num_tokens": 10, "token_loss_pct": 44.44, "normalized_content": "russia threatens to ban gta 6 if rockstar doesn't remove male strippers and other immoral content."}
{"title": "East coast could soon get rolling blackouts during summer because data centers have pushed electric grid to the limit", "url": "https://www.reddit.com/r/technology/comments/1qflq0x/east_coast_could_soon_get_rolling_blackouts/", "content": "East coast could soon get rolling blackouts during summer because data centers have pushed electric grid to the limit.", "source": "Reddit", "date": "2026-01-17T20:01:52", "author": "MetaKnowing", "score": 14179, "tokens": ["east", "coast", "soon", "roll", "blackout", "summer", "data", "center", "push", "electric", "grid", "limit"], "num_tokens": 12, "token_loss_pct": 40.0, "normalized_content": "east coast could soon get rolling blackouts during summer because data centers have pushed electric grid to the limit."}
{"title": "Majority of CEOs report zero payoff from AI splurge", "url": "https://www.reddit.com/r/technology/comments/1qi422a/majority_of_ceos_report_zero_payoff_from_ai/", "content": "Majority of CEOs report zero payoff from AI splurge.", "source": "Reddit", "date": "2026-01-20T16:27:10", "author": "kim82352", "score": 13728, "tokens": ["majority", "ceo", "report", "zero", "payoff", "ai", "splurge"], "num_tokens": 7, "token_loss_pct": 30.0, "normalized_content": "majority of ceos report zero payoff from ai splurge."}
{"title": "Financial Expert Says OpenAI Is on the Verge of Running Out of Money", "url": "https://www.reddit.com/r/technology/comments/1qe7vop/financial_expert_says_openai_is_on_the_verge_of/", "content": "Financial Expert Says OpenAI Is on the Verge of Running Out of Money.", "source": "Reddit", "date": "2026-01-16T06:52:47", "author": "Infinityy100b", "score": 13488, "tokens": ["financial", "expert", "say", "openai", "verge", "run", "money"], "num_tokens": 7, "token_loss_pct": 50.0, "normalized_content": "financial expert says openai is on the verge of running out of money."}
{"title": "Six months later, President Mobile still hasn’t delivered preordered phones | Lawmakers seek FTC investigation, but President has taken control of the agency.", "url": "https://www.reddit.com/r/technology/comments/1qdsy8g/six_months_later_president_mobile_still_hasnt/", "content": "Six months later, President Mobile still hasn’t delivered preordered phones | Lawmakers seek FTC investigation, but President has taken control of the agency..", "source": "Reddit", "date": "2026-01-15T20:19:25", "author": "ControlCAD", "score": 11904, "tokens": ["month", "later", "president", "mobile", "not", "deliver", "preordere", "phone", "lawmaker", "seek", "ftc", "investigation", "president", "take", "control", "agency"], "num_tokens": 16, "token_loss_pct": 36.0, "normalized_content": "six months later president mobile still hasnt delivered preordered phones  lawmakers seek ftc investigation but president has taken control of the agency.."}
{"title": "DOGE Cuts “Unexpectedly and Significantly Impacted” Critical Pentagon Unit | Staffing problems caused by DOGE resulted in the Defense Information Systems Agency warning of “extreme risk for loss of service” across the military", "url": "https://www.reddit.com/r/technology/comments/1qh7v3o/doge_cuts_unexpectedly_and_significantly_impacted/", "content": "DOGE Cuts “Unexpectedly and Significantly Impacted” Critical Pentagon Unit | Staffing problems caused by DOGE resulted in the Defense Information Systems Agency warning of “extreme risk for loss of service” across the military.", "source": "Reddit", "date": "2026-01-19T16:53:22", "author": "Hrmbee", "score": 11375, "tokens": ["doge", "cut", "unexpectedly", "significantly", "impact", "critical", "pentagon", "unit", "staffing", "problem", "cause", "doge", "result", "defense", "information", "system", "agency", "warning", "extreme", "risk", "loss", "service", "military"], "num_tokens": 23, "token_loss_pct": 32.35, "normalized_content": "doge cuts unexpectedly and significantly impacted critical pentagon unit  staffing problems caused by doge resulted in the defense information systems agency warning of extreme risk for loss of service across the military."}
{"title": "Bandcamp becomes the first major music platform to ban AI content", "url": "https://www.reddit.com/r/technology/comments/1qcuxli/bandcamp_becomes_the_first_major_music_platform/", "content": "Bandcamp becomes the first major music platform to ban AI content.", "source": "Reddit", "date": "2026-01-14T19:20:54", "author": "ornithobiography", "score": 10563, "tokens": ["bandcamp", "major", "music", "platform", "ban", "ai", "content"], "num_tokens": 7, "token_loss_pct": 41.67, "normalized_content": "bandcamp becomes the first major music platform to ban ai content."}
{"title": "FBI fights leaks by seizing Washington Post reporter’s phone, laptops, and watch | FBI searches home and devices of reporter who has over 1,100 government contacts.", "url": "https://www.reddit.com/r/technology/comments/1qd57lv/fbi_fights_leaks_by_seizing_washington_post/", "content": "FBI fights leaks by seizing Washington Post reporter’s phone, laptops, and watch | FBI searches home and devices of reporter who has over 1,100 government contacts..", "source": "Reddit", "date": "2026-01-15T01:57:43", "author": "ControlCAD", "score": 9449, "tokens": ["fbi", "fight", "leak", "seize", "washington", "post", "reporter", "phone", "laptop", "watch", "fbi", "search", "home", "device", "reporter", "1100", "government", "contact"], "num_tokens": 18, "token_loss_pct": 33.33, "normalized_content": "fbi fights leaks by seizing washington post reporters phone laptops and watch  fbi searches home and devices of reporter who has over 1100 government contacts.."}
{"title": "AI boom could falter without wider adoption, Microsoft chief Satya Nadella warns", "url": "https://www.reddit.com/r/technology/comments/1qi5si0/ai_boom_could_falter_without_wider_adoption/", "content": "AI boom could falter without wider adoption, Microsoft chief Satya Nadella warns.", "source": "Reddit", "date": "2026-01-20T17:30:26", "author": "PaiDuck", "score": 8692, "tokens": ["ai", "boom", "falter", "wide", "adoption", "microsoft", "chief", "satya", "nadella", "warn"], "num_tokens": 10, "token_loss_pct": 23.08, "normalized_content": "ai boom could falter without wider adoption microsoft chief satya nadella warns."}
{"title": "Data Centers Will Consume 70 Percent Of Memory Chips made in 2026, RAM Shortage Will Last Until Until Atleast 2029 As Manafacturing Capacity For RAM In 2028 That Hasnt Even Been Made Yet Is Already being Sold", "url": "https://www.reddit.com/r/technology/comments/1qgolhs/data_centers_will_consume_70_percent_of_memory/", "content": "Data Centers Will Consume 70 Percent Of Memory Chips made in 2026, RAM Shortage Will Last Until Until Atleast 2029 As Manafacturing Capacity For RAM In 2028 That Hasnt Even Been Made Yet Is Already being Sold.", "source": "Reddit", "date": "2026-01-19T01:03:04", "author": "Shogouki", "score": 8226, "tokens": ["data", "center", "consume", "70", "percent", "memory", "chip", "2026", "ram", "shortage", "atleast", "2029", "manafacture", "capacity", "ram", "2028", "not", "sell"], "num_tokens": 18, "token_loss_pct": 53.85, "normalized_content": "data centers will consume 70 percent of memory chips made in 2026 ram shortage will last until until atleast 2029 as manafacturing capacity for ram in 2028 that hasnt even been made yet is already being sold."}
{"title": "Natural language processing", "url": "https://en.wikipedia.org/wiki/Natural_language_processing", "content": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly.\nMajor processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.\n\nHistory\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n\nSymbolic NLP (1950s – early 1990s)\nThe premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n\n1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.\n1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.\n1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first chatterbots were written (e.g., PARRY).\n1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.\n\nStatistical NLP (1990s–present)\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. \n\n1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems o", "source": "Wikipedia", "date": null, "author": null, "score": null, "tokens": ["natural", "language", "processing", "nlp", "processing", "natural", "language", "information", "computer", "nlp", "subfield", "computer", "science", "closely", "associate", "artificial", "intelligence", "nlp", "relate", "information", "retrieval", "knowledge", "representation", "computational", "linguistic", "linguistic", "broadly", "major", "processing", "task", "nlp", "system", "include", "speech", "recognition", "text", "classification", "natural", "language", "understanding", "natural", "language", "generation", "history", "natural", "language", "processing", "root", "1950s", "1950", "alan", "ture", "publish", "article", "title", "compute", "machinery", "intelligence", "propose", "call", "ture", "test", "criterion", "intelligence", "time", "articulate", "problem", "separate", "artificial", "intelligence", "propose", "test", "include", "task", "involve", "automate", "interpretation", "generation", "natural", "language", "symbolic", "nlp", "1950s", "early", "1990s", "premise", "symbolic", "nlp", "illustrate", "john", "searle", "chinese", "room", "think", "experiment", "give", "collection", "rule", "e.g.", "chinese", "phrasebook", "question", "matching", "answer", "computer", "emulate", "natural", "language", "understanding", "nlp", "task", "apply", "rule", "datum", "confront", "1950", "georgetown", "experiment", "1954", "involve", "fully", "automatic", "translation", "russian", "sentence", "english", "author", "claim", "year", "machine", "translation", "solved", "problem", "real", "progress", "slow", "alpac", "report", "1966", "find", "year", "research", "fail", "fulfill", "expectation", "fund", "machine", "translation", "dramatically", "reduce", "little", "research", "machine", "translation", "conduct", "america", "research", "continue", "japan", "europe", "late", "1980", "statistical", "machine", "translation", "system", "develop", "1960", "notably", "successful", "natural", "language", "processing", "system", "develop", "1960", "shrdlu", "natural", "language", "system", "work", "restrict", "block", "world", "restrict", "vocabulary", "eliza", "simulation", "rogerian", "psychotherapy", "write", "joseph", "weizenbaum", "1964", "1966", "despite", "minimal", "information", "human", "thought", "emotion", "eliza", "able", "produce", "interaction", "appear", "human", "like", "patient", "exceed", "small", "knowledge", "base", "eliza", "provide", "generic", "response", "example", "respond", "head", "hurt", "head", "hurt", "ross", "quillian", "successful", "work", "natural", "language", "demonstrate", "vocabulary", "word", "fit", "computer", "memory", "time", "1970", "1970", "programmer", "begin", "write", "conceptual", "ontology", "structure", "real", "world", "information", "computer", "understandable", "datum", "example", "margie", "schank", "1975", "sam", "cullingford", "1978", "pam", "wilensky", "1978", "talespin", "meehan", "1976", "qualm", "lehnert", "1977", "politic", "carbonell", "1979", "plot", "unit", "lehnert", "1981", "time", "chatterbot", "write", "e.g.", "parry", "1980s", "1980", "early", "1990", "mark", "heyday", "symbolic", "method", "nlp", "focus", "area", "time", "include", "research", "rule", "base", "parse", "e.g.", "development", "hpsg", "computational", "operationalization", "generative", "grammar", "morphology", "e.g.", "level", "morphology", "semantic", "e.g.", "lesk", "algorithm", "reference", "e.g.", "center", "theory", "area", "natural", "language", "understanding", "e.g.", "rhetorical", "structure", "theory", "line", "research", "continue", "e.g.", "development", "chatterbot", "racter", "jabberwacky", "important", "development", "eventually", "lead", "statistical", "turn", "1990", "rise", "importance", "quantitative", "evaluation", "period", "statistical", "nlp", "1990spresent", "1980s", "natural", "language", "processing", "system", "base", "complex", "set", "hand", "write", "rule", "start", "late", "1980", "revolution", "natural", "language", "processing", "introduction", "machine", "learn", "algorithm", "language", "processing", "shift", "influence", "increase", "computational", "power", "moore", "law", "decline", "dominance", "chomskyan", "linguistic", "theory", "e.g.", "transformational", "grammar", "theoretical", "underpinning", "discourage", "sort", "corpus", "linguistic", "underlie", "machine", "learn", "approach", "language", "processing", "1990", "notable", "early", "success", "statistical", "method", "nlp", "occur", "field", "machine", "translation", "especially", "work", "ibm", "research", "ibm", "alignment", "model", "system", "able", "advantage", "exist", "multilingual", "textual", "corpora", "produce", "parliament", "canada", "european", "union", "result", "law", "call", "translation", "governmental", "proceeding", "official", "language", "correspond", "system"], "num_tokens": 436, "token_loss_pct": 43.01, "normalized_content": "natural language processing nlp is the processing of natural language information by a computer. nlp is a subfield of computer science and is closely associated with artificial intelligence. nlp is also related to information retrieval knowledge representation computational linguistics and linguistics more broadly. major processing tasks in an nlp system include speech recognition text classification natural language understanding and natural language generation. history natural language processing has its roots in the 1950s. already in 1950 alan turing published an article titled computing machinery and intelligence which proposed what is now called the turing test as a criterion of intelligence though at the time that was not articulated as a problem separate from artificial intelligence. the proposed test includes a task that involves the automated interpretation and generation of natural language. symbolic nlp 1950s  early 1990s the premise of symbolic nlp is often illustrated using john searle's chinese room thought experiment given a collection of rules e.g. a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it confronts. 1950s the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years machine translation would be a solved problem. however real progress was much slower and after the alpac report in 1966 which found that ten years of research had failed to fulfill the expectations funding for machine translation was dramatically reduced. little further research in machine translation was conducted in america though some research continued elsewhere such as japan and europe until the late 1980s when the first statistical machine translation systems were developed. 1960s some notably successful natural language processing systems developed in the 1960s were shrdlu a natural language system working in restricted blocks worlds with restricted vocabularies and eliza a simulation of a rogerian psychotherapy written by joseph weizenbaum between 1964 and 1966. despite using minimal information about human thought or emotion eliza was able to produce interactions that appeared human-like. when the patient exceeded the very small knowledge base eliza might provide a generic response for example responding to my head hurts with why do you say your head hurts. ross quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words because that was all that would fit in a computer memory at the time. 1970s during the 1970s many programmers began to write conceptual ontologies which structured real-world information into computer-understandable data. examples are margie schank 1975 sam cullingford 1978 pam wilensky 1978 talespin meehan 1976 qualm lehnert 1977 politics carbonell 1979 and plot units lehnert 1981. during this time the first chatterbots were written e.g. parry. 1980s the 1980s and early 1990s mark the heyday of symbolic methods in nlp. focus areas of the time included research on rule-based parsing e.g. the development of hpsg as a computational operationalization of generative grammar morphology e.g. two-level morphology semantics e.g. lesk algorithm reference e.g. within centering theory and other areas of natural language understanding e.g. in the rhetorical structure theory. other lines of research were continued e.g. the development of chatterbots with racter and jabberwacky. an important development that eventually led to the statistical turn in the 1990s was the rising importance of quantitative evaluation in this period. statistical nlp 1990spresent up until the 1980s most natural language processing systems were based on complex sets of hand-written rules. starting in the late 1980s however there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. this shift was influenced by increasing computational power see moore's law and a decline in the dominance of chomskyan linguistic theories... e.g. transformational grammar whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 1990s many of the notable early successes in statistical methods in nlp occurred in the field of machine translation due especially to work at ibm research such as ibm alignment models. these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems o"}
{"title": "Deep learning", "url": "https://en.wikipedia.org/wiki/Deep_learning", "content": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n\nOverview\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\nFundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\nImportantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.\nThe term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated.\n\nInterpretations\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\nThe classic u", "source": "Wikipedia", "date": null, "author": null, "score": null, "tokens": ["machine", "learn", "deep", "learning", "focus", "utilize", "multilayere", "neural", "network", "perform", "task", "classification", "regression", "representation", "learning", "field", "take", "inspiration", "biological", "neuroscience", "revolve", "stack", "artificial", "neuron", "layer", "train", "process", "datum", "adjective", "deep", "refer", "use", "multiple", "layer", "range", "thousand", "network", "method", "supervise", "semi", "supervised", "unsupervised", "common", "deep", "learning", "network", "architecture", "include", "fully", "connect", "network", "deep", "belief", "network", "recurrent", "neural", "network", "convolutional", "neural", "network", "generative", "adversarial", "network", "transformer", "neural", "radiance", "field", "architecture", "apply", "field", "include", "computer", "vision", "speech", "recognition", "natural", "language", "processing", "machine", "translation", "bioinformatics", "drug", "design", "medical", "image", "analysis", "climate", "science", "material", "inspection", "board", "game", "program", "produce", "result", "comparable", "case", "surpass", "human", "expert", "performance", "early", "form", "neural", "network", "inspire", "information", "processing", "distribute", "communication", "node", "biological", "system", "particularly", "human", "brain", "current", "neural", "network", "intend", "model", "brain", "function", "organism", "generally", "see", "low", "quality", "model", "purpose", "overview", "modern", "deep", "learning", "model", "base", "multi", "layered", "neural", "network", "convolutional", "neural", "network", "transformer", "include", "propositional", "formula", "latent", "variable", "organize", "layer", "wise", "deep", "generative", "model", "node", "deep", "belief", "network", "deep", "boltzmann", "machine", "fundamentally", "deep", "learning", "refer", "class", "machine", "learn", "algorithm", "hierarchy", "layer", "transform", "input", "datum", "progressively", "abstract", "composite", "representation", "example", "image", "recognition", "model", "raw", "input", "image", "represent", "tensor", "pixel", "representational", "layer", "attempt", "identify", "basic", "shape", "line", "circle", "second", "layer", "compose", "encode", "arrangement", "edge", "layer", "encode", "nose", "eye", "fourth", "layer", "recognize", "image", "contain", "face", "importantly", "deep", "learning", "process", "learn", "feature", "optimally", "place", "level", "prior", "deep", "learning", "machine", "learn", "technique", "involve", "hand", "craft", "feature", "engineering", "transform", "datum", "suitable", "representation", "classification", "algorithm", "operate", "deep", "learning", "approach", "feature", "hand", "craft", "model", "discover", "useful", "feature", "representation", "datum", "automatically", "eliminate", "need", "hand", "tuning", "example", "vary", "number", "layer", "layer", "size", "provide", "different", "degree", "abstraction", "word", "deep", "deep", "learning", "refer", "number", "layer", "data", "transform", "precisely", "deep", "learning", "system", "substantial", "credit", "assignment", "path", "cap", "depth", "cap", "chain", "transformation", "input", "output", "cap", "describe", "potentially", "causal", "connection", "input", "output", "feedforward", "neural", "network", "depth", "cap", "network", "number", "hide", "layer", "plus", "output", "layer", "parameterize", "recurrent", "neural", "network", "signal", "propagate", "layer", "cap", "depth", "potentially", "unlimited", "universally", "agree", "threshold", "depth", "divide", "shallow", "learning", "deep", "learning", "researcher", "agree", "deep", "learning", "involve", "cap", "depth", "high", "cap", "depth", "show", "universal", "approximator", "sense", "emulate", "function", "layer", "add", "function", "approximator", "ability", "network", "deep", "model", "cap", "able", "extract", "well", "feature", "shallow", "model", "extra", "layer", "help", "learn", "feature", "effectively", "deep", "learning", "architecture", "construct", "greedy", "layer", "layer", "method", "deep", "learning", "help", "disentangle", "abstraction", "pick", "feature", "improve", "performance", "deep", "learning", "algorithm", "apply", "unsupervised", "learning", "task", "important", "benefit", "unlabeled", "datum", "abundant", "label", "datum", "example", "deep", "structure", "train", "unsupervised", "manner", "deep", "belief", "network", "term", "deep", "learning", "introduce", "machine", "learn", "community", "rina", "dechter", "1986", "artificial", "neural", "network", "igor", "aizenberg", "colleague", "2000", "context", "boolean", "threshold", "neuron", "history", "appearance", "apparently", "complicated", "interpretation", "deep", "neural", "network", "generally", "interpret", "term", "universal", "approximation", "theorem", "probabilistic", "inference", "classic"], "num_tokens": 445, "token_loss_pct": 44.1, "normalized_content": "in machine learning deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification regression and representation learning. the field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and training them to process data. the adjective deep refers to the use of multiple layers ranging from three to several hundred or thousands in the network. methods used can be supervised semi-supervised or unsupervised. some common deep learning network architectures include fully connected networks deep belief networks recurrent neural networks convolutional neural networks generative adversarial networks transformers and neural radiance fields. these architectures have been applied to fields including computer vision speech recognition natural language processing machine translation bioinformatics drug design medical image analysis climate science material inspection and board game programs where they have produced results comparable to and in some cases surpassing human expert performance. early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems particularly the human brain. however current neural networks do not intend to model the brain function of organisms and are generally seen as low-quality models for that purpose. overview most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep boltzmann machines. fundamentally deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. for example in an image recognition model the raw input may be an image represented as a tensor of pixels. the first representational layer may attempt to identify basic shapes such as lines and circles the second layer may compose and encode arrangements of edges the third layer may encode a nose and eyes and the fourth layer may recognize that the image contains a face. importantly a deep learning process can learn which features to optimally place at which level on its own. prior to deep learning machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. in the deep learning approach features are not hand-crafted and the model discovers useful feature representations from the data automatically. this does not eliminate the need for hand-tuning for example varying numbers of layers and layer sizes can provide different degrees of abstraction. the word deep in deep learning refers to the number of layers through which the data is transformed. more precisely deep learning systems have a substantial credit assignment path cap depth. the cap is the chain of transformations from input to output. caps describe potentially causal connections between input and output. for a feedforward neural network the depth of the caps is that of the network and is the number of hidden layers plus one as the output layer is also parameterized. for recurrent neural networks in which a signal may propagate through a layer more than once the cap depth is potentially unlimited. no universally agreed-upon threshold of depth divides shallow learning from deep learning but most researchers agree that deep learning involves cap depth higher than two. cap of depth two has been shown to be a universal approximator in the sense that it can emulate any function. beyond that more layers do not add to the function approximator ability of the network. deep models cap  two are able to extract better features than shallow models and hence extra layers help in learning the features effectively. deep learning architectures can be constructed with a greedy layer-by-layer method. deep learning helps to disentangle these abstractions and pick out which features improve performance. deep learning algorithms can be applied to unsupervised learning tasks. this is an important benefit because unlabeled data is more abundant than the labeled data. examples of deep structures that can be trained in an unsupervised manner are deep belief networks. the term deep learning was introduced to the machine learning community by rina dechter in 1986 and to artificial neural networks by igor aizenberg and colleagues in 2000 in the context of boolean threshold neurons. although the history of its appearance is apparently more complicated. interpretations deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. the classic u"}
{"title": "Machine learning", "url": "https://en.wikipedia.org/wiki/Machine_learning", "content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.\nFrom a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework.\n\nHistory\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used during this time period.\nThe earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think?\", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\".\nModern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.\n\nCurrent Supervised Learning Algorithms have objectives of classification and regression.\nCurrent Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.\nCurrent Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods.\nIn 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques.\n\nRelationships to other fields\nArtificial intelligence\nAs a scientific endeavour, machine learning grew out of the quest for artifi", "source": "Wikipedia", "date": null, "author": null, "score": null, "tokens": ["machine", "learning", "ml", "field", "study", "artificial", "intelligence", "concern", "development", "study", "statistical", "algorithm", "learn", "datum", "generalise", "unseen", "datum", "perform", "task", "explicit", "instruction", "subdiscipline", "machine", "learning", "advance", "field", "deep", "learning", "allow", "neural", "network", "class", "statistical", "algorithm", "surpass", "previous", "machine", "learn", "approach", "performance", "ml", "find", "application", "field", "include", "natural", "language", "processing", "computer", "vision", "speech", "recognition", "email", "filter", "agriculture", "medicine", "application", "ml", "business", "problem", "know", "predictive", "analytic", "statistic", "mathematical", "optimisation", "mathematical", "programming", "method", "comprise", "foundation", "machine", "learning", "datum", "mining", "related", "field", "study", "focus", "exploratory", "datum", "analysis", "eda", "unsupervised", "learning", "theoretical", "viewpoint", "probably", "approximately", "correct", "learning", "provide", "mathematical", "statistical", "framework", "describe", "machine", "learning", "traditional", "machine", "learning", "deep", "learning", "algorithm", "describe", "empirical", "risk", "minimisation", "framework", "history", "term", "machine", "learning", "coin", "1959", "arthur", "samuel", "ibm", "employee", "pioneer", "field", "computer", "gaming", "artificial", "intelligence", "synonym", "self", "teach", "computer", "time", "period", "early", "machine", "learning", "program", "introduce", "1950", "arthur", "samuel", "invent", "computer", "program", "calculate", "win", "chance", "checker", "history", "machine", "learn", "root", "decade", "human", "desire", "effort", "study", "human", "cognitive", "process", "1949", "canadian", "psychologist", "donald", "hebb", "publish", "book", "organization", "behavior", "introduce", "theoretical", "neural", "structure", "form", "certain", "interaction", "nerve", "cell", "hebb", "model", "neuron", "interact", "set", "groundwork", "ais", "machine", "learning", "algorithm", "work", "node", "artificial", "neuron", "computer", "communicate", "datum", "researcher", "study", "human", "cognitive", "system", "contribute", "modern", "machine", "learn", "technology", "include", "logician", "walter", "pitt", "warren", "mcculloch", "propose", "early", "mathematical", "model", "neural", "network", "come", "algorithm", "mirror", "human", "thought", "process", "early", "1960", "experimental", "learn", "machine", "punch", "tape", "memory", "call", "cybertron", "develop", "raytheon", "company", "analyse", "sonar", "signal", "electrocardiogram", "speech", "pattern", "rudimentary", "reinforcement", "learning", "repetitively", "train", "human", "operatorteacher", "recognise", "pattern", "equip", "goof", "button", "cause", "reevaluate", "incorrect", "decision", "representative", "book", "research", "machine", "learning", "1960", "nil", "nilsson", "book", "learn", "machine", "deal", "machine", "learn", "pattern", "classification", "interest", "relate", "pattern", "recognition", "continue", "1970", "describe", "duda", "hart", "1973", "1981", "report", "give", "teaching", "strategy", "artificial", "neural", "network", "learn", "recognise", "40", "character", "26", "letter", "10", "digit", "special", "symbol", "computer", "terminal", "tom", "m.", "mitchell", "provide", "widely", "quote", "formal", "definition", "algorithm", "study", "machine", "learn", "field", "computer", "program", "say", "learn", "experience", "respect", "class", "task", "performance", "measure", "performance", "task", "measure", "improve", "experience", "e.", "definition", "task", "machine", "learning", "concern", "offer", "fundamentally", "operational", "definition", "define", "field", "cognitive", "term", "follow", "alan", "turing", "proposal", "paper", "computing", "machinery", "intelligence", "question", "machine", "think", "replace", "question", "machine", "think", "entity", "modern", "day", "machine", "learning", "algorithm", "break", "algorithm", "type", "supervise", "learning", "algorithm", "unsupervised", "learn", "algorithm", "reinforcement", "learning", "algorithm", "current", "supervised", "learning", "algorithm", "objective", "classification", "regression", "current", "unsupervised", "learning", "algorithm", "objective", "cluster", "dimensionality", "reduction", "association", "rule", "current", "reinforcement", "learning", "algorithm", "focus", "decision", "respect", "previous", "unknown", "time", "break", "study", "model", "base", "method", "model", "free", "method", "2014", "ian", "goodfellow", "introduce", "generative", "adversarial", "network", "gan", "realistic", "datum", "synthesis", "2016", "alphago", "obtain", "victory", "human", "player", "reinforcement", "learning", "technique", "relationship", "field", "artificial", "intelligence", "scientific", "endeavour", "machine", "learning", "grow", "quest", "artifi"], "num_tokens": 443, "token_loss_pct": 41.4, "normalized_content": "machine learning ml is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data and thus perform tasks without explicit instructions. within a subdiscipline in machine learning advances in the field of deep learning have allowed neural networks a class of statistical algorithms to surpass many previous machine learning approaches in performance. ml finds application in many fields including natural language processing computer vision speech recognition email filtering agriculture and medicine. the application of ml to business problems is known as predictive analytics. statistics and mathematical optimisation mathematical programming methods comprise the foundations of machine learning. data mining is a related field of study focusing on exploratory data analysis eda through unsupervised learning. from a theoretical viewpoint probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework. history the term machine learning was coined in 1959 by arthur samuel an ibm employee and pioneer in the field of computer gaming and artificial intelligence. the synonym self-teaching computers was also used during this time period. the earliest machine learning program was introduced in the 1950s when arthur samuel invented a computer program that calculated the winning chance in checkers for each side but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. in 1949 canadian psychologist donald hebb published the book the organization of behavior in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. hebb's model of neurons interacting with one another set a groundwork for how ais and machine learning algorithms work under nodes or artificial neurons used by computers to communicate data. other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well including logician walter pitts and warren mcculloch who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. by the early 1960s an experimental learning machine with punched tape memory called cybertron had been developed by raytheon company to analyse sonar signals electrocardiograms and speech patterns using rudimentary reinforcement learning. it was repetitively trained by a human operatorteacher to recognise patterns and equipped with a goof button to cause it to reevaluate incorrect decisions. a representative book on research into machine learning during the 1960s was nils nilsson's book on learning machines dealing mostly with machine learning for pattern classification. interest related to pattern recognition continued into the 1970s as described by duda and hart in 1973. in 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters 26 letters 10 digits and 4 special symbols from a computer terminal. tom m. mitchell provided a widely quoted more formal definition of the algorithms studied in the machine learning field a computer program is said to learn from experience e with respect to some class of tasks t and performance measure p if its performance at tasks in t as measured by p improves with experience e. this definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. this follows alan turing's proposal in his paper computing machinery and intelligence in which the question can machines think is replaced with the question can machines do what we as thinking entities can do. modern-day machine learning algorithms are broken into 3 algorithm types supervised learning algorithms unsupervised learning algorithms and reinforcement learning algorithms. current supervised learning algorithms have objectives of classification and regression. current unsupervised learning algorithms have objectives of clustering dimensionality reduction and association rule. current reinforcement learning algorithms focus on decisions that must be made with respect to some previous unknown time and are broken down to either be studies of model-based methods or model-free methods. in 2014 ian goodfellow and others introduced generative adversarial networks gans with realistic data synthesis. by 2016 alphago obtained victory against top human players using reinforcement learning techniques. relationships to other fields artificial intelligence as a scientific endeavour machine learning grew out of the quest for artifi"}
{"title": "Artificial intelligence", "url": "https://en.wikipedia.org/wiki/Artificial_intelligence", "content": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\nGoals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\nReasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\nKnowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the", "source": "Wikipedia", "date": null, "author": null, "score": null, "tokens": ["artificial", "intelligence", "ai", "capability", "computational", "system", "perform", "task", "typically", "associate", "human", "intelligence", "learn", "reasoning", "problem", "solve", "perception", "decision", "making", "field", "research", "computer", "science", "develop", "study", "method", "software", "enable", "machine", "perceive", "environment", "use", "learning", "intelligence", "action", "maximize", "chance", "achieve", "define", "goal", "high", "profile", "application", "ai", "include", "advanced", "web", "search", "engine", "e.g.", "google", "search", "recommendation", "system", "youtube", "amazon", "netflix", "virtual", "assistant", "e.g.", "google", "assistant", "siri", "alexa", "autonomous", "vehicle", "e.g.", "waymo", "generative", "creative", "tool", "e.g.", "language", "model", "ai", "art", "superhuman", "play", "analysis", "strategy", "game", "e.g.", "chess", "ai", "application", "perceive", "ai", "lot", "cut", "edge", "ai", "filter", "general", "application", "call", "ai", "useful", "common", "label", "ai", "anymore", "subfield", "ai", "research", "center", "particular", "goal", "use", "particular", "tool", "traditional", "goal", "ai", "research", "include", "learn", "reasoning", "knowledge", "representation", "plan", "natural", "language", "processing", "perception", "support", "robotic", "reach", "goal", "ai", "researcher", "adapt", "integrate", "wide", "range", "technique", "include", "search", "mathematical", "optimization", "formal", "logic", "artificial", "neural", "network", "method", "base", "statistic", "operation", "research", "economic", "ai", "draw", "psychology", "linguistic", "philosophy", "neuroscience", "field", "company", "openai", "google", "deepmind", "meta", "aim", "create", "artificial", "general", "intelligence", "agi", "ai", "complete", "virtually", "cognitive", "task", "human", "artificial", "intelligence", "found", "academic", "discipline", "1956", "field", "go", "multiple", "cycle", "optimism", "history", "follow", "period", "disappointment", "loss", "funding", "know", "ai", "winter", "funding", "interest", "vastly", "increase", "2012", "graphic", "processing", "unit", "start", "accelerate", "neural", "network", "deep", "learning", "outperform", "previous", "ai", "technique", "growth", "accelerate", "2017", "transformer", "architecture", "2020", "ongoing", "period", "rapid", "progress", "advanced", "generative", "ai", "know", "ai", "boom", "generative", "ai", "ability", "create", "modify", "content", "lead", "unintended", "consequence", "harm", "ethical", "concern", "raise", "ai", "long", "term", "effect", "potential", "existential", "risk", "prompt", "discussion", "regulatory", "policy", "ensure", "safety", "benefit", "technology", "goal", "general", "problem", "simulate", "create", "intelligence", "break", "subproblem", "consist", "particular", "trait", "capability", "researcher", "expect", "intelligent", "system", "display", "trait", "describe", "receive", "attention", "cover", "scope", "ai", "research", "reasoning", "problem", "solve", "early", "researcher", "develop", "algorithm", "imitate", "step", "step", "reasoning", "human", "use", "solve", "puzzle", "logical", "deduction", "late", "1980", "1990s", "method", "develop", "deal", "uncertain", "incomplete", "information", "employ", "concept", "probability", "economic", "algorithm", "insufficient", "solve", "large", "reasoning", "problem", "experience", "combinatorial", "explosion", "exponentially", "slow", "problem", "grow", "human", "rarely", "use", "step", "step", "deduction", "early", "ai", "research", "model", "solve", "problem", "fast", "intuitive", "judgment", "accurate", "efficient", "reasoning", "unsolved", "problem", "knowledge", "representation", "knowledge", "representation", "knowledge", "engineering", "allow", "ai", "program", "answer", "question", "intelligently", "deduction", "real", "world", "fact", "formal", "knowledge", "representation", "content", "base", "indexing", "retrieval", "scene", "interpretation", "clinical", "decision", "support", "knowledge", "discovery", "mining", "interesting", "actionable", "inference", "large", "database", "area", "knowledge", "base", "body", "knowledge", "represent", "form", "program", "ontology", "set", "object", "relation", "concept", "property", "particular", "domain", "knowledge", "knowledge", "basis", "need", "represent", "thing", "object", "property", "category", "relation", "object", "situation", "event", "state", "time", "cause", "effect", "knowledge", "knowledge", "know", "people", "know", "default", "reasoning", "thing", "human", "assume", "true", "tell", "differently", "remain", "true", "fact", "change", "aspect", "domain", "knowledge", "difficult", "problem", "knowledge", "representation", "breadth", "commonsense", "knowledge"], "num_tokens": 440, "token_loss_pct": 42.56, "normalized_content": "artificial intelligence ai is the capability of computational systems to perform tasks typically associated with human intelligence such as learning reasoning problem-solving perception and decision-making. it is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. high-profile applications of ai include advanced web search engines e.g. google search recommendation systems used by youtube amazon and netflix virtual assistants e.g. google assistant siri and alexa autonomous vehicles e.g. waymo generative and creative tools e.g. language models and ai art and superhuman play and analysis in strategy games e.g. chess and go. however many ai applications are not perceived as ai a lot of cutting edge ai has filtered into general applications often without being called ai because once something becomes useful enough and common enough it's not labeled ai anymore. various subfields of ai research are centered around particular goals and the use of particular tools. the traditional goals of ai research include learning reasoning knowledge representation planning natural language processing perception and support for robotics. to reach these goals ai researchers have adapted and integrated a wide range of techniques including search and mathematical optimization formal logic artificial neural networks and methods based on statistics operations research and economics. ai also draws upon psychology linguistics philosophy neuroscience and other fields. some companies such as openai google deepmind and meta aim to create artificial general intelligence agi  ai that can complete virtually any cognitive task at least as well as a human. artificial intelligence was founded as an academic discipline in 1956 and the field went through multiple cycles of optimism throughout its history followed by periods of disappointment and loss of funding known as ai winters. funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous ai techniques. this growth accelerated further after 2017 with the transformer architecture. in the 2020s an ongoing period of rapid progress in advanced generative ai became known as the ai boom. generative ai's ability to create and modify content has led to several unintended consequences and harms. ethical concerns have been raised about ai's long-term effects and potential existential risks prompting discussions about regulatory policies to ensure the safety and benefits of the technology. goals the general problem of simulating or creating intelligence has been broken into subproblems. these consist of particular traits or capabilities that researchers expect an intelligent system to display. the traits described below have received the most attention and cover the scope of ai research. reasoning and problem-solving early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. by the late 1980s and 1990s methods were developed for dealing with uncertain or incomplete information employing concepts from probability and economics. many of these algorithms are insufficient for solving large reasoning problems because they experience a combinatorial explosion they become exponentially slower as the problems grow. even humans rarely use the step-by-step deduction that early ai research could model. they solve most of their problems using fast intuitive judgments. accurate and efficient reasoning is an unsolved problem. knowledge representation knowledge representation and knowledge engineering allow ai programs to answer questions intelligently and make deductions about real-world facts. formal knowledge representations are used in content-based indexing and retrieval scene interpretation clinical decision support knowledge discovery mining interesting and actionable inferences from large databases and other areas. a knowledge base is a body of knowledge represented in a form that can be used by a program. an ontology is the set of objects relations concepts and properties used by a particular domain of knowledge. knowledge bases need to represent things such as objects properties categories and relations between objects situations events states and time causes and effects knowledge about knowledge what we know about what other people know default reasoning things that humans assume are true until they are told differently and will remain true even when other facts are changing and many other aspects and domains of knowledge. among the most difficult problems in knowledge representation are the breadth of commonsense knowledge the"}
