{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e62581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests->wikipedia-api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests->wikipedia-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests->wikipedia-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests->wikipedia-api) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wikipedia-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b8a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 11.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 12.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 12.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 11.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.8 MB/s  0:00:01\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec97d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d37c37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "tf_keras OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tf-keras\n",
    "\n",
    "import tf_keras\n",
    "print(\"tf_keras OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d6e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (5.29.4)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-6.33.4-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 12.8 MB/s  0:00:00\n",
      "Using cached protobuf-6.33.4-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Installing collected packages: sentencepiece, protobuf\n",
      "\n",
      "   ---------------------------------------- 0/2 [sentencepiece]\n",
      "  Attempting uninstall: protobuf\n",
      "   ---------------------------------------- 0/2 [sentencepiece]\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "   ---------------------------------------- 0/2 [sentencepiece]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [protobuf]\n",
      "   ---------------------------------------- 2/2 [protobuf]\n",
      "\n",
      "Successfully installed protobuf-6.33.4 sentencepiece-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\DELL\\Desktop\\ml\\.conda\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "paddlepaddle-gpu 2.6.2 requires protobuf<=3.20.2,>=3.1.0; platform_system == \"Windows\", but you have protobuf 6.33.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U sentencepiece protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97232d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\desktop\\ml\\.conda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239cd0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21 14:37:12,365 | __main__ | INFO | üöÄ Pipeline Veille NLP - D√©marrage\n",
      "2026-01-21 14:37:12,380 | __main__ | INFO | Configuration charg√©e depuis: config.json\n",
      "2026-01-21 14:37:12,380 | __main__ | INFO | Reddit enabled? True\n",
      "2026-01-21 14:37:12,380 | __main__ | INFO | \n",
      "===========================================================================\n",
      "2026-01-21 14:37:12,380 | __main__ | INFO | √âTAPE 1 : COLLECTE ARTICLES\n",
      "2026-01-21 14:37:12,380 | __main__ | INFO | ===========================================================================\n",
      "2026-01-21 14:37:12,380 | news_collector | INFO | ======================================================================\n",
      "2026-01-21 14:37:12,380 | news_collector | INFO | üì• COLLECTE ARTICLES - D√©but\n",
      "2026-01-21 14:37:12,380 | news_collector | INFO | ======================================================================\n",
      "2026-01-21 14:37:12,380 | news_collector | INFO | üîÑ Collecte HackerNews...\n",
      "2026-01-21 14:37:12,380 | news_collector | INFO |   Scraping page 1/10\n",
      "2026-01-21 14:37:28,663 | news_collector | INFO |   Scraping page 2/10\n",
      "2026-01-21 14:37:40,133 | news_collector | INFO |   Scraping page 3/10\n",
      "2026-01-21 14:37:52,634 | news_collector | INFO |   Scraping page 4/10\n",
      "2026-01-21 14:38:04,742 | news_collector | INFO |   Scraping page 5/10\n",
      "2026-01-21 14:38:19,575 | news_collector | INFO |   Scraping page 6/10\n",
      "2026-01-21 14:38:35,880 | news_collector | INFO |   Scraping page 7/10\n",
      "2026-01-21 14:38:47,409 | news_collector | INFO |   Scraping page 8/10\n",
      "2026-01-21 14:39:12,098 | news_collector | INFO |   Scraping page 9/10\n",
      "2026-01-21 14:39:23,620 | news_collector | INFO |   Scraping page 10/10\n",
      "2026-01-21 14:39:50,184 | news_collector | INFO | ‚úÖ HackerNews: 300 articles collect√©s\n",
      "2026-01-21 14:39:50,186 | news_collector | INFO | üîÑ Collecte RSS: https://pycoders.com/feed\n",
      "2026-01-21 14:39:50,699 | news_collector | INFO | ‚úÖ RSS: 3 articles collect√©s\n",
      "2026-01-21 14:39:50,699 | news_collector | INFO | üîÑ Collecte Reddit...\n",
      "2026-01-21 14:39:51,402 | news_collector | WARNING |   ‚úó Reddit ArtificialIntelligence: 404 Client Error: Not Found for url: https://www.reddit.com/r/ArtificialIntelligence/top.json?limit=20&t=week\n",
      "2026-01-21 14:39:52,687 | news_collector | INFO | ‚úÖ Reddit: 80 articles collect√©s\n",
      "2026-01-21 14:39:52,689 | news_collector | INFO | üîÑ Collecte Wikipedia...\n",
      "2026-01-21 14:39:52,690 | wikipediaapi | INFO | Wikipedia: language=en, user_agent: Mozilla/5.0 (compatible; NewsCollector/1.0; wikipedia-api) (Wikipedia-API/0.8.1; https://github.com/martin-majlis/Wikipedia-API/), extract_format=1\n",
      "2026-01-21 14:39:52,691 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=info&titles=Natural Language Processing&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle|varianttitles\n",
      "2026-01-21 14:39:52,957 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=extracts&titles=Natural language processing&explaintext=1&exsectionformat=wiki\n",
      "2026-01-21 14:39:53,202 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=info&titles=Deep Learning&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle|varianttitles\n",
      "2026-01-21 14:39:53,502 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=extracts&titles=Deep learning&explaintext=1&exsectionformat=wiki\n",
      "2026-01-21 14:39:53,858 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=info&titles=Machine Learning&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle|varianttitles\n",
      "2026-01-21 14:39:54,385 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=extracts&titles=Machine learning&explaintext=1&exsectionformat=wiki\n",
      "2026-01-21 14:39:54,781 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=info&titles=Artificial Intelligence&inprop=protection|talkid|watched|watchers|visitingwatchers|notificationtimestamp|subjectid|url|readable|preload|displaytitle|varianttitles\n",
      "2026-01-21 14:39:55,265 | wikipediaapi | INFO | Request URL: https://en.wikipedia.org/w/api.php?format=json&redirects=1&action=query&prop=extracts&titles=Artificial intelligence&explaintext=1&exsectionformat=wiki\n",
      "2026-01-21 14:39:55,614 | news_collector | INFO | ‚úÖ Wikipedia: 4 articles collect√©s\n",
      "2026-01-21 14:39:55,615 | news_collector | INFO | ======================================================================\n",
      "2026-01-21 14:39:55,616 | news_collector | INFO | üìä R√âSUM√â COLLECTE\n",
      "2026-01-21 14:39:55,617 | news_collector | INFO |    Total articles : 387\n",
      "2026-01-21 14:39:55,617 | news_collector | INFO |    Total erreurs : 1\n",
      "2026-01-21 14:39:55,618 | news_collector | INFO | ======================================================================\n",
      "2026-01-21 14:39:55,634 | news_collector | INFO | ‚úÖ Articles sauvegard√©s: data/articles_raw.jsonl\n",
      "2026-01-21 14:39:55,636 | news_collector | INFO | ‚úÖ Erreurs sauvegard√©es: data/collection_errors.json\n",
      "2026-01-21 14:39:55,637 | __main__ | INFO | ‚úÖ √âTAPE 1 COMPL√âT√âE : 387 articles collect√©s\n",
      "2026-01-21 14:39:55,640 | __main__ | INFO | \n",
      "===========================================================================\n",
      "2026-01-21 14:39:55,641 | __main__ | INFO | √âTAPE 2 : PR√âTRAITEMENT NLP\n",
      "2026-01-21 14:39:55,642 | __main__ | INFO | ===========================================================================\n",
      "2026-01-21 14:39:57,056 | text_preprocessor | INFO | ‚úÖ Mod√®le spaCy charg√©: en_core_web_sm\n",
      "2026-01-21 14:39:57,056 | text_preprocessor | INFO | üîß Preprocessing 387 articles...\n",
      "2026-01-21 14:40:00,279 | text_preprocessor | INFO |   ‚úì 20/387 articles\n",
      "2026-01-21 14:40:02,967 | text_preprocessor | INFO |   ‚úì 40/387 articles\n",
      "2026-01-21 14:40:05,294 | text_preprocessor | INFO |   ‚úì 60/387 articles\n",
      "2026-01-21 14:40:08,139 | text_preprocessor | INFO |   ‚úì 80/387 articles\n",
      "2026-01-21 14:40:10,837 | text_preprocessor | INFO |   ‚úì 100/387 articles\n",
      "2026-01-21 14:40:13,481 | text_preprocessor | INFO |   ‚úì 120/387 articles\n",
      "2026-01-21 14:40:16,050 | text_preprocessor | INFO |   ‚úì 140/387 articles\n",
      "2026-01-21 14:40:18,951 | text_preprocessor | INFO |   ‚úì 160/387 articles\n",
      "2026-01-21 14:40:21,757 | text_preprocessor | INFO |   ‚úì 180/387 articles\n",
      "2026-01-21 14:40:24,812 | text_preprocessor | INFO |   ‚úì 200/387 articles\n",
      "2026-01-21 14:40:27,220 | text_preprocessor | INFO |   ‚úì 220/387 articles\n",
      "2026-01-21 14:40:29,787 | text_preprocessor | INFO |   ‚úì 240/387 articles\n",
      "2026-01-21 14:40:32,034 | text_preprocessor | INFO |   ‚úì 260/387 articles\n",
      "2026-01-21 14:40:34,784 | text_preprocessor | INFO |   ‚úì 280/387 articles\n",
      "2026-01-21 14:40:37,476 | text_preprocessor | INFO |   ‚úì 300/387 articles\n",
      "2026-01-21 14:40:40,161 | text_preprocessor | INFO |   ‚úì 320/387 articles\n",
      "2026-01-21 14:40:40,773 | text_preprocessor | INFO |   ‚úì 340/387 articles\n",
      "2026-01-21 14:40:43,469 | text_preprocessor | INFO |   ‚úì 360/387 articles\n",
      "2026-01-21 14:40:44,293 | text_preprocessor | INFO |   ‚úì 380/387 articles\n",
      "2026-01-21 14:40:45,810 | text_preprocessor | INFO | ‚úÖ Preprocessing termin√©: 387 articles\n",
      "\n",
      "======================================================================\n",
      "üìä RAPPORT QUALIT√â PR√âTRAITEMENT\n",
      "======================================================================\n",
      "Articles trait√©s: 387\n",
      "Perte tokens moyenne: 48.75%\n",
      "  Min: 17.02%\n",
      "  Max: 68.2%\n",
      "  M√©diane: 49.57%\n",
      "======================================================================\n",
      "2026-01-21 14:40:45,823 | __main__ | INFO | ‚úÖ √âTAPE 2 COMPL√âT√âE : 387 articles trait√©s\n",
      "2026-01-21 14:40:45,840 | __main__ | INFO | \n",
      "===========================================================================\n",
      "2026-01-21 14:40:45,841 | __main__ | INFO | √âTAPE 3 : CLASSIFICATION & EXTRACTION\n",
      "2026-01-21 14:40:45,842 | __main__ | INFO | ===========================================================================\n",
      "2026-01-21 14:40:45,844 | news_classifier | INFO | üì• Chargement mod√®le: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21 14:40:47,200 | news_classifier | INFO | ‚úÖ Mod√®le charg√©: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21 14:40:55,471 | news_classifier | INFO | ‚úÖ Sentiment classifier charg√©\n",
      "2026-01-21 14:40:55,473 | news_classifier | INFO | ü§ñ Classification 387 articles...\n",
      "2026-01-21 14:45:20,102 | news_classifier | INFO |   ‚úì 10/387 articles\n",
      "2026-01-21 14:50:31,515 | news_classifier | INFO |   ‚úì 20/387 articles\n",
      "2026-01-21 14:55:10,212 | news_classifier | INFO |   ‚úì 30/387 articles\n",
      "2026-01-21 14:59:37,751 | news_classifier | INFO |   ‚úì 40/387 articles\n",
      "2026-01-21 15:04:18,356 | news_classifier | INFO |   ‚úì 50/387 articles\n",
      "2026-01-21 15:07:05,649 | news_classifier | INFO |   ‚úì 60/387 articles\n",
      "2026-01-21 15:12:40,023 | news_classifier | INFO |   ‚úì 70/387 articles\n",
      "2026-01-21 15:16:10,721 | news_classifier | INFO |   ‚úì 80/387 articles\n",
      "2026-01-21 15:20:37,306 | news_classifier | INFO |   ‚úì 90/387 articles\n",
      "2026-01-21 15:25:22,406 | news_classifier | INFO |   ‚úì 100/387 articles\n",
      "2026-01-21 15:29:08,760 | news_classifier | INFO |   ‚úì 110/387 articles\n",
      "2026-01-21 15:34:56,357 | news_classifier | INFO |   ‚úì 120/387 articles\n",
      "2026-01-21 15:39:13,656 | news_classifier | INFO |   ‚úì 130/387 articles\n",
      "2026-01-21 15:43:28,045 | news_classifier | INFO |   ‚úì 140/387 articles\n",
      "2026-01-21 15:48:03,250 | news_classifier | INFO |   ‚úì 150/387 articles\n",
      "2026-01-21 15:53:31,597 | news_classifier | INFO |   ‚úì 160/387 articles\n",
      "2026-01-21 15:58:34,550 | news_classifier | INFO |   ‚úì 170/387 articles\n",
      "2026-01-21 16:17:41,331 | news_classifier | INFO |   ‚úì 180/387 articles\n",
      "2026-01-21 16:22:47,555 | news_classifier | INFO |   ‚úì 190/387 articles\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# main.py\n",
    "\"\"\"\n",
    "üöÄ MAIN SCRIPT - Pipeline Complet Veille NLP\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Forcer UTF-8 (OK terminal Windows, ignore si Jupyter)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "for _stream_name in (\"stdout\", \"stderr\"):\n",
    "    stream = getattr(sys, _stream_name)\n",
    "    if hasattr(stream, \"reconfigure\"):\n",
    "        stream.reconfigure(encoding=\"utf-8\", errors=\"replace\")\n",
    "    elif hasattr(stream, \"buffer\"):\n",
    "        setattr(\n",
    "            sys,\n",
    "            _stream_name,\n",
    "            io.TextIOWrapper(stream.buffer, encoding=\"utf-8\", errors=\"replace\"),\n",
    "        )\n",
    "    else:\n",
    "        # Jupyter: pas de buffer -> on laisse tel quel\n",
    "        pass\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Ajouter src au path (compatible script + notebook)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Notebook / interactive (pas de __file__)\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "SRC_DIR = BASE_DIR / \"src\"\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "from news_collector import NewsCollector\n",
    "from text_preprocessor import TextPreprocessor\n",
    "from news_classifier import NewsClassifier\n",
    "from report_generator import ReportGenerator\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# SETUP LOGGING\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def setup_logging(config: dict):\n",
    "    \"\"\"Configurer logging\"\"\"\n",
    "    log_level = config.get(\"logging\", {}).get(\"level\", \"INFO\")\n",
    "    log_file = config.get(\"logging\", {}).get(\"log_file\", \"veille_system.log\")\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level, logging.INFO),\n",
    "        format=\"%(asctime)s | %(name)s | %(levelname)s | %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, encoding=\"utf-8\"),\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"üöÄ Pipeline Veille NLP - D√©marrage\")\n",
    "    logger.info(\"Configuration charg√©e depuis: config.json\")\n",
    "    return logger\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# √âTAPE 1 : COLLECTE\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def step_collect(config: dict, logger) -> list:\n",
    "    logger.info(\"\\n\" + \"=\" * 75)\n",
    "    logger.info(\"√âTAPE 1 : COLLECTE ARTICLES\")\n",
    "    logger.info(\"=\" * 75)\n",
    "\n",
    "    collector = NewsCollector(config)\n",
    "    articles = collector.collect_all()\n",
    "\n",
    "    collector.save_to_jsonl(\"data/articles_raw.jsonl\")\n",
    "    collector.save_errors_log(\"data/collection_errors.json\")\n",
    "\n",
    "    logger.info(f\"‚úÖ √âTAPE 1 COMPL√âT√âE : {len(articles)} articles collect√©s\")\n",
    "    return articles\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# √âTAPE 2 : PR√âTRAITEMENT\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def step_preprocess(articles: list, config: dict, logger) -> list:\n",
    "    logger.info(\"\\n\" + \"=\" * 75)\n",
    "    logger.info(\"√âTAPE 2 : PR√âTRAITEMENT NLP\")\n",
    "    logger.info(\"=\" * 75)\n",
    "\n",
    "    preprocessor = TextPreprocessor(config)\n",
    "    processed_articles = preprocessor.process_batch(articles)\n",
    "\n",
    "    preprocessor.print_quality_report()\n",
    "\n",
    "    with open(\"data/articles_processed.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for article in processed_articles:\n",
    "            f.write(json.dumps(article, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    logger.info(f\"‚úÖ √âTAPE 2 COMPL√âT√âE : {len(processed_articles)} articles trait√©s\")\n",
    "    return processed_articles\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# √âTAPE 3 : CLASSIFICATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def step_classify(articles: list, config: dict, logger) -> list:\n",
    "    logger.info(\"\\n\" + \"=\" * 75)\n",
    "    logger.info(\"√âTAPE 3 : CLASSIFICATION & EXTRACTION\")\n",
    "    logger.info(\"=\" * 75)\n",
    "\n",
    "    classifier = NewsClassifier(config)\n",
    "    classified_articles = classifier.classify_batch(articles)\n",
    "\n",
    "    classifier.print_classification_summary(classified_articles)\n",
    "\n",
    "    with open(\"data/articles_classified.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for article in classified_articles:\n",
    "            f.write(json.dumps(article, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    logger.info(f\"‚úÖ √âTAPE 3 COMPL√âT√âE : {len(classified_articles)} articles classifi√©s\")\n",
    "    return classified_articles\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# √âTAPE 4 : G√âN√âRATION RAPPORT\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def step_generate_report(articles: list, config: dict, logger) -> str:\n",
    "    logger.info(\"\\n\" + \"=\" * 75)\n",
    "    logger.info(\"√âTAPE 4 : G√âN√âRATION RAPPORT\")\n",
    "    logger.info(\"=\" * 75)\n",
    "\n",
    "    generator = ReportGenerator(config)\n",
    "    report = generator.generate(articles)\n",
    "\n",
    "    output_file = config.get(\"output\", {}).get(\"report_name\", \"veille_report.txt\")\n",
    "    output_dir = config.get(\"output\", {}).get(\"output_dir\", \"./output\")\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    output_path = Path(output_dir) / output_file\n",
    "\n",
    "    generator.save_report(report, str(output_path))\n",
    "    logger.info(\"‚úÖ √âTAPE 4 COMPL√âT√âE : Rapport sauvegard√©\")\n",
    "\n",
    "    return report\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# MAIN\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def main():\n",
    "    # Cr√©er dossiers\n",
    "    Path(\"data\").mkdir(exist_ok=True)\n",
    "    Path(\"output\").mkdir(exist_ok=True)\n",
    "\n",
    "    config_path = BASE_DIR / \"config.json\"\n",
    "\n",
    "    try:\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = json.load(f)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå ERREUR : config.json non trouv√©\")\n",
    "        print(f\"Chemin attendu : {config_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logger = setup_logging(config)\n",
    "    logger.info(\n",
    "        f\"Reddit enabled? {config.get('collection', {}).get('reddit', {}).get('enabled')}\"\n",
    "    )\n",
    "    try:\n",
    "        articles = step_collect(config, logger)\n",
    "        if not articles:\n",
    "            logger.warning(\"‚ö†Ô∏è Aucun article collect√©\")\n",
    "            return\n",
    "\n",
    "        processed_articles = step_preprocess(articles, config, logger)\n",
    "        if not processed_articles:\n",
    "            logger.error(\"‚ùå Aucun article apr√®s pr√©traitement\")\n",
    "            return\n",
    "\n",
    "        classified_articles = step_classify(processed_articles, config, logger)\n",
    "        if not classified_articles:\n",
    "            logger.error(\"‚ùå Aucun article apr√®s classification\")\n",
    "            return\n",
    "\n",
    "        report = step_generate_report(classified_articles, config, logger)\n",
    "\n",
    "        logger.info(\"\\n\" + \"=\" * 75)\n",
    "        logger.info(\"‚úÖ PIPELINE COMPLET - SUCC√àS\")\n",
    "        logger.info(\"=\" * 75)\n",
    "        logger.info(f\"Rapport g√©n√©r√©: output/{config.get('output', {}).get('report_name', 'veille_report.txt')}\")\n",
    "        logger.info(f\"Articles trait√©s: {len(classified_articles)}\")\n",
    "        logger.info(f\"Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        logger.info(\"=\" * 75)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 75)\n",
    "        print(\"üìÑ APER√áU RAPPORT\")\n",
    "        print(\"=\" * 75)\n",
    "        print(\"\\n\".join(report.split(\"\\n\")[:30]))\n",
    "        print(\"...\")\n",
    "        print(f\"\\n‚úÖ Rapport complet sauvegard√© en output/{config.get('output', {}).get('report_name', 'veille_report.txt')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\n‚ùå ERREUR FATALE : {str(e)}\")\n",
    "        logger.exception(\"Traceback:\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
