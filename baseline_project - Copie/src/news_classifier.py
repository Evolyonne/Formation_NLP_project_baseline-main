# src/news_classifier.py
"""
ğŸ¤– Module Classification & Extraction NLP

Tasks :
1. Topic Classification : DÃ©butant / IntermÃ©diaire / AvancÃ©
2. Sentiment Analysis : Positif / Neutre / Critique
3. Duplicate Detection : SimilaritÃ© cosinus entre articles

Usage:
    classifier = NewsClassifier(config)
    classified_articles = classifier.classify_batch(articles)
"""

import logging
import torch
from typing import List, Dict, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline
import json

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEWS CLASSIFIER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class NewsClassifier:
    """
    Classification articles avec HuggingFace + similaritÃ© cosinus
    
    DESIGN DECISIONS :
    
    1. Topic Classification : Zero-shot vs Fine-tuned?
        â†’ Choix : Zero-shot (pas besoin donnÃ©es entraÃ®nement)
        â†’ Avantage : Rapide, flexible sur labels
        â†’ Limitation : Accuracy infÃ©rieure Ã  fine-tuned
    
    2. ModÃ¨le multilingue : distilbert-base-multilingual-uncased
        â†’ Support franÃ§ais + anglais + vitesse
        â†’ Alternative : roberta (meilleur accuracy, plus lent)
    
    3. Duplicate detection : Cosine similarity vs Semantic?
        â†’ Choix : TF-IDF cosine (fast, transparent)
        â†’ Alternative : Embeddings (better but slower)
    
    4. Seuil similaritÃ© : 0.85
        â†’ Validation : Tester manuellement sur 20 paires
        â†’ Justification : Ã‰quilibre false positives/negatives
    """
    
    def __init__(self, config: Dict):
        self.config = config

        model_name = config.get("classification", {}).get(
            "model_name",
            "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli"
        )

        # --- Zero-shot topic (FORCER PyTorch) ---
        try:
            logger.info(f"ğŸ“¥ Chargement modÃ¨le: {model_name}...")
            self.classifier = pipeline(
                "zero-shot-classification",
                model=model_name,
                device=-1,
                framework="pt"
            )
            logger.info(f"âœ… ModÃ¨le chargÃ©: {model_name}")
        except Exception as e:
            logger.error(f"âŒ Erreur chargement modÃ¨le: {str(e)}")
            self.classifier = None

        # --- Sentiment (FORCER PyTorch) ---
        try:
            self.sentiment_classifier = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-xlm-roberta-base-sentiment",
                device=-1,
                framework="pt"
            )
            logger.info("âœ… Sentiment classifier chargÃ©")
        except Exception as e:
            logger.warning(f"âš ï¸ Sentiment classifier: {str(e)}")
            self.sentiment_classifier = None

        # --- Vectorizer pour duplicate detection ---
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            lowercase=True,
            stop_words="english"
        )
        self.tfidf_matrix = None

    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TASK 1 : TOPIC CLASSIFICATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def classify_topic(self, text: str) -> Dict:
        """
        Classifier article par niveau : DÃ©butant/IntermÃ©diaire/AvancÃ©
        
        Args:
            text: Contenu article Ã  classifier
        
        Returns:
            {
                'predicted_label': str,
                'confidence': float,
                'all_scores': dict de tous labels et scores
            }
        """
        if not text or not self.classifier:
            return {
                'predicted_label': 'Unknown',
                'confidence': 0.0,
                'all_scores': {}
            }
        
        try:
            labels = self.config.get('classification', {}).get('labels', 
                                                               ['Beginner', 'Intermediate', 'Advanced'])
            
            # Limiter texte Ã  512 tokens (limite BERT)
            text_truncated = ' '.join(text.split()[:400])
            
            result = self.classifier(text_truncated, labels)
            
            # result format:
            # {
            #   'sequence': texte original,
            #   'labels': ['Intermediate', 'Beginner', 'Advanced'],
            #   'scores': [0.95, 0.04, 0.01]
            # }
            
            return {
                'predicted_label': result['labels'][0],
                'confidence': round(result['scores'][0], 4),
                'all_scores': {label: round(score, 4) 
                              for label, score in zip(result['labels'], result['scores'])}
            }
        
        except Exception as e:
            logger.warning(f"Erreur classification: {str(e)}")
            return {
                'predicted_label': 'Unknown',
                'confidence': 0.0,
                'all_scores': {}
            }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TASK 2 : SENTIMENT ANALYSIS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
    def analyze_sentiment(self, text: str) -> Dict:
        if not text or not self.sentiment_classifier:
            return {'sentiment': 'NEUTRAL', 'score': 0.5, 'label': 'Neutre'}

        try:
            # Tronquer proprement avec le tokenizer (Ã©vite >512 tokens)
            tokenizer = self.sentiment_classifier.tokenizer
            inputs = tokenizer(
                text,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            truncated_text = tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)

            result = self.sentiment_classifier(truncated_text)[0]

            hf_label = str(result.get('label', '')).lower()
            score = float(result.get('score', 0.0))

            # Certains modÃ¨les renvoient LABEL_0/1/2 -> mapping
            if hf_label in ["label_2"]:
                hf_label = "positive"
            elif hf_label in ["label_0"]:
                hf_label = "negative"
            elif hf_label in ["label_1"]:
                hf_label = "neutral"

            if hf_label == "positive":
                return {'sentiment': 'POSITIVE', 'score': round(score, 4), 'label': 'Positif'}
            elif hf_label == "negative":
                return {'sentiment': 'NEGATIVE', 'score': round(score, 4), 'label': 'Critique'}
            else:
                return {'sentiment': 'NEUTRAL', 'score': round(score, 4), 'label': 'Neutre'}

        except Exception as e:
            logger.warning(f"Erreur sentiment: {str(e)}")
            return {'sentiment': 'NEUTRAL', 'score': 0.5, 'label': 'Neutre'}


    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TASK 3 : DUPLICATE DETECTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def detect_duplicates(self, articles: List[Dict], threshold: float = 0.85) -> List[Dict]:
        """
        DÃ©tecter articles dupliquÃ©s via similaritÃ© cosinus
        
        JUSTIFICATION APPROCHE :
        - TF-IDF cosine : Fast, transparent, interpretable
        - Alternative : Embeddings (plus lent, meilleur accuracy)
        
        Args:
            articles: Liste articles Ã  dÃ©dupliquer
            threshold: Seuil similaritÃ© (0-1)
        
        Returns:
            Articles avec flag 'is_duplicate' ajoutÃ©
        """
        logger.info(f"ğŸ” Duplicate detection (threshold={threshold})...")
        
        if len(articles) < 2:
            for article in articles:
                article['is_duplicate'] = False
            return articles
        
        try:
            # Extraire contenus
            contents = [article.get('normalized_content', article.get('title', '')) 
                       for article in articles]
            
            # Construire matrice TF-IDF
            self.tfidf_matrix = self.vectorizer.fit_transform(contents)
            
            # Calculer similaritÃ©s pairwise
            similarity_matrix = cosine_similarity(self.tfidf_matrix)
            
            # Marquer doublons
            duplicates = set()
            for i in range(len(articles)):
                for j in range(i + 1, len(articles)):
                    if similarity_matrix[i][j] >= threshold:
                        # Marquer le plus jeune comme doublon
                        duplicates.add(j)
                        
                        logger.debug(f"  Doublon dÃ©tectÃ©: {i} â†” {j} (sim={similarity_matrix[i][j]:.3f})")
            
            # Ajouter flag
            for idx, article in enumerate(articles):
                article['is_duplicate'] = idx in duplicates
                article['duplicate_score'] = max(similarity_matrix[idx]) if len(articles) > 1 else 0.0
            
            logger.info(f"âœ… Duplicate detection: {len(duplicates)} doublons trouvÃ©s")
            return articles
        
        except Exception as e:
            logger.error(f"âŒ Erreur duplicate detection: {str(e)}")
            for article in articles:
                article['is_duplicate'] = False
            return articles
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BATCH CLASSIFICATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def classify_batch(self, articles: List[Dict]) -> List[Dict]:
        """
        Classifier batch d'articles
        
        Ajoute Ã  chaque article :
        - topic_prediction
        - topic_confidence
        - topic_scores
        - sentiment
        - sentiment_score
        - sentiment_label
        """
        logger.info(f"ğŸ¤– Classification {len(articles)} articles...")
        
        classified = []
        
        for i, article in enumerate(articles):
            try:
                # Topic -> texte normalisÃ© (OK)
                text_topic = article.get("normalized_content", article.get("content", article.get("title", "")))

                # Sentiment -> texte BRUT (title + content) (mieux pour Ã©viter "Neutre")
                text_sentiment = f"{article.get('title','')} {article.get('content','')}".strip()
                if not text_sentiment:
                    text_sentiment = article.get("title", "")

                # Task 1: Topic
                topic_result = self.classify_topic(text_topic)
                article["topic_prediction"] = topic_result["predicted_label"]
                article["topic_confidence"] = topic_result["confidence"]
                article["topic_scores"] = topic_result["all_scores"]
 
                # Task 2: Sentiment
                if article.get("source") == "Wikipedia":
                    # Wikipedia = encyclopÃ©dique â†’ sentiment neutre par dÃ©faut
                    sentiment_result = {
                        "sentiment": "NEUTRAL",
                        "score": 0.5,
                        "label": "Neutre"
                    }
                else:
                    sentiment_result = self.analyze_sentiment(text_sentiment)

                article["sentiment"] = sentiment_result["sentiment"]
                article["sentiment_score"] = sentiment_result["score"]
                article["sentiment_label"] = sentiment_result["label"]



                classified.append(article)

                if (i + 1) % 10 == 0:
                    logger.info(f"  âœ“ {i + 1}/{len(articles)} articles")

            except Exception as e:
                logger.warning(f"  âœ— Article {i}: {str(e)}")
                continue

        
        # Task 3: Duplicate detection
        classified = self.detect_duplicates(classified, 
                                           threshold=self.config.get('deduplication', {}).get('threshold', 0.85))
        
        logger.info(f"âœ… Classification terminÃ©e: {len(classified)} articles")
        return classified
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰VALUATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def print_classification_summary(self, articles: List[Dict]):
        """Afficher rÃ©sumÃ© classification"""
        if not articles:
            return
        
        # Compter par topic
        topics = {}
        for article in articles:
            topic = article.get('topic_prediction', 'Unknown')
            topics[topic] = topics.get(topic, 0) + 1
        
        # Compter par sentiment
        sentiments = {}
        for article in articles:
            sentiment = article.get('sentiment_label', 'Unknown')
            sentiments[sentiment] = sentiments.get(sentiment, 0) + 1
        
        # Compter doublons
        duplicates = sum(1 for a in articles if a.get('is_duplicate', False))
        
        print("\n" + "="*70)
        print("ğŸ“Š RÃ‰SUMÃ‰ CLASSIFICATION")
        print("="*70)
        print("\nğŸ“Œ DISTRIBUTION TOPICS:")
        for topic, count in sorted(topics.items(), key=lambda x: x[1], reverse=True):
            pct = (count / len(articles)) * 100
            print(f"  {topic:15s}: {count:3d} ({pct:5.1f}%)")
        
        print("\nğŸ˜Š DISTRIBUTION SENTIMENTS:")
        for sentiment, count in sorted(sentiments.items(), key=lambda x: x[1], reverse=True):
            pct = (count / len(articles)) * 100
            print(f"  {sentiment:15s}: {count:3d} ({pct:5.1f}%)")
        
        print(f"\nğŸ”„ DUPLICATES: {duplicates} ({(duplicates/len(articles)*100):.1f}%)")
        print("="*70)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN - TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import json
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
    )
    
    # Charger config
    with open('config.json', 'r') as f:
        config = json.load(f)
    
    # CrÃ©er classifier
    classifier = NewsClassifier(config)
    
    # Test articles
    test_articles = [
        {
            'title': 'Beginner Guide to Python',
            'content': 'This is a simple tutorial for beginners learning Python programming...',
            'normalized_content': 'simple tutorial beginners learning python programming'
        },
        {
            'title': 'Advanced Fine-tuning Techniques',
            'content': 'Advanced techniques for fine-tuning large language models...',
            'normalized_content': 'advanced techniques fine-tuning large language models'
        }
    ]
    
    # Classifier
    classified = classifier.classify_batch(test_articles)
    
    # Afficher rÃ©sultats
    for article in classified:
        print(f"\nArticle: {article['title']}")
        print(f"  Topic: {article['topic_prediction']} ({article['topic_confidence']})")
        print(f"  Sentiment: {article['sentiment_label']}")
