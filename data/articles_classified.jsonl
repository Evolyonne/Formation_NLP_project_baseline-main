{"title": "Running Claude Code dangerously (safely)", "url": "https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/", "content": "Running Claude Code dangerously (safely)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["running", "claude", "code", "dangerously", "safely"], "num_tokens": 5, "token_loss_pct": 0.0, "normalized_content": "running claude code dangerously safely", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9994, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "I'm addicted to being useful", "url": "https://www.seangoedecke.com/addicted-to-being-useful/", "content": "I'm addicted to being useful", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["i'", "addicted", "to", "being", "useful"], "num_tokens": 5, "token_loss_pct": 16.67, "normalized_content": "i'm addicted to being useful", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9284, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Linux kernel framework for PCIe device emulation, in userspace", "url": "https://github.com/cakehonolulu/pciem", "content": "Linux kernel framework for PCIe device emulation, in userspace", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["linux", "kernel", "framework", "for", "pcie", "devic", "emulation", "in", "userspace"], "num_tokens": 9, "token_loss_pct": 0.0, "normalized_content": "linux kernel framework for pcie device emulation in userspace", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9884, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "url": "https://github.com/majcheradam/ocrbase", "content": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["show", "hn", "ocrbase", "pdf", ".md.json", "document", "ocr", "and", "structured", "extraction", "api"], "num_tokens": 11, "token_loss_pct": 15.38, "normalized_content": "show hn ocrbase  pdf  .md.json document ocr and structured extraction api", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9928, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "The Zen of Reticulum", "url": "https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md", "content": "The Zen of Reticulum", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["the", "zen", "of", "reticulum"], "num_tokens": 4, "token_loss_pct": 0.0, "normalized_content": "the zen of reticulum", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9788, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Level S4 solar radiation event", "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026", "content": "Level S4 solar radiation event", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["level", "s4", "solar", "radiation", "event"], "num_tokens": 5, "token_loss_pct": 0.0, "normalized_content": "level s4 solar radiation event", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9475, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Reticulum, a secure and anonymous mesh networking stack", "url": "https://github.com/markqvist/Reticulum", "content": "Reticulum, a secure and anonymous mesh networking stack", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["reticulum", "secure", "and", "anonymou", "mesh", "networking", "stack"], "num_tokens": 7, "token_loss_pct": 12.5, "normalized_content": "reticulum a secure and anonymous mesh networking stack", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9781, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "IP over Avian Carriers with Quality of Service (1999)", "url": "https://www.rfc-editor.org/rfc/rfc2549.html", "content": "IP over Avian Carriers with Quality of Service (1999)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["ip", "over", "avian", "carrier", "with", "quality", "of", "service", "1999"], "num_tokens": 9, "token_loss_pct": 0.0, "normalized_content": "ip over avian carriers with quality of service 1999", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "POSITIVE", "sentiment_score": 0.9866, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Apple testing new App Store design that blurs the line between ads and results", "url": "https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/", "content": "Apple testing new App Store design that blurs the line between ads and results", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["apple", "testing", "new", "app", "store", "design", "that", "blur", "the", "line", "between", "ads", "and", "results"], "num_tokens": 14, "token_loss_pct": 0.0, "normalized_content": "apple testing new app store design that blurs the line between ads and results", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9949, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "url": "https://trendsfp.github.io/papers/tfp26-paper-12.pdf", "content": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["benchmarking", "baseline", "fully", "in", "place", "functional", "languag", "compiler", "pdf"], "num_tokens": 9, "token_loss_pct": 25.0, "normalized_content": "benchmarking a baseline fully-in-place functional language compiler pdf", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.5983, "sentiment_label": "Super Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Channel3 (YC S25) Is Hiring", "url": "https://www.ycombinator.com/companies/channel3/jobs/3DIAYYY-backend-engineer", "content": "Channel3 (YC S25) Is Hiring", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["channel3", "yc", "s25", "is", "hiring"], "num_tokens": 5, "token_loss_pct": 0.0, "normalized_content": "channel3 yc s25 is hiring", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9123, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Increasing the performance of WebAssembly Text Format parser by 350%", "url": "https://blog.gplane.win/posts/improve-wat-parser-perf.html", "content": "Increasing the performance of WebAssembly Text Format parser by 350%", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["increasing", "the", "performance", "of", "webassembly", "text", "format", "parser", "by", "350"], "num_tokens": 10, "token_loss_pct": 0.0, "normalized_content": "increasing the performance of webassembly text format parser by 350", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "POSITIVE", "sentiment_score": 0.9893, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "What came first: the CNAME or the A record?", "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/", "content": "What came first: the CNAME or the A record?", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["what", "came", "first", "the", "cname", "the", "record"], "num_tokens": 7, "token_loss_pct": 22.22, "normalized_content": "what came first the cname or the a record", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.5901, "sentiment_label": "Super Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The Overcomplexity of the Shadcn Radio Button", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["the", "overcomplexity", "of", "the", "shadcn", "radio", "button"], "num_tokens": 7, "token_loss_pct": 0.0, "normalized_content": "the overcomplexity of the shadcn radio button", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9991, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "url": "https://github.com/jordanhubbard/nanolang", "content": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nanolang", "tiny", "experimental", "languag", "designed", "to", "b", "targeted", "by", "coding", "llm"], "num_tokens": 11, "token_loss_pct": 8.33, "normalized_content": "nanolang a tiny experimental language designed to be targeted by coding llms", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9911, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "The coming industrialisation of exploit generation with LLMs", "url": "https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/", "content": "The coming industrialisation of exploit generation with LLMs", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["the", "coming", "industrialisation", "of", "exploit", "generation", "with", "llms"], "num_tokens": 8, "token_loss_pct": 0.0, "normalized_content": "the coming industrialisation of exploit generation with llms", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "POSITIVE", "sentiment_score": 0.8417, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "x86 prefixes and escape opcodes flowchart", "url": "https://soc.me/interfaces/x86-prefixes-and-escape-opcodes-flowchart.html", "content": "x86 prefixes and escape opcodes flowchart", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["x86", "prefixe", "and", "escape", "opcoder", "flowchart"], "num_tokens": 6, "token_loss_pct": 0.0, "normalized_content": "x86 prefixes and escape opcodes flowchart", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9893, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Notes on Apple's Nano Texture (2025)", "url": "https://jon.bo/posts/nano-texture/", "content": "Notes on Apple's Nano Texture (2025)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["note", "apple'", "nano", "texture", "2025"], "num_tokens": 5, "token_loss_pct": 28.57, "normalized_content": "notes on apple's nano texture 2025", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9753, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Scaling long-running autonomous coding", "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/", "content": "Scaling long-running autonomous coding", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["scaling", "long", "running", "autonomou", "coding"], "num_tokens": 5, "token_loss_pct": 16.67, "normalized_content": "scaling long-running autonomous coding", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.8879, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "String theory can now describe a universe that has dark energy?", "url": "https://www.quantamagazine.org/string-theory-can-now-describe-a-universe-that-has-dark-energy-20260114/", "content": "String theory can now describe a universe that has dark energy?", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["string", "theory", "can", "now", "describe", "universe", "that", "has", "dark", "energy"], "num_tokens": 10, "token_loss_pct": 9.09, "normalized_content": "string theory can now describe a universe that has dark energy", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9377, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "3D printing my laptop ergonomic setup", "url": "https://www.ntietz.com/blog/3d-printing-my-laptop-ergonomic-setup/", "content": "3D printing my laptop ergonomic setup", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["3d", "printing", "my", "laptop", "ergonomic", "setup"], "num_tokens": 6, "token_loss_pct": 0.0, "normalized_content": "3d printing my laptop ergonomic setup", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9941, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Giving university exams in the age of chatbots", "url": "https://ploum.net/2026-01-19-exam-with-chatbots.html", "content": "Giving university exams in the age of chatbots", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["giving", "university", "examen", "in", "the", "age", "of", "chatbot"], "num_tokens": 8, "token_loss_pct": 0.0, "normalized_content": "giving university exams in the age of chatbots", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9867, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Prediction markets are ushering in a world in which news becomes about gambling", "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/", "content": "Prediction markets are ushering in a world in which news becomes about gambling", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["prediction", "market", "are", "ushering", "in", "world", "in", "which", "new", "become", "about", "gambling"], "num_tokens": 12, "token_loss_pct": 7.69, "normalized_content": "prediction markets are ushering in a world in which news becomes about gambling", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9391, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Targeted Bets: An alternative approach to the job hunt", "url": "https://www.seanmuirhead.com/blog/targeted-bets", "content": "Targeted Bets: An alternative approach to the job hunt", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["targeted", "bet", "an", "alternative", "approach", "to", "the", "job", "hunt"], "num_tokens": 9, "token_loss_pct": 0.0, "normalized_content": "targeted bets an alternative approach to the job hunt", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9696, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Understanding ZFS Scrubs and Data Integrity", "url": "https://klarasystems.com/articles/understanding-zfs-scrubs-and-data-integrity/", "content": "Understanding ZFS Scrubs and Data Integrity", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["understanding", "zf", "scrub", "and", "dater", "integrity"], "num_tokens": 6, "token_loss_pct": 0.0, "normalized_content": "understanding zfs scrubs and data integrity", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9799, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "British redcoat's lost memoir reveals realities of life as a disabled veteran", "url": "https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html", "content": "British redcoat's lost memoir reveals realities of life as a disabled veteran", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["british", "redcoat'", "lost", "memoir", "reveal", "realitier", "of", "life", "disabled", "veteran"], "num_tokens": 10, "token_loss_pct": 23.08, "normalized_content": "british redcoat's lost memoir reveals realities of life as a disabled veteran", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "POSITIVE", "sentiment_score": 0.9917, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Kahan on the 8087 and designing Intel's floating point (2016) [video]", "url": "https://www.youtube.com/watch?v=L-QVgbdt_qg", "content": "Kahan on the 8087 and designing Intel's floating point (2016) [video]", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["kahan", "the", "8087", "and", "designing", "intel'", "floating", "point", "2016", "video"], "num_tokens": 10, "token_loss_pct": 16.67, "normalized_content": "kahan on the 8087 and designing intel's floating point 2016 video", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "POSITIVE", "sentiment_score": 0.9901, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000004}
{"title": "Nova Launcher added Facebook and Google Ads tracking", "url": "https://lemdro.id/post/lemdro.id/35049920", "content": "Nova Launcher added Facebook and Google Ads tracking", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["nover", "launcher", "added", "facebook", "and", "google", "ads", "tracking"], "num_tokens": 8, "token_loss_pct": 0.0, "normalized_content": "nova launcher added facebook and google ads tracking", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.8788, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "The microstructure of wealth transfer in prediction markets", "url": "https://www.jbecker.dev/research/prediction-market-microstructure", "content": "The microstructure of wealth transfer in prediction markets", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["the", "microstructure", "of", "wealth", "transfer", "in", "prediction", "market"], "num_tokens": 8, "token_loss_pct": 0.0, "normalized_content": "the microstructure of wealth transfer in prediction markets", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.5372, "sentiment_label": "Super Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars", "url": "https://newsroom.porsche.com/en/2026/company/porsche-deliveries-2025-41516.html", "content": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["porsche", "sold", "more", "electrified", "car", "in", "europe", "in", "2025", "than", "pur", "ger", "powered", "car"], "num_tokens": 14, "token_loss_pct": 6.67, "normalized_content": "porsche sold more electrified cars in europe in 2025 than pure gas-powered cars", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9329, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "The assistant axis: situating and stabilizing the character of LLMs", "url": "https://www.anthropic.com/research/assistant-axis", "content": "The assistant axis: situating and stabilizing the character of LLMs", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["the", "assister", "axis", "situating", "and", "stabilizing", "the", "character", "of", "llm"], "num_tokens": 10, "token_loss_pct": 0.0, "normalized_content": "the assistant axis situating and stabilizing the character of llms", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9828, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "From Nevada to Kansas by Glider", "url": "https://www.weglide.org/flight/978820", "content": "From Nevada to Kansas by Glider", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["from", "nevader", "to", "kanser", "by", "glider"], "num_tokens": 6, "token_loss_pct": 0.0, "normalized_content": "from nevada to kansas by glider", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9599, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Kiss Launcher – fast launcher for Android", "url": "https://kisslauncher.com/", "content": "Kiss Launcher – fast launcher for Android", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["kiss", "launcher", "fast", "launcher", "for", "android"], "num_tokens": 6, "token_loss_pct": 14.29, "normalized_content": "kiss launcher  fast launcher for android", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9365, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Show HN: Artificial Ivy in the Browser", "url": "https://da.nmcardle.com/grow", "content": "Show HN: Artificial Ivy in the Browser", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["show", "hn", "artificial", "ivy", "in", "the", "browser"], "num_tokens": 7, "token_loss_pct": 0.0, "normalized_content": "show hn artificial ivy in the browser", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9981, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "I set all 376 Vim options and I'm still a fool", "url": "https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/", "content": "I set all 376 Vim options and I'm still a fool", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["set", "all", "376", "vim", "option", "and", "i'", "still", "fool"], "num_tokens": 9, "token_loss_pct": 25.0, "normalized_content": "i set all 376 vim options and i'm still a fool", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9996, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Face as a QR Code", "url": "https://bookofjoe2.blogspot.com/2025/12/your-face-as-qr-code.html", "content": "Face as a QR Code", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["face", "qr", "code"], "num_tokens": 3, "token_loss_pct": 40.0, "normalized_content": "face as a qr code", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.991, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "How we made Python's packaging library 3x faster", "url": "https://iscinumpy.dev/post/packaging-faster/", "content": "How we made Python's packaging library 3x faster", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["how", "w", "made", "python'", "packaging", "library", "3x", "faster"], "num_tokens": 8, "token_loss_pct": 11.11, "normalized_content": "how we made python's packaging library 3x faster", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9806, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "The Antarctic Snow Cruiser", "url": "https://www.amusingplanet.com/2026/01/the-antarctic-snow-cruiser.html", "content": "The Antarctic Snow Cruiser", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["the", "antarctic", "snow", "cruiser"], "num_tokens": 4, "token_loss_pct": 0.0, "normalized_content": "the antarctic snow cruiser", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "POSITIVE", "sentiment_score": 0.9757, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "San Francisco coyote swims to Alcatraz", "url": "https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php", "content": "San Francisco coyote swims to Alcatraz", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["san", "francisco", "coyote", "swim", "to", "alcatraz"], "num_tokens": 6, "token_loss_pct": 0.0, "normalized_content": "san francisco coyote swims to alcatraz", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9354, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Sending Data over Offline Finding Networks", "url": "https://cc-sw.com/find-my-and-find-hub-network-research/", "content": "Sending Data over Offline Finding Networks", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["sending", "dater", "over", "offline", "finding", "network"], "num_tokens": 6, "token_loss_pct": 0.0, "normalized_content": "sending data over offline finding networks", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9831, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "url": "https://github.com/minimaxir/ballin", "content": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["show", "hn", "an", "interactif", "physic", "simulator", "with", "1000s", "of", "balls", "in", "your", "terminal"], "num_tokens": 13, "token_loss_pct": 0.0, "normalized_content": "show hn an interactive physics simulator with 1000s of balls in your terminal", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "POSITIVE", "sentiment_score": 0.6495, "sentiment_label": "Super Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "King – man + woman is queen; but why? (2017)", "url": "https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/", "content": "King – man + woman is queen; but why? (2017)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["king", "man", "woman", "i", "queen", "but", "why", "2017"], "num_tokens": 8, "token_loss_pct": 20.0, "normalized_content": "king  man  woman is queen but why 2017", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.974, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "How to be a good conference talk audience member (2022)", "url": "https://www.mooreds.com/wordpress/archives/3522", "content": "How to be a good conference talk audience member (2022)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["how", "to", "b", "good", "conference", "talk", "audience", "member", "2022"], "num_tokens": 9, "token_loss_pct": 10.0, "normalized_content": "how to be a good conference talk audience member 2022", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9989, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Conditions in the Intel 8087 floating-point chip's microcode", "url": "https://www.righto.com/2025/12/8087-microcode-conditions.html", "content": "Conditions in the Intel 8087 floating-point chip's microcode", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["condition", "in", "the", "intel", "8087", "floating", "point", "chip'", "microcode"], "num_tokens": 9, "token_loss_pct": 18.18, "normalized_content": "conditions in the intel 8087 floating-point chip's microcode", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9954, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Flux 2 Klein pure C inference", "url": "https://github.com/antirez/flux2.c", "content": "Flux 2 Klein pure C inference", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["flux", "klein", "pur", "inference"], "num_tokens": 4, "token_loss_pct": 33.33, "normalized_content": "flux 2 klein pure c inference", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.8269, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "CSS Web Components for marketing sites (2024)", "url": "https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/", "content": "CSS Web Components for marketing sites (2024)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["cs", "web", "component", "for", "marketing", "site", "2024"], "num_tokens": 7, "token_loss_pct": 0.0, "normalized_content": "css web components for marketing sites 2024", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9312, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Show HN: Subth.ink – write something and see how many others wrote the same", "url": "https://subth.ink/", "content": "Show HN: Subth.ink – write something and see how many others wrote the same", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["show", "hn", "subth.ink", "write", "something", "and", "se", "how", "many", "other", "wrote", "the", "same"], "num_tokens": 13, "token_loss_pct": 7.14, "normalized_content": "show hn subth.ink  write something and see how many others wrote the same", "topic_prediction": "Advanced", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.0, "Advanced": 1.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9908, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Show HN: E80: an 8-bit CPU in structural VHDL", "url": "https://github.com/Stokpan/E80", "content": "Show HN: E80: an 8-bit CPU in structural VHDL", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["show", "hn", "e80", "an", "8-bit", "cpu", "in", "structural", "vhdl"], "num_tokens": 9, "token_loss_pct": 0.0, "normalized_content": "show hn e80 an 8-bit cpu in structural vhdl", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9953, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Bypassing Gemma and Qwen safety with raw strings", "url": "https://teendifferent.substack.com/p/apply_chat_template-is-the-safety", "content": "Bypassing Gemma and Qwen safety with raw strings", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["bypassing", "gemma", "and", "qwen", "safety", "with", "raw", "string"], "num_tokens": 8, "token_loss_pct": 0.0, "normalized_content": "bypassing gemma and qwen safety with raw strings", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.8904, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Selling SaaS in Japan", "url": "https://embedworkflow.com/blog/what-saas-founders-should-know-about-entering-the-japanese-market/", "content": "Selling SaaS in Japan", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["selling", "saas", "in", "japan"], "num_tokens": 4, "token_loss_pct": 0.0, "normalized_content": "selling saas in japan", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9251, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "F-16 Falcon Strike", "url": "https://webchrono.pl/F16FalconStrike/index.html", "content": "F-16 Falcon Strike", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["f-16", "falcon", "strik"], "num_tokens": 3, "token_loss_pct": 0.0, "normalized_content": "f-16 falcon strike", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9757, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Show HN: Pipenet – A Modern Alternative to Localtunnel", "url": "https://pipenet.dev/", "content": "Show HN: Pipenet – A Modern Alternative to Localtunnel", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["show", "hn", "pipenet", "modern", "alternative", "to", "localtunnel"], "num_tokens": 7, "token_loss_pct": 22.22, "normalized_content": "show hn pipenet  a modern alternative to localtunnel", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9901, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Understanding C++ Ownership System", "url": "https://blog.aiono.dev/posts/understanding-c++-ownership-system.html", "content": "Understanding C++ Ownership System", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["understanding", "ownership", "system"], "num_tokens": 3, "token_loss_pct": 25.0, "normalized_content": "understanding c ownership system", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9989, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Weight Transfer for RL Post-Training in under 2 seconds", "url": "https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds", "content": "Weight Transfer for RL Post-Training in under 2 seconds", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["weight", "transfer", "for", "rl", "post-training", "in", "under", "second"], "num_tokens": 8, "token_loss_pct": 11.11, "normalized_content": "weight transfer for rl post-training in under 2 seconds", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9744, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Legal Structures for Latin American Startups (2021)", "url": "https://latamlist.com/legal-structures-for-latin-american-startups/", "content": "Legal Structures for Latin American Startups (2021)", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["legal", "structure", "for", "latin", "american", "startups", "2021"], "num_tokens": 7, "token_loss_pct": 0.0, "normalized_content": "legal structures for latin american startups 2021", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9746, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Use Social Media Mindfully", "url": "https://danielleheberling.xyz/blog/mindful-social-media/", "content": "Use Social Media Mindfully", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["us", "social", "media", "mindfully"], "num_tokens": 4, "token_loss_pct": 0.0, "normalized_content": "use social media mindfully", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "POSITIVE", "sentiment_score": 0.9581, "sentiment_label": "Positif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Chatbot Psychosis", "url": "https://en.wikipedia.org/wiki/Chatbot_psychosis", "content": "Chatbot Psychosis", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["chatbot", "psychosi"], "num_tokens": 2, "token_loss_pct": 0.0, "normalized_content": "chatbot psychosis", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9899, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0000000000000002}
{"title": "Radboud University selects Fairphone as standard smartphone for employees", "url": "https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees", "content": "Radboud University selects Fairphone as standard smartphone for employees", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["radboud", "university", "select", "fairphone", "standard", "smartphone", "for", "employee"], "num_tokens": 8, "token_loss_pct": 11.11, "normalized_content": "radboud university selects fairphone as standard smartphone for employees", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9727, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Graphics In Flatland – 2D ray tracing [video]", "url": "https://www.youtube.com/watch?v=WYTOykSqf2Y", "content": "Graphics In Flatland – 2D ray tracing [video]", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["graphics", "in", "flatland", "2d", "ray", "tracing", "video"], "num_tokens": 7, "token_loss_pct": 12.5, "normalized_content": "graphics in flatland  2d ray tracing video", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.997, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Iterative image reconstruction using random cubic bézier strokes", "url": "https://tangled.org/luthenwald.tngl.sh/splined", "content": "Iterative image reconstruction using random cubic bézier strokes", "source": "HackerNews", "date": null, "author": null, "score": null, "tokens": ["iterative", "image", "reconstruction", "using", "random", "cubic", "bézier", "stroke"], "num_tokens": 8, "token_loss_pct": 0.0, "normalized_content": "iterative image reconstruction using random cubic bézier strokes", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9867, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Issue #717: Unit Testing Performance, Cursor, Recursive match, and More (Jan. 13, 2026)", "url": "https://pycoders.com/issues/717", "content": "<p> <span>#717 – JANUARY 13, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/717/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s We", "source": "RSS", "date": "2026-01-13T19:30:00+00:00", "author": null, "score": null, "tokens": ["hashtag", "january", "13", "2026", "view", "in", "browser", "img", "altthe", "pycoderrsquo", "we"], "num_tokens": 11, "token_loss_pct": 15.38, "normalized_content": "hashtag  january 13 2026 view in browser  img altthe pycoderrsquos we", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9904, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Issue #716: Performance Numbers, async Web Apps, uv Speed, and More (Jan. 6, 2026)", "url": "https://pycoders.com/issues/716", "content": "<p> <span>#716 – JANUARY 6, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/716/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s Wee", "source": "RSS", "date": "2026-01-06T19:30:00+00:00", "author": null, "score": null, "tokens": ["hashtag", "january", "2026", "view", "in", "browser", "img", "altthe", "pycoderrsquos", "wee"], "num_tokens": 10, "token_loss_pct": 23.08, "normalized_content": "hashtag  january 6 2026 view in browser  img altthe pycoderrsquos wee", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9926, "sentiment_label": "Negatif", "is_duplicate": true, "duplicate_score": 1.0}
{"title": "Issue #715: Top 5 of 2025, LlamaIndex, Python 3.15 Speed, and More (Dec. 30, 2025)", "url": "https://pycoders.com/issues/715", "content": "<p> <span>#715 – DECEMBER 30, 2025</span><br /> <span><a href=\"https://pycoders.com/issues/715/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s W", "source": "RSS", "date": "2025-12-30T19:30:00+00:00", "author": null, "score": null, "tokens": ["hashtag", "december", "30", "2025", "view", "in", "browser", "img", "altthe", "pycoderrsquos"], "num_tokens": 10, "token_loss_pct": 23.08, "normalized_content": "hashtag  december 30 2025 view in browser  img altthe pycoderrsquos w", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9916, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 1.0}
{"title": "Traitement automatique des langues", "url": "https://fr.wikipedia.org/wiki/Traitement_automatique_des_langues", "content": "Le traitement automatique des langues (TAL ou TALN), en anglais natural language processing ou NLP, est un domaine multidisciplinaire impliquant la linguistique, l'informatique et l'intelligence artificielle, qui vise à créer des outils de traitement du langage naturel pour diverses applications. Il ne doit pas être confondu avec la linguistique informatique, qui vise à analyser les langues au moyen d'outils informatiques.\nLe TALN est sorti des laboratoires de recherche pour être progressivement mis en œuvre dans des applications informatiques nécessitant l'intégration du langage humain à la machine. Aussi le TALN est-il parfois appelé ingénierie linguistique.\n\nHistoire\nAnnées 1950-1960\nLes premiers travaux en traitement automatique du langage naturel commencent dans les années 1950, principalement aux États-Unis où le contexte politique, lié à la guerre froide, est propice au développement de la thématique de la traduction automatique.\nLes premières applications informatiques sont liées au traitement automatique des conversations. En 1950, dans son article fondateur de l'intelligence artificielle, « Computing machinery and intelligence », Alan Turing expose une méthode d'évaluation qui sera appelée par la suite « test de Turing » ou « critère de Turing ». Ce test mesure le degré d'intelligence d'une machine, à partir de la capacité d'un programme conversationnel à se faire passer pour un être humain : dans un échange de messages écrits, un sujet humain doit déterminer si son interlocuteur est une machine ou non. La base employée est cependant fragile pour évaluer l'intelligence artificielle, car l'impression d'un unique utilisateur dépend de trop de facteurs liés au milieu ambiant pour être érigée en règle.\nEn 1954, l'expérience Georgetown-IBM, réalisée conjointement par l'université de Georgetown et par la société IBM, comporte la traduction complètement automatique, en anglais, de plus de soixante phrases russes romanisées relatives aux domaines de la politique, du droit, des mathématiques et de la science. Les auteurs prétendent que dans un délai de trois à cinq ans, la traduction automatique ne sera plus un problème. Il apparaît cependant que les énoncés en russe ont été choisis avec soin et que nombre des opérations effectuées pour la démonstration ont été adaptées à des mots et des phrases particuliers. De plus, il n'y a pas d'analyse relationnelle ou syntaxique permettant d'identifier la structure des phrases. La méthode employée est une méthode essentiellement lexicographique reposant sur un dictionnaire où un mot donné est relié à des règles et des démarches spécifiques.\nLes notions introduites par Turing permirent à Joseph Weizenbaum de mettre au point, de 1964 à 1966, le premier automate conversationnel à tromper un être humain quant à sa nature. Simulant un psychothérapeute rogérien, l'automate, du nom d'ELIZA, bien que n'employant presque aucune information sur la pensée ou l'émotion humaine, parvient parfois à établir une interaction étonnamment similaire à l'interaction humaine. Ainsi, quand le « patient » dépasse les faibles capacités de la base de connaissances, ELIZA peut fournir une réponse générique, comme « Pourquoi dites-vous avoir mal à la tête ? » en réponse à « J'ai mal à la tête ».\nÀ la fin des années 1960, Terry Winograd, un chercheur du MIT, met au point un programme en langage naturel du nom de SHRDLU (prononcer « chreudeul »), qui permet à son utilisateur de converser avec un ordinateur pour gérer un « monde de cubes de construction » (a blocks world) s'affichant sur un des premiers écrans. C’est le premier programme qui sache comprendre et exécuter des ordres complexes en langage naturel. Mais les seules opérations qu'il peut faire, c’est de prendre des cubes, les déplacer, les rassembler ou les disperser. Il ne pourra jamais comprendre tout ce que les humains peuvent faire avec des objets physiques.\nLes progrès réels sont donc décevants. Le rapport ALPAC (en) de 1966 constate qu'en dix ans de recherches les buts n'ont pas été atteints. Cette prise de conscience de l'extrême complexité des langues a considérablement réduit l'ambition des travaux de recherche.\n\nAnnées 1970-1980\nPendant les années 1970 beaucoup de programmeurs ont commencé à écrire des « ontologies conceptuelles », dont le but était de structurer l'information en données compréhensibles par l'ordinateur. C'est le cas de MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), SCRUPULE (Lehnert, 1977), Politics (Carbonell, 1979), Plot Units (Lehnert, 1981).\n\nAnnées 1990-2000\nDepuis les années 2010\nEn janvier 2018, des modèles d'intelligence artificielle développés par Microsoft et Alibaba réussissent chacun de leur côté à battre les humains dans un test de lecture et de compréhension de l’université Stanford. Le traitement automatique du langage naturel imite la compréhension humaine des mots et des phrases et permet maintenant aux modèles d'apprentissage automatique de trai", "source": "Wikipedia", "date": null, "author": null, "score": null, "tokens": ["traitement", "automatique", "langue", "tal", "taln", "anglais", "natural", "languag", "processing", "nlp", "domaine", "multidisciplinaire", "impliquer", "linguistique", "informatique", "intelligence", "artificiel", "viser", "créer", "outil", "traitement", "langage", "naturel", "application", "confondre", "linguistique", "informatique", "viser", "analyser", "langue", "moyen", "outil", "informatique", "taln", "sortir", "laboratoire", "recherche", "progressivemer", "mettre", "œuvre", "application", "informatique", "nécessiter", "intégration", "langage", "humain", "machine", "taln", "il", "appeler", "ingénierie", "linguistique", "histoire", "anner", "1950", "1960", "premier", "travail", "traitement", "automatique", "langage", "naturel", "commencer", "année", "1950", "principalement", "états-unis", "contexte", "politique", "lier", "guerre", "froid", "propice", "développement", "thématique", "traduction", "automatique", "premier", "application", "informatique", "lier", "traitement", "automatique", "conversation", "1950", "article", "fondateur", "intelligence", "artificiel", "computing", "machinery", "and", "intelligence", "alan", "turing", "expose", "méthode", "évaluation", "appeler", "suite", "test", "turing", "critère", "turing", "test", "mesure", "degré", "intelligence", "machine", "partir", "capacité", "programme", "conversationnel", "faire", "passer", "humain", "échange", "message", "écrire", "sujet", "humain", "déterminer", "interlocuteur", "machine", "non", "base", "employer", "fragile", "évaluer", "intelligence", "artificiel", "impression", "unique", "utilisateur", "dépendre", "trop", "facteur", "lier", "milieu", "ambier", "ériger", "régler", "1954", "expérience", "georgetown", "ibm", "réaliser", "conjointemer", "université", "georgetown", "société", "ibm", "comporte", "traduction", "complètement", "automatique", "anglais", "phrase", "russe", "romaniser", "relatif", "domaine", "politique", "droit", "mathématique", "science", "auteur", "prétendre", "délai", "an", "traduction", "automatique", "problème", "apparaître", "énoncé", "russe", "être", "choisir", "soin", "nombre", "opération", "effectuer", "démonstration", "être", "adapter", "mot", "phrase", "particulier", "analyse", "relationnel", "syntaxique", "permettre", "identifier", "structure", "phrase", "méthode", "employer", "méthod", "essentiellement", "lexicographique", "reposer", "dictionnaire", "mot", "donner", "relier", "règle", "démarche", "notion", "introduire", "turing", "permettre", "joseph", "weizenbaum", "mettre", "point", "1964", "1966", "automate", "conversationnel", "tromper", "humain", "nature", "simuler", "psychothérapeute", "rogérien", "automate", "nom", "eliza", "bien", "employer", "presque", "aucun", "information", "pensée", "émotion", "humain", "parvenir", "établir", "interaction", "étonnammer", "similaire", "interaction", "humain", "patient", "dépasser", "faible", "capacité", "base", "connaissance", "eliza", "fournir", "réponse", "générique", "dire", "vous", "mal", "tête", "  ", "réponse", "mal", "tête", "fin", "année", "1960", "terry", "winograd", "chercheur", "mit", "mettre", "point", "programme", "langage", "naturel", "nom", "shrdlu", "prononcer", "chreudeul", "utilisateur", "converser", "ordinateur", "gérer", "monde", "cube", "construction", "block", "world", "afficher", "premier", "écran", "cest", "programme", "savoir", "comprendre", "exécuter", "ordre", "complexe", "langage", "naturel", "opération", "faire", "cest", "prendre", "cube", "déplacer", "rassembler", "disperser", "pouvoir", "jamais", "comprendre", "humain", "faire", "objet", "physique", "progrès", "réel", "décevant", "rapport", "alpac", "1966", "constater", "an", "recherche", "but", "être", "atteindre", "prise", "conscience", "extrême", "complexité", "langue", "considérablement", "réduire", "ambition", "travail", "recherche", "année", "1970", "1980", "année", "1970", "beaucoup", "programmeur", "commencer", "écrire", "ontologie", "conceptuel", "but", "structurer", "information", "donnée", "compréhensibler", "ordinateur", "cas", "margie", "schank", "1975", "sam", "cullingford", "1978", "pam", "wilensky", "1978", "talespin", "meehan", "1976", "scrupul", "lehnert", "1977", "politic", "carbonell", "1979", "plot", "units", "lehnert", "1981", "année", "1990", "2000", "année", "2010", "janvier", "2018", "modèle", "intelligence", "artificiel", "développer", "microsoft", "alibaber", "réussir", "côté", "battr", "humain", "test", "lecture", "compréhension", "luniversité", "stanford", "traitement", "automatique", "langage", "naturel", "imit", "compréhension", "humain", "mot", "phrase", "modèle", "apprentissage", "automatique", "trai"], "num_tokens": 410, "token_loss_pct": 50.54, "normalized_content": "le traitement automatique des langues tal ou taln en anglais natural language processing ou nlp est un domaine multidisciplinaire impliquant la linguistique l'informatique et l'intelligence artificielle qui vise à créer des outils de traitement du langage naturel pour diverses applications. il ne doit pas être confondu avec la linguistique informatique qui vise à analyser les langues au moyen d'outils informatiques. le taln est sorti des laboratoires de recherche pour être progressivement mis en œuvre dans des applications informatiques nécessitant l'intégration du langage humain à la machine. aussi le taln est-il parfois appelé ingénierie linguistique. histoire années 1950-1960 les premiers travaux en traitement automatique du langage naturel commencent dans les années 1950 principalement aux états-unis où le contexte politique lié à la guerre froide est propice au développement de la thématique de la traduction automatique. les premières applications informatiques sont liées au traitement automatique des conversations. en 1950 dans son article fondateur de l'intelligence artificielle  computing machinery and intelligence  alan turing expose une méthode d'évaluation qui sera appelée par la suite  test de turing  ou  critère de turing . ce test mesure le degré d'intelligence d'une machine à partir de la capacité d'un programme conversationnel à se faire passer pour un être humain  dans un échange de messages écrits un sujet humain doit déterminer si son interlocuteur est une machine ou non. la base employée est cependant fragile pour évaluer l'intelligence artificielle car l'impression d'un unique utilisateur dépend de trop de facteurs liés au milieu ambiant pour être érigée en règle. en 1954 l'expérience georgetown-ibm réalisée conjointement par l'université de georgetown et par la société ibm comporte la traduction complètement automatique en anglais de plus de soixante phrases russes romanisées relatives aux domaines de la politique du droit des mathématiques et de la science. les auteurs prétendent que dans un délai de trois à cinq ans la traduction automatique ne sera plus un problème. il apparaît cependant que les énoncés en russe ont été choisis avec soin et que nombre des opérations effectuées pour la démonstration ont été adaptées à des mots et des phrases particuliers. de plus il n'y a pas d'analyse relationnelle ou syntaxique permettant d'identifier la structure des phrases. la méthode employée est une méthode essentiellement lexicographique reposant sur un dictionnaire où un mot donné est relié à des règles et des démarches spécifiques. les notions introduites par turing permirent à joseph weizenbaum de mettre au point de 1964 à 1966 le premier automate conversationnel à tromper un être humain quant à sa nature. simulant un psychothérapeute rogérien l'automate du nom d'eliza bien que n'employant presque aucune information sur la pensée ou l'émotion humaine parvient parfois à établir une interaction étonnamment similaire à l'interaction humaine. ainsi quand le  patient  dépasse les faibles capacités de la base de connaissances eliza peut fournir une réponse générique comme  pourquoi dites-vous avoir mal à la tête   en réponse à  j'ai mal à la tête . à la fin des années 1960 terry winograd un chercheur du mit met au point un programme en langage naturel du nom de shrdlu prononcer  chreudeul  qui permet à son utilisateur de converser avec un ordinateur pour gérer un  monde de cubes de construction  a blocks world s'affichant sur un des premiers écrans. cest le premier programme qui sache comprendre et exécuter des ordres complexes en langage naturel. mais les seules opérations qu'il peut faire cest de prendre des cubes les déplacer les rassembler ou les disperser. il ne pourra jamais comprendre tout ce que les humains peuvent faire avec des objets physiques. les progrès réels sont donc décevants. le rapport alpac en de 1966 constate qu'en dix ans de recherches les buts n'ont pas été atteints. cette prise de conscience de l'extrême complexité des langues a considérablement réduit l'ambition des travaux de recherche. années 1970-1980 pendant les années 1970 beaucoup de programmeurs ont commencé à écrire des  ontologies conceptuelles  dont le but était de structurer l'information en données compréhensibles par l'ordinateur. c'est le cas de margie schank 1975 sam cullingford 1978 pam wilensky 1978 talespin meehan 1976 scrupule lehnert 1977 politics carbonell 1979 plot units lehnert 1981. années 1990-2000 depuis les années 2010 en janvier 2018 des modèles d'intelligence artificielle développés par microsoft et alibaba réussissent chacun de leur côté à battre les humains dans un test de lecture et de compréhension de luniversité stanford. le traitement automatique du langage naturel imite la compréhension humaine des mots et des phrases et permet maintenant aux modèles d'apprentissage automatique de trai", "topic_prediction": "Unknown", "topic_confidence": 0.0, "topic_scores": {}, "sentiment": "NEGATIVE", "sentiment_score": 0.9791, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 0.9999999999999966}
{"title": "Apprentissage automatique", "url": "https://fr.wikipedia.org/wiki/Apprentissage_automatique", "content": "L'apprentissage automatique (en anglais : machine learning, litt. « apprentissage machine »), apprentissage artificiel ou apprentissage statistique est un champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes. On parle d'apprentissage statistique car l'apprentissage consiste à créer un modèle dont l'erreur statistique moyenne est la plus faible possible.\nL'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement préalable à l'utilisation pratique du modèle. La seconde phase est la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. \nCertains systèmes peuvent continuer à apprendre une fois en production, s'ils disposent d'un retour sur la qualité des résultats produits. C'est l'apprentissage en ligne, ou l'apprentissage continu.\nSelon le type de données utilisées pour l'apprentissage, on distingue :\n\nl'apprentissage supervisé : l'algorithme apprend à partir de données étiquetées (la réponse à la tâche, qui est la donnée de sortie, est donc connue pour chaque données d'entrée). L'objectif est de prédire les sorties pour de nouvelles données ;\nl'apprentissage non supervisé : l'algorithme apprend à partir de données non étiquetées. Il cherche à découvrir des structures sous-jacentes, cachées (qui peuvent par exemple être une densité de probabilité) ; des motifs dans les données permettent la classification ou le classement des données ;\nl'apprentissage semi-supervisé : il tire parti d'une grande quantité de données non étiquetées pour améliorer la performance du modèle, tout en utilisant une moindre quantité de données étiquetées pour guider son apprentissage. Il diminue les coûts d'étiquetage manuel des données ;\nl'apprentissage auto-supervisé : c'est une forme d'apprentissage non supervisé, où le modèle génère ses propres étiquettes à partir des données brutes. Le modèle peut ainsi créer des représentations internes utiles, sans nécessiter de données étiquetées manuellement.\nL'apprentissage automatique peut être appliqué à divers types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.\nSi le modèle apprend de manière incrémentale, en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement.\n\nHistorique\nDepuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.\nLa concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 1936, qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 1950, dans lequel il développe, entre autres, le test de Turing.\nEn 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux.\nArthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis.\nUne avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de", "source": "Wikipedia", "date": null, "author": null, "score": null, "tokens": ["apprentissage", "automatique", "anglais", "machine", "learning", "litt", "apprentissage", "machine", "apprentissage", "artificiel", "apprentissage", "statistique", "champ", "étude", "intelligence", "artificiel", "fondre", "approche", "mathématique", "statistique", "donner", "ordinateur", "capacité", "apprendre", "partir", "donnée", "améliorer", "performance", "résoudre", "tâche", "explicitement", "programmer", "largement", "concerner", "conception", "analyse", "optimisation", "développement", "implémentation", "méthode", "apprentissage", "statistique", "apprentissage", "consister", "créer", "modèle", "erreur", "statistique", "moyen", "faible", "apprentissage", "automatique", "comporte", "généralement", "phase", "consister", "estimer", "modèle", "partir", "donnée", "appeler", "observation", "disponible", "nombre", "finir", "phase", "conception", "système", "estimation", "modèle", "consister", "résoudre", "tâche", "pratique", "traduire", "discours", "estimer", "densité", "probabilité", "reconnaître", "présence", "chat", "photographie", "participer", "conduite", "véhicule", "autonome", "phase", "apprentissage", "entraînement", "généralement", "utilisation", "pratique", "modèle", "second", "phase", "mise", "production", "modèle", "déterminé", "nouveau", "donnée", "soumettre", "obtenir", "résultat", "correspondre", "tâche", "souhaiter", "système", "continuer", "apprendre", "fois", "production", "disposer", "qualité", "résultat", "produire", "apprentissage", "ligne", "apprentissage", "continu", "type", "donnée", "utiliser", "apprentissage", "distinguer", "apprentissage", "superviser", "algorithme", "apprendre", "partir", "donnée", "étiqueter", "réponse", "tâche", "donnée", "sortie", "connaître", "donnée", "entrée", "objectif", "prédir", "sortie", "nouveau", "donnée", "apprentissage", "non", "superviser", "algorithme", "apprendre", "partir", "donnée", "non", "étiqueter", "cherche", "découvrir", "structure", "sous-jacent", "cacher", "exemple", "densité", "probabilité", "motif", "donnée", "permettre", "classification", "classement", "donnée", "apprentissage", "semi-superviser", "tire", "partir", "grand", "quantité", "donnée", "non", "étiqueter", "améliorer", "performance", "modèle", "utiliser", "moindre", "quantité", "donnée", "étiqueter", "guider", "apprentissage", "diminuer", "coût", "étiquetage", "manuel", "donnée", "apprentissage", "auto-supervisé", "forme", "apprentissage", "non", "superviser", "modèle", "générer", "propre", "étiquette", "partir", "donnée", "brut", "modèle", "créer", "représentation", "interne", "utile", "nécessiter", "donnée", "étiqueter", "manuellemer", "apprentissage", "automatique", "appliquer", "type", "donnée", "graphe", "arbre", "courbe", "simplement", "vecteur", "caractéristique", "variable", "qualitatif", "quantitative", "continu", "discret", "modèle", "apprendre", "manière", "incrémental", "fonction", "récompense", "recevoir", "programme", "action", "entrepris", "apprentissage", "renforcement", "historique", "antiquité", "sujet", "machine", "pensant", "préoccuper", "esprit", "concept", "base", "pensée", "devenir", "ensuite", "intelligence", "artificiel", "sous-branche", "apprentissage", "automatique", "concrétisation", "idée", "principalement", "devoir", "alan", "turing", "mathématicien", "cryptologue", "britannique", "concept", "machine", "universel", "1936", "base", "ordinateur", "aujourd'hui", "continuer", "poser", "base", "apprentissage", "automatique", "article", "ordinateur", "intelligence", "1950", "développer", "test", "turing", "1943", "neurophysiologiste", "warren", "mcculloch", "mathématicien", "walter", "pitt", "publier", "article", "décrire", "fonctionnement", "neurone", "représentant", "aide", "circuit", "électrique", "représentation", "base", "théorique", "réseau", "neuronal", "arthur", "samuel", "informaticien", "américain", "pionnier", "secteur", "intelligence", "artificiel", "faire", "usage", "expression", "machine", "learning", "français", "apprentissage", "automatique", "1959", "suite", "création", "programme", "ibm", "1952", "programme", "jouer", "jeu", "dame", "améliorer", "jouant", "terme", "parvenir", "battre", "4e", "meilleur", "joueur", "états-unis", "avancée", "majeur", "secteur", "intelligence", "machine", "succès", "ordinateur", "développé", "ibm", "deep", "blu", "vaincre", "champion", "mondial", "échec", "garry", "kasparov", "1997", "projet", "deep", "blue", "inspirer", "nombre", "cadre", "intelligence", "artificiel", "particulièrement", "grand", "défi", "ibm", "watson", "ordinateur", "but"], "num_tokens": 376, "token_loss_pct": 54.15, "normalized_content": "l'apprentissage automatique en anglais  machine learning litt.  apprentissage machine  apprentissage artificiel ou apprentissage statistique est un champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d' apprendre  à partir de données c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. plus largement il concerne la conception l'analyse l'optimisation le développement et l'implémentation de telles méthodes. on parle d'apprentissage statistique car l'apprentissage consiste à créer un modèle dont l'erreur statistique moyenne est la plus faible possible. l'apprentissage automatique comporte généralement deux phases. la première consiste à estimer un modèle à partir de données appelées observations qui sont disponibles et en nombre fini lors de la phase de conception du système. l'estimation du modèle consiste à résoudre une tâche pratique telle que traduire un discours estimer une densité de probabilité reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. cette phase dite  d'apprentissage  ou  d'entraînement  est généralement préalable à l'utilisation pratique du modèle. la seconde phase est la mise en production  le modèle étant déterminé de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. certains systèmes peuvent continuer à apprendre une fois en production s'ils disposent d'un retour sur la qualité des résultats produits. c'est l'apprentissage en ligne ou l'apprentissage continu. selon le type de données utilisées pour l'apprentissage on distingue  l'apprentissage supervisé  l'algorithme apprend à partir de données étiquetées la réponse à la tâche qui est la donnée de sortie est donc connue pour chaque données d'entrée. l'objectif est de prédire les sorties pour de nouvelles données  l'apprentissage non supervisé  l'algorithme apprend à partir de données non étiquetées. il cherche à découvrir des structures sous-jacentes cachées qui peuvent par exemple être une densité de probabilité  des motifs dans les données permettent la classification ou le classement des données  l'apprentissage semi-supervisé  il tire parti d'une grande quantité de données non étiquetées pour améliorer la performance du modèle tout en utilisant une moindre quantité de données étiquetées pour guider son apprentissage. il diminue les coûts d'étiquetage manuel des données  l'apprentissage auto-supervisé  c'est une forme d'apprentissage non supervisé où le modèle génère ses propres étiquettes à partir des données brutes. le modèle peut ainsi créer des représentations internes utiles sans nécessiter de données étiquetées manuellement. l'apprentissage automatique peut être appliqué à divers types de données tels des graphes des arbres des courbes ou plus simplement des vecteurs de caractéristiques qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes. si le modèle apprend de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises on parle d'apprentissage par renforcement. historique depuis l'antiquité le sujet des machines pensantes préoccupe les esprits. ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle ainsi qu'une de ses sous-branches  l'apprentissage automatique. la concrétisation de cette idée est principalement due à alan turing mathématicien et cryptologue britannique et à son concept de la  machine universelle  en 1936 qui est à la base des ordinateurs d'aujourd'hui. il continuera à poser les bases de l'apprentissage automatique avec son article sur  l'ordinateur et l'intelligence  en 1950 dans lequel il développe entre autres le test de turing. en 1943 le neurophysiologiste warren mcculloch et le mathématicien walter pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. cette représentation sera la base théorique des réseaux neuronaux. arthur samuel informaticien américain pionnier dans le secteur de l'intelligence artificielle est le premier à faire usage de l'expression machine learning en français  apprentissage automatique  en 1959 à la suite de la création de son programme pour ibm en 1952. le programme jouait au jeu de dames et s'améliorait en jouant. à terme il parvint à battre le 4e meilleur joueur des états-unis. une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par ibm deep blue qui est le premier à vaincre le champion mondial d'échecs garry kasparov en 1997. le projet deep blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle particulièrement un autre grand défi  ibm watson l'ordinateur dont le but est de", "topic_prediction": "Intermediate", "topic_confidence": 1.0, "topic_scores": {"Beginner": 0.0, "Intermediate": 1.0, "Advanced": 0.0}, "sentiment": "NEGATIVE", "sentiment_score": 0.9735, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 0.9999999999999988}
{"title": "Apprentissage profond", "url": "https://fr.wikipedia.org/wiki/Apprentissage_profond", "content": "L'apprentissage profond ou apprentissage en profondeur (en anglais : deep learning) est un sous-domaine de l’intelligence artificielle qui utilise des réseaux neuronaux artificiels formant de nombreuses couches pour résoudre des tâches complexes. L'apprentissage profond permet des progrès importants et rapides dans les domaines de l'analyse du signal sonore ou visuel, notamment de la reconnaissance faciale, de la reconnaissance vocale, de la vision par ordinateur, du traitement automatisé du langage. Les développements de l'apprentissage profond sont rendus possibles par des investissements privés et publics importants, notamment de la part des GAFAM (Google, Apple, Facebook, Amazon, Microsoft), durant les années 2000.\n\nDéfinition\nPour créer un modèle informatique prédictif de manière classique, on modélise les données par extraction de caractéristiques, cette dernière étant souvent effectuée au moyen d'un algorithme. Selon la méthode de l'apprentissage profond, l'extraction de caractéristiques résulte elle-même d'un processus d'apprentissage : on parle donc d'apprentissage de représentations. En pratique, la machine apprend des représentations hiérarchisées, souvent dans les couches cachées de réseaux de neurones artificiels, chacune étant définie à partir de représentations plus simples. Ces représentations étant apprises directement à partir des données, cela évite que les humains aient à expliciter la manière de les construire au moyen d'un algorithme. Si l'on représente la manière dont ces représentations sont construites les unes à partir des autres au moyen d'un graphe, celui-ci contiendra de multiples couches, justifiant ainsi la qualification de « profond ».\n\nHistorique\nL'apprentissage profond est considéré comme « la troisième vague » de développement, après le « cybernétique » des années 1940-1960, puis le « connexionniste »  des années 1980, chacun ayant été suivi par un hiver de l'intelligence artificielle.\nLe concept d'apprentissage profond prend forme dans les années 2010, avec la convergence de trois facteurs :\n\ndes avancées théoriques, notamment dues à Geoffrey Hinton, qui a proposé des approches de pré-entraînement permettant d'apprendre des architectures profondes ;\nle phénomène de Big data, qui a permis la mise à disposition de volumes colossaux de données numériques, nécessaires pour apprendre les architectures profondes ;\nl'avènement du GPGPU, consistant à effectuer des calculs génériques et utiles pour l'apprentissage d'architectures profondes au moyen de processeurs graphiques qui accélèrent les calculs.\nEn 2012, le modèle AlexNet, conçu par Alex Krizhevsky, Ilya Sutskever et leur directeur de thèse Geoffrey Hinton, obtient les meilleures performances lors de la campagne d'évaluation internationale ImageNet de reconnaissance d'images. Le réseau surpasse largement le deuxième et popularise ainsi les approches par apprentissage profond en vision par ordinateur.\nEn 2015, le programme AlphaGo, un modèle neuronal profond qui a « appris » à jouer au jeu de go grâce à l'apprentissage par renforcement, bat le champion européen Fan Hui par cinq parties à zéro. En mars 2016, le même programme bat le champion du monde Lee Sedol par 4 parties à 1. Ces matches ont eu un fort retentissement dans le grand public, en particulier en Asie.\nEn 2017, à la conférence NIPS, des chercheurs travaillant pour la plupart dans des équipes de recherche de Google proposent l'architecture transformeur, qui servira peu de temps après de base aux grands modèles de langage. L'année suivante, l'entreprise propose le modèle BERT, basée sur la partie « encodeur » du transformeur. Ce modèle de langage permettra une amélioration significative des performances en traitement automatique des langues. La même année, OpenAI propose le modèle GPT, qui est pour sa part fondé sur la partie « décodeur » des transformeurs.\nEn 2018, Yann Le Cun, Yoshua Bengio et Geoffrey Hinton sont récipiendaires du prix Turing « Pour les percées conceptuelles et techniques qui ont fait des réseaux neuronaux profonds une composante essentielle de l'informatique ». \nEn 2019, OpenAI publie GPT-2, un modèle de fondation capable de générer du texte. Tout en exprimant leurs inquiétudes sur les détournements possibles de ce type de technologie, les chercheurs de l'association renoncent à partager la version complète.\n\nDomaines d'application\nL'apprentissage profond s'applique à divers secteurs des NTIC, notamment :\n\nen vision par ordinateur, la reconnaissance de formes visuelles, par exemple la reconnaissance d'un panneau de signalisation par un robot ou une voiture autonome, ou la reconnaissance d'emplacements dans une image en combinant ses caractéristiques, comme un lit, une fenêtre et des affiches peuvent indiquer une chambre. Elle aide à prédire certaines propriétés (ex. : les propriétés d'un sol filmé par un robot) ;\nla reconnaissance ou la comparaison de formes ou d'objets hautement déformables ;\nl'analyse de mouvements et positions des doigts d'un", "source": "Wikipedia", "date": null, "author": null, "score": null, "tokens": ["apprentissage", "profond", "apprentissage", "profondeur", "anglais", "deep", "learning", "sous-domaine", "lintelligence", "artificiel", "utiliser", "réseau", "neuronal", "artificiel", "former", "couche", "résoudre", "tâche", "complexe", "apprentissage", "profond", "progrès", "important", "rapide", "domaine", "analyse", "signal", "sonore", "visuel", "reconnaissance", "facial", "reconnaissance", "vocal", "vision", "ordinateur", "traitement", "automatiser", "langage", "développement", "apprentissage", "profond", "rendre", "investissement", "priver", "public", "important", "part", "gafam", "google", "apple", "facebook", "amazon", "microsoft", "année", "2000", "définition", "créer", "modèle", "informatique", "prédictif", "manière", "classique", "modélise", "donnée", "extraction", "caractéristique", "dernier", "effectuer", "moyen", "algorithme", "méthode", "apprentissage", "profond", "extraction", "caractéristique", "résulter", "processus", "apprentissage", "apprentissage", "représentation", "pratique", "machine", "apprendre", "représentation", "hiérarchiser", "couche", "cacher", "réseau", "neurone", "artificiel", "définir", "partir", "représentation", "simple", "représentation", "apprendre", "partir", "donnée", "éviter", "humain", "avoir", "expliciter", "manière", "construire", "moyen", "algorithme", "représenter", "manière", "représentation", "construire", "partir", "moyen", "graphe", "contenir", "multiple", "couche", "justifier", "qualification", "profond", "historique", "apprentissage", "profond", "considérer", "vague", "développement", "cybernétique", "année", "1940", "1960", "connexionniste", "année", "1980", "être", "suivre", "hiver", "intelligence", "artificiel", "concept", "apprentissage", "profond", "prendre", "forme", "année", "2010", "convergence", "facteur", "avancée", "théorique", "devoir", "geoffrey", "hinton", "proposer", "approche", "pré-entraînement", "permettre", "apprendre", "architecture", "profonde", "phénomène", "big", "dater", "permettre", "mise", "disposition", "volume", "colossal", "donnée", "numérique", "nécessaire", "apprendre", "architecture", "profonde", "avènement", "gpgpu", "consister", "effectuer", "calcul", "générique", "utile", "apprentissage", "architecture", "profonde", "moyen", "processeur", "graphique", "accélérer", "calcul", "2012", "modèle", "alexnet", "concevoir", "ale", "krizhevsky", "ilya", "sutskever", "directeur", "thèse", "geoffrey", "hinton", "obtenir", "meilleur", "performance", "campagne", "évaluation", "international", "imagenet", "reconnaissance", "image", "réseau", "surpasser", "largement", "popularise", "approche", "apprentissage", "profond", "vision", "ordinateur", "2015", "programme", "alphago", "modèle", "neuronal", "profond", "apprendre", "jouer", "jeu", "go", "grâce", "apprentissage", "renforcement", "champion", "européen", "fan", "party", "zéro", "mars", "2016", "programme", "champion", "monde", "lee", "sedol", "party", "match", "fort", "retentissement", "grand", "public", "particulier", "asie", "2017", "conférence", "nip", "chercheur", "travailler", "plupart", "équipe", "recherche", "google", "proposer", "architecture", "transformeur", "servir", "temps", "base", "grand", "modèle", "langage", "année", "entreprise", "proposer", "modèle", "bert", "baser", "partie", "encodeur", "transformeur", "modèle", "langage", "permettre", "amélioration", "significatif", "performance", "traitement", "automatique", "langue", "année", "opener", "proposer", "modèle", "gpt", "part", "fonder", "partie", "décodeur", "transformeur", "2018", "yann", "cun", "yoshua", "bengio", "geoffrey", "hinton", "récipiendairer", "prix", "turing", "percée", "conceptuel", "technique", "réseau", "neuronal", "profond", "composant", "essentielle", "informatique", "2019", "openai", "publier", "gpt-2", "modèle", "fondation", "capable", "générer", "texte", "exprimer", "inquiétude", "détournement", "type", "technologie", "chercheur", "association", "renoncer", "partager", "version", "complet", "domaine", "application", "apprentissage", "profond", "applique", "secteur", "ntic", "vision", "ordinateur", "reconnaissance", "forme", "visuel", "exemple", "reconnaissance", "panneau", "signalisation", "robot", "voiture", "autonome", "reconnaissance", "emplacement", "image", "combiner", "caractéristique", "lit", "fenêtre", "affiche", "indiquer", "chambre", "aide", "prédir", "propriété", "ex", "propriété", "sol", "filmer", "robot", "reconnaissance", "comparaison", "forme", "objet", "hautement", "déformabler", "analyse", "mouvement", "position", "doigt"], "num_tokens": 382, "token_loss_pct": 52.01, "normalized_content": "l'apprentissage profond ou apprentissage en profondeur en anglais  deep learning est un sous-domaine de lintelligence artificielle qui utilise des réseaux neuronaux artificiels formant de nombreuses couches pour résoudre des tâches complexes. l'apprentissage profond permet des progrès importants et rapides dans les domaines de l'analyse du signal sonore ou visuel notamment de la reconnaissance faciale de la reconnaissance vocale de la vision par ordinateur du traitement automatisé du langage. les développements de l'apprentissage profond sont rendus possibles par des investissements privés et publics importants notamment de la part des gafam google apple facebook amazon microsoft durant les années 2000. définition pour créer un modèle informatique prédictif de manière classique on modélise les données par extraction de caractéristiques cette dernière étant souvent effectuée au moyen d'un algorithme. selon la méthode de l'apprentissage profond l'extraction de caractéristiques résulte elle-même d'un processus d'apprentissage  on parle donc d'apprentissage de représentations. en pratique la machine apprend des représentations hiérarchisées souvent dans les couches cachées de réseaux de neurones artificiels chacune étant définie à partir de représentations plus simples. ces représentations étant apprises directement à partir des données cela évite que les humains aient à expliciter la manière de les construire au moyen d'un algorithme. si l'on représente la manière dont ces représentations sont construites les unes à partir des autres au moyen d'un graphe celui-ci contiendra de multiples couches justifiant ainsi la qualification de  profond . historique l'apprentissage profond est considéré comme  la troisième vague  de développement après le  cybernétique  des années 1940-1960 puis le  connexionniste  des années 1980 chacun ayant été suivi par un hiver de l'intelligence artificielle. le concept d'apprentissage profond prend forme dans les années 2010 avec la convergence de trois facteurs  des avancées théoriques notamment dues à geoffrey hinton qui a proposé des approches de pré-entraînement permettant d'apprendre des architectures profondes  le phénomène de big data qui a permis la mise à disposition de volumes colossaux de données numériques nécessaires pour apprendre les architectures profondes  l'avènement du gpgpu consistant à effectuer des calculs génériques et utiles pour l'apprentissage d'architectures profondes au moyen de processeurs graphiques qui accélèrent les calculs. en 2012 le modèle alexnet conçu par alex krizhevsky ilya sutskever et leur directeur de thèse geoffrey hinton obtient les meilleures performances lors de la campagne d'évaluation internationale imagenet de reconnaissance d'images. le réseau surpasse largement le deuxième et popularise ainsi les approches par apprentissage profond en vision par ordinateur. en 2015 le programme alphago un modèle neuronal profond qui a  appris  à jouer au jeu de go grâce à l'apprentissage par renforcement bat le champion européen fan hui par cinq parties à zéro. en mars 2016 le même programme bat le champion du monde lee sedol par 4 parties à 1. ces matches ont eu un fort retentissement dans le grand public en particulier en asie. en 2017 à la conférence nips des chercheurs travaillant pour la plupart dans des équipes de recherche de google proposent l'architecture transformeur qui servira peu de temps après de base aux grands modèles de langage. l'année suivante l'entreprise propose le modèle bert basée sur la partie  encodeur  du transformeur. ce modèle de langage permettra une amélioration significative des performances en traitement automatique des langues. la même année openai propose le modèle gpt qui est pour sa part fondé sur la partie  décodeur  des transformeurs. en 2018 yann le cun yoshua bengio et geoffrey hinton sont récipiendaires du prix turing  pour les percées conceptuelles et techniques qui ont fait des réseaux neuronaux profonds une composante essentielle de l'informatique . en 2019 openai publie gpt-2 un modèle de fondation capable de générer du texte. tout en exprimant leurs inquiétudes sur les détournements possibles de ce type de technologie les chercheurs de l'association renoncent à partager la version complète. domaines d'application l'apprentissage profond s'applique à divers secteurs des ntic notamment  en vision par ordinateur la reconnaissance de formes visuelles par exemple la reconnaissance d'un panneau de signalisation par un robot ou une voiture autonome ou la reconnaissance d'emplacements dans une image en combinant ses caractéristiques comme un lit une fenêtre et des affiches peuvent indiquer une chambre. elle aide à prédire certaines propriétés ex.  les propriétés d'un sol filmé par un robot  la reconnaissance ou la comparaison de formes ou d'objets hautement déformables  l'analyse de mouvements et positions des doigts d'un", "topic_prediction": "Intermediate", "topic_confidence": 0.5586, "topic_scores": {"Beginner": 0.0, "Intermediate": 0.5586, "Advanced": 0.4414}, "sentiment": "NEGATIVE", "sentiment_score": 0.9874, "sentiment_label": "Negatif", "is_duplicate": false, "duplicate_score": 0.999999999999998}
