{"title": "IP Addresses Through 2025", "url": "https://www.potaroo.net/ispcol/2026-01/addr2025.html", "content": "IP Addresses Through 2025", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Danish pension fund divesting US Treasuries", "url": "https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/", "content": "Danish pension fund divesting US Treasuries", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Zen of Reticulum", "url": "https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md", "content": "The Zen of Reticulum", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I'm addicted to being useful", "url": "https://www.seangoedecke.com/addicted-to-being-useful/", "content": "I'm addicted to being useful", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "url": "https://github.com/majcheradam/ocrbase", "content": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Linux kernel framework for PCIe device emulation, in userspace", "url": "https://github.com/cakehonolulu/pciem", "content": "Linux kernel framework for PCIe device emulation, in userspace", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Running Claude Code dangerously (safely)", "url": "https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/", "content": "Running Claude Code dangerously (safely)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Level S4 solar radiation event", "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026", "content": "Level S4 solar radiation event", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP over Avian Carriers with Quality of Service (1999)", "url": "https://www.rfc-editor.org/rfc/rfc2549.html", "content": "IP over Avian Carriers with Quality of Service (1999)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Reticulum, a secure and anonymous mesh networking stack", "url": "https://github.com/markqvist/Reticulum", "content": "Reticulum, a secure and anonymous mesh networking stack", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Apple testing new App Store design that blurs the line between ads and results", "url": "https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/", "content": "Apple testing new App Store design that blurs the line between ads and results", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Channel3 (YC S25) Is Hiring", "url": "https://www.ycombinator.com/companies/channel3/jobs/3DIAYYY-backend-engineer", "content": "Channel3 (YC S25) Is Hiring", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Increasing the performance of WebAssembly Text Format parser by 350%", "url": "https://blog.gplane.win/posts/improve-wat-parser-perf.html", "content": "Increasing the performance of WebAssembly Text Format parser by 350%", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "url": "https://trendsfp.github.io/papers/tfp26-paper-12.pdf", "content": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "What came first: the CNAME or the A record?", "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/", "content": "What came first: the CNAME or the A record?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "url": "https://github.com/jordanhubbard/nanolang", "content": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The Overcomplexity of the Shadcn Radio Button", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Europe could 'weaponize' $10T of US assets over Greenland", "url": "https://www.bloomberg.com/news/articles/2026-01-19/-weaponizing-10-trillion-of-us-assets-is-tough-ask-for-europe", "content": "Europe could 'weaponize' $10T of US assets over Greenland", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The coming industrialisation of exploit generation with LLMs", "url": "https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/", "content": "The coming industrialisation of exploit generation with LLMs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Scaling long-running autonomous coding", "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/", "content": "Scaling long-running autonomous coding", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Notes on Apple's Nano Texture (2025)", "url": "https://jon.bo/posts/nano-texture/", "content": "Notes on Apple's Nano Texture (2025)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "3D printing my laptop ergonomic setup", "url": "https://www.ntietz.com/blog/3d-printing-my-laptop-ergonomic-setup/", "content": "3D printing my laptop ergonomic setup", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "x86 prefixes and escape opcodes flowchart", "url": "https://soc.me/interfaces/x86-prefixes-and-escape-opcodes-flowchart.html", "content": "x86 prefixes and escape opcodes flowchart", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Prediction markets are ushering in a world in which news becomes about gambling", "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/", "content": "Prediction markets are ushering in a world in which news becomes about gambling", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nuudel: Non-Tracking Appointment Tool", "url": "https://nuudel.digitalcourage.de/", "content": "Nuudel: Non-Tracking Appointment Tool", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Giving university exams in the age of chatbots", "url": "https://ploum.net/2026-01-19-exam-with-chatbots.html", "content": "Giving university exams in the age of chatbots", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Targeted Bets: An alternative approach to the job hunt", "url": "https://www.seanmuirhead.com/blog/targeted-bets", "content": "Targeted Bets: An alternative approach to the job hunt", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "British redcoat's lost memoir reveals realities of life as a disabled veteran", "url": "https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html", "content": "British redcoat's lost memoir reveals realities of life as a disabled veteran", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Understanding ZFS Scrubs and Data Integrity", "url": "https://klarasystems.com/articles/understanding-zfs-scrubs-and-data-integrity/", "content": "Understanding ZFS Scrubs and Data Integrity", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars", "url": "https://newsroom.porsche.com/en/2026/company/porsche-deliveries-2025-41516.html", "content": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Kahan on the 8087 and designing Intel's floating point (2016) [video]", "url": "https://www.youtube.com/watch?v=L-QVgbdt_qg", "content": "Kahan on the 8087 and designing Intel's floating point (2016) [video]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The assistant axis: situating and stabilizing the character of LLMs", "url": "https://www.anthropic.com/research/assistant-axis", "content": "The assistant axis: situating and stabilizing the character of LLMs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The microstructure of wealth transfer in prediction markets", "url": "https://www.jbecker.dev/research/prediction-market-microstructure", "content": "The microstructure of wealth transfer in prediction markets", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "From Nevada to Kansas by Glider", "url": "https://www.weglide.org/flight/978820", "content": "From Nevada to Kansas by Glider", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I set all 376 Vim options and I'm still a fool", "url": "https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/", "content": "I set all 376 Vim options and I'm still a fool", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nova Launcher added Facebook and Google Ads tracking", "url": "https://lemdro.id/post/lemdro.id/35049920", "content": "Nova Launcher added Facebook and Google Ads tracking", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Kiss Launcher – fast launcher for Android", "url": "https://kisslauncher.com/", "content": "Kiss Launcher – fast launcher for Android", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Artificial Ivy in the Browser", "url": "https://da.nmcardle.com/grow", "content": "Show HN: Artificial Ivy in the Browser", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Face as a QR Code", "url": "https://bookofjoe2.blogspot.com/2025/12/your-face-as-qr-code.html", "content": "Face as a QR Code", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "San Francisco coyote swims to Alcatraz", "url": "https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php", "content": "San Francisco coyote swims to Alcatraz", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "url": "https://github.com/minimaxir/ballin", "content": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Sending Data over Offline Finding Networks", "url": "https://cc-sw.com/find-my-and-find-hub-network-research/", "content": "Sending Data over Offline Finding Networks", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "King – man + woman is queen; but why? (2017)", "url": "https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/", "content": "King – man + woman is queen; but why? (2017)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How we made Python's packaging library 3x faster", "url": "https://iscinumpy.dev/post/packaging-faster/", "content": "How we made Python's packaging library 3x faster", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Conditions in the Intel 8087 floating-point chip's microcode", "url": "https://www.righto.com/2025/12/8087-microcode-conditions.html", "content": "Conditions in the Intel 8087 floating-point chip's microcode", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Flux 2 Klein pure C inference", "url": "https://github.com/antirez/flux2.c", "content": "Flux 2 Klein pure C inference", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "CSS Web Components for marketing sites (2024)", "url": "https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/", "content": "CSS Web Components for marketing sites (2024)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Antarctic Snow Cruiser", "url": "https://www.amusingplanet.com/2026/01/the-antarctic-snow-cruiser.html", "content": "The Antarctic Snow Cruiser", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Subth.ink – write something and see how many others wrote the same", "url": "https://subth.ink/", "content": "Show HN: Subth.ink – write something and see how many others wrote the same", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Bypassing Gemma and Qwen safety with raw strings", "url": "https://teendifferent.substack.com/p/apply_chat_template-is-the-safety", "content": "Bypassing Gemma and Qwen safety with raw strings", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Selling SaaS in Japan", "url": "https://embedworkflow.com/blog/what-saas-founders-should-know-about-entering-the-japanese-market/", "content": "Selling SaaS in Japan", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: E80: an 8-bit CPU in structural VHDL", "url": "https://github.com/Stokpan/E80", "content": "Show HN: E80: an 8-bit CPU in structural VHDL", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Pipenet – A Modern Alternative to Localtunnel", "url": "https://pipenet.dev/", "content": "Show HN: Pipenet – A Modern Alternative to Localtunnel", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How to be a good conference talk audience member (2022)", "url": "https://www.mooreds.com/wordpress/archives/3522", "content": "How to be a good conference talk audience member (2022)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Use Social Media Mindfully", "url": "https://danielleheberling.xyz/blog/mindful-social-media/", "content": "Use Social Media Mindfully", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Radboud University selects Fairphone as standard smartphone for employees", "url": "https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees", "content": "Radboud University selects Fairphone as standard smartphone for employees", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "ASCII characters are not pixels: a deep dive into ASCII rendering", "url": "https://alexharri.com/blog/ascii-rendering", "content": "ASCII characters are not pixels: a deep dive into ASCII rendering", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "F-16 Falcon Strike", "url": "https://webchrono.pl/F16FalconStrike/index.html", "content": "F-16 Falcon Strike", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Weight Transfer for RL Post-Training in under 2 seconds", "url": "https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds", "content": "Weight Transfer for RL Post-Training in under 2 seconds", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Iterative image reconstruction using random cubic bézier strokes", "url": "https://tangled.org/luthenwald.tngl.sh/splined", "content": "Iterative image reconstruction using random cubic bézier strokes", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Issue #717: Unit Testing Performance, Cursor, Recursive match, and More (Jan. 13, 2026)", "url": "https://pycoders.com/issues/717", "content": "<p> <span>#717 – JANUARY 13, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/717/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s We", "source": "RSS", "date": "2026-01-13T19:30:00+00:00", "author": null, "score": null}
{"title": "Issue #716: Performance Numbers, async Web Apps, uv Speed, and More (Jan. 6, 2026)", "url": "https://pycoders.com/issues/716", "content": "<p> <span>#716 – JANUARY 6, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/716/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s Wee", "source": "RSS", "date": "2026-01-06T19:30:00+00:00", "author": null, "score": null}
{"title": "Issue #715: Top 5 of 2025, LlamaIndex, Python 3.15 Speed, and More (Dec. 30, 2025)", "url": "https://pycoders.com/issues/715", "content": "<p> <span>#715 – DECEMBER 30, 2025</span><br /> <span><a href=\"https://pycoders.com/issues/715/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s W", "source": "RSS", "date": "2025-12-30T19:30:00+00:00", "author": null, "score": null}
{"title": "Natural language processing", "url": "https://en.wikipedia.org/wiki/Natural_language_processing", "content": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly.\nMajor processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.\n\nHistory\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n\nSymbolic NLP (1950s – early 1990s)\nThe premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n\n1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.\n1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.\n1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first chatterbots were written (e.g., PARRY).\n1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.\n\nStatistical NLP (1990s–present)\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. \n\n1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems o", "source": "Wikipedia", "date": null, "author": null, "score": null}
{"title": "Machine learning", "url": "https://en.wikipedia.org/wiki/Machine_learning", "content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.\nFrom a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework.\n\nHistory\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used during this time period.\nThe earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think?\", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\".\nModern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.\n\nCurrent Supervised Learning Algorithms have objectives of classification and regression.\nCurrent Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.\nCurrent Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods.\nIn 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques.\n\nRelationships to other fields\nArtificial intelligence\nAs a scientific endeavour, machine learning grew out of the quest for artifi", "source": "Wikipedia", "date": null, "author": null, "score": null}
{"title": "Deep learning", "url": "https://en.wikipedia.org/wiki/Deep_learning", "content": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n\nOverview\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\nFundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\nImportantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.\nThe term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated.\n\nInterpretations\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\nThe classic u", "source": "Wikipedia", "date": null, "author": null, "score": null}
{"title": "BERT (language model)", "url": "https://en.wikipedia.org/wiki/BERT_(language_model)", "content": "Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state of the art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \nBERT is trained by masked token prediction and next sentence prediction. With this training, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It improved on ELMo and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.\n\nArchitecture\nBERT is an \"encoder-only\" transformer architecture. At a high level, BERT consists of 4 modules: \n\nTokenizer: This module converts a piece of English text into a sequence of integers (\"tokens\").\nEmbedding: This module converts the sequence of tokens into an array of real-valued vectors representing the tokens. It represents the conversion of discrete token types into a lower-dimensional Euclidean space.\nEncoder: a stack of Transformer blocks with self-attention, but without causal masking.\nTask head: This module converts the final representation vectors into one-hot encoded tokens again by producing a predicted probability distribution over the token types. It can be viewed as a simple decoder, decoding the latent representation into token types, or as an \"un-embedding layer\".\nThe task head is necessary for pre-training, but it is often unnecessary for so-called \"downstream tasks,\" such as question answering or sentiment classification. Instead, one removes the task head and replaces it with a newly initialized module suited for the task, and finetune the new module. The latent vector representation of the model is directly fed into this new module, allowing for sample-efficient transfer learning.\n\nEmbedding\nThis section describes the embedding used by BERTBASE. The other one, BERTLARGE, is similar, just larger.\nThe tokenizer of BERT is WordPiece, which is a sub-word strategy like byte-pair encoding. Its vocabulary size is 30,000, and any token not appearing in its vocabulary is replaced by [UNK] (\"unknown\"). \n\nThe first layer is the embedding layer, which contains three components: token type embeddings, position embeddings, and segment type embeddings. \n\nToken type: The token type is a standard embedding layer, translating a one-hot vector into a dense vector based on its token type.\nPosition: The position embeddings are based on a token's position in the sequence. BERT uses absolute position embeddings, where each position in a sequence is mapped to a real-valued vector. Each dimension of the vector consists of a sinusoidal function that takes the position in the sequence as input.\nSegment type: Using a vocabulary of just 0 or 1, this embedding layer produces a dense vector based on whether the token belongs to the first or second text segment in that input. In other words, type-1 tokens are all tokens that appear after the [SEP] special token. All prior tokens are type-0.\nThe three embedding vectors are added together representing the initial token representation as a function of these three pieces of information. After embedding, the vector representation is normalized using a LayerNorm operation, outputting a 768-dimensional vector for each input token. After this, the representation vectors are passed forward through 12 Transformer encoder blocks, and are decoded back to 30,000-dimensional vocabulary space using a basic affine transformation layer.\n\nArchitectural family\nThe encoder stack of BERT has 2 free parameters: \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, the number of layers, and \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n, the hidden size. There are always \n  \n    \n      \n        H\n        \n          /\n        \n        64\n      \n    \n    {\\displaystyle H/64}\n  \n self-attention heads, and the feed-forward/filter size is always \n  \n    \n      \n        4\n        H\n      \n    \n    {\\displaystyle 4H}\n  \n. By varying these two numbers, one obtains an entire family of BERT models.\nFor BERT:\n\nthe feed-forward size and filter size are synonymous. Both of them denote the number of dimensions in the middle layer of the feed-forward network.\nthe hidden size and embedding size are synonymous. Both of them denote ", "source": "Wikipedia", "date": null, "author": null, "score": null}
{"title": "Artificial intelligence", "url": "https://en.wikipedia.org/wiki/Artificial_intelligence", "content": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\nGoals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\nReasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\nKnowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the", "source": "Wikipedia", "date": null, "author": null, "score": null}
